__BRYTHON__.use_VFS = true;
var scripts = {"$timestamp": 1734992701233, "typing_extensions": [".py", "import abc\nimport collections\nimport collections.abc\nimport contextlib\nimport functools\nimport inspect\nimport operator\nimport sys\nimport types as _types\nimport typing\nimport warnings\n\n__all__=[\n\n'Any',\n'ClassVar',\n'Concatenate',\n'Final',\n'LiteralString',\n'ParamSpec',\n'ParamSpecArgs',\n'ParamSpecKwargs',\n'Self',\n'Type',\n'TypeVar',\n'TypeVarTuple',\n'Unpack',\n\n\n'Awaitable',\n'AsyncIterator',\n'AsyncIterable',\n'Coroutine',\n'AsyncGenerator',\n'AsyncContextManager',\n'Buffer',\n'ChainMap',\n\n\n'ContextManager',\n'Counter',\n'Deque',\n'DefaultDict',\n'NamedTuple',\n'OrderedDict',\n'TypedDict',\n\n\n'SupportsAbs',\n'SupportsBytes',\n'SupportsComplex',\n'SupportsFloat',\n'SupportsIndex',\n'SupportsInt',\n'SupportsRound',\n\n\n'Annotated',\n'assert_never',\n'assert_type',\n'clear_overloads',\n'dataclass_transform',\n'deprecated',\n'Doc',\n'get_overloads',\n'final',\n'get_args',\n'get_origin',\n'get_original_bases',\n'get_protocol_members',\n'get_type_hints',\n'IntVar',\n'is_protocol',\n'is_typeddict',\n'Literal',\n'NewType',\n'overload',\n'override',\n'Protocol',\n'reveal_type',\n'runtime',\n'runtime_checkable',\n'Text',\n'TypeAlias',\n'TypeAliasType',\n'TypeGuard',\n'TypeIs',\n'TYPE_CHECKING',\n'Never',\n'NoReturn',\n'ReadOnly',\n'Required',\n'NotRequired',\n\n\n'AbstractSet',\n'AnyStr',\n'BinaryIO',\n'Callable',\n'Collection',\n'Container',\n'Dict',\n'ForwardRef',\n'FrozenSet',\n'Generator',\n'Generic',\n'Hashable',\n'IO',\n'ItemsView',\n'Iterable',\n'Iterator',\n'KeysView',\n'List',\n'Mapping',\n'MappingView',\n'Match',\n'MutableMapping',\n'MutableSequence',\n'MutableSet',\n'NoDefault',\n'Optional',\n'Pattern',\n'Reversible',\n'Sequence',\n'Set',\n'Sized',\n'TextIO',\n'Tuple',\n'Union',\n'ValuesView',\n'cast',\n'no_type_check',\n'no_type_check_decorator',\n]\n\n\nPEP_560=True\nGenericMeta=type\n_PEP_696_IMPLEMENTED=sys.version_info >=(3,13,0,\"beta\")\n\n\n\n\n\nclass _Sentinel:\n def __repr__(self):\n  return \"<sentinel>\"\n  \n  \n_marker=_Sentinel()\n\n\nif sys.version_info >=(3,10):\n def _should_collect_from_parameters(t):\n  return isinstance(\n  t,(typing._GenericAlias,_types.GenericAlias,_types.UnionType)\n  )\nelif sys.version_info >=(3,9):\n def _should_collect_from_parameters(t):\n  return isinstance(t,(typing._GenericAlias,_types.GenericAlias))\nelse:\n def _should_collect_from_parameters(t):\n  return isinstance(t,typing._GenericAlias)and not t._special\n  \n  \nNoReturn=typing.NoReturn\n\n\n\nT=typing.TypeVar('T')\nKT=typing.TypeVar('KT')\nVT=typing.TypeVar('VT')\nT_co=typing.TypeVar('T_co',covariant=True)\nT_contra=typing.TypeVar('T_contra',contravariant=True)\n\n\nif sys.version_info >=(3,11):\n from typing import Any\nelse:\n\n class _AnyMeta(type):\n  def __instancecheck__(self,obj):\n   if self is Any:\n    raise TypeError(\"typing_extensions.Any cannot be used with isinstance()\")\n   return super().__instancecheck__(obj)\n   \n  def __repr__(self):\n   if self is Any:\n    return \"typing_extensions.Any\"\n   return super().__repr__()\n   \n class Any(metaclass=_AnyMeta):\n  ''\n\n\n\n\n\n\n  \n  def __new__(cls,*args,**kwargs):\n   if cls is Any:\n    raise TypeError(\"Any cannot be instantiated\")\n   return super().__new__(cls,*args,**kwargs)\n   \n   \nClassVar=typing.ClassVar\n\n\nclass _ExtensionsSpecialForm(typing._SpecialForm,_root=True):\n def __repr__(self):\n  return 'typing_extensions.'+self._name\n  \n  \nFinal=typing.Final\n\nif sys.version_info >=(3,11):\n final=typing.final\nelse:\n\n\n\n def final(f):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  try:\n   f.__final__=True\n  except(AttributeError,TypeError):\n  \n  \n  \n   pass\n  return f\n  \n  \ndef IntVar(name):\n return typing.TypeVar(name)\n \n \n \nif sys.version_info >=(3,10,1):\n Literal=typing.Literal\nelse:\n def _flatten_literal_params(parameters):\n  ''\n  params=[]\n  for p in parameters:\n   if isinstance(p,_LiteralGenericAlias):\n    params.extend(p.__args__)\n   else:\n    params.append(p)\n  return tuple(params)\n  \n def _value_and_type_iter(params):\n  for p in params:\n   yield p,type(p)\n   \n class _LiteralGenericAlias(typing._GenericAlias,_root=True):\n  def __eq__(self,other):\n   if not isinstance(other,_LiteralGenericAlias):\n    return NotImplemented\n   these_args_deduped=set(_value_and_type_iter(self.__args__))\n   other_args_deduped=set(_value_and_type_iter(other.__args__))\n   return these_args_deduped ==other_args_deduped\n   \n  def __hash__(self):\n   return hash(frozenset(_value_and_type_iter(self.__args__)))\n   \n class _LiteralForm(_ExtensionsSpecialForm,_root=True):\n  def __init__(self,doc:str):\n   self._name='Literal'\n   self._doc=self.__doc__=doc\n   \n  def __getitem__(self,parameters):\n   if not isinstance(parameters,tuple):\n    parameters=(parameters,)\n    \n   parameters=_flatten_literal_params(parameters)\n   \n   val_type_pairs=list(_value_and_type_iter(parameters))\n   try:\n    deduped_pairs=set(val_type_pairs)\n   except TypeError:\n   \n    pass\n   else:\n   \n    if len(deduped_pairs)<len(val_type_pairs):\n     new_parameters=[]\n     for pair in val_type_pairs:\n      if pair in deduped_pairs:\n       new_parameters.append(pair[0])\n       deduped_pairs.remove(pair)\n     assert not deduped_pairs,deduped_pairs\n     parameters=tuple(new_parameters)\n     \n   return _LiteralGenericAlias(self,parameters)\n   \n Literal=_LiteralForm(doc=\"\"\"\\\n                           A type that can be used to indicate to type checkers\n                           that the corresponding value has a value literally equivalent\n                           to the provided parameter. For example:\n\n                               var: Literal[4] = 4\n\n                           The type checker understands that 'var' is literally equal to\n                           the value 4 and no other value.\n\n                           Literal[...] cannot be subclassed. There is no runtime\n                           checking verifying that the parameter is actually a value\n                           instead of a type.\"\"\")\n \n \n_overload_dummy=typing._overload_dummy\n\n\nif hasattr(typing,\"get_overloads\"):\n overload=typing.overload\n get_overloads=typing.get_overloads\n clear_overloads=typing.clear_overloads\nelse:\n\n _overload_registry=collections.defaultdict(\n functools.partial(collections.defaultdict,dict)\n )\n \n def overload(func):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  f=getattr(func,\"__func__\",func)\n  try:\n   _overload_registry[f.__module__][f.__qualname__][\n   f.__code__.co_firstlineno\n   ]=func\n  except AttributeError:\n  \n   pass\n  return _overload_dummy\n  \n def get_overloads(func):\n  ''\n  \n  f=getattr(func,\"__func__\",func)\n  if f.__module__ not in _overload_registry:\n   return[]\n  mod_dict=_overload_registry[f.__module__]\n  if f.__qualname__ not in mod_dict:\n   return[]\n  return list(mod_dict[f.__qualname__].values())\n  \n def clear_overloads():\n  ''\n  _overload_registry.clear()\n  \n  \n  \nType=typing.Type\n\n\n\nAwaitable=typing.Awaitable\nCoroutine=typing.Coroutine\nAsyncIterable=typing.AsyncIterable\nAsyncIterator=typing.AsyncIterator\nDeque=typing.Deque\nDefaultDict=typing.DefaultDict\nOrderedDict=typing.OrderedDict\nCounter=typing.Counter\nChainMap=typing.ChainMap\nText=typing.Text\nTYPE_CHECKING=typing.TYPE_CHECKING\n\n\nif sys.version_info >=(3,13,0,\"beta\"):\n from typing import AsyncContextManager,AsyncGenerator,ContextManager,Generator\nelse:\n def _is_dunder(attr):\n  return attr.startswith('__')and attr.endswith('__')\n  \n  \n _special_generic_alias_base=getattr(\n typing,\"_SpecialGenericAlias\",typing._GenericAlias\n )\n \n class _SpecialGenericAlias(_special_generic_alias_base,_root=True):\n  def __init__(self,origin,nparams,*,inst=True,name=None,defaults=()):\n   if _special_generic_alias_base is typing._GenericAlias:\n   \n    self.__origin__=origin\n    self._nparams=nparams\n    super().__init__(origin,nparams,special=True,inst=inst,name=name)\n   else:\n   \n    super().__init__(origin,nparams,inst=inst,name=name)\n   self._defaults=defaults\n   \n  def __setattr__(self,attr,val):\n   allowed_attrs={'_name','_inst','_nparams','_defaults'}\n   if _special_generic_alias_base is typing._GenericAlias:\n   \n    allowed_attrs.add(\"__origin__\")\n   if _is_dunder(attr)or attr in allowed_attrs:\n    object.__setattr__(self,attr,val)\n   else:\n    setattr(self.__origin__,attr,val)\n    \n  @typing._tp_cache\n  def __getitem__(self,params):\n   if not isinstance(params,tuple):\n    params=(params,)\n   msg=\"Parameters to generic types must be types.\"\n   params=tuple(typing._type_check(p,msg)for p in params)\n   if(\n   self._defaults\n   and len(params)<self._nparams\n   and len(params)+len(self._defaults)>=self._nparams\n   ):\n    params=(*params,*self._defaults[len(params)-self._nparams:])\n   actual_len=len(params)\n   \n   if actual_len !=self._nparams:\n    if self._defaults:\n     expected=f\"at least {self._nparams -len(self._defaults)}\"\n    else:\n     expected=str(self._nparams)\n    if not self._nparams:\n     raise TypeError(f\"{self} is not a generic class\")\n    raise TypeError(\n    f\"Too {'many'if actual_len >self._nparams else 'few'}\"\n    f\" arguments for {self};\"\n    f\" actual {actual_len}, expected {expected}\"\n    )\n   return self.copy_with(params)\n   \n _NoneType=type(None)\n Generator=_SpecialGenericAlias(\n collections.abc.Generator,3,defaults=(_NoneType,_NoneType)\n )\n AsyncGenerator=_SpecialGenericAlias(\n collections.abc.AsyncGenerator,2,defaults=(_NoneType,)\n )\n ContextManager=_SpecialGenericAlias(\n contextlib.AbstractContextManager,\n 2,\n name=\"ContextManager\",\n defaults=(typing.Optional[bool],)\n )\n AsyncContextManager=_SpecialGenericAlias(\n contextlib.AbstractAsyncContextManager,\n 2,\n name=\"AsyncContextManager\",\n defaults=(typing.Optional[bool],)\n )\n \n \n_PROTO_ALLOWLIST={\n'collections.abc':[\n'Callable','Awaitable','Iterable','Iterator','AsyncIterable',\n'Hashable','Sized','Container','Collection','Reversible','Buffer',\n],\n'contextlib':['AbstractContextManager','AbstractAsyncContextManager'],\n'typing_extensions':['Buffer'],\n}\n\n\n_EXCLUDED_ATTRS=frozenset(typing.EXCLUDED_ATTRIBUTES)|{\n\"__match_args__\",\"__protocol_attrs__\",\"__non_callable_proto_members__\",\n\"__final__\",\n}\n\n\ndef _get_protocol_attrs(cls):\n attrs=set()\n for base in cls.__mro__[:-1]:\n  if base.__name__ in{'Protocol','Generic'}:\n   continue\n  annotations=getattr(base,'__annotations__',{})\n  for attr in(*base.__dict__,*annotations):\n   if(not attr.startswith('_abc_')and attr not in _EXCLUDED_ATTRS):\n    attrs.add(attr)\n return attrs\n \n \ndef _caller(depth=2):\n try:\n  return sys._getframe(depth).f_globals.get('__name__','__main__')\n except(AttributeError,ValueError):\n  return None\n  \n  \n  \n  \nif sys.version_info >=(3,13):\n Protocol=typing.Protocol\nelse:\n def _allow_reckless_class_checks(depth=3):\n  ''\n\n\n  \n  return _caller(depth)in{'abc','functools',None}\n  \n def _no_init(self,*args,**kwargs):\n  if type(self)._is_protocol:\n   raise TypeError('Protocols cannot be instantiated')\n   \n def _type_check_issubclass_arg_1(arg):\n  ''\n\n\n\n\n\n\n\n\n\n  \n  if not isinstance(arg,type):\n  \n   raise TypeError('issubclass() arg 1 must be a class')\n   \n   \n   \n   \n class _ProtocolMeta(type(typing.Protocol)):\n \n \n \n \n \n \n  def __new__(mcls,name,bases,namespace,**kwargs):\n   if name ==\"Protocol\"and len(bases)<2:\n    pass\n   elif{Protocol,typing.Protocol}&set(bases):\n    for base in bases:\n     if not(\n     base in{object,typing.Generic,Protocol,typing.Protocol}\n     or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__,[])\n     or is_protocol(base)\n     ):\n      raise TypeError(\n      f\"Protocols can only inherit from other protocols, \"\n      f\"got {base !r}\"\n      )\n   return abc.ABCMeta.__new__(mcls,name,bases,namespace,**kwargs)\n   \n  def __init__(cls,*args,**kwargs):\n   abc.ABCMeta.__init__(cls,*args,**kwargs)\n   if getattr(cls,\"_is_protocol\",False):\n    cls.__protocol_attrs__=_get_protocol_attrs(cls)\n    \n  def __subclasscheck__(cls,other):\n   if cls is Protocol:\n    return type.__subclasscheck__(cls,other)\n   if(\n   getattr(cls,'_is_protocol',False)\n   and not _allow_reckless_class_checks()\n   ):\n    if not getattr(cls,'_is_runtime_protocol',False):\n     _type_check_issubclass_arg_1(other)\n     raise TypeError(\n     \"Instance and class checks can only be used with \"\n     \"@runtime_checkable protocols\"\n     )\n    if(\n    \n    cls.__non_callable_proto_members__\n    and cls.__dict__.get(\"__subclasshook__\")is _proto_hook\n    ):\n     _type_check_issubclass_arg_1(other)\n     non_method_attrs=sorted(cls.__non_callable_proto_members__)\n     raise TypeError(\n     \"Protocols with non-method members don't support issubclass().\"\n     f\" Non-method members: {str(non_method_attrs)[1:-1]}.\"\n     )\n   return abc.ABCMeta.__subclasscheck__(cls,other)\n   \n  def __instancecheck__(cls,instance):\n  \n  \n   if cls is Protocol:\n    return type.__instancecheck__(cls,instance)\n   if not getattr(cls,\"_is_protocol\",False):\n   \n    return abc.ABCMeta.__instancecheck__(cls,instance)\n    \n   if(\n   not getattr(cls,'_is_runtime_protocol',False)and\n   not _allow_reckless_class_checks()\n   ):\n    raise TypeError(\"Instance and class checks can only be used with\"\n    \" @runtime_checkable protocols\")\n    \n   if abc.ABCMeta.__instancecheck__(cls,instance):\n    return True\n    \n   for attr in cls.__protocol_attrs__:\n    try:\n     val=inspect.getattr_static(instance,attr)\n    except AttributeError:\n     break\n     \n    if val is None and attr not in cls.__non_callable_proto_members__:\n     break\n   else:\n    return True\n    \n   return False\n   \n  def __eq__(cls,other):\n  \n  \n  \n   if abc.ABCMeta.__eq__(cls,other)is True:\n    return True\n   return cls is Protocol and other is typing.Protocol\n   \n   \n   \n   \n  def __hash__(cls)->int:\n   return type.__hash__(cls)\n   \n @classmethod\n def _proto_hook(cls,other):\n  if not cls.__dict__.get('_is_protocol',False):\n   return NotImplemented\n   \n  for attr in cls.__protocol_attrs__:\n   for base in other.__mro__:\n   \n    if attr in base.__dict__:\n     if base.__dict__[attr]is None:\n      return NotImplemented\n     break\n     \n     \n    annotations=getattr(base,'__annotations__',{})\n    if(\n    isinstance(annotations,collections.abc.Mapping)\n    and attr in annotations\n    and is_protocol(other)\n    ):\n     break\n   else:\n    return NotImplemented\n  return True\n  \n class Protocol(typing.Generic,metaclass=_ProtocolMeta):\n  __doc__=typing.Protocol.__doc__\n  __slots__=()\n  _is_protocol=True\n  _is_runtime_protocol=False\n  \n  def __init_subclass__(cls,*args,**kwargs):\n   super().__init_subclass__(*args,**kwargs)\n   \n   \n   if not cls.__dict__.get('_is_protocol',False):\n    cls._is_protocol=any(b is Protocol for b in cls.__bases__)\n    \n    \n   if '__subclasshook__'not in cls.__dict__:\n    cls.__subclasshook__=_proto_hook\n    \n    \n   if cls._is_protocol and cls.__init__ is Protocol.__init__:\n    cls.__init__=_no_init\n    \n    \nif sys.version_info >=(3,13):\n runtime_checkable=typing.runtime_checkable\nelse:\n def runtime_checkable(cls):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if not issubclass(cls,typing.Generic)or not getattr(cls,'_is_protocol',False):\n   raise TypeError(f'@runtime_checkable can be only applied to protocol classes,'\n   f' got {cls !r}')\n  cls._is_runtime_protocol=True\n  \n  \n  \n  \n  \n  \n  \n  \n  if isinstance(cls,_ProtocolMeta)or sys.version_info >=(3,12,2):\n  \n  \n  \n  \n   cls.__non_callable_proto_members__=set()\n   for attr in cls.__protocol_attrs__:\n    try:\n     is_callable=callable(getattr(cls,attr,None))\n    except Exception as e:\n     raise TypeError(\n     f\"Failed to determine whether protocol member {attr !r} \"\n     \"is a method member\"\n     )from e\n    else:\n     if not is_callable:\n      cls.__non_callable_proto_members__.add(attr)\n      \n  return cls\n  \n  \n  \nruntime=runtime_checkable\n\n\n\nif sys.version_info >=(3,12):\n SupportsInt=typing.SupportsInt\n SupportsFloat=typing.SupportsFloat\n SupportsComplex=typing.SupportsComplex\n SupportsBytes=typing.SupportsBytes\n SupportsIndex=typing.SupportsIndex\n SupportsAbs=typing.SupportsAbs\n SupportsRound=typing.SupportsRound\nelse:\n @runtime_checkable\n class SupportsInt(Protocol):\n  ''\n  __slots__=()\n  \n  @abc.abstractmethod\n  def __int__(self)->int:\n   pass\n   \n @runtime_checkable\n class SupportsFloat(Protocol):\n  ''\n  __slots__=()\n  \n  @abc.abstractmethod\n  def __float__(self)->float:\n   pass\n   \n @runtime_checkable\n class SupportsComplex(Protocol):\n  ''\n  __slots__=()\n  \n  @abc.abstractmethod\n  def __complex__(self)->complex:\n   pass\n   \n @runtime_checkable\n class SupportsBytes(Protocol):\n  ''\n  __slots__=()\n  \n  @abc.abstractmethod\n  def __bytes__(self)->bytes:\n   pass\n   \n @runtime_checkable\n class SupportsIndex(Protocol):\n  __slots__=()\n  \n  @abc.abstractmethod\n  def __index__(self)->int:\n   pass\n   \n @runtime_checkable\n class SupportsAbs(Protocol[T_co]):\n  ''\n\n  \n  __slots__=()\n  \n  @abc.abstractmethod\n  def __abs__(self)->T_co:\n   pass\n   \n @runtime_checkable\n class SupportsRound(Protocol[T_co]):\n  ''\n\n  \n  __slots__=()\n  \n  @abc.abstractmethod\n  def __round__(self,ndigits:int=0)->T_co:\n   pass\n   \n   \ndef _ensure_subclassable(mro_entries):\n def inner(func):\n  if sys.implementation.name ==\"pypy\"and sys.version_info <(3,9):\n   cls_dict={\n   \"__call__\":staticmethod(func),\n   \"__mro_entries__\":staticmethod(mro_entries)\n   }\n   t=type(func.__name__,(),cls_dict)\n   return functools.update_wrapper(t(),func)\n  else:\n   func.__mro_entries__=mro_entries\n   return func\n return inner\n \n \n \n \n_PEP_728_IMPLEMENTED=False\n\nif _PEP_728_IMPLEMENTED:\n\n\n\n\n\n\n\n\n\n\n\n\n TypedDict=typing.TypedDict\n _TypedDictMeta=typing._TypedDictMeta\n is_typeddict=typing.is_typeddict\nelse:\n\n _TAKES_MODULE=\"module\"in inspect.signature(typing._type_check).parameters\n \n def _get_typeddict_qualifiers(annotation_type):\n  while True:\n   annotation_origin=get_origin(annotation_type)\n   if annotation_origin is Annotated:\n    annotation_args=get_args(annotation_type)\n    if annotation_args:\n     annotation_type=annotation_args[0]\n    else:\n     break\n   elif annotation_origin is Required:\n    yield Required\n    annotation_type,=get_args(annotation_type)\n   elif annotation_origin is NotRequired:\n    yield NotRequired\n    annotation_type,=get_args(annotation_type)\n   elif annotation_origin is ReadOnly:\n    yield ReadOnly\n    annotation_type,=get_args(annotation_type)\n   else:\n    break\n    \n class _TypedDictMeta(type):\n  def __new__(cls,name,bases,ns,*,total=True,closed=False):\n   ''\n\n\n\n\n\n   \n   for base in bases:\n    if type(base)is not _TypedDictMeta and base is not typing.Generic:\n     raise TypeError('cannot inherit from both a TypedDict type '\n     'and a non-TypedDict base class')\n     \n   if any(issubclass(b,typing.Generic)for b in bases):\n    generic_base=(typing.Generic,)\n   else:\n    generic_base=()\n    \n    \n    \n   tp_dict=type.__new__(_TypedDictMeta,\"Protocol\",(*generic_base,dict),ns)\n   tp_dict.__name__=name\n   if tp_dict.__qualname__ ==\"Protocol\":\n    tp_dict.__qualname__=name\n    \n   if not hasattr(tp_dict,'__orig_bases__'):\n    tp_dict.__orig_bases__=bases\n    \n   annotations={}\n   if \"__annotations__\"in ns:\n    own_annotations=ns[\"__annotations__\"]\n   elif \"__annotate__\"in ns:\n   \n    own_annotations=ns[\"__annotate__\"](1)\n   else:\n    own_annotations={}\n   msg=\"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n   if _TAKES_MODULE:\n    own_annotations={\n    n:typing._type_check(tp,msg,module=tp_dict.__module__)\n    for n,tp in own_annotations.items()\n    }\n   else:\n    own_annotations={\n    n:typing._type_check(tp,msg)\n    for n,tp in own_annotations.items()\n    }\n   required_keys=set()\n   optional_keys=set()\n   readonly_keys=set()\n   mutable_keys=set()\n   extra_items_type=None\n   \n   for base in bases:\n    base_dict=base.__dict__\n    \n    annotations.update(base_dict.get('__annotations__',{}))\n    required_keys.update(base_dict.get('__required_keys__',()))\n    optional_keys.update(base_dict.get('__optional_keys__',()))\n    readonly_keys.update(base_dict.get('__readonly_keys__',()))\n    mutable_keys.update(base_dict.get('__mutable_keys__',()))\n    base_extra_items_type=base_dict.get('__extra_items__',None)\n    if base_extra_items_type is not None:\n     extra_items_type=base_extra_items_type\n     \n   if closed and extra_items_type is None:\n    extra_items_type=Never\n   if closed and \"__extra_items__\"in own_annotations:\n    annotation_type=own_annotations.pop(\"__extra_items__\")\n    qualifiers=set(_get_typeddict_qualifiers(annotation_type))\n    if Required in qualifiers:\n     raise TypeError(\n     \"Special key __extra_items__ does not support \"\n     \"Required\"\n     )\n    if NotRequired in qualifiers:\n     raise TypeError(\n     \"Special key __extra_items__ does not support \"\n     \"NotRequired\"\n     )\n    extra_items_type=annotation_type\n    \n   annotations.update(own_annotations)\n   for annotation_key,annotation_type in own_annotations.items():\n    qualifiers=set(_get_typeddict_qualifiers(annotation_type))\n    \n    if Required in qualifiers:\n     required_keys.add(annotation_key)\n    elif NotRequired in qualifiers:\n     optional_keys.add(annotation_key)\n    elif total:\n     required_keys.add(annotation_key)\n    else:\n     optional_keys.add(annotation_key)\n    if ReadOnly in qualifiers:\n     mutable_keys.discard(annotation_key)\n     readonly_keys.add(annotation_key)\n    else:\n     mutable_keys.add(annotation_key)\n     readonly_keys.discard(annotation_key)\n     \n   tp_dict.__annotations__=annotations\n   tp_dict.__required_keys__=frozenset(required_keys)\n   tp_dict.__optional_keys__=frozenset(optional_keys)\n   tp_dict.__readonly_keys__=frozenset(readonly_keys)\n   tp_dict.__mutable_keys__=frozenset(mutable_keys)\n   if not hasattr(tp_dict,'__total__'):\n    tp_dict.__total__=total\n   tp_dict.__closed__=closed\n   tp_dict.__extra_items__=extra_items_type\n   return tp_dict\n   \n  __call__=dict\n  \n  def __subclasscheck__(cls,other):\n  \n   raise TypeError('TypedDict does not support instance and class checks')\n   \n  __instancecheck__=__subclasscheck__\n  \n _TypedDict=type.__new__(_TypedDictMeta,'TypedDict',(),{})\n \n @_ensure_subclassable(lambda bases:(_TypedDict,))\n def TypedDict(typename,fields=_marker,/,*,total=True,closed=False,**kwargs):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if fields is _marker or fields is None:\n   if fields is _marker:\n    deprecated_thing=\"Failing to pass a value for the 'fields' parameter\"\n   else:\n    deprecated_thing=\"Passing `None` as the 'fields' parameter\"\n    \n   example=f\"`{typename} = TypedDict({typename !r}, {{}})`\"\n   deprecation_msg=(\n   f\"{deprecated_thing} is deprecated and will be disallowed in \"\n   \"Python 3.15. To create a TypedDict class with 0 fields \"\n   \"using the functional syntax, pass an empty dictionary, e.g. \"\n   )+example+\".\"\n   warnings.warn(deprecation_msg,DeprecationWarning,stacklevel=2)\n   if closed is not False and closed is not True:\n    kwargs[\"closed\"]=closed\n    closed=False\n   fields=kwargs\n  elif kwargs:\n   raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n   \" but not both\")\n  if kwargs:\n   if sys.version_info >=(3,13):\n    raise TypeError(\"TypedDict takes no keyword arguments\")\n   warnings.warn(\n   \"The kwargs-based syntax for TypedDict definitions is deprecated \"\n   \"in Python 3.11, will be removed in Python 3.13, and may not be \"\n   \"understood by third-party type checkers.\",\n   DeprecationWarning,\n   stacklevel=2,\n   )\n   \n  ns={'__annotations__':dict(fields)}\n  module=_caller()\n  if module is not None:\n  \n   ns['__module__']=module\n   \n  td=_TypedDictMeta(typename,(),ns,total=total,closed=closed)\n  td.__orig_bases__=(TypedDict,)\n  return td\n  \n if hasattr(typing,\"_TypedDictMeta\"):\n  _TYPEDDICT_TYPES=(typing._TypedDictMeta,_TypedDictMeta)\n else:\n  _TYPEDDICT_TYPES=(_TypedDictMeta,)\n  \n def is_typeddict(tp):\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  if hasattr(typing,\"TypedDict\")and tp is typing.TypedDict:\n   return False\n  return isinstance(tp,_TYPEDDICT_TYPES)\n  \n  \nif hasattr(typing,\"assert_type\"):\n assert_type=typing.assert_type\n \nelse:\n def assert_type(val,typ,/):\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return val\n  \n  \nif hasattr(typing,\"ReadOnly\"):\n get_type_hints=typing.get_type_hints\nelse:\n\n def _strip_extras(t):\n  ''\n  if isinstance(t,_AnnotatedAlias):\n   return _strip_extras(t.__origin__)\n  if hasattr(t,\"__origin__\")and t.__origin__ in(Required,NotRequired,ReadOnly):\n   return _strip_extras(t.__args__[0])\n  if isinstance(t,typing._GenericAlias):\n   stripped_args=tuple(_strip_extras(a)for a in t.__args__)\n   if stripped_args ==t.__args__:\n    return t\n   return t.copy_with(stripped_args)\n  if hasattr(_types,\"GenericAlias\")and isinstance(t,_types.GenericAlias):\n   stripped_args=tuple(_strip_extras(a)for a in t.__args__)\n   if stripped_args ==t.__args__:\n    return t\n   return _types.GenericAlias(t.__origin__,stripped_args)\n  if hasattr(_types,\"UnionType\")and isinstance(t,_types.UnionType):\n   stripped_args=tuple(_strip_extras(a)for a in t.__args__)\n   if stripped_args ==t.__args__:\n    return t\n   return functools.reduce(operator.or_,stripped_args)\n   \n  return t\n  \n def get_type_hints(obj,globalns=None,localns=None,include_extras=False):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if hasattr(typing,\"Annotated\"):\n   hint=typing.get_type_hints(\n   obj,globalns=globalns,localns=localns,include_extras=True\n   )\n  else:\n   hint=typing.get_type_hints(obj,globalns=globalns,localns=localns)\n  if include_extras:\n   return hint\n  return{k:_strip_extras(t)for k,t in hint.items()}\n  \n  \n  \nif hasattr(typing,'Annotated'):\n Annotated=typing.Annotated\n \n \n _AnnotatedAlias=typing._AnnotatedAlias\n \nelse:\n class _AnnotatedAlias(typing._GenericAlias,_root=True):\n  ''\n\n\n\n\n\n  \n  def __init__(self,origin,metadata):\n   if isinstance(origin,_AnnotatedAlias):\n    metadata=origin.__metadata__+metadata\n    origin=origin.__origin__\n   super().__init__(origin,origin)\n   self.__metadata__=metadata\n   \n  def copy_with(self,params):\n   assert len(params)==1\n   new_type=params[0]\n   return _AnnotatedAlias(new_type,self.__metadata__)\n   \n  def __repr__(self):\n   return(f\"typing_extensions.Annotated[{typing._type_repr(self.__origin__)}, \"\n   f\"{', '.join(repr(a)for a in self.__metadata__)}]\")\n   \n  def __reduce__(self):\n   return operator.getitem,(\n   Annotated,(self.__origin__,*self.__metadata__)\n   )\n   \n  def __eq__(self,other):\n   if not isinstance(other,_AnnotatedAlias):\n    return NotImplemented\n   if self.__origin__ !=other.__origin__:\n    return False\n   return self.__metadata__ ==other.__metadata__\n   \n  def __hash__(self):\n   return hash((self.__origin__,self.__metadata__))\n   \n class Annotated:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  __slots__=()\n  \n  def __new__(cls,*args,**kwargs):\n   raise TypeError(\"Type Annotated cannot be instantiated.\")\n   \n  @typing._tp_cache\n  def __class_getitem__(cls,params):\n   if not isinstance(params,tuple)or len(params)<2:\n    raise TypeError(\"Annotated[...] should be used \"\n    \"with at least two arguments (a type and an \"\n    \"annotation).\")\n   allowed_special_forms=(ClassVar,Final)\n   if get_origin(params[0])in allowed_special_forms:\n    origin=params[0]\n   else:\n    msg=\"Annotated[t, ...]: t must be a type.\"\n    origin=typing._type_check(params[0],msg)\n   metadata=tuple(params[1:])\n   return _AnnotatedAlias(origin,metadata)\n   \n  def __init_subclass__(cls,*args,**kwargs):\n   raise TypeError(\n   f\"Cannot subclass {cls.__module__}.Annotated\"\n   )\n   \n   \n   \n   \nif sys.version_info[:2]>=(3,10):\n get_origin=typing.get_origin\n get_args=typing.get_args\n \nelse:\n try:\n \n  from typing import _BaseGenericAlias\n except ImportError:\n  _BaseGenericAlias=typing._GenericAlias\n try:\n \n  from typing import GenericAlias as _typing_GenericAlias\n except ImportError:\n  _typing_GenericAlias=typing._GenericAlias\n  \n def get_origin(tp):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if isinstance(tp,_AnnotatedAlias):\n   return Annotated\n  if isinstance(tp,(typing._GenericAlias,_typing_GenericAlias,_BaseGenericAlias,\n  ParamSpecArgs,ParamSpecKwargs)):\n   return tp.__origin__\n  if tp is typing.Generic:\n   return typing.Generic\n  return None\n  \n def get_args(tp):\n  ''\n\n\n\n\n\n\n\n\n  \n  if isinstance(tp,_AnnotatedAlias):\n   return(tp.__origin__,*tp.__metadata__)\n  if isinstance(tp,(typing._GenericAlias,_typing_GenericAlias)):\n   if getattr(tp,\"_special\",False):\n    return()\n   res=tp.__args__\n   if get_origin(tp)is collections.abc.Callable and res[0]is not Ellipsis:\n    res=(list(res[:-1]),res[-1])\n   return res\n  return()\n  \n  \n  \nif hasattr(typing,'TypeAlias'):\n TypeAlias=typing.TypeAlias\n \nelif sys.version_info[:2]>=(3,9):\n @_ExtensionsSpecialForm\n def TypeAlias(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n  \n  raise TypeError(f\"{self} is not subscriptable\")\n  \nelse:\n TypeAlias=_ExtensionsSpecialForm(\n 'TypeAlias',\n doc=\"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example\n        above.\"\"\"\n )\n \n \nif hasattr(typing,\"NoDefault\"):\n NoDefault=typing.NoDefault\nelse:\n class NoDefaultTypeMeta(type):\n  def __setattr__(cls,attr,value):\n  \n   raise TypeError(\n   f\"cannot set {attr !r} attribute of immutable type {cls.__name__ !r}\"\n   )\n   \n class NoDefaultType(metaclass=NoDefaultTypeMeta):\n  ''\n  \n  __slots__=()\n  \n  def __new__(cls):\n   return globals().get(\"NoDefault\")or object.__new__(cls)\n   \n  def __repr__(self):\n   return \"typing_extensions.NoDefault\"\n   \n  def __reduce__(self):\n   return \"NoDefault\"\n   \n NoDefault=NoDefaultType()\n del NoDefaultType,NoDefaultTypeMeta\n \n \ndef _set_default(type_param,default):\n type_param.has_default=lambda:default is not NoDefault\n type_param.__default__=default\n \n \ndef _set_module(typevarlike):\n\n def_mod=_caller(depth=3)\n if def_mod !='typing_extensions':\n  typevarlike.__module__=def_mod\n  \n  \nclass _DefaultMixin:\n ''\n \n __slots__=()\n __init__=_set_default\n \n \n \nclass _TypeVarLikeMeta(type):\n def __instancecheck__(cls,__instance:Any)->bool:\n  return isinstance(__instance,cls._backported_typevarlike)\n  \n  \nif _PEP_696_IMPLEMENTED:\n from typing import TypeVar\nelse:\n\n class TypeVar(metaclass=_TypeVarLikeMeta):\n  ''\n  \n  _backported_typevarlike=typing.TypeVar\n  \n  def __new__(cls,name,*constraints,bound=None,\n  covariant=False,contravariant=False,\n  default=NoDefault,infer_variance=False):\n   if hasattr(typing,\"TypeAliasType\"):\n   \n    typevar=typing.TypeVar(name,*constraints,bound=bound,\n    covariant=covariant,contravariant=contravariant,\n    infer_variance=infer_variance)\n   else:\n    typevar=typing.TypeVar(name,*constraints,bound=bound,\n    covariant=covariant,contravariant=contravariant)\n    if infer_variance and(covariant or contravariant):\n     raise ValueError(\"Variance cannot be specified with infer_variance.\")\n    typevar.__infer_variance__=infer_variance\n    \n   _set_default(typevar,default)\n   _set_module(typevar)\n   \n   def _tvar_prepare_subst(alias,args):\n    if(\n    typevar.has_default()\n    and alias.__parameters__.index(typevar)==len(args)\n    ):\n     args +=(typevar.__default__,)\n    return args\n    \n   typevar.__typing_prepare_subst__=_tvar_prepare_subst\n   return typevar\n   \n  def __init_subclass__(cls)->None:\n   raise TypeError(f\"type '{__name__}.TypeVar' is not an acceptable base type\")\n   \n   \n   \nif hasattr(typing,'ParamSpecArgs'):\n ParamSpecArgs=typing.ParamSpecArgs\n ParamSpecKwargs=typing.ParamSpecKwargs\n \nelse:\n class _Immutable:\n  ''\n  __slots__=()\n  \n  def __copy__(self):\n   return self\n   \n  def __deepcopy__(self,memo):\n   return self\n   \n class ParamSpecArgs(_Immutable):\n  ''\n\n\n\n\n\n\n\n\n\n  \n  def __init__(self,origin):\n   self.__origin__=origin\n   \n  def __repr__(self):\n   return f\"{self.__origin__.__name__}.args\"\n   \n  def __eq__(self,other):\n   if not isinstance(other,ParamSpecArgs):\n    return NotImplemented\n   return self.__origin__ ==other.__origin__\n   \n class ParamSpecKwargs(_Immutable):\n  ''\n\n\n\n\n\n\n\n\n\n  \n  def __init__(self,origin):\n   self.__origin__=origin\n   \n  def __repr__(self):\n   return f\"{self.__origin__.__name__}.kwargs\"\n   \n  def __eq__(self,other):\n   if not isinstance(other,ParamSpecKwargs):\n    return NotImplemented\n   return self.__origin__ ==other.__origin__\n   \n   \nif _PEP_696_IMPLEMENTED:\n from typing import ParamSpec\n \n \nelif hasattr(typing,'ParamSpec'):\n\n\n class ParamSpec(metaclass=_TypeVarLikeMeta):\n  ''\n  \n  _backported_typevarlike=typing.ParamSpec\n  \n  def __new__(cls,name,*,bound=None,\n  covariant=False,contravariant=False,\n  infer_variance=False,default=NoDefault):\n   if hasattr(typing,\"TypeAliasType\"):\n   \n    paramspec=typing.ParamSpec(name,bound=bound,\n    covariant=covariant,\n    contravariant=contravariant,\n    infer_variance=infer_variance)\n   else:\n    paramspec=typing.ParamSpec(name,bound=bound,\n    covariant=covariant,\n    contravariant=contravariant)\n    paramspec.__infer_variance__=infer_variance\n    \n   _set_default(paramspec,default)\n   _set_module(paramspec)\n   \n   def _paramspec_prepare_subst(alias,args):\n    params=alias.__parameters__\n    i=params.index(paramspec)\n    if i ==len(args)and paramspec.has_default():\n     args=[*args,paramspec.__default__]\n    if i >=len(args):\n     raise TypeError(f\"Too few arguments for {alias}\")\n     \n    if len(params)==1 and not typing._is_param_expr(args[0]):\n     assert i ==0\n     args=(args,)\n     \n    elif isinstance(args[i],list):\n     args=(*args[:i],tuple(args[i]),*args[i+1:])\n    return args\n    \n   paramspec.__typing_prepare_subst__=_paramspec_prepare_subst\n   return paramspec\n   \n  def __init_subclass__(cls)->None:\n   raise TypeError(f\"type '{__name__}.ParamSpec' is not an acceptable base type\")\n   \n   \nelse:\n\n\n class ParamSpec(list,_DefaultMixin):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  __class__=typing.TypeVar\n  \n  @property\n  def args(self):\n   return ParamSpecArgs(self)\n   \n  @property\n  def kwargs(self):\n   return ParamSpecKwargs(self)\n   \n  def __init__(self,name,*,bound=None,covariant=False,contravariant=False,\n  infer_variance=False,default=NoDefault):\n   list.__init__(self,[self])\n   self.__name__=name\n   self.__covariant__=bool(covariant)\n   self.__contravariant__=bool(contravariant)\n   self.__infer_variance__=bool(infer_variance)\n   if bound:\n    self.__bound__=typing._type_check(bound,'Bound must be a type.')\n   else:\n    self.__bound__=None\n   _DefaultMixin.__init__(self,default)\n   \n   \n   def_mod=_caller()\n   if def_mod !='typing_extensions':\n    self.__module__=def_mod\n    \n  def __repr__(self):\n   if self.__infer_variance__:\n    prefix=''\n   elif self.__covariant__:\n    prefix='+'\n   elif self.__contravariant__:\n    prefix='-'\n   else:\n    prefix='~'\n   return prefix+self.__name__\n   \n  def __hash__(self):\n   return object.__hash__(self)\n   \n  def __eq__(self,other):\n   return self is other\n   \n  def __reduce__(self):\n   return self.__name__\n   \n   \n  def __call__(self,*args,**kwargs):\n   pass\n   \n   \n   \nif not hasattr(typing,'Concatenate'):\n\n class _ConcatenateGenericAlias(list):\n \n \n  __class__=typing._GenericAlias\n  \n  \n  _special=False\n  \n  def __init__(self,origin,args):\n   super().__init__(args)\n   self.__origin__=origin\n   self.__args__=args\n   \n  def __repr__(self):\n   _type_repr=typing._type_repr\n   return(f'{_type_repr(self.__origin__)}'\n   f'[{\", \".join(_type_repr(arg)for arg in self.__args__)}]')\n   \n  def __hash__(self):\n   return hash((self.__origin__,self.__args__))\n   \n   \n  def __call__(self,*args,**kwargs):\n   pass\n   \n  @property\n  def __parameters__(self):\n   return tuple(\n   tp for tp in self.__args__ if isinstance(tp,(typing.TypeVar,ParamSpec))\n   )\n   \n   \n   \n@typing._tp_cache\ndef _concatenate_getitem(self,parameters):\n if parameters ==():\n  raise TypeError(\"Cannot take a Concatenate of no types.\")\n if not isinstance(parameters,tuple):\n  parameters=(parameters,)\n if not isinstance(parameters[-1],ParamSpec):\n  raise TypeError(\"The last parameter to Concatenate should be a \"\n  \"ParamSpec variable.\")\n msg=\"Concatenate[arg, ...]: each arg must be a type.\"\n parameters=tuple(typing._type_check(p,msg)for p in parameters)\n return _ConcatenateGenericAlias(self,parameters)\n \n \n \nif hasattr(typing,'Concatenate'):\n Concatenate=typing.Concatenate\n _ConcatenateGenericAlias=typing._ConcatenateGenericAlias\n \nelif sys.version_info[:2]>=(3,9):\n @_ExtensionsSpecialForm\n def Concatenate(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n  \n  return _concatenate_getitem(self,parameters)\n  \nelse:\n class _ConcatenateForm(_ExtensionsSpecialForm,_root=True):\n  def __getitem__(self,parameters):\n   return _concatenate_getitem(self,parameters)\n   \n Concatenate=_ConcatenateForm(\n 'Concatenate',\n doc=\"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\")\n \n \nif hasattr(typing,'TypeGuard'):\n TypeGuard=typing.TypeGuard\n \nelif sys.version_info[:2]>=(3,9):\n @_ExtensionsSpecialForm\n def TypeGuard(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  item=typing._type_check(parameters,f'{self} accepts only a single type.')\n  return typing._GenericAlias(self,(item,))\n  \nelse:\n class _TypeGuardForm(_ExtensionsSpecialForm,_root=True):\n  def __getitem__(self,parameters):\n   item=typing._type_check(parameters,\n   f'{self._name} accepts only a single type')\n   return typing._GenericAlias(self,(item,))\n   \n TypeGuard=_TypeGuardForm(\n 'TypeGuard',\n doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\")\n \n \nif hasattr(typing,'TypeIs'):\n TypeIs=typing.TypeIs\n \nelif sys.version_info[:2]>=(3,9):\n @_ExtensionsSpecialForm\n def TypeIs(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  item=typing._type_check(parameters,f'{self} accepts only a single type.')\n  return typing._GenericAlias(self,(item,))\n  \nelse:\n class _TypeIsForm(_ExtensionsSpecialForm,_root=True):\n  def __getitem__(self,parameters):\n   item=typing._type_check(parameters,\n   f'{self._name} accepts only a single type')\n   return typing._GenericAlias(self,(item,))\n   \n TypeIs=_TypeIsForm(\n 'TypeIs',\n doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type narrower function.  ``TypeIs`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeIs`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeIs[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeIs`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the intersection of the type inside ``TypeGuard`` and the argument's\n        previously known type.\n\n        For example::\n\n            def is_awaitable(val: object) -> TypeIs[Awaitable[Any]]:\n                return hasattr(val, '__await__')\n\n            def f(val: Union[int, Awaitable[int]]) -> int:\n                if is_awaitable(val):\n                    assert_type(val, Awaitable[int])\n                else:\n                    assert_type(val, int)\n\n        ``TypeIs`` also works with type variables.  For more information, see\n        PEP 742 (Narrowing types with TypeIs).\n        \"\"\")\n \n \n \nclass _SpecialForm(typing._Final,_root=True):\n __slots__=('_name','__doc__','_getitem')\n \n def __init__(self,getitem):\n  self._getitem=getitem\n  self._name=getitem.__name__\n  self.__doc__=getitem.__doc__\n  \n def __getattr__(self,item):\n  if item in{'__name__','__qualname__'}:\n   return self._name\n   \n  raise AttributeError(item)\n  \n def __mro_entries__(self,bases):\n  raise TypeError(f\"Cannot subclass {self !r}\")\n  \n def __repr__(self):\n  return f'typing_extensions.{self._name}'\n  \n def __reduce__(self):\n  return self._name\n  \n def __call__(self,*args,**kwds):\n  raise TypeError(f\"Cannot instantiate {self !r}\")\n  \n def __or__(self,other):\n  return typing.Union[self,other]\n  \n def __ror__(self,other):\n  return typing.Union[other,self]\n  \n def __instancecheck__(self,obj):\n  raise TypeError(f\"{self} cannot be used with isinstance()\")\n  \n def __subclasscheck__(self,cls):\n  raise TypeError(f\"{self} cannot be used with issubclass()\")\n  \n @typing._tp_cache\n def __getitem__(self,parameters):\n  return self._getitem(self,parameters)\n  \n  \nif hasattr(typing,\"LiteralString\"):\n LiteralString=typing.LiteralString\nelse:\n @_SpecialForm\n def LiteralString(self,params):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise TypeError(f\"{self} is not subscriptable\")\n  \n  \nif hasattr(typing,\"Self\"):\n Self=typing.Self\nelse:\n @_SpecialForm\n def Self(self,params):\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  \n  raise TypeError(f\"{self} is not subscriptable\")\n  \n  \nif hasattr(typing,\"Never\"):\n Never=typing.Never\nelse:\n @_SpecialForm\n def Never(self,params):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  raise TypeError(f\"{self} is not subscriptable\")\n  \n  \nif hasattr(typing,'Required'):\n Required=typing.Required\n NotRequired=typing.NotRequired\nelif sys.version_info[:2]>=(3,9):\n @_ExtensionsSpecialForm\n def Required(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  item=typing._type_check(parameters,f'{self._name} accepts only a single type.')\n  return typing._GenericAlias(self,(item,))\n  \n @_ExtensionsSpecialForm\n def NotRequired(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  item=typing._type_check(parameters,f'{self._name} accepts only a single type.')\n  return typing._GenericAlias(self,(item,))\n  \nelse:\n class _RequiredForm(_ExtensionsSpecialForm,_root=True):\n  def __getitem__(self,parameters):\n   item=typing._type_check(parameters,\n   f'{self._name} accepts only a single type.')\n   return typing._GenericAlias(self,(item,))\n   \n Required=_RequiredForm(\n 'Required',\n doc=\"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\")\n NotRequired=_RequiredForm(\n 'NotRequired',\n doc=\"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\")\n \n \nif hasattr(typing,'ReadOnly'):\n ReadOnly=typing.ReadOnly\nelif sys.version_info[:2]>=(3,9):\n @_ExtensionsSpecialForm\n def ReadOnly(self,parameters):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  item=typing._type_check(parameters,f'{self._name} accepts only a single type.')\n  return typing._GenericAlias(self,(item,))\n  \nelse:\n class _ReadOnlyForm(_ExtensionsSpecialForm,_root=True):\n  def __getitem__(self,parameters):\n   item=typing._type_check(parameters,\n   f'{self._name} accepts only a single type.')\n   return typing._GenericAlias(self,(item,))\n   \n ReadOnly=_ReadOnlyForm(\n 'ReadOnly',\n doc=\"\"\"A special typing construct to mark a key of a TypedDict as read-only.\n\n        For example:\n\n            class Movie(TypedDict):\n                title: ReadOnly[str]\n                year: int\n\n            def mutate_movie(m: Movie) -> None:\n                m[\"year\"] = 1992  # allowed\n                m[\"title\"] = \"The Matrix\"  # typechecker error\n\n        There is no runtime checking for this propery.\n        \"\"\")\n \n \n_UNPACK_DOC=\"\"\"\\\nType unpack operator.\n\nThe type unpack operator takes the child types from some container type,\nsuch as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'. For\nexample:\n\n  # For some generic class `Foo`:\n  Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n  Ts = TypeVarTuple('Ts')\n  # Specifies that `Bar` is generic in an arbitrary number of types.\n  # (Think of `Ts` as a tuple of an arbitrary number of individual\n  #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n  #  `Generic[]`.)\n  class Bar(Generic[Unpack[Ts]]): ...\n  Bar[int]  # Valid\n  Bar[int, str]  # Also valid\n\nFrom Python 3.11, this can also be done using the `*` operator:\n\n    Foo[*tuple[int, str]]\n    class Bar(Generic[*Ts]): ...\n\nThe operator can also be used along with a `TypedDict` to annotate\n`**kwargs` in a function signature. For instance:\n\n  class Movie(TypedDict):\n    name: str\n    year: int\n\n  # This function expects two keyword arguments - *name* of type `str` and\n  # *year* of type `int`.\n  def foo(**kwargs: Unpack[Movie]): ...\n\nNote that there is only some runtime checking of this operator. Not\neverything the runtime allows may be accepted by static type checkers.\n\nFor more information, see PEP 646 and PEP 692.\n\"\"\"\n\n\nif sys.version_info >=(3,12):\n Unpack=typing.Unpack\n \n def _is_unpack(obj):\n  return get_origin(obj)is Unpack\n  \nelif sys.version_info[:2]>=(3,9):\n class _UnpackSpecialForm(_ExtensionsSpecialForm,_root=True):\n  def __init__(self,getitem):\n   super().__init__(getitem)\n   self.__doc__=_UNPACK_DOC\n   \n class _UnpackAlias(typing._GenericAlias,_root=True):\n  __class__=typing.TypeVar\n  \n  @property\n  def __typing_unpacked_tuple_args__(self):\n   assert self.__origin__ is Unpack\n   assert len(self.__args__)==1\n   arg,=self.__args__\n   if isinstance(arg,(typing._GenericAlias,_types.GenericAlias)):\n    if arg.__origin__ is not tuple:\n     raise TypeError(\"Unpack[...] must be used with a tuple type\")\n    return arg.__args__\n   return None\n   \n @_UnpackSpecialForm\n def Unpack(self,parameters):\n  item=typing._type_check(parameters,f'{self._name} accepts only a single type.')\n  return _UnpackAlias(self,(item,))\n  \n def _is_unpack(obj):\n  return isinstance(obj,_UnpackAlias)\n  \nelse:\n class _UnpackAlias(typing._GenericAlias,_root=True):\n  __class__=typing.TypeVar\n  \n class _UnpackForm(_ExtensionsSpecialForm,_root=True):\n  def __getitem__(self,parameters):\n   item=typing._type_check(parameters,\n   f'{self._name} accepts only a single type.')\n   return _UnpackAlias(self,(item,))\n   \n Unpack=_UnpackForm('Unpack',doc=_UNPACK_DOC)\n \n def _is_unpack(obj):\n  return isinstance(obj,_UnpackAlias)\n  \n  \nif _PEP_696_IMPLEMENTED:\n from typing import TypeVarTuple\n \nelif hasattr(typing,\"TypeVarTuple\"):\n\n def _unpack_args(*args):\n  newargs=[]\n  for arg in args:\n   subargs=getattr(arg,'__typing_unpacked_tuple_args__',None)\n   if subargs is not None and not(subargs and subargs[-1]is ...):\n    newargs.extend(subargs)\n   else:\n    newargs.append(arg)\n  return newargs\n  \n  \n class TypeVarTuple(metaclass=_TypeVarLikeMeta):\n  ''\n  \n  _backported_typevarlike=typing.TypeVarTuple\n  \n  def __new__(cls,name,*,default=NoDefault):\n   tvt=typing.TypeVarTuple(name)\n   _set_default(tvt,default)\n   _set_module(tvt)\n   \n   def _typevartuple_prepare_subst(alias,args):\n    params=alias.__parameters__\n    typevartuple_index=params.index(tvt)\n    for param in params[typevartuple_index+1:]:\n     if isinstance(param,TypeVarTuple):\n      raise TypeError(\n      f\"More than one TypeVarTuple parameter in {alias}\"\n      )\n      \n    alen=len(args)\n    plen=len(params)\n    left=typevartuple_index\n    right=plen -typevartuple_index -1\n    var_tuple_index=None\n    fillarg=None\n    for k,arg in enumerate(args):\n     if not isinstance(arg,type):\n      subargs=getattr(arg,'__typing_unpacked_tuple_args__',None)\n      if subargs and len(subargs)==2 and subargs[-1]is ...:\n       if var_tuple_index is not None:\n        raise TypeError(\n        \"More than one unpacked \"\n        \"arbitrary-length tuple argument\"\n        )\n       var_tuple_index=k\n       fillarg=subargs[0]\n    if var_tuple_index is not None:\n     left=min(left,var_tuple_index)\n     right=min(right,alen -var_tuple_index -1)\n    elif left+right >alen:\n     raise TypeError(f\"Too few arguments for {alias};\"\n     f\" actual {alen}, expected at least {plen -1}\")\n    if left ==alen -right and tvt.has_default():\n     replacement=_unpack_args(tvt.__default__)\n    else:\n     replacement=args[left:alen -right]\n     \n    return(\n    *args[:left],\n    *([fillarg]*(typevartuple_index -left)),\n    replacement,\n    *([fillarg]*(plen -right -left -typevartuple_index -1)),\n    *args[alen -right:],\n    )\n    \n   tvt.__typing_prepare_subst__=_typevartuple_prepare_subst\n   return tvt\n   \n  def __init_subclass__(self,*args,**kwds):\n   raise TypeError(\"Cannot subclass special typing classes\")\n   \nelse:\n class TypeVarTuple(_DefaultMixin):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  __class__=typing.TypeVar\n  \n  def __iter__(self):\n   yield self.__unpacked__\n   \n  def __init__(self,name,*,default=NoDefault):\n   self.__name__=name\n   _DefaultMixin.__init__(self,default)\n   \n   \n   def_mod=_caller()\n   if def_mod !='typing_extensions':\n    self.__module__=def_mod\n    \n   self.__unpacked__=Unpack[self]\n   \n  def __repr__(self):\n   return self.__name__\n   \n  def __hash__(self):\n   return object.__hash__(self)\n   \n  def __eq__(self,other):\n   return self is other\n   \n  def __reduce__(self):\n   return self.__name__\n   \n  def __init_subclass__(self,*args,**kwds):\n   if '_root'not in kwds:\n    raise TypeError(\"Cannot subclass special typing classes\")\n    \n    \nif hasattr(typing,\"reveal_type\"):\n reveal_type=typing.reveal_type\nelse:\n def reveal_type(obj:T,/)->T:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  print(f\"Runtime type is {type(obj).__name__ !r}\",file=sys.stderr)\n  return obj\n  \n  \nif hasattr(typing,\"_ASSERT_NEVER_REPR_MAX_LENGTH\"):\n _ASSERT_NEVER_REPR_MAX_LENGTH=typing._ASSERT_NEVER_REPR_MAX_LENGTH\nelse:\n _ASSERT_NEVER_REPR_MAX_LENGTH=100\n \n \nif hasattr(typing,\"assert_never\"):\n assert_never=typing.assert_never\nelse:\n def assert_never(arg:Never,/)->Never:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  value=repr(arg)\n  if len(value)>_ASSERT_NEVER_REPR_MAX_LENGTH:\n   value=value[:_ASSERT_NEVER_REPR_MAX_LENGTH]+'...'\n  raise AssertionError(f\"Expected code to be unreachable, but got: {value}\")\n  \n  \nif sys.version_info >=(3,12):\n\n dataclass_transform=typing.dataclass_transform\nelse:\n def dataclass_transform(\n *,\n eq_default:bool=True,\n order_default:bool=False,\n kw_only_default:bool=False,\n frozen_default:bool=False,\n field_specifiers:typing.Tuple[\n typing.Union[typing.Type[typing.Any],typing.Callable[...,typing.Any]],\n ...\n ]=(),\n **kwargs:typing.Any,\n )->typing.Callable[[T],T]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  def decorator(cls_or_fn):\n   cls_or_fn.__dataclass_transform__={\n   \"eq_default\":eq_default,\n   \"order_default\":order_default,\n   \"kw_only_default\":kw_only_default,\n   \"frozen_default\":frozen_default,\n   \"field_specifiers\":field_specifiers,\n   \"kwargs\":kwargs,\n   }\n   return cls_or_fn\n  return decorator\n  \n  \nif hasattr(typing,\"override\"):\n override=typing.override\nelse:\n _F=typing.TypeVar(\"_F\",bound=typing.Callable[...,typing.Any])\n \n def override(arg:_F,/)->_F:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  try:\n   arg.__override__=True\n  except(AttributeError,TypeError):\n  \n  \n  \n   pass\n  return arg\n  \n  \nif hasattr(warnings,\"deprecated\"):\n deprecated=warnings.deprecated\nelse:\n _T=typing.TypeVar(\"_T\")\n \n class deprecated:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  def __init__(\n  self,\n  message:str,\n  /,\n  *,\n  category:typing.Optional[typing.Type[Warning]]=DeprecationWarning,\n  stacklevel:int=1,\n  )->None:\n   if not isinstance(message,str):\n    raise TypeError(\n    \"Expected an object of type str for 'message', not \"\n    f\"{type(message).__name__ !r}\"\n    )\n   self.message=message\n   self.category=category\n   self.stacklevel=stacklevel\n   \n  def __call__(self,arg:_T,/)->_T:\n  \n  \n   msg=self.message\n   category=self.category\n   stacklevel=self.stacklevel\n   if category is None:\n    arg.__deprecated__=msg\n    return arg\n   elif isinstance(arg,type):\n    import functools\n    from types import MethodType\n    \n    original_new=arg.__new__\n    \n    @functools.wraps(original_new)\n    def __new__(cls,*args,**kwargs):\n     if cls is arg:\n      warnings.warn(msg,category=category,stacklevel=stacklevel+1)\n     if original_new is not object.__new__:\n      return original_new(cls,*args,**kwargs)\n      \n     elif cls.__init__ is object.__init__ and(args or kwargs):\n      raise TypeError(f\"{cls.__name__}() takes no arguments\")\n     else:\n      return original_new(cls)\n      \n    arg.__new__=staticmethod(__new__)\n    \n    original_init_subclass=arg.__init_subclass__\n    \n    \n    if isinstance(original_init_subclass,MethodType):\n     original_init_subclass=original_init_subclass.__func__\n     \n     @functools.wraps(original_init_subclass)\n     def __init_subclass__(*args,**kwargs):\n      warnings.warn(msg,category=category,stacklevel=stacklevel+1)\n      return original_init_subclass(*args,**kwargs)\n      \n     arg.__init_subclass__=classmethod(__init_subclass__)\n     \n     \n    else:\n     @functools.wraps(original_init_subclass)\n     def __init_subclass__(*args,**kwargs):\n      warnings.warn(msg,category=category,stacklevel=stacklevel+1)\n      return original_init_subclass(*args,**kwargs)\n      \n     arg.__init_subclass__=__init_subclass__\n     \n    arg.__deprecated__=__new__.__deprecated__=msg\n    __init_subclass__.__deprecated__=msg\n    return arg\n   elif callable(arg):\n    import functools\n    \n    @functools.wraps(arg)\n    def wrapper(*args,**kwargs):\n     warnings.warn(msg,category=category,stacklevel=stacklevel+1)\n     return arg(*args,**kwargs)\n     \n    arg.__deprecated__=wrapper.__deprecated__=msg\n    return wrapper\n   else:\n    raise TypeError(\n    \"@deprecated decorator with non-None category must be applied to \"\n    f\"a class or callable, not {arg !r}\"\n    )\n    \n    \n    \n    \n    \n    \n    \n    \n    \nif not hasattr(typing,\"TypeVarTuple\"):\n def _check_generic(cls,parameters,elen=_marker):\n  ''\n\n\n  \n  if not elen:\n   raise TypeError(f\"{cls} is not a generic class\")\n  if elen is _marker:\n   if not hasattr(cls,\"__parameters__\")or not cls.__parameters__:\n    raise TypeError(f\"{cls} is not a generic class\")\n   elen=len(cls.__parameters__)\n  alen=len(parameters)\n  if alen !=elen:\n   expect_val=elen\n   if hasattr(cls,\"__parameters__\"):\n    parameters=[p for p in cls.__parameters__ if not _is_unpack(p)]\n    num_tv_tuples=sum(isinstance(p,TypeVarTuple)for p in parameters)\n    if(num_tv_tuples >0)and(alen >=elen -num_tv_tuples):\n     return\n     \n     \n     \n    if alen <elen:\n    \n    \n     if(\n     getattr(parameters[alen],'__default__',NoDefault)\n     is not NoDefault\n     ):\n      return\n      \n     num_default_tv=sum(getattr(p,'__default__',NoDefault)\n     is not NoDefault for p in parameters)\n     \n     elen -=num_default_tv\n     \n     expect_val=f\"at least {elen}\"\n     \n   things=\"arguments\"if sys.version_info >=(3,10)else \"parameters\"\n   raise TypeError(f\"Too {'many'if alen >elen else 'few'} {things}\"\n   f\" for {cls}; actual {alen}, expected {expect_val}\")\nelse:\n\n\n def _check_generic(cls,parameters,elen):\n  ''\n\n\n  \n  if not elen:\n   raise TypeError(f\"{cls} is not a generic class\")\n  alen=len(parameters)\n  if alen !=elen:\n   expect_val=elen\n   if hasattr(cls,\"__parameters__\"):\n    parameters=[p for p in cls.__parameters__ if not _is_unpack(p)]\n    \n    \n    \n    if alen <elen:\n    \n    \n     if(\n     getattr(parameters[alen],'__default__',NoDefault)\n     is not NoDefault\n     ):\n      return\n      \n     num_default_tv=sum(getattr(p,'__default__',NoDefault)\n     is not NoDefault for p in parameters)\n     \n     elen -=num_default_tv\n     \n     expect_val=f\"at least {elen}\"\n     \n   raise TypeError(f\"Too {'many'if alen >elen else 'few'} arguments\"\n   f\" for {cls}; actual {alen}, expected {expect_val}\")\n   \nif not _PEP_696_IMPLEMENTED:\n typing._check_generic=_check_generic\n \n \ndef _has_generic_or_protocol_as_origin()->bool:\n try:\n  frame=sys._getframe(2)\n  \n  \n  \n except(AttributeError,ValueError):\n  return False\n else:\n \n \n  if frame.f_globals.get(\"__name__\")!=\"typing\":\n   return False\n  origin=frame.f_locals.get(\"origin\")\n  \n  \n  return origin is typing.Generic or origin is Protocol or origin is typing.Protocol\n  \n  \n_TYPEVARTUPLE_TYPES={TypeVarTuple,getattr(typing,\"TypeVarTuple\",None)}\n\n\ndef _is_unpacked_typevartuple(x)->bool:\n if get_origin(x)is not Unpack:\n  return False\n args=get_args(x)\n return(\n bool(args)\n and len(args)==1\n and type(args[0])in _TYPEVARTUPLE_TYPES\n )\n \n \n \nif hasattr(typing,'_collect_type_vars'):\n def _collect_type_vars(types,typevar_types=None):\n  ''\n\n\n\n  \n  if typevar_types is None:\n   typevar_types=typing.TypeVar\n  tvars=[]\n  \n  \n  \n  enforce_default_ordering=_has_generic_or_protocol_as_origin()\n  default_encountered=False\n  \n  \n  type_var_tuple_encountered=False\n  \n  for t in types:\n   if _is_unpacked_typevartuple(t):\n    type_var_tuple_encountered=True\n   elif isinstance(t,typevar_types)and t not in tvars:\n    if enforce_default_ordering:\n     has_default=getattr(t,'__default__',NoDefault)is not NoDefault\n     if has_default:\n      if type_var_tuple_encountered:\n       raise TypeError('Type parameter with a default'\n       ' follows TypeVarTuple')\n      default_encountered=True\n     elif default_encountered:\n      raise TypeError(f'Type parameter {t !r} without a default'\n      ' follows type parameter with a default')\n      \n    tvars.append(t)\n   if _should_collect_from_parameters(t):\n    tvars.extend([t for t in t.__parameters__ if t not in tvars])\n  return tuple(tvars)\n  \n typing._collect_type_vars=_collect_type_vars\nelse:\n def _collect_parameters(args):\n  ''\n\n\n\n\n\n  \n  parameters=[]\n  \n  \n  \n  enforce_default_ordering=_has_generic_or_protocol_as_origin()\n  default_encountered=False\n  \n  \n  type_var_tuple_encountered=False\n  \n  for t in args:\n   if isinstance(t,type):\n   \n    pass\n   elif isinstance(t,tuple):\n   \n   \n    for x in t:\n     for collected in _collect_parameters([x]):\n      if collected not in parameters:\n       parameters.append(collected)\n   elif hasattr(t,'__typing_subst__'):\n    if t not in parameters:\n     if enforce_default_ordering:\n      has_default=(\n      getattr(t,'__default__',NoDefault)is not NoDefault\n      )\n      \n      if type_var_tuple_encountered and has_default:\n       raise TypeError('Type parameter with a default'\n       ' follows TypeVarTuple')\n       \n      if has_default:\n       default_encountered=True\n      elif default_encountered:\n       raise TypeError(f'Type parameter {t !r} without a default'\n       ' follows type parameter with a default')\n       \n     parameters.append(t)\n   else:\n    if _is_unpacked_typevartuple(t):\n     type_var_tuple_encountered=True\n    for x in getattr(t,'__parameters__',()):\n     if x not in parameters:\n      parameters.append(x)\n      \n  return tuple(parameters)\n  \n if not _PEP_696_IMPLEMENTED:\n  typing._collect_parameters=_collect_parameters\n  \n  \n  \n  \n  \n  \nif sys.version_info >=(3,13):\n NamedTuple=typing.NamedTuple\nelse:\n def _make_nmtuple(name,types,module,defaults=()):\n  fields=[n for n,t in types]\n  annotations={n:typing._type_check(t,f\"field {n} annotation must be a type\")\n  for n,t in types}\n  nm_tpl=collections.namedtuple(name,fields,\n  defaults=defaults,module=module)\n  nm_tpl.__annotations__=nm_tpl.__new__.__annotations__=annotations\n  \n  \n  if sys.version_info <(3,9):\n   nm_tpl._field_types=annotations\n  return nm_tpl\n  \n _prohibited_namedtuple_fields=typing._prohibited\n _special_namedtuple_fields=frozenset({'__module__','__name__','__annotations__'})\n \n class _NamedTupleMeta(type):\n  def __new__(cls,typename,bases,ns):\n   assert _NamedTuple in bases\n   for base in bases:\n    if base is not _NamedTuple and base is not typing.Generic:\n     raise TypeError(\n     'can only inherit from a NamedTuple type and Generic')\n   bases=tuple(tuple if base is _NamedTuple else base for base in bases)\n   if \"__annotations__\"in ns:\n    types=ns[\"__annotations__\"]\n   elif \"__annotate__\"in ns:\n   \n    types=ns[\"__annotate__\"](1)\n   else:\n    types={}\n   default_names=[]\n   for field_name in types:\n    if field_name in ns:\n     default_names.append(field_name)\n    elif default_names:\n     raise TypeError(f\"Non-default namedtuple field {field_name} \"\n     f\"cannot follow default field\"\n     f\"{'s'if len(default_names)>1 else ''} \"\n     f\"{', '.join(default_names)}\")\n   nm_tpl=_make_nmtuple(\n   typename,types.items(),\n   defaults=[ns[n]for n in default_names],\n   module=ns['__module__']\n   )\n   nm_tpl.__bases__=bases\n   if typing.Generic in bases:\n    if hasattr(typing,'_generic_class_getitem'):\n     nm_tpl.__class_getitem__=classmethod(typing._generic_class_getitem)\n    else:\n     class_getitem=typing.Generic.__class_getitem__.__func__\n     nm_tpl.__class_getitem__=classmethod(class_getitem)\n     \n   for key,val in ns.items():\n    if key in _prohibited_namedtuple_fields:\n     raise AttributeError(\"Cannot overwrite NamedTuple attribute \"+key)\n    elif key not in _special_namedtuple_fields:\n     if key not in nm_tpl._fields:\n      setattr(nm_tpl,key,ns[key])\n     try:\n      set_name=type(val).__set_name__\n     except AttributeError:\n      pass\n     else:\n      try:\n       set_name(val,nm_tpl,key)\n      except BaseException as e:\n       msg=(\n       f\"Error calling __set_name__ on {type(val).__name__ !r} \"\n       f\"instance {key !r} in {typename !r}\"\n       )\n       \n       \n       \n       \n       \n       if sys.version_info >=(3,12):\n        e.add_note(msg)\n        raise\n       else:\n        raise RuntimeError(msg)from e\n        \n   if typing.Generic in bases:\n    nm_tpl.__init_subclass__()\n   return nm_tpl\n   \n _NamedTuple=type.__new__(_NamedTupleMeta,'NamedTuple',(),{})\n \n def _namedtuple_mro_entries(bases):\n  assert NamedTuple in bases\n  return(_NamedTuple,)\n  \n @_ensure_subclassable(_namedtuple_mro_entries)\n def NamedTuple(typename,fields=_marker,/,**kwargs):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if fields is _marker:\n   if kwargs:\n    deprecated_thing=\"Creating NamedTuple classes using keyword arguments\"\n    deprecation_msg=(\n    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n    \"Use the class-based or functional syntax instead.\"\n    )\n   else:\n    deprecated_thing=\"Failing to pass a value for the 'fields' parameter\"\n    example=f\"`{typename} = NamedTuple({typename !r}, [])`\"\n    deprecation_msg=(\n    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n    \"To create a NamedTuple class with 0 fields \"\n    \"using the functional syntax, \"\n    \"pass an empty list, e.g. \"\n    )+example+\".\"\n  elif fields is None:\n   if kwargs:\n    raise TypeError(\n    \"Cannot pass `None` as the 'fields' parameter \"\n    \"and also specify fields using keyword arguments\"\n    )\n   else:\n    deprecated_thing=\"Passing `None` as the 'fields' parameter\"\n    example=f\"`{typename} = NamedTuple({typename !r}, [])`\"\n    deprecation_msg=(\n    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n    \"To create a NamedTuple class with 0 fields \"\n    \"using the functional syntax, \"\n    \"pass an empty list, e.g. \"\n    )+example+\".\"\n  elif kwargs:\n   raise TypeError(\"Either list of fields or keywords\"\n   \" can be provided to NamedTuple, not both\")\n  if fields is _marker or fields is None:\n   warnings.warn(\n   deprecation_msg.format(name=deprecated_thing,remove=\"3.15\"),\n   DeprecationWarning,\n   stacklevel=2,\n   )\n   fields=kwargs.items()\n  nt=_make_nmtuple(typename,fields,module=_caller())\n  nt.__orig_bases__=(NamedTuple,)\n  return nt\n  \n  \nif hasattr(collections.abc,\"Buffer\"):\n Buffer=collections.abc.Buffer\nelse:\n class Buffer(abc.ABC):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n Buffer.register(memoryview)\n Buffer.register(bytearray)\n Buffer.register(bytes)\n \n \n \nif hasattr(_types,\"get_original_bases\"):\n get_original_bases=_types.get_original_bases\nelse:\n def get_original_bases(cls,/):\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  try:\n   return cls.__dict__.get(\"__orig_bases__\",cls.__bases__)\n  except AttributeError:\n   raise TypeError(\n   f'Expected an instance of type, not {type(cls).__name__ !r}'\n   )from None\n   \n   \n   \n   \nif sys.version_info >=(3,11):\n NewType=typing.NewType\nelse:\n class NewType:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  \n  def __call__(self,obj,/):\n   return obj\n   \n  def __init__(self,name,tp):\n   self.__qualname__=name\n   if '.'in name:\n    name=name.rpartition('.')[-1]\n   self.__name__=name\n   self.__supertype__=tp\n   def_mod=_caller()\n   if def_mod !='typing_extensions':\n    self.__module__=def_mod\n    \n  def __mro_entries__(self,bases):\n  \n  \n   supercls_name=self.__name__\n   \n   class Dummy:\n    def __init_subclass__(cls):\n     subcls_name=cls.__name__\n     raise TypeError(\n     f\"Cannot subclass an instance of NewType. \"\n     f\"Perhaps you were looking for: \"\n     f\"`{subcls_name} = NewType({subcls_name !r}, {supercls_name})`\"\n     )\n     \n   return(Dummy,)\n   \n  def __repr__(self):\n   return f'{self.__module__}.{self.__qualname__}'\n   \n  def __reduce__(self):\n   return self.__qualname__\n   \n  if sys.version_info >=(3,10):\n  \n  \n  \n   def __or__(self,other):\n    return typing.Union[self,other]\n    \n   def __ror__(self,other):\n    return typing.Union[other,self]\n    \n    \nif hasattr(typing,\"TypeAliasType\"):\n TypeAliasType=typing.TypeAliasType\nelse:\n def _is_unionable(obj):\n  ''\n  return obj is None or isinstance(obj,(\n  type,\n  _types.GenericAlias,\n  _types.UnionType,\n  TypeAliasType,\n  ))\n  \n class TypeAliasType:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  def __init__(self,name:str,value,*,type_params=()):\n   if not isinstance(name,str):\n    raise TypeError(\"TypeAliasType name must be a string\")\n   self.__value__=value\n   self.__type_params__=type_params\n   \n   parameters=[]\n   for type_param in type_params:\n    if isinstance(type_param,TypeVarTuple):\n     parameters.extend(type_param)\n    else:\n     parameters.append(type_param)\n   self.__parameters__=tuple(parameters)\n   def_mod=_caller()\n   if def_mod !='typing_extensions':\n    self.__module__=def_mod\n    \n   self.__name__=name\n   \n  def __setattr__(self,name:str,value:object,/)->None:\n   if hasattr(self,\"__name__\"):\n    self._raise_attribute_error(name)\n   super().__setattr__(name,value)\n   \n  def __delattr__(self,name:str,/)->Never:\n   self._raise_attribute_error(name)\n   \n  def _raise_attribute_error(self,name:str)->Never:\n  \n   if name ==\"__name__\":\n    raise AttributeError(\"readonly attribute\")\n   elif name in{\"__value__\",\"__type_params__\",\"__parameters__\",\"__module__\"}:\n    raise AttributeError(\n    f\"attribute '{name}' of 'typing.TypeAliasType' objects \"\n    \"is not writable\"\n    )\n   else:\n    raise AttributeError(\n    f\"'typing.TypeAliasType' object has no attribute '{name}'\"\n    )\n    \n  def __repr__(self)->str:\n   return self.__name__\n   \n  def __getitem__(self,parameters):\n   if not isinstance(parameters,tuple):\n    parameters=(parameters,)\n   parameters=[\n   typing._type_check(\n   item,f'Subscripting {self.__name__} requires a type.'\n   )\n   for item in parameters\n   ]\n   return typing._GenericAlias(self,tuple(parameters))\n   \n  def __reduce__(self):\n   return self.__name__\n   \n  def __init_subclass__(cls,*args,**kwargs):\n   raise TypeError(\n   \"type 'typing_extensions.TypeAliasType' is not an acceptable base type\"\n   )\n   \n   \n   \n  def __call__(self):\n   raise TypeError(\"Type alias is not callable\")\n   \n  if sys.version_info >=(3,10):\n   def __or__(self,right):\n   \n   \n    if not _is_unionable(right):\n     return NotImplemented\n    return typing.Union[self,right]\n    \n   def __ror__(self,left):\n    if not _is_unionable(left):\n     return NotImplemented\n    return typing.Union[left,self]\n    \n    \nif hasattr(typing,\"is_protocol\"):\n is_protocol=typing.is_protocol\n get_protocol_members=typing.get_protocol_members\nelse:\n def is_protocol(tp:type,/)->bool:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  return(\n  isinstance(tp,type)\n  and getattr(tp,'_is_protocol',False)\n  and tp is not Protocol\n  and tp is not typing.Protocol\n  )\n  \n def get_protocol_members(tp:type,/)->typing.FrozenSet[str]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  if not is_protocol(tp):\n   raise TypeError(f'{tp !r} is not a Protocol')\n  if hasattr(tp,'__protocol_attrs__'):\n   return frozenset(tp.__protocol_attrs__)\n  return frozenset(_get_protocol_attrs(tp))\n  \n  \nif hasattr(typing,\"Doc\"):\n Doc=typing.Doc\nelse:\n class Doc:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  def __init__(self,documentation:str,/)->None:\n   self.documentation=documentation\n   \n  def __repr__(self)->str:\n   return f\"Doc({self.documentation !r})\"\n   \n  def __hash__(self)->int:\n   return hash(self.documentation)\n   \n  def __eq__(self,other:object)->bool:\n   if not isinstance(other,Doc):\n    return NotImplemented\n   return self.documentation ==other.documentation\n   \n   \n_CapsuleType=getattr(_types,\"CapsuleType\",None)\n\nif _CapsuleType is None:\n try:\n  import _socket\n except ImportError:\n  pass\n else:\n  _CAPI=getattr(_socket,\"CAPI\",None)\n  if _CAPI is not None:\n   _CapsuleType=type(_CAPI)\n   \nif _CapsuleType is not None:\n CapsuleType=_CapsuleType\n __all__.append(\"CapsuleType\")\n \n \n \n \n \n \nAbstractSet=typing.AbstractSet\nAnyStr=typing.AnyStr\nBinaryIO=typing.BinaryIO\nCallable=typing.Callable\nCollection=typing.Collection\nContainer=typing.Container\nDict=typing.Dict\nForwardRef=typing.ForwardRef\nFrozenSet=typing.FrozenSet\nGeneric=typing.Generic\nHashable=typing.Hashable\nIO=typing.IO\nItemsView=typing.ItemsView\nIterable=typing.Iterable\nIterator=typing.Iterator\nKeysView=typing.KeysView\nList=typing.List\nMapping=typing.Mapping\nMappingView=typing.MappingView\nMatch=typing.Match\nMutableMapping=typing.MutableMapping\nMutableSequence=typing.MutableSequence\nMutableSet=typing.MutableSet\nOptional=typing.Optional\nPattern=typing.Pattern\nReversible=typing.Reversible\nSequence=typing.Sequence\nSet=typing.Set\nSized=typing.Sized\nTextIO=typing.TextIO\nTuple=typing.Tuple\nUnion=typing.Union\nValuesView=typing.ValuesView\ncast=typing.cast\nno_type_check=typing.no_type_check\nno_type_check_decorator=typing.no_type_check_decorator\n", ["_socket", "abc", "collections", "collections.abc", "contextlib", "functools", "inspect", "operator", "sys", "types", "typing", "warnings"]], "distro": [".py", "from.distro import(\nNORMALIZED_DISTRO_ID,\nNORMALIZED_LSB_ID,\nNORMALIZED_OS_ID,\nLinuxDistribution,\n__version__,\nbuild_number,\ncodename,\ndistro_release_attr,\ndistro_release_info,\nid,\ninfo,\nlike,\nlinux_distribution,\nlsb_release_attr,\nlsb_release_info,\nmajor_version,\nminor_version,\nname,\nos_release_attr,\nos_release_info,\nuname_attr,\nuname_info,\nversion,\nversion_parts,\n)\n\n__all__=[\n\"NORMALIZED_DISTRO_ID\",\n\"NORMALIZED_LSB_ID\",\n\"NORMALIZED_OS_ID\",\n\"LinuxDistribution\",\n\"build_number\",\n\"codename\",\n\"distro_release_attr\",\n\"distro_release_info\",\n\"id\",\n\"info\",\n\"like\",\n\"linux_distribution\",\n\"lsb_release_attr\",\n\"lsb_release_info\",\n\"major_version\",\n\"minor_version\",\n\"name\",\n\"os_release_attr\",\n\"os_release_info\",\n\"uname_attr\",\n\"uname_info\",\n\"version\",\n\"version_parts\",\n]\n\n__version__=__version__\n", ["distro.distro"], 1], "distro.__main__": [".py", "from.distro import main\n\nif __name__ ==\"__main__\":\n main()\n", ["distro.distro"]], "distro.distro": [".py", "#!/usr/bin/env python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"\"\"\nThe ``distro`` package (``distro`` stands for Linux Distribution) provides\ninformation about the Linux distribution it runs on, such as a reliable\nmachine-readable distro ID, or version information.\n\nIt is the recommended replacement for Python's original\n:py:func:`platform.linux_distribution` function, but it provides much more\nfunctionality. An alternative implementation became necessary because Python\n3.5 deprecated this function, and Python 3.8 removed it altogether. Its\npredecessor function :py:func:`platform.dist` was already deprecated since\nPython 2.6 and removed in Python 3.8. Still, there are many cases in which\naccess to OS distribution information is needed. See `Python issue 1322\n<https://bugs.python.org/issue1322>`_ for more information.\n\"\"\"\n\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport shlex\nimport subprocess\nimport sys\nimport warnings\nfrom typing import(\nAny,\nCallable,\nDict,\nIterable,\nOptional,\nSequence,\nTextIO,\nTuple,\nType,\n)\n\ntry:\n from typing import TypedDict\nexcept ImportError:\n\n TypedDict=dict\n \n__version__=\"1.9.0\"\n\n\nclass VersionDict(TypedDict):\n major:str\n minor:str\n build_number:str\n \n \nclass InfoDict(TypedDict):\n id:str\n version:str\n version_parts:VersionDict\n like:str\n codename:str\n \n \n_UNIXCONFDIR=os.environ.get(\"UNIXCONFDIR\",\"/etc\")\n_UNIXUSRLIBDIR=os.environ.get(\"UNIXUSRLIBDIR\",\"/usr/lib\")\n_OS_RELEASE_BASENAME=\"os-release\"\n\n\n\n\n\n\n\n\nNORMALIZED_OS_ID={\n\"ol\":\"oracle\",\n\"opensuse-leap\":\"opensuse\",\n}\n\n\n\n\n\n\n\n\nNORMALIZED_LSB_ID={\n\"enterpriseenterpriseas\":\"oracle\",\n\"enterpriseenterpriseserver\":\"oracle\",\n\"redhatenterpriseworkstation\":\"rhel\",\n\"redhatenterpriseserver\":\"rhel\",\n\"redhatenterprisecomputenode\":\"rhel\",\n}\n\n\n\n\n\n\n\n\nNORMALIZED_DISTRO_ID={\n\"redhat\":\"rhel\",\n}\n\n\n_DISTRO_RELEASE_CONTENT_REVERSED_PATTERN=re.compile(\nr\"(?:[^)]*\\)(.*)\\()? *(?:STL )?([\\d.+\\-a-z]*\\d) *(?:esaeler *)?(.+)\"\n)\n\n\n_DISTRO_RELEASE_BASENAME_PATTERN=re.compile(r\"(\\w+)[-_](release|version)$\")\n\n\n_DISTRO_RELEASE_BASENAMES=[\n\"SuSE-release\",\n\"altlinux-release\",\n\"arch-release\",\n\"base-release\",\n\"centos-release\",\n\"fedora-release\",\n\"gentoo-release\",\n\"mageia-release\",\n\"mandrake-release\",\n\"mandriva-release\",\n\"mandrivalinux-release\",\n\"manjaro-release\",\n\"oracle-release\",\n\"redhat-release\",\n\"rocky-release\",\n\"sl-release\",\n\"slackware-version\",\n]\n\n\n_DISTRO_RELEASE_IGNORE_BASENAMES=(\n\"debian_version\",\n\"lsb-release\",\n\"oem-release\",\n_OS_RELEASE_BASENAME,\n\"system-release\",\n\"plesk-release\",\n\"iredmail-release\",\n\"board-release\",\n\"ec2_version\",\n)\n\n\ndef linux_distribution(full_distribution_name:bool=True)->Tuple[str,str,str]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n warnings.warn(\n \"distro.linux_distribution() is deprecated. It should only be used as a \"\n \"compatibility shim with Python's platform.linux_distribution(). Please use \"\n \"distro.id(), distro.version() and distro.name() instead.\",\n DeprecationWarning,\n stacklevel=2,\n )\n return _distro.linux_distribution(full_distribution_name)\n \n \ndef id()->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.id()\n \n \ndef name(pretty:bool=False)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.name(pretty)\n \n \ndef version(pretty:bool=False,best:bool=False)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.version(pretty,best)\n \n \ndef version_parts(best:bool=False)->Tuple[str,str,str]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.version_parts(best)\n \n \ndef major_version(best:bool=False)->str:\n ''\n\n\n\n\n\n\n\n \n return _distro.major_version(best)\n \n \ndef minor_version(best:bool=False)->str:\n ''\n\n\n\n\n\n\n\n \n return _distro.minor_version(best)\n \n \ndef build_number(best:bool=False)->str:\n ''\n\n\n\n\n\n\n\n \n return _distro.build_number(best)\n \n \ndef like()->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.like()\n \n \ndef codename()->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.codename()\n \n \ndef info(pretty:bool=False,best:bool=False)->InfoDict:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.info(pretty,best)\n \n \ndef os_release_info()->Dict[str,str]:\n ''\n\n\n\n\n \n return _distro.os_release_info()\n \n \ndef lsb_release_info()->Dict[str,str]:\n ''\n\n\n\n\n\n \n return _distro.lsb_release_info()\n \n \ndef distro_release_info()->Dict[str,str]:\n ''\n\n\n\n\n \n return _distro.distro_release_info()\n \n \ndef uname_info()->Dict[str,str]:\n ''\n\n\n \n return _distro.uname_info()\n \n \ndef os_release_attr(attribute:str)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.os_release_attr(attribute)\n \n \ndef lsb_release_attr(attribute:str)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.lsb_release_attr(attribute)\n \n \ndef distro_release_attr(attribute:str)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.distro_release_attr(attribute)\n \n \ndef uname_attr(attribute:str)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n \n return _distro.uname_attr(attribute)\n \n \ntry:\n from functools import cached_property\nexcept ImportError:\n\n class cached_property:\n  ''\n\n\n  \n  \n  def __init__(self,f:Callable[[Any],Any])->None:\n   self._fname=f.__name__\n   self._f=f\n   \n  def __get__(self,obj:Any,owner:Type[Any])->Any:\n   assert obj is not None,f\"call {self._fname} on an instance\"\n   ret=obj.__dict__[self._fname]=self._f(obj)\n   return ret\n   \n   \nclass LinuxDistribution:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n include_lsb:Optional[bool]=None,\n os_release_file:str=\"\",\n distro_release_file:str=\"\",\n include_uname:Optional[bool]=None,\n root_dir:Optional[str]=None,\n include_oslevel:Optional[bool]=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  self.root_dir=root_dir\n  self.etc_dir=os.path.join(root_dir,\"etc\")if root_dir else _UNIXCONFDIR\n  self.usr_lib_dir=(\n  os.path.join(root_dir,\"usr/lib\")if root_dir else _UNIXUSRLIBDIR\n  )\n  \n  if os_release_file:\n   self.os_release_file=os_release_file\n  else:\n   etc_dir_os_release_file=os.path.join(self.etc_dir,_OS_RELEASE_BASENAME)\n   usr_lib_os_release_file=os.path.join(\n   self.usr_lib_dir,_OS_RELEASE_BASENAME\n   )\n   \n   \n   \n   if os.path.isfile(etc_dir_os_release_file)or not os.path.isfile(\n   usr_lib_os_release_file\n   ):\n    self.os_release_file=etc_dir_os_release_file\n   else:\n    self.os_release_file=usr_lib_os_release_file\n    \n  self.distro_release_file=distro_release_file or \"\"\n  \n  is_root_dir_defined=root_dir is not None\n  if is_root_dir_defined and(include_lsb or include_uname or include_oslevel):\n   raise ValueError(\n   \"Including subprocess data sources from specific root_dir is disallowed\"\n   \" to prevent false information\"\n   )\n  self.include_lsb=(\n  include_lsb if include_lsb is not None else not is_root_dir_defined\n  )\n  self.include_uname=(\n  include_uname if include_uname is not None else not is_root_dir_defined\n  )\n  self.include_oslevel=(\n  include_oslevel if include_oslevel is not None else not is_root_dir_defined\n  )\n  \n def __repr__(self)->str:\n  ''\n  return(\n  \"LinuxDistribution(\"\n  \"os_release_file={self.os_release_file!r}, \"\n  \"distro_release_file={self.distro_release_file!r}, \"\n  \"include_lsb={self.include_lsb!r}, \"\n  \"include_uname={self.include_uname!r}, \"\n  \"include_oslevel={self.include_oslevel!r}, \"\n  \"root_dir={self.root_dir!r}, \"\n  \"_os_release_info={self._os_release_info!r}, \"\n  \"_lsb_release_info={self._lsb_release_info!r}, \"\n  \"_distro_release_info={self._distro_release_info!r}, \"\n  \"_uname_info={self._uname_info!r}, \"\n  \"_oslevel_info={self._oslevel_info!r})\".format(self=self)\n  )\n  \n def linux_distribution(\n self,full_distribution_name:bool=True\n )->Tuple[str,str,str]:\n  ''\n\n\n\n\n\n  \n  return(\n  self.name()if full_distribution_name else self.id(),\n  self.version(),\n  self._os_release_info.get(\"release_codename\")or self.codename(),\n  )\n  \n def id(self)->str:\n  ''\n\n\n  \n  \n  def normalize(distro_id:str,table:Dict[str,str])->str:\n   distro_id=distro_id.lower().replace(\" \",\"_\")\n   return table.get(distro_id,distro_id)\n   \n  distro_id=self.os_release_attr(\"id\")\n  if distro_id:\n   return normalize(distro_id,NORMALIZED_OS_ID)\n   \n  distro_id=self.lsb_release_attr(\"distributor_id\")\n  if distro_id:\n   return normalize(distro_id,NORMALIZED_LSB_ID)\n   \n  distro_id=self.distro_release_attr(\"id\")\n  if distro_id:\n   return normalize(distro_id,NORMALIZED_DISTRO_ID)\n   \n  distro_id=self.uname_attr(\"id\")\n  if distro_id:\n   return normalize(distro_id,NORMALIZED_DISTRO_ID)\n   \n  return \"\"\n  \n def name(self,pretty:bool=False)->str:\n  ''\n\n\n\n  \n  name=(\n  self.os_release_attr(\"name\")\n  or self.lsb_release_attr(\"distributor_id\")\n  or self.distro_release_attr(\"name\")\n  or self.uname_attr(\"name\")\n  )\n  if pretty:\n   name=self.os_release_attr(\"pretty_name\")or self.lsb_release_attr(\n   \"description\"\n   )\n   if not name:\n    name=self.distro_release_attr(\"name\")or self.uname_attr(\"name\")\n    version=self.version(pretty=True)\n    if version:\n     name=f\"{name} {version}\"\n  return name or \"\"\n  \n def version(self,pretty:bool=False,best:bool=False)->str:\n  ''\n\n\n\n  \n  versions=[\n  self.os_release_attr(\"version_id\"),\n  self.lsb_release_attr(\"release\"),\n  self.distro_release_attr(\"version_id\"),\n  self._parse_distro_release_content(self.os_release_attr(\"pretty_name\")).get(\n  \"version_id\",\"\"\n  ),\n  self._parse_distro_release_content(\n  self.lsb_release_attr(\"description\")\n  ).get(\"version_id\",\"\"),\n  self.uname_attr(\"release\"),\n  ]\n  if self.uname_attr(\"id\").startswith(\"aix\"):\n  \n   versions.insert(0,self.oslevel_info())\n  elif self.id()==\"debian\"or \"debian\"in self.like().split():\n  \n   versions.append(self._debian_version)\n  version=\"\"\n  if best:\n  \n  \n  \n  \n   for v in versions:\n    if v.count(\".\")>version.count(\".\")or version ==\"\":\n     version=v\n  else:\n   for v in versions:\n    if v !=\"\":\n     version=v\n     break\n  if pretty and version and self.codename():\n   version=f\"{version} ({self.codename()})\"\n  return version\n  \n def version_parts(self,best:bool=False)->Tuple[str,str,str]:\n  ''\n\n\n\n\n  \n  version_str=self.version(best=best)\n  if version_str:\n   version_regex=re.compile(r\"(\\d+)\\.?(\\d+)?\\.?(\\d+)?\")\n   matches=version_regex.match(version_str)\n   if matches:\n    major,minor,build_number=matches.groups()\n    return major,minor or \"\",build_number or \"\"\n  return \"\",\"\",\"\"\n  \n def major_version(self,best:bool=False)->str:\n  ''\n\n\n\n  \n  return self.version_parts(best)[0]\n  \n def minor_version(self,best:bool=False)->str:\n  ''\n\n\n\n  \n  return self.version_parts(best)[1]\n  \n def build_number(self,best:bool=False)->str:\n  ''\n\n\n\n  \n  return self.version_parts(best)[2]\n  \n def like(self)->str:\n  ''\n\n\n\n  \n  return self.os_release_attr(\"id_like\")or \"\"\n  \n def codename(self)->str:\n  ''\n\n\n\n  \n  try:\n  \n  \n   return self._os_release_info[\"codename\"]\n  except KeyError:\n   return(\n   self.lsb_release_attr(\"codename\")\n   or self.distro_release_attr(\"codename\")\n   or \"\"\n   )\n   \n def info(self,pretty:bool=False,best:bool=False)->InfoDict:\n  ''\n\n\n\n\n  \n  return InfoDict(\n  id=self.id(),\n  version=self.version(pretty,best),\n  version_parts=VersionDict(\n  major=self.major_version(best),\n  minor=self.minor_version(best),\n  build_number=self.build_number(best),\n  ),\n  like=self.like(),\n  codename=self.codename(),\n  )\n  \n def os_release_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n  \n  return self._os_release_info\n  \n def lsb_release_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n\n  \n  return self._lsb_release_info\n  \n def distro_release_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n\n  \n  return self._distro_release_info\n  \n def uname_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n  \n  return self._uname_info\n  \n def oslevel_info(self)->str:\n  ''\n\n  \n  return self._oslevel_info\n  \n def os_release_attr(self,attribute:str)->str:\n  ''\n\n\n\n\n  \n  return self._os_release_info.get(attribute,\"\")\n  \n def lsb_release_attr(self,attribute:str)->str:\n  ''\n\n\n\n\n  \n  return self._lsb_release_info.get(attribute,\"\")\n  \n def distro_release_attr(self,attribute:str)->str:\n  ''\n\n\n\n\n  \n  return self._distro_release_info.get(attribute,\"\")\n  \n def uname_attr(self,attribute:str)->str:\n  ''\n\n\n\n\n  \n  return self._uname_info.get(attribute,\"\")\n  \n @cached_property\n def _os_release_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n  \n  if os.path.isfile(self.os_release_file):\n   with open(self.os_release_file,encoding=\"utf-8\")as release_file:\n    return self._parse_os_release_content(release_file)\n  return{}\n  \n @staticmethod\n def _parse_os_release_content(lines:TextIO)->Dict[str,str]:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  props={}\n  lexer=shlex.shlex(lines,posix=True)\n  lexer.whitespace_split=True\n  \n  tokens=list(lexer)\n  for token in tokens:\n  \n  \n  \n  \n  \n  \n  \n   if \"=\"in token:\n    k,v=token.split(\"=\",1)\n    props[k.lower()]=v\n    \n  if \"version\"in props:\n  \n   match=re.search(r\"\\((\\D+)\\)|,\\s*(\\D+)\",props[\"version\"])\n   if match:\n    release_codename=match.group(1)or match.group(2)\n    props[\"codename\"]=props[\"release_codename\"]=release_codename\n    \n  if \"version_codename\"in props:\n  \n  \n  \n  \n   props[\"codename\"]=props[\"version_codename\"]\n  elif \"ubuntu_codename\"in props:\n  \n   props[\"codename\"]=props[\"ubuntu_codename\"]\n   \n  return props\n  \n @cached_property\n def _lsb_release_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n  \n  if not self.include_lsb:\n   return{}\n  try:\n   cmd=(\"lsb_release\",\"-a\")\n   stdout=subprocess.check_output(cmd,stderr=subprocess.DEVNULL)\n   \n  except(OSError,subprocess.CalledProcessError):\n   return{}\n  content=self._to_str(stdout).splitlines()\n  return self._parse_lsb_release_content(content)\n  \n @staticmethod\n def _parse_lsb_release_content(lines:Iterable[str])->Dict[str,str]:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  props={}\n  for line in lines:\n   kv=line.strip(\"\\n\").split(\":\",1)\n   if len(kv)!=2:\n   \n    continue\n   k,v=kv\n   props.update({k.replace(\" \",\"_\").lower():v.strip()})\n  return props\n  \n @cached_property\n def _uname_info(self)->Dict[str,str]:\n  if not self.include_uname:\n   return{}\n  try:\n   cmd=(\"uname\",\"-rs\")\n   stdout=subprocess.check_output(cmd,stderr=subprocess.DEVNULL)\n  except OSError:\n   return{}\n  content=self._to_str(stdout).splitlines()\n  return self._parse_uname_content(content)\n  \n @cached_property\n def _oslevel_info(self)->str:\n  if not self.include_oslevel:\n   return \"\"\n  try:\n   stdout=subprocess.check_output(\"oslevel\",stderr=subprocess.DEVNULL)\n  except(OSError,subprocess.CalledProcessError):\n   return \"\"\n  return self._to_str(stdout).strip()\n  \n @cached_property\n def _debian_version(self)->str:\n  try:\n   with open(\n   os.path.join(self.etc_dir,\"debian_version\"),encoding=\"ascii\"\n   )as fp:\n    return fp.readline().rstrip()\n  except FileNotFoundError:\n   return \"\"\n   \n @staticmethod\n def _parse_uname_content(lines:Sequence[str])->Dict[str,str]:\n  if not lines:\n   return{}\n  props={}\n  match=re.search(r\"^([^\\s]+)\\s+([\\d\\.]+)\",lines[0].strip())\n  if match:\n   name,version=match.groups()\n   \n   \n   \n   \n   if name ==\"Linux\":\n    return{}\n   props[\"id\"]=name.lower()\n   props[\"name\"]=name\n   props[\"release\"]=version\n  return props\n  \n @staticmethod\n def _to_str(bytestring:bytes)->str:\n  encoding=sys.getfilesystemencoding()\n  return bytestring.decode(encoding)\n  \n @cached_property\n def _distro_release_info(self)->Dict[str,str]:\n  ''\n\n\n\n\n  \n  if self.distro_release_file:\n  \n  \n   distro_info=self._parse_distro_release_file(self.distro_release_file)\n   basename=os.path.basename(self.distro_release_file)\n   \n   \n   \n   \n   match=_DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n  else:\n   try:\n    basenames=[\n    basename\n    for basename in os.listdir(self.etc_dir)\n    if basename not in _DISTRO_RELEASE_IGNORE_BASENAMES\n    and os.path.isfile(os.path.join(self.etc_dir,basename))\n    ]\n    \n    \n    \n    basenames.sort()\n   except OSError:\n   \n   \n   \n   \n    basenames=_DISTRO_RELEASE_BASENAMES\n   for basename in basenames:\n    match=_DISTRO_RELEASE_BASENAME_PATTERN.match(basename)\n    if match is None:\n     continue\n    filepath=os.path.join(self.etc_dir,basename)\n    distro_info=self._parse_distro_release_file(filepath)\n    \n    if \"name\"not in distro_info:\n     continue\n    self.distro_release_file=filepath\n    break\n   else:\n    return{}\n    \n  if match is not None:\n   distro_info[\"id\"]=match.group(1)\n   \n   \n  if \"cloudlinux\"in distro_info.get(\"name\",\"\").lower():\n   distro_info[\"id\"]=\"cloudlinux\"\n   \n  return distro_info\n  \n def _parse_distro_release_file(self,filepath:str)->Dict[str,str]:\n  ''\n\n\n\n\n\n\n\n\n  \n  try:\n   with open(filepath,encoding=\"utf-8\")as fp:\n   \n   \n    return self._parse_distro_release_content(fp.readline())\n  except OSError:\n  \n  \n  \n   return{}\n   \n @staticmethod\n def _parse_distro_release_content(line:str)->Dict[str,str]:\n  ''\n\n\n\n\n\n\n\n\n  \n  matches=_DISTRO_RELEASE_CONTENT_REVERSED_PATTERN.match(line.strip()[::-1])\n  distro_info={}\n  if matches:\n  \n   distro_info[\"name\"]=matches.group(3)[::-1]\n   if matches.group(2):\n    distro_info[\"version_id\"]=matches.group(2)[::-1]\n   if matches.group(1):\n    distro_info[\"codename\"]=matches.group(1)[::-1]\n  elif line:\n   distro_info[\"name\"]=line.strip()\n  return distro_info\n  \n  \n_distro=LinuxDistribution()\n\n\ndef main()->None:\n logger=logging.getLogger(__name__)\n logger.setLevel(logging.DEBUG)\n logger.addHandler(logging.StreamHandler(sys.stdout))\n \n parser=argparse.ArgumentParser(description=\"OS distro info tool\")\n parser.add_argument(\n \"--json\",\"-j\",help=\"Output in machine readable format\",action=\"store_true\"\n )\n \n parser.add_argument(\n \"--root-dir\",\n \"-r\",\n type=str,\n dest=\"root_dir\",\n help=\"Path to the root filesystem directory (defaults to /)\",\n )\n \n args=parser.parse_args()\n \n if args.root_dir:\n  dist=LinuxDistribution(\n  include_lsb=False,\n  include_uname=False,\n  include_oslevel=False,\n  root_dir=args.root_dir,\n  )\n else:\n  dist=_distro\n  \n if args.json:\n  logger.info(json.dumps(dist.info(),indent=4,sort_keys=True))\n else:\n  logger.info(\"Name: %s\",dist.name(pretty=True))\n  distribution_version=dist.version(pretty=True)\n  logger.info(\"Version: %s\",distribution_version)\n  distribution_codename=dist.codename()\n  logger.info(\"Codename: %s\",distribution_codename)\n  \n  \nif __name__ ==\"__main__\":\n main()\n", ["argparse", "functools", "json", "logging", "os", "re", "shlex", "subprocess", "sys", "typing", "warnings"]], "pydantic.main": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport operator\nimport sys\nimport types\nimport typing\nimport warnings\nfrom copy import copy,deepcopy\nfrom functools import cached_property\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nClassVar,\nDict,\nGenerator,\nLiteral,\nMapping,\nSet,\nTuple,\nTypeVar,\nUnion,\ncast,\noverload,\n)\n\nimport pydantic_core\nimport typing_extensions\nfrom pydantic_core import PydanticUndefined\nfrom typing_extensions import Self,TypeAlias,Unpack\n\nfrom._internal import(\n_config,\n_decorators,\n_fields,\n_forward_ref,\n_generics,\n_import_utils,\n_mock_val_ser,\n_model_construction,\n_namespace_utils,\n_repr,\n_typing_extra,\n_utils,\n)\nfrom._migration import getattr_migration\nfrom.aliases import AliasChoices,AliasPath\nfrom.annotated_handlers import GetCoreSchemaHandler,GetJsonSchemaHandler\nfrom.config import ConfigDict\nfrom.errors import PydanticUndefinedAnnotation,PydanticUserError\nfrom.json_schema import DEFAULT_REF_TEMPLATE,GenerateJsonSchema,JsonSchemaMode,JsonSchemaValue,model_json_schema\nfrom.plugin._schema_validator import PluggableSchemaValidator\nfrom.warnings import PydanticDeprecatedSince20\n\nif TYPE_CHECKING:\n from inspect import Signature\n from pathlib import Path\n \n from pydantic_core import CoreSchema,SchemaSerializer,SchemaValidator\n \n from._internal._namespace_utils import MappingNamespace\n from._internal._utils import AbstractSetIntStr,MappingIntStrAny\n from.deprecated.parse import Protocol as DeprecatedParseProtocol\n from.fields import ComputedFieldInfo,FieldInfo,ModelPrivateAttr\nelse:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n__all__='BaseModel','create_model'\n\n\nTupleGenerator:TypeAlias=Generator[Tuple[str,Any],None,None]\n\n\n\nIncEx:TypeAlias=Union[Set[int],Set[str],Mapping[int,Union['IncEx',bool]],Mapping[str,Union['IncEx',bool]]]\n\n_object_setattr=_model_construction.object_setattr\n\n\nclass BaseModel(metaclass=_model_construction.ModelMetaclass):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n model_config:ClassVar[ConfigDict]=ConfigDict()\n ''\n\n \n \n \n \n __class_vars__:ClassVar[set[str]]\n '' \n \n __private_attributes__:ClassVar[Dict[str,ModelPrivateAttr]]\n '' \n \n __signature__:ClassVar[Signature]\n '' \n \n __pydantic_complete__:ClassVar[bool]=False\n '' \n \n __pydantic_core_schema__:ClassVar[CoreSchema]\n '' \n \n __pydantic_custom_init__:ClassVar[bool]\n '' \n \n \n __pydantic_decorators__:ClassVar[_decorators.DecoratorInfos]=_decorators.DecoratorInfos()\n ''\n \n \n __pydantic_generic_metadata__:ClassVar[_generics.PydanticGenericMetadata]\n ''\n \n \n __pydantic_parent_namespace__:ClassVar[Dict[str,Any]|None]=None\n '' \n \n __pydantic_post_init__:ClassVar[None |Literal['model_post_init']]\n '' \n \n __pydantic_root_model__:ClassVar[bool]=False\n '' \n \n __pydantic_serializer__:ClassVar[SchemaSerializer]\n '' \n \n __pydantic_validator__:ClassVar[SchemaValidator |PluggableSchemaValidator]\n '' \n \n __pydantic_fields__:ClassVar[Dict[str,FieldInfo]]\n ''\n\n \n \n __pydantic_computed_fields__:ClassVar[Dict[str,ComputedFieldInfo]]\n '' \n \n __pydantic_extra__:dict[str,Any]|None=_model_construction.NoInitField(init=False)\n '' \n \n __pydantic_fields_set__:set[str]=_model_construction.NoInitField(init=False)\n '' \n \n __pydantic_private__:dict[str,Any]|None=_model_construction.NoInitField(init=False)\n '' \n \n if not TYPE_CHECKING:\n \n \n  __pydantic_core_schema__=_mock_val_ser.MockCoreSchema(\n  'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly',\n  code='base-model-instantiated',\n  )\n  __pydantic_validator__=_mock_val_ser.MockValSer(\n  'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly',\n  val_or_ser='validator',\n  code='base-model-instantiated',\n  )\n  __pydantic_serializer__=_mock_val_ser.MockValSer(\n  'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly',\n  val_or_ser='serializer',\n  code='base-model-instantiated',\n  )\n  \n __slots__='__dict__','__pydantic_fields_set__','__pydantic_extra__','__pydantic_private__'\n \n def __init__(self,/,**data:Any)->None:\n  ''\n\n\n\n\n\n  \n  \n  __tracebackhide__=True\n  validated_self=self.__pydantic_validator__.validate_python(data,self_instance=self)\n  if self is not validated_self:\n   warnings.warn(\n   'A custom validator is returning a value other than `self`.\\n'\n   \"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.\\n\"\n   'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n   stacklevel=2,\n   )\n   \n   \n __init__.__pydantic_base_init__=True\n \n if TYPE_CHECKING:\n  model_fields:ClassVar[dict[str,FieldInfo]]\n  model_computed_fields:ClassVar[dict[str,ComputedFieldInfo]]\n else:\n \n \n \n \n  @property\n  def model_fields(self)->dict[str,FieldInfo]:\n   ''\n\n\n\n\n\n\n   \n   \n   return getattr(self,'__pydantic_fields__',{})\n   \n  @property\n  def model_computed_fields(self)->dict[str,ComputedFieldInfo]:\n   ''\n\n\n\n\n\n\n   \n   \n   return getattr(self,'__pydantic_computed_fields__',{})\n   \n @property\n def model_extra(self)->dict[str,Any]|None:\n  ''\n\n\n\n  \n  return self.__pydantic_extra__\n  \n @property\n def model_fields_set(self)->set[str]:\n  ''\n\n\n\n\n  \n  return self.__pydantic_fields_set__\n  \n @classmethod\n def model_construct(cls,_fields_set:set[str]|None=None,**values:Any)->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  m=cls.__new__(cls)\n  fields_values:dict[str,Any]={}\n  fields_set=set()\n  \n  for name,field in cls.__pydantic_fields__.items():\n   if field.alias is not None and field.alias in values:\n    fields_values[name]=values.pop(field.alias)\n    fields_set.add(name)\n    \n   if(name not in fields_set)and(field.validation_alias is not None):\n    validation_aliases:list[str |AliasPath]=(\n    field.validation_alias.choices\n    if isinstance(field.validation_alias,AliasChoices)\n    else[field.validation_alias]\n    )\n    \n    for alias in validation_aliases:\n     if isinstance(alias,str)and alias in values:\n      fields_values[name]=values.pop(alias)\n      fields_set.add(name)\n      break\n     elif isinstance(alias,AliasPath):\n      value=alias.search_dict_for_path(values)\n      if value is not PydanticUndefined:\n       fields_values[name]=value\n       fields_set.add(name)\n       break\n       \n   if name not in fields_set:\n    if name in values:\n     fields_values[name]=values.pop(name)\n     fields_set.add(name)\n    elif not field.is_required():\n     fields_values[name]=field.get_default(call_default_factory=True,validated_data=fields_values)\n  if _fields_set is None:\n   _fields_set=fields_set\n   \n  _extra:dict[str,Any]|None=values if cls.model_config.get('extra')=='allow'else None\n  _object_setattr(m,'__dict__',fields_values)\n  _object_setattr(m,'__pydantic_fields_set__',_fields_set)\n  if not cls.__pydantic_root_model__:\n   _object_setattr(m,'__pydantic_extra__',_extra)\n   \n  if cls.__pydantic_post_init__:\n   m.model_post_init(None)\n   \n   if hasattr(m,'__pydantic_private__')and m.__pydantic_private__ is not None:\n    for k,v in values.items():\n     if k in m.__private_attributes__:\n      m.__pydantic_private__[k]=v\n      \n  elif not cls.__pydantic_root_model__:\n  \n  \n   _object_setattr(m,'__pydantic_private__',None)\n   \n  return m\n  \n def model_copy(self,*,update:Mapping[str,Any]|None=None,deep:bool=False)->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  copied=self.__deepcopy__()if deep else self.__copy__()\n  if update:\n   if self.model_config.get('extra')=='allow':\n    for k,v in update.items():\n     if k in self.__pydantic_fields__:\n      copied.__dict__[k]=v\n     else:\n      if copied.__pydantic_extra__ is None:\n       copied.__pydantic_extra__={}\n      copied.__pydantic_extra__[k]=v\n   else:\n    copied.__dict__.update(update)\n   copied.__pydantic_fields_set__.update(update.keys())\n  return copied\n  \n def model_dump(\n self,\n *,\n mode:Literal['json','python']|str='python',\n include:IncEx |None=None,\n exclude:IncEx |None=None,\n context:Any |None=None,\n by_alias:bool=False,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n round_trip:bool=False,\n warnings:bool |Literal['none','warn','error']=True,\n serialize_as_any:bool=False,\n )->dict[str,Any]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.__pydantic_serializer__.to_python(\n  self,\n  mode=mode,\n  by_alias=by_alias,\n  include=include,\n  exclude=exclude,\n  context=context,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  round_trip=round_trip,\n  warnings=warnings,\n  serialize_as_any=serialize_as_any,\n  )\n  \n def model_dump_json(\n self,\n *,\n indent:int |None=None,\n include:IncEx |None=None,\n exclude:IncEx |None=None,\n context:Any |None=None,\n by_alias:bool=False,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n round_trip:bool=False,\n warnings:bool |Literal['none','warn','error']=True,\n serialize_as_any:bool=False,\n )->str:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.__pydantic_serializer__.to_json(\n  self,\n  indent=indent,\n  include=include,\n  exclude=exclude,\n  context=context,\n  by_alias=by_alias,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  round_trip=round_trip,\n  warnings=warnings,\n  serialize_as_any=serialize_as_any,\n  ).decode()\n  \n @classmethod\n def model_json_schema(\n cls,\n by_alias:bool=True,\n ref_template:str=DEFAULT_REF_TEMPLATE,\n schema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\n mode:JsonSchemaMode='validation',\n )->dict[str,Any]:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return model_json_schema(\n  cls,by_alias=by_alias,ref_template=ref_template,schema_generator=schema_generator,mode=mode\n  )\n  \n @classmethod\n def model_parametrized_name(cls,params:tuple[type[Any],...])->str:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if not issubclass(cls,typing.Generic):\n   raise TypeError('Concrete names should only be generated for generic models.')\n   \n   \n   \n   \n  param_names=[param if isinstance(param,str)else _repr.display_as_type(param)for param in params]\n  params_component=', '.join(param_names)\n  return f'{cls.__name__}[{params_component}]'\n  \n def model_post_init(self,__context:Any)->None:\n  ''\n\n  \n  pass\n  \n @classmethod\n def model_rebuild(\n cls,\n *,\n force:bool=False,\n raise_errors:bool=True,\n _parent_namespace_depth:int=2,\n _types_namespace:MappingNamespace |None=None,\n )->bool |None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if not force and cls.__pydantic_complete__:\n   return None\n   \n  if '__pydantic_core_schema__'in cls.__dict__:\n   delattr(cls,'__pydantic_core_schema__')\n   \n  if _types_namespace is not None:\n   rebuild_ns=_types_namespace\n  elif _parent_namespace_depth >0:\n   rebuild_ns=_typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth,force=True)or{}\n  else:\n   rebuild_ns={}\n   \n  parent_ns=_model_construction.unpack_lenient_weakvaluedict(cls.__pydantic_parent_namespace__)or{}\n  \n  ns_resolver=_namespace_utils.NsResolver(\n  parent_namespace={**rebuild_ns,**parent_ns},\n  )\n  \n  \n  config={**cls.model_config,'defer_build':False}\n  return _model_construction.complete_model_class(\n  cls,\n  cls.__name__,\n  _config.ConfigWrapper(config,check=False),\n  raise_errors=raise_errors,\n  ns_resolver=ns_resolver,\n  )\n  \n @classmethod\n def model_validate(\n cls,\n obj:Any,\n *,\n strict:bool |None=None,\n from_attributes:bool |None=None,\n context:Any |None=None,\n )->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  __tracebackhide__=True\n  return cls.__pydantic_validator__.validate_python(\n  obj,strict=strict,from_attributes=from_attributes,context=context\n  )\n  \n @classmethod\n def model_validate_json(\n cls,\n json_data:str |bytes |bytearray,\n *,\n strict:bool |None=None,\n context:Any |None=None,\n )->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  __tracebackhide__=True\n  return cls.__pydantic_validator__.validate_json(json_data,strict=strict,context=context)\n  \n @classmethod\n def model_validate_strings(\n cls,\n obj:Any,\n *,\n strict:bool |None=None,\n context:Any |None=None,\n )->Self:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  __tracebackhide__=True\n  return cls.__pydantic_validator__.validate_strings(obj,strict=strict,context=context)\n  \n @classmethod\n def __get_pydantic_core_schema__(cls,source:type[BaseModel],handler:GetCoreSchemaHandler,/)->CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  \n  schema=cls.__dict__.get('__pydantic_core_schema__')\n  if schema is not None and not isinstance(schema,_mock_val_ser.MockCoreSchema):\n  \n  \n  \n   if not cls.__pydantic_generic_metadata__['origin']:\n    return cls.__pydantic_core_schema__\n    \n  return handler(source)\n  \n @classmethod\n def __get_pydantic_json_schema__(\n cls,\n core_schema:CoreSchema,\n handler:GetJsonSchemaHandler,\n /,\n )->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return handler(core_schema)\n  \n @classmethod\n def __pydantic_init_subclass__(cls,**kwargs:Any)->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  pass\n  \n def __class_getitem__(\n cls,typevar_values:type[Any]|tuple[type[Any],...]\n )->type[BaseModel]|_forward_ref.PydanticRecursiveRef:\n  cached=_generics.get_cached_generic_type_early(cls,typevar_values)\n  if cached is not None:\n   return cached\n   \n  if cls is BaseModel:\n   raise TypeError('Type parameters should be placed on typing.Generic, not BaseModel')\n  if not hasattr(cls,'__parameters__'):\n   raise TypeError(f'{cls} cannot be parametrized because it does not inherit from typing.Generic')\n  if not cls.__pydantic_generic_metadata__['parameters']and typing.Generic not in cls.__bases__:\n   raise TypeError(f'{cls} is not a generic class')\n   \n  if not isinstance(typevar_values,tuple):\n   typevar_values=(typevar_values,)\n  _generics.check_parameters_count(cls,typevar_values)\n  \n  \n  typevars_map:dict[TypeVar,type[Any]]=dict(\n  zip(cls.__pydantic_generic_metadata__['parameters'],typevar_values)\n  )\n  \n  if _utils.all_identical(typevars_map.keys(),typevars_map.values())and typevars_map:\n   submodel=cls\n   _generics.set_cached_generic_type(cls,typevar_values,submodel)\n  else:\n   parent_args=cls.__pydantic_generic_metadata__['args']\n   if not parent_args:\n    args=typevar_values\n   else:\n    args=tuple(_generics.replace_types(arg,typevars_map)for arg in parent_args)\n    \n   origin=cls.__pydantic_generic_metadata__['origin']or cls\n   model_name=origin.model_parametrized_name(args)\n   params=tuple(\n   {param:None for param in _generics.iter_contained_typevars(typevars_map.values())}\n   )\n   \n   with _generics.generic_recursion_self_type(origin,args)as maybe_self_type:\n   \n    cached=_generics.get_cached_generic_type_late(cls,typevar_values,origin,args)\n    if cached is not None:\n     return cached\n     \n    if maybe_self_type is not None:\n     return maybe_self_type\n     \n     \n    try:\n    \n    \n    \n    \n    \n     parent_ns=_typing_extra.parent_frame_namespace(parent_depth=2)or{}\n     origin.model_rebuild(_types_namespace=parent_ns)\n    except PydanticUndefinedAnnotation:\n    \n    \n     pass\n     \n    submodel=_generics.create_generic_submodel(model_name,origin,args,params)\n    \n    \n    _generics.set_cached_generic_type(cls,typevar_values,submodel,origin,args)\n    \n  return submodel\n  \n def __copy__(self)->Self:\n  ''\n  cls=type(self)\n  m=cls.__new__(cls)\n  _object_setattr(m,'__dict__',copy(self.__dict__))\n  _object_setattr(m,'__pydantic_extra__',copy(self.__pydantic_extra__))\n  _object_setattr(m,'__pydantic_fields_set__',copy(self.__pydantic_fields_set__))\n  \n  if not hasattr(self,'__pydantic_private__')or self.__pydantic_private__ is None:\n   _object_setattr(m,'__pydantic_private__',None)\n  else:\n   _object_setattr(\n   m,\n   '__pydantic_private__',\n   {k:v for k,v in self.__pydantic_private__.items()if v is not PydanticUndefined},\n   )\n   \n  return m\n  \n def __deepcopy__(self,memo:dict[int,Any]|None=None)->Self:\n  ''\n  cls=type(self)\n  m=cls.__new__(cls)\n  _object_setattr(m,'__dict__',deepcopy(self.__dict__,memo=memo))\n  _object_setattr(m,'__pydantic_extra__',deepcopy(self.__pydantic_extra__,memo=memo))\n  \n  \n  _object_setattr(m,'__pydantic_fields_set__',copy(self.__pydantic_fields_set__))\n  \n  if not hasattr(self,'__pydantic_private__')or self.__pydantic_private__ is None:\n   _object_setattr(m,'__pydantic_private__',None)\n  else:\n   _object_setattr(\n   m,\n   '__pydantic_private__',\n   deepcopy({k:v for k,v in self.__pydantic_private__.items()if v is not PydanticUndefined},memo=memo),\n   )\n   \n  return m\n  \n if not TYPE_CHECKING:\n \n \n \n  def __getattr__(self,item:str)->Any:\n   private_attributes=object.__getattribute__(self,'__private_attributes__')\n   if item in private_attributes:\n    attribute=private_attributes[item]\n    if hasattr(attribute,'__get__'):\n     return attribute.__get__(self,type(self))\n     \n    try:\n    \n     return self.__pydantic_private__[item]\n    except KeyError as exc:\n     raise AttributeError(f'{type(self).__name__ !r} object has no attribute {item !r}')from exc\n   else:\n   \n   \n    try:\n     pydantic_extra=object.__getattribute__(self,'__pydantic_extra__')\n    except AttributeError:\n     pydantic_extra=None\n     \n    if pydantic_extra:\n     try:\n      return pydantic_extra[item]\n     except KeyError as exc:\n      raise AttributeError(f'{type(self).__name__ !r} object has no attribute {item !r}')from exc\n    else:\n     if hasattr(self.__class__,item):\n      return super().__getattribute__(item)\n     else:\n     \n      raise AttributeError(f'{type(self).__name__ !r} object has no attribute {item !r}')\n      \n  def __setattr__(self,name:str,value:Any)->None:\n   if name in self.__class_vars__:\n    raise AttributeError(\n    f'{name !r} is a ClassVar of `{self.__class__.__name__}` and cannot be set on an instance. '\n    f'If you want to set a value on the class, use `{self.__class__.__name__}.{name} = value`.'\n    )\n   elif not _fields.is_valid_field_name(name):\n    if self.__pydantic_private__ is None or name not in self.__private_attributes__:\n     _object_setattr(self,name,value)\n    else:\n     attribute=self.__private_attributes__[name]\n     if hasattr(attribute,'__set__'):\n      attribute.__set__(self,value)\n     else:\n      self.__pydantic_private__[name]=value\n    return\n    \n   self._check_frozen(name,value)\n   \n   attr=getattr(self.__class__,name,None)\n   \n   \n   \n   \n   if isinstance(attr,property):\n    attr.__set__(self,value)\n   elif isinstance(attr,cached_property):\n    self.__dict__[name]=value\n   elif self.model_config.get('validate_assignment',None):\n    self.__pydantic_validator__.validate_assignment(self,name,value)\n   elif self.model_config.get('extra')!='allow'and name not in self.__pydantic_fields__:\n   \n    raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n   elif self.model_config.get('extra')=='allow'and name not in self.__pydantic_fields__:\n    if self.model_extra and name in self.model_extra:\n     self.__pydantic_extra__[name]=value\n    else:\n     try:\n      getattr(self,name)\n     except AttributeError:\n     \n      self.__pydantic_extra__[name]=value\n     else:\n     \n      _object_setattr(self,name,value)\n   else:\n    self.__dict__[name]=value\n    self.__pydantic_fields_set__.add(name)\n    \n  def __delattr__(self,item:str)->Any:\n   if item in self.__private_attributes__:\n    attribute=self.__private_attributes__[item]\n    if hasattr(attribute,'__delete__'):\n     attribute.__delete__(self)\n     return\n     \n    try:\n    \n     del self.__pydantic_private__[item]\n     return\n    except KeyError as exc:\n     raise AttributeError(f'{type(self).__name__ !r} object has no attribute {item !r}')from exc\n     \n   self._check_frozen(item,None)\n   \n   if item in self.__pydantic_fields__:\n    object.__delattr__(self,item)\n   elif self.__pydantic_extra__ is not None and item in self.__pydantic_extra__:\n    del self.__pydantic_extra__[item]\n   else:\n    try:\n     object.__delattr__(self,item)\n    except AttributeError:\n     raise AttributeError(f'{type(self).__name__ !r} object has no attribute {item !r}')\n     \n     \n     \n  def __replace__(self,**changes:Any)->Self:\n   return self.model_copy(update=changes)\n   \n def _check_frozen(self,name:str,value:Any)->None:\n  if self.model_config.get('frozen',None):\n   typ='frozen_instance'\n  elif getattr(self.__pydantic_fields__.get(name),'frozen',False):\n   typ='frozen_field'\n  else:\n   return\n  error:pydantic_core.InitErrorDetails={\n  'type':typ,\n  'loc':(name,),\n  'input':value,\n  }\n  raise pydantic_core.ValidationError.from_exception_data(self.__class__.__name__,[error])\n  \n def __getstate__(self)->dict[Any,Any]:\n  private=self.__pydantic_private__\n  if private:\n   private={k:v for k,v in private.items()if v is not PydanticUndefined}\n  return{\n  '__dict__':self.__dict__,\n  '__pydantic_extra__':self.__pydantic_extra__,\n  '__pydantic_fields_set__':self.__pydantic_fields_set__,\n  '__pydantic_private__':private,\n  }\n  \n def __setstate__(self,state:dict[Any,Any])->None:\n  _object_setattr(self,'__pydantic_fields_set__',state.get('__pydantic_fields_set__',{}))\n  _object_setattr(self,'__pydantic_extra__',state.get('__pydantic_extra__',{}))\n  _object_setattr(self,'__pydantic_private__',state.get('__pydantic_private__',{}))\n  _object_setattr(self,'__dict__',state.get('__dict__',{}))\n  \n if not TYPE_CHECKING:\n \n  def __eq__(self,other:Any)->bool:\n   if isinstance(other,BaseModel):\n   \n   \n   \n    self_type=self.__pydantic_generic_metadata__['origin']or self.__class__\n    other_type=other.__pydantic_generic_metadata__['origin']or other.__class__\n    \n    \n    if not(\n    self_type ==other_type\n    and getattr(self,'__pydantic_private__',None)==getattr(other,'__pydantic_private__',None)\n    and self.__pydantic_extra__ ==other.__pydantic_extra__\n    ):\n     return False\n     \n     \n     \n     \n     \n     \n    if self.__dict__ ==other.__dict__:\n    \n     return True\n     \n     \n     \n    model_fields=type(self).__pydantic_fields__.keys()\n    if self.__dict__.keys()<=model_fields and other.__dict__.keys()<=model_fields:\n     return False\n     \n     \n     \n     \n     \n     \n     \n     \n    getter=operator.itemgetter(*model_fields)if model_fields else lambda _:_utils._SENTINEL\n    try:\n     return getter(self.__dict__)==getter(other.__dict__)\n    except KeyError:\n    \n    \n    \n    \n    \n     self_fields_proxy=_utils.SafeGetItemProxy(self.__dict__)\n     other_fields_proxy=_utils.SafeGetItemProxy(other.__dict__)\n     return getter(self_fields_proxy)==getter(other_fields_proxy)\n     \n     \n   else:\n    return NotImplemented\n    \n if TYPE_CHECKING:\n \n \n \n \n  def __init_subclass__(cls,**kwargs:Unpack[ConfigDict]):\n   ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n   \n def __iter__(self)->TupleGenerator:\n  ''\n  yield from[(k,v)for(k,v)in self.__dict__.items()if not k.startswith('_')]\n  extra=self.__pydantic_extra__\n  if extra:\n   yield from extra.items()\n   \n def __repr__(self)->str:\n  return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n  \n def __repr_args__(self)->_repr.ReprArgs:\n  for k,v in self.__dict__.items():\n   field=self.__pydantic_fields__.get(k)\n   if field and field.repr:\n    if v is not self:\n     yield k,v\n    else:\n     yield k,self.__repr_recursion__(v)\n     \n     \n     \n     \n  try:\n   pydantic_extra=object.__getattribute__(self,'__pydantic_extra__')\n  except AttributeError:\n   pydantic_extra=None\n   \n  if pydantic_extra is not None:\n   yield from((k,v)for k,v in pydantic_extra.items())\n  yield from((k,getattr(self,k))for k,v in self.__pydantic_computed_fields__.items()if v.repr)\n  \n  \n __repr_name__=_repr.Representation.__repr_name__\n __repr_recursion__=_repr.Representation.__repr_recursion__\n __repr_str__=_repr.Representation.__repr_str__\n __pretty__=_repr.Representation.__pretty__\n __rich_repr__=_repr.Representation.__rich_repr__\n \n def __str__(self)->str:\n  return self.__repr_str__(' ')\n  \n  \n @property\n @typing_extensions.deprecated(\n 'The `__fields__` attribute is deprecated, use `model_fields` instead.',category=None\n )\n def __fields__(self)->dict[str,FieldInfo]:\n  warnings.warn(\n  'The `__fields__` attribute is deprecated, use `model_fields` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return self.model_fields\n  \n @property\n @typing_extensions.deprecated(\n 'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.',\n category=None,\n )\n def __fields_set__(self)->set[str]:\n  warnings.warn(\n  'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return self.__pydantic_fields_set__\n  \n @typing_extensions.deprecated('The `dict` method is deprecated; use `model_dump` instead.',category=None)\n def dict(\n self,\n *,\n include:IncEx |None=None,\n exclude:IncEx |None=None,\n by_alias:bool=False,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n )->Dict[str,Any]:\n  warnings.warn(\n  'The `dict` method is deprecated; use `model_dump` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return self.model_dump(\n  include=include,\n  exclude=exclude,\n  by_alias=by_alias,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  )\n  \n @typing_extensions.deprecated('The `json` method is deprecated; use `model_dump_json` instead.',category=None)\n def json(\n self,\n *,\n include:IncEx |None=None,\n exclude:IncEx |None=None,\n by_alias:bool=False,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n encoder:Callable[[Any],Any]|None=PydanticUndefined,\n models_as_dict:bool=PydanticUndefined,\n **dumps_kwargs:Any,\n )->str:\n  warnings.warn(\n  'The `json` method is deprecated; use `model_dump_json` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  if encoder is not PydanticUndefined:\n   raise TypeError('The `encoder` argument is no longer supported; use field serializers instead.')\n  if models_as_dict is not PydanticUndefined:\n   raise TypeError('The `models_as_dict` argument is no longer supported; use a model serializer instead.')\n  if dumps_kwargs:\n   raise TypeError('`dumps_kwargs` keyword arguments are no longer supported.')\n  return self.model_dump_json(\n  include=include,\n  exclude=exclude,\n  by_alias=by_alias,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  )\n  \n @classmethod\n @typing_extensions.deprecated('The `parse_obj` method is deprecated; use `model_validate` instead.',category=None)\n def parse_obj(cls,obj:Any)->Self:\n  warnings.warn(\n  'The `parse_obj` method is deprecated; use `model_validate` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return cls.model_validate(obj)\n  \n @classmethod\n @typing_extensions.deprecated(\n 'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '\n 'otherwise load the data then use `model_validate` instead.',\n category=None,\n )\n def parse_raw(\n cls,\n b:str |bytes,\n *,\n content_type:str |None=None,\n encoding:str='utf8',\n proto:DeprecatedParseProtocol |None=None,\n allow_pickle:bool=False,\n )->Self:\n  warnings.warn(\n  'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '\n  'otherwise load the data then use `model_validate` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import parse\n  \n  try:\n   obj=parse.load_str_bytes(\n   b,\n   proto=proto,\n   content_type=content_type,\n   encoding=encoding,\n   allow_pickle=allow_pickle,\n   )\n  except(ValueError,TypeError)as exc:\n   import json\n   \n   \n   if isinstance(exc,UnicodeDecodeError):\n    type_str='value_error.unicodedecode'\n   elif isinstance(exc,json.JSONDecodeError):\n    type_str='value_error.jsondecode'\n   elif isinstance(exc,ValueError):\n    type_str='value_error'\n   else:\n    type_str='type_error'\n    \n    \n   error:pydantic_core.InitErrorDetails={\n   \n   'type':pydantic_core.PydanticCustomError(type_str,str(exc)),\n   'loc':('__root__',),\n   'input':b,\n   }\n   raise pydantic_core.ValidationError.from_exception_data(cls.__name__,[error])\n  return cls.model_validate(obj)\n  \n @classmethod\n @typing_extensions.deprecated(\n 'The `parse_file` method is deprecated; load the data from file, then if your data is JSON '\n 'use `model_validate_json`, otherwise `model_validate` instead.',\n category=None,\n )\n def parse_file(\n cls,\n path:str |Path,\n *,\n content_type:str |None=None,\n encoding:str='utf8',\n proto:DeprecatedParseProtocol |None=None,\n allow_pickle:bool=False,\n )->Self:\n  warnings.warn(\n  'The `parse_file` method is deprecated; load the data from file, then if your data is JSON '\n  'use `model_validate_json`, otherwise `model_validate` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import parse\n  \n  obj=parse.load_file(\n  path,\n  proto=proto,\n  content_type=content_type,\n  encoding=encoding,\n  allow_pickle=allow_pickle,\n  )\n  return cls.parse_obj(obj)\n  \n @classmethod\n @typing_extensions.deprecated(\n 'The `from_orm` method is deprecated; set '\n \"`model_config['from_attributes']=True` and use `model_validate` instead.\",\n category=None,\n )\n def from_orm(cls,obj:Any)->Self:\n  warnings.warn(\n  'The `from_orm` method is deprecated; set '\n  \"`model_config['from_attributes']=True` and use `model_validate` instead.\",\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  if not cls.model_config.get('from_attributes',None):\n   raise PydanticUserError(\n   'You must set the config attribute `from_attributes=True` to use from_orm',code=None\n   )\n  return cls.model_validate(obj)\n  \n @classmethod\n @typing_extensions.deprecated('The `construct` method is deprecated; use `model_construct` instead.',category=None)\n def construct(cls,_fields_set:set[str]|None=None,**values:Any)->Self:\n  warnings.warn(\n  'The `construct` method is deprecated; use `model_construct` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return cls.model_construct(_fields_set=_fields_set,**values)\n  \n @typing_extensions.deprecated(\n 'The `copy` method is deprecated; use `model_copy` instead. '\n 'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.',\n category=None,\n )\n def copy(\n self,\n *,\n include:AbstractSetIntStr |MappingIntStrAny |None=None,\n exclude:AbstractSetIntStr |MappingIntStrAny |None=None,\n update:Dict[str,Any]|None=None,\n deep:bool=False,\n )->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  warnings.warn(\n  'The `copy` method is deprecated; use `model_copy` instead. '\n  'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import copy_internals\n  \n  values=dict(\n  copy_internals._iter(\n  self,to_dict=False,by_alias=False,include=include,exclude=exclude,exclude_unset=False\n  ),\n  **(update or{}),\n  )\n  if self.__pydantic_private__ is None:\n   private=None\n  else:\n   private={k:v for k,v in self.__pydantic_private__.items()if v is not PydanticUndefined}\n   \n  if self.__pydantic_extra__ is None:\n   extra:dict[str,Any]|None=None\n  else:\n   extra=self.__pydantic_extra__.copy()\n   for k in list(self.__pydantic_extra__):\n    if k not in values:\n     extra.pop(k)\n   for k in list(values):\n    if k in self.__pydantic_extra__:\n     extra[k]=values.pop(k)\n     \n     \n  if update:\n   fields_set=self.__pydantic_fields_set__ |update.keys()\n  else:\n   fields_set=set(self.__pydantic_fields_set__)\n   \n   \n  if exclude:\n   fields_set -=set(exclude)\n   \n  return copy_internals._copy_and_set_values(self,values,fields_set,extra,private,deep=deep)\n  \n @classmethod\n @typing_extensions.deprecated('The `schema` method is deprecated; use `model_json_schema` instead.',category=None)\n def schema(\n cls,by_alias:bool=True,ref_template:str=DEFAULT_REF_TEMPLATE\n )->Dict[str,Any]:\n  warnings.warn(\n  'The `schema` method is deprecated; use `model_json_schema` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return cls.model_json_schema(by_alias=by_alias,ref_template=ref_template)\n  \n @classmethod\n @typing_extensions.deprecated(\n 'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.',\n category=None,\n )\n def schema_json(\n cls,*,by_alias:bool=True,ref_template:str=DEFAULT_REF_TEMPLATE,**dumps_kwargs:Any\n )->str:\n  warnings.warn(\n  'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  import json\n  \n  from.deprecated.json import pydantic_encoder\n  \n  return json.dumps(\n  cls.model_json_schema(by_alias=by_alias,ref_template=ref_template),\n  default=pydantic_encoder,\n  **dumps_kwargs,\n  )\n  \n @classmethod\n @typing_extensions.deprecated('The `validate` method is deprecated; use `model_validate` instead.',category=None)\n def validate(cls,value:Any)->Self:\n  warnings.warn(\n  'The `validate` method is deprecated; use `model_validate` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return cls.model_validate(value)\n  \n @classmethod\n @typing_extensions.deprecated(\n 'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.',\n category=None,\n )\n def update_forward_refs(cls,**localns:Any)->None:\n  warnings.warn(\n  'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  if localns:\n   raise TypeError('`localns` arguments are not longer accepted.')\n  cls.model_rebuild(force=True)\n  \n @typing_extensions.deprecated(\n 'The private method `_iter` will be removed and should no longer be used.',category=None\n )\n def _iter(self,*args:Any,**kwargs:Any)->Any:\n  warnings.warn(\n  'The private method `_iter` will be removed and should no longer be used.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import copy_internals\n  \n  return copy_internals._iter(self,*args,**kwargs)\n  \n @typing_extensions.deprecated(\n 'The private method `_copy_and_set_values` will be removed and should no longer be used.',\n category=None,\n )\n def _copy_and_set_values(self,*args:Any,**kwargs:Any)->Any:\n  warnings.warn(\n  'The private method `_copy_and_set_values` will be removed and should no longer be used.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import copy_internals\n  \n  return copy_internals._copy_and_set_values(self,*args,**kwargs)\n  \n @classmethod\n @typing_extensions.deprecated(\n 'The private method `_get_value` will be removed and should no longer be used.',\n category=None,\n )\n def _get_value(cls,*args:Any,**kwargs:Any)->Any:\n  warnings.warn(\n  'The private method `_get_value` will be removed and should no longer be used.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import copy_internals\n  \n  return copy_internals._get_value(cls,*args,**kwargs)\n  \n @typing_extensions.deprecated(\n 'The private method `_calculate_keys` will be removed and should no longer be used.',\n category=None,\n )\n def _calculate_keys(self,*args:Any,**kwargs:Any)->Any:\n  warnings.warn(\n  'The private method `_calculate_keys` will be removed and should no longer be used.',\n  category=PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  from.deprecated import copy_internals\n  \n  return copy_internals._calculate_keys(self,*args,**kwargs)\n  \n  \nModelT=TypeVar('ModelT',bound=BaseModel)\n\n\n@overload\ndef create_model(\nmodel_name:str,\n/,\n*,\n__config__:ConfigDict |None=None,\n__doc__:str |None=None,\n__base__:None=None,\n__module__:str=__name__,\n__validators__:dict[str,Callable[...,Any]]|None=None,\n__cls_kwargs__:dict[str,Any]|None=None,\n**field_definitions:Any,\n)->type[BaseModel]:...\n\n\n@overload\ndef create_model(\nmodel_name:str,\n/,\n*,\n__config__:ConfigDict |None=None,\n__doc__:str |None=None,\n__base__:type[ModelT]|tuple[type[ModelT],...],\n__module__:str=__name__,\n__validators__:dict[str,Callable[...,Any]]|None=None,\n__cls_kwargs__:dict[str,Any]|None=None,\n**field_definitions:Any,\n)->type[ModelT]:...\n\n\ndef create_model(\nmodel_name:str,\n/,\n*,\n__config__:ConfigDict |None=None,\n__doc__:str |None=None,\n__base__:type[ModelT]|tuple[type[ModelT],...]|None=None,\n__module__:str |None=None,\n__validators__:dict[str,Callable[...,Any]]|None=None,\n__cls_kwargs__:dict[str,Any]|None=None,\n__slots__:tuple[str,...]|None=None,\n**field_definitions:Any,\n)->type[ModelT]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if __slots__ is not None:\n \n  warnings.warn('__slots__ should not be passed to create_model',RuntimeWarning)\n  \n if __base__ is not None:\n  if __config__ is not None:\n   raise PydanticUserError(\n   'to avoid confusion `__config__` and `__base__` cannot be used together',\n   code='create-model-config-base',\n   )\n  if not isinstance(__base__,tuple):\n   __base__=(__base__,)\n else:\n  __base__=(cast('type[ModelT]',BaseModel),)\n  \n __cls_kwargs__=__cls_kwargs__ or{}\n \n fields={}\n annotations={}\n \n for f_name,f_def in field_definitions.items():\n  if not _fields.is_valid_field_name(f_name):\n   warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"',RuntimeWarning)\n  if isinstance(f_def,tuple):\n   f_def=cast('tuple[str, Any]',f_def)\n   try:\n    f_annotation,f_value=f_def\n   except ValueError as e:\n    raise PydanticUserError(\n    'Field definitions should be a `(<type>, <default>)`.',\n    code='create-model-field-definitions',\n    )from e\n    \n  elif _typing_extra.is_annotated(f_def):\n   (f_annotation,f_value,*_)=typing_extensions.get_args(\n   f_def\n   )\n   FieldInfo=_import_utils.import_cached_field_info()\n   \n   if not isinstance(f_value,FieldInfo):\n    raise PydanticUserError(\n    'Field definitions should be a Annotated[<type>, <FieldInfo>]',\n    code='create-model-field-definitions',\n    )\n    \n  else:\n   f_annotation,f_value=None,f_def\n   \n  if f_annotation:\n   annotations[f_name]=f_annotation\n  fields[f_name]=f_value\n  \n if __module__ is None:\n  f=sys._getframe(1)\n  __module__=f.f_globals['__name__']\n  \n namespace:dict[str,Any]={'__annotations__':annotations,'__module__':__module__}\n if __doc__:\n  namespace.update({'__doc__':__doc__})\n if __validators__:\n  namespace.update(__validators__)\n namespace.update(fields)\n if __config__:\n  namespace['model_config']=_config.ConfigWrapper(__config__).config_dict\n resolved_bases=types.resolve_bases(__base__)\n meta,ns,kwds=types.prepare_class(model_name,resolved_bases,kwds=__cls_kwargs__)\n if resolved_bases is not __base__:\n  ns['__orig_bases__']=__base__\n namespace.update(ns)\n \n return meta(\n model_name,\n resolved_bases,\n namespace,\n __pydantic_reset_parent_namespace__=False,\n _create_model_module=__module__,\n **kwds,\n )\n \n \n__getattr__=getattr_migration(__name__)\n", ["__future__", "copy", "functools", "inspect", "json", "operator", "pathlib", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._decorators", "pydantic._internal._fields", "pydantic._internal._forward_ref", "pydantic._internal._generics", "pydantic._internal._import_utils", "pydantic._internal._mock_val_ser", "pydantic._internal._model_construction", "pydantic._internal._namespace_utils", "pydantic._internal._repr", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic._migration", "pydantic.aliases", "pydantic.annotated_handlers", "pydantic.config", "pydantic.deprecated", "pydantic.deprecated.copy_internals", "pydantic.deprecated.json", "pydantic.deprecated.parse", "pydantic.errors", "pydantic.fields", "pydantic.json_schema", "pydantic.plugin._schema_validator", "pydantic.warnings", "pydantic_core", "sys", "types", "typing", "typing_extensions", "warnings"]], "pydantic": [".py", "import typing\nfrom importlib import import_module\nfrom warnings import warn\n\nfrom._migration import getattr_migration\nfrom.version import VERSION\n\nif typing.TYPE_CHECKING:\n\n\n import pydantic_core\n from pydantic_core.core_schema import(\n FieldSerializationInfo,\n SerializationInfo,\n SerializerFunctionWrapHandler,\n ValidationInfo,\n ValidatorFunctionWrapHandler,\n )\n \n from. import dataclasses\n from.aliases import AliasChoices,AliasGenerator,AliasPath\n from.annotated_handlers import GetCoreSchemaHandler,GetJsonSchemaHandler\n from.config import ConfigDict,with_config\n from.errors import *\n from.fields import Field,PrivateAttr,computed_field\n from.functional_serializers import(\n PlainSerializer,\n SerializeAsAny,\n WrapSerializer,\n field_serializer,\n model_serializer,\n )\n from.functional_validators import(\n AfterValidator,\n BeforeValidator,\n InstanceOf,\n ModelWrapValidatorHandler,\n PlainValidator,\n SkipValidation,\n WrapValidator,\n field_validator,\n model_validator,\n )\n from.json_schema import WithJsonSchema\n from.main import *\n from.networks import *\n from.type_adapter import TypeAdapter\n from.types import *\n from.validate_call_decorator import validate_call\n from.warnings import(\n PydanticDeprecatedSince20,\n PydanticDeprecatedSince26,\n PydanticDeprecatedSince29,\n PydanticDeprecationWarning,\n PydanticExperimentalWarning,\n )\n \n \n ValidationError=pydantic_core.ValidationError\n from.deprecated.class_validators import root_validator,validator\n from.deprecated.config import BaseConfig,Extra\n from.deprecated.tools import *\n from.root_model import RootModel\n \n__version__=VERSION\n__all__=(\n\n'dataclasses',\n\n'field_validator',\n'model_validator',\n'AfterValidator',\n'BeforeValidator',\n'PlainValidator',\n'WrapValidator',\n'SkipValidation',\n'InstanceOf',\n'ModelWrapValidatorHandler',\n\n'WithJsonSchema',\n\n'root_validator',\n'validator',\n\n'field_serializer',\n'model_serializer',\n'PlainSerializer',\n'SerializeAsAny',\n'WrapSerializer',\n\n'ConfigDict',\n'with_config',\n\n'BaseConfig',\n'Extra',\n\n'validate_call',\n\n'PydanticErrorCodes',\n'PydanticUserError',\n'PydanticSchemaGenerationError',\n'PydanticImportError',\n'PydanticUndefinedAnnotation',\n'PydanticInvalidForJsonSchema',\n\n'Field',\n'computed_field',\n'PrivateAttr',\n\n'AliasChoices',\n'AliasGenerator',\n'AliasPath',\n\n'BaseModel',\n'create_model',\n\n'AnyUrl',\n'AnyHttpUrl',\n'FileUrl',\n'HttpUrl',\n'FtpUrl',\n'WebsocketUrl',\n'AnyWebsocketUrl',\n'UrlConstraints',\n'EmailStr',\n'NameEmail',\n'IPvAnyAddress',\n'IPvAnyInterface',\n'IPvAnyNetwork',\n'PostgresDsn',\n'CockroachDsn',\n'AmqpDsn',\n'RedisDsn',\n'MongoDsn',\n'KafkaDsn',\n'NatsDsn',\n'MySQLDsn',\n'MariaDBDsn',\n'ClickHouseDsn',\n'SnowflakeDsn',\n'validate_email',\n\n'RootModel',\n\n'parse_obj_as',\n'schema_of',\n'schema_json_of',\n\n'Strict',\n'StrictStr',\n'conbytes',\n'conlist',\n'conset',\n'confrozenset',\n'constr',\n'StringConstraints',\n'ImportString',\n'conint',\n'PositiveInt',\n'NegativeInt',\n'NonNegativeInt',\n'NonPositiveInt',\n'confloat',\n'PositiveFloat',\n'NegativeFloat',\n'NonNegativeFloat',\n'NonPositiveFloat',\n'FiniteFloat',\n'condecimal',\n'condate',\n'UUID1',\n'UUID3',\n'UUID4',\n'UUID5',\n'FilePath',\n'DirectoryPath',\n'NewPath',\n'Json',\n'Secret',\n'SecretStr',\n'SecretBytes',\n'SocketPath',\n'StrictBool',\n'StrictBytes',\n'StrictInt',\n'StrictFloat',\n'PaymentCardNumber',\n'ByteSize',\n'PastDate',\n'FutureDate',\n'PastDatetime',\n'FutureDatetime',\n'AwareDatetime',\n'NaiveDatetime',\n'AllowInfNan',\n'EncoderProtocol',\n'EncodedBytes',\n'EncodedStr',\n'Base64Encoder',\n'Base64Bytes',\n'Base64Str',\n'Base64UrlBytes',\n'Base64UrlStr',\n'GetPydanticSchema',\n'Tag',\n'Discriminator',\n'JsonValue',\n'FailFast',\n\n'TypeAdapter',\n\n'__version__',\n'VERSION',\n\n'PydanticDeprecatedSince20',\n'PydanticDeprecatedSince26',\n'PydanticDeprecatedSince29',\n'PydanticDeprecationWarning',\n'PydanticExperimentalWarning',\n\n'GetCoreSchemaHandler',\n'GetJsonSchemaHandler',\n\n'ValidationError',\n'ValidationInfo',\n'SerializationInfo',\n'ValidatorFunctionWrapHandler',\n'FieldSerializationInfo',\n'SerializerFunctionWrapHandler',\n'OnErrorOmit',\n)\n\n\n_dynamic_imports:'dict[str, tuple[str, str]]'={\n'dataclasses':(__spec__.parent,'__module__'),\n\n'field_validator':(__spec__.parent,'.functional_validators'),\n'model_validator':(__spec__.parent,'.functional_validators'),\n'AfterValidator':(__spec__.parent,'.functional_validators'),\n'BeforeValidator':(__spec__.parent,'.functional_validators'),\n'PlainValidator':(__spec__.parent,'.functional_validators'),\n'WrapValidator':(__spec__.parent,'.functional_validators'),\n'SkipValidation':(__spec__.parent,'.functional_validators'),\n'InstanceOf':(__spec__.parent,'.functional_validators'),\n'ModelWrapValidatorHandler':(__spec__.parent,'.functional_validators'),\n\n'WithJsonSchema':(__spec__.parent,'.json_schema'),\n\n'field_serializer':(__spec__.parent,'.functional_serializers'),\n'model_serializer':(__spec__.parent,'.functional_serializers'),\n'PlainSerializer':(__spec__.parent,'.functional_serializers'),\n'SerializeAsAny':(__spec__.parent,'.functional_serializers'),\n'WrapSerializer':(__spec__.parent,'.functional_serializers'),\n\n'ConfigDict':(__spec__.parent,'.config'),\n'with_config':(__spec__.parent,'.config'),\n\n'validate_call':(__spec__.parent,'.validate_call_decorator'),\n\n'PydanticErrorCodes':(__spec__.parent,'.errors'),\n'PydanticUserError':(__spec__.parent,'.errors'),\n'PydanticSchemaGenerationError':(__spec__.parent,'.errors'),\n'PydanticImportError':(__spec__.parent,'.errors'),\n'PydanticUndefinedAnnotation':(__spec__.parent,'.errors'),\n'PydanticInvalidForJsonSchema':(__spec__.parent,'.errors'),\n\n'Field':(__spec__.parent,'.fields'),\n'computed_field':(__spec__.parent,'.fields'),\n'PrivateAttr':(__spec__.parent,'.fields'),\n\n'AliasChoices':(__spec__.parent,'.aliases'),\n'AliasGenerator':(__spec__.parent,'.aliases'),\n'AliasPath':(__spec__.parent,'.aliases'),\n\n'BaseModel':(__spec__.parent,'.main'),\n'create_model':(__spec__.parent,'.main'),\n\n'AnyUrl':(__spec__.parent,'.networks'),\n'AnyHttpUrl':(__spec__.parent,'.networks'),\n'FileUrl':(__spec__.parent,'.networks'),\n'HttpUrl':(__spec__.parent,'.networks'),\n'FtpUrl':(__spec__.parent,'.networks'),\n'WebsocketUrl':(__spec__.parent,'.networks'),\n'AnyWebsocketUrl':(__spec__.parent,'.networks'),\n'UrlConstraints':(__spec__.parent,'.networks'),\n'EmailStr':(__spec__.parent,'.networks'),\n'NameEmail':(__spec__.parent,'.networks'),\n'IPvAnyAddress':(__spec__.parent,'.networks'),\n'IPvAnyInterface':(__spec__.parent,'.networks'),\n'IPvAnyNetwork':(__spec__.parent,'.networks'),\n'PostgresDsn':(__spec__.parent,'.networks'),\n'CockroachDsn':(__spec__.parent,'.networks'),\n'AmqpDsn':(__spec__.parent,'.networks'),\n'RedisDsn':(__spec__.parent,'.networks'),\n'MongoDsn':(__spec__.parent,'.networks'),\n'KafkaDsn':(__spec__.parent,'.networks'),\n'NatsDsn':(__spec__.parent,'.networks'),\n'MySQLDsn':(__spec__.parent,'.networks'),\n'MariaDBDsn':(__spec__.parent,'.networks'),\n'ClickHouseDsn':(__spec__.parent,'.networks'),\n'SnowflakeDsn':(__spec__.parent,'.networks'),\n'validate_email':(__spec__.parent,'.networks'),\n\n'RootModel':(__spec__.parent,'.root_model'),\n\n'Strict':(__spec__.parent,'.types'),\n'StrictStr':(__spec__.parent,'.types'),\n'conbytes':(__spec__.parent,'.types'),\n'conlist':(__spec__.parent,'.types'),\n'conset':(__spec__.parent,'.types'),\n'confrozenset':(__spec__.parent,'.types'),\n'constr':(__spec__.parent,'.types'),\n'StringConstraints':(__spec__.parent,'.types'),\n'ImportString':(__spec__.parent,'.types'),\n'conint':(__spec__.parent,'.types'),\n'PositiveInt':(__spec__.parent,'.types'),\n'NegativeInt':(__spec__.parent,'.types'),\n'NonNegativeInt':(__spec__.parent,'.types'),\n'NonPositiveInt':(__spec__.parent,'.types'),\n'confloat':(__spec__.parent,'.types'),\n'PositiveFloat':(__spec__.parent,'.types'),\n'NegativeFloat':(__spec__.parent,'.types'),\n'NonNegativeFloat':(__spec__.parent,'.types'),\n'NonPositiveFloat':(__spec__.parent,'.types'),\n'FiniteFloat':(__spec__.parent,'.types'),\n'condecimal':(__spec__.parent,'.types'),\n'condate':(__spec__.parent,'.types'),\n'UUID1':(__spec__.parent,'.types'),\n'UUID3':(__spec__.parent,'.types'),\n'UUID4':(__spec__.parent,'.types'),\n'UUID5':(__spec__.parent,'.types'),\n'FilePath':(__spec__.parent,'.types'),\n'DirectoryPath':(__spec__.parent,'.types'),\n'NewPath':(__spec__.parent,'.types'),\n'Json':(__spec__.parent,'.types'),\n'Secret':(__spec__.parent,'.types'),\n'SecretStr':(__spec__.parent,'.types'),\n'SecretBytes':(__spec__.parent,'.types'),\n'StrictBool':(__spec__.parent,'.types'),\n'StrictBytes':(__spec__.parent,'.types'),\n'StrictInt':(__spec__.parent,'.types'),\n'StrictFloat':(__spec__.parent,'.types'),\n'PaymentCardNumber':(__spec__.parent,'.types'),\n'ByteSize':(__spec__.parent,'.types'),\n'PastDate':(__spec__.parent,'.types'),\n'SocketPath':(__spec__.parent,'.types'),\n'FutureDate':(__spec__.parent,'.types'),\n'PastDatetime':(__spec__.parent,'.types'),\n'FutureDatetime':(__spec__.parent,'.types'),\n'AwareDatetime':(__spec__.parent,'.types'),\n'NaiveDatetime':(__spec__.parent,'.types'),\n'AllowInfNan':(__spec__.parent,'.types'),\n'EncoderProtocol':(__spec__.parent,'.types'),\n'EncodedBytes':(__spec__.parent,'.types'),\n'EncodedStr':(__spec__.parent,'.types'),\n'Base64Encoder':(__spec__.parent,'.types'),\n'Base64Bytes':(__spec__.parent,'.types'),\n'Base64Str':(__spec__.parent,'.types'),\n'Base64UrlBytes':(__spec__.parent,'.types'),\n'Base64UrlStr':(__spec__.parent,'.types'),\n'GetPydanticSchema':(__spec__.parent,'.types'),\n'Tag':(__spec__.parent,'.types'),\n'Discriminator':(__spec__.parent,'.types'),\n'JsonValue':(__spec__.parent,'.types'),\n'OnErrorOmit':(__spec__.parent,'.types'),\n'FailFast':(__spec__.parent,'.types'),\n\n'TypeAdapter':(__spec__.parent,'.type_adapter'),\n\n'PydanticDeprecatedSince20':(__spec__.parent,'.warnings'),\n'PydanticDeprecatedSince26':(__spec__.parent,'.warnings'),\n'PydanticDeprecatedSince29':(__spec__.parent,'.warnings'),\n'PydanticDeprecationWarning':(__spec__.parent,'.warnings'),\n'PydanticExperimentalWarning':(__spec__.parent,'.warnings'),\n\n'GetCoreSchemaHandler':(__spec__.parent,'.annotated_handlers'),\n'GetJsonSchemaHandler':(__spec__.parent,'.annotated_handlers'),\n\n'ValidationError':('pydantic_core','.'),\n'ValidationInfo':('pydantic_core','.core_schema'),\n'SerializationInfo':('pydantic_core','.core_schema'),\n'ValidatorFunctionWrapHandler':('pydantic_core','.core_schema'),\n'FieldSerializationInfo':('pydantic_core','.core_schema'),\n'SerializerFunctionWrapHandler':('pydantic_core','.core_schema'),\n\n'root_validator':(__spec__.parent,'.deprecated.class_validators'),\n'validator':(__spec__.parent,'.deprecated.class_validators'),\n'BaseConfig':(__spec__.parent,'.deprecated.config'),\n'Extra':(__spec__.parent,'.deprecated.config'),\n'parse_obj_as':(__spec__.parent,'.deprecated.tools'),\n'schema_of':(__spec__.parent,'.deprecated.tools'),\n'schema_json_of':(__spec__.parent,'.deprecated.tools'),\n\n'FieldValidationInfo':('pydantic_core','.core_schema'),\n'GenerateSchema':(__spec__.parent,'._internal._generate_schema'),\n}\n_deprecated_dynamic_imports={'FieldValidationInfo','GenerateSchema'}\n\n_getattr_migration=getattr_migration(__name__)\n\n\ndef __getattr__(attr_name:str)->object:\n if attr_name in _deprecated_dynamic_imports:\n  warn(\n  f'Importing {attr_name} from `pydantic` is deprecated. This feature is either no longer supported, or is not public.',\n  DeprecationWarning,\n  stacklevel=2,\n  )\n  \n dynamic_attr=_dynamic_imports.get(attr_name)\n if dynamic_attr is None:\n  return _getattr_migration(attr_name)\n  \n package,module_name=dynamic_attr\n \n if module_name =='__module__':\n  result=import_module(f'.{attr_name}',package=package)\n  globals()[attr_name]=result\n  return result\n else:\n  module=import_module(module_name,package=package)\n  result=getattr(module,attr_name)\n  g=globals()\n  for k,(_,v_module_name)in _dynamic_imports.items():\n   if v_module_name ==module_name and k not in _deprecated_dynamic_imports:\n    g[k]=getattr(module,k)\n  return result\n  \n  \ndef __dir__()->'list[str]':\n return list(__all__)\n", ["importlib", "pydantic", "pydantic._migration", "pydantic.aliases", "pydantic.annotated_handlers", "pydantic.config", "pydantic.dataclasses", "pydantic.deprecated.class_validators", "pydantic.deprecated.config", "pydantic.deprecated.tools", "pydantic.errors", "pydantic.fields", "pydantic.functional_serializers", "pydantic.functional_validators", "pydantic.json_schema", "pydantic.main", "pydantic.networks", "pydantic.root_model", "pydantic.type_adapter", "pydantic.types", "pydantic.validate_call_decorator", "pydantic.version", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "typing", "warnings"], 1], "pydantic.version": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\n__all__='VERSION','version_info'\n\nVERSION='2.10.4'\n''\n\n\ndef version_short()->str:\n ''\n\n\n \n return '.'.join(VERSION.split('.')[:2])\n \n \ndef version_info()->str:\n ''\n import importlib.metadata as importlib_metadata\n import os\n import platform\n import sys\n from pathlib import Path\n \n import pydantic_core._pydantic_core as pdc\n \n from._internal import _git as git\n \n \n package_names={\n 'email-validator',\n 'fastapi',\n 'mypy',\n 'pydantic-extra-types',\n 'pydantic-settings',\n 'pyright',\n 'typing_extensions',\n }\n related_packages=[]\n \n for dist in importlib_metadata.distributions():\n  name=dist.metadata['Name']\n  if name in package_names:\n   related_packages.append(f'{name}-{dist.version}')\n   \n pydantic_dir=os.path.abspath(os.path.dirname(os.path.dirname(__file__)))\n most_recent_commit=(\n git.git_revision(pydantic_dir)if git.is_git_repo(pydantic_dir)and git.have_git()else 'unknown'\n )\n \n info={\n 'pydantic version':VERSION,\n 'pydantic-core version':pdc.__version__,\n 'pydantic-core build':getattr(pdc,'build_info',None)or pdc.build_profile,\n 'install path':Path(__file__).resolve().parent,\n 'python version':sys.version,\n 'platform':platform.platform(),\n 'related packages':' '.join(related_packages),\n 'commit':most_recent_commit,\n }\n return '\\n'.join('{:>30} {}'.format(k+':',str(v).replace('\\n',' '))for k,v in info.items())\n \n \ndef parse_mypy_version(version:str)->tuple[int,int,int]:\n ''\n\n\n\n\n\n\n\n\n\n \n return tuple(map(int,version.partition('+')[0].split('.')))\n", ["__future__", "importlib.metadata", "os", "pathlib", "platform", "pydantic._internal", "pydantic._internal._git", "pydantic_core._pydantic_core", "sys"]], "pydantic.parse": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.generics": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.env_settings": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.decorator": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.class_validators": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.json": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.validate_call_decorator": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nfrom functools import partial\nfrom types import BuiltinFunctionType\nfrom typing import TYPE_CHECKING,Any,Callable,TypeVar,cast,overload\n\nfrom._internal import _generate_schema,_typing_extra,_validate_call\nfrom.errors import PydanticUserError\n\n__all__=('validate_call',)\n\nif TYPE_CHECKING:\n from.config import ConfigDict\n \n AnyCallableT=TypeVar('AnyCallableT',bound=Callable[...,Any])\n \n \n_INVALID_TYPE_ERROR_CODE='validate-call-type'\n\n\ndef _check_function_type(function:object)->None:\n ''\n if isinstance(function,_generate_schema.VALIDATE_CALL_SUPPORTED_TYPES):\n  try:\n   inspect.signature(cast(_generate_schema.ValidateCallSupportedTypes,function))\n  except ValueError:\n   raise PydanticUserError(\n   f\"Input function `{function}` doesn't have a valid signature\",code=_INVALID_TYPE_ERROR_CODE\n   )\n   \n  if isinstance(function,partial):\n   try:\n    assert not isinstance(partial.func,partial),'Partial of partial'\n    _check_function_type(function.func)\n   except PydanticUserError as e:\n    raise PydanticUserError(\n    f'Partial of `{function.func}` is invalid because the type of `{function.func}` is not supported by `validate_call`',\n    code=_INVALID_TYPE_ERROR_CODE,\n    )from e\n    \n  return\n  \n if isinstance(function,BuiltinFunctionType):\n  raise PydanticUserError(f'Input built-in function `{function}` is not supported',code=_INVALID_TYPE_ERROR_CODE)\n if isinstance(function,(classmethod,staticmethod,property)):\n  name=type(function).__name__\n  raise PydanticUserError(\n  f'The `@{name}` decorator should be applied after `@validate_call` (put `@{name}` on top)',\n  code=_INVALID_TYPE_ERROR_CODE,\n  )\n  \n if inspect.isclass(function):\n  raise PydanticUserError(\n  f'Unable to validate {function}: `validate_call` should be applied to functions, not classes (put `@validate_call` on top of `__init__` or `__new__` instead)',\n  code=_INVALID_TYPE_ERROR_CODE,\n  )\n if callable(function):\n  raise PydanticUserError(\n  f'Unable to validate {function}: `validate_call` should be applied to functions, not instances or other callables. Use `validate_call` explicitly on `__call__` instead.',\n  code=_INVALID_TYPE_ERROR_CODE,\n  )\n  \n raise PydanticUserError(\n f'Unable to validate {function}: `validate_call` should be applied to one of the following: function, method, partial, or lambda',\n code=_INVALID_TYPE_ERROR_CODE,\n )\n \n \n@overload\ndef validate_call(\n*,config:ConfigDict |None=None,validate_return:bool=False\n)->Callable[[AnyCallableT],AnyCallableT]:...\n\n\n@overload\ndef validate_call(func:AnyCallableT,/)->AnyCallableT:...\n\n\ndef validate_call(\nfunc:AnyCallableT |None=None,\n/,\n*,\nconfig:ConfigDict |None=None,\nvalidate_return:bool=False,\n)->AnyCallableT |Callable[[AnyCallableT],AnyCallableT]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n parent_namespace=_typing_extra.parent_frame_namespace()\n \n def validate(function:AnyCallableT)->AnyCallableT:\n  _check_function_type(function)\n  validate_call_wrapper=_validate_call.ValidateCallWrapper(\n  cast(_generate_schema.ValidateCallSupportedTypes,function),config,validate_return,parent_namespace\n  )\n  return _validate_call.update_wrapper_attributes(function,validate_call_wrapper.__call__)\n  \n if func is not None:\n  return validate(func)\n else:\n  return validate\n", ["__future__", "functools", "inspect", "pydantic._internal", "pydantic._internal._generate_schema", "pydantic._internal._typing_extra", "pydantic._internal._validate_call", "pydantic.config", "pydantic.errors", "types", "typing"]], "pydantic.types": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport base64\nimport dataclasses as _dataclasses\nimport re\nfrom datetime import date,datetime\nfrom decimal import Decimal\nfrom enum import Enum\nfrom pathlib import Path\nfrom types import ModuleType\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nClassVar,\nDict,\nFrozenSet,\nGeneric,\nHashable,\nIterator,\nList,\nPattern,\nSet,\nTypeVar,\nUnion,\ncast,\nget_args,\nget_origin,\n)\nfrom uuid import UUID\n\nimport annotated_types\nfrom annotated_types import BaseMetadata,MaxLen,MinLen\nfrom pydantic_core import CoreSchema,PydanticCustomError,SchemaSerializer,core_schema\nfrom typing_extensions import Annotated,Literal,Protocol,TypeAlias,TypeAliasType,deprecated\n\nfrom._internal import _core_utils,_fields,_internal_dataclass,_typing_extra,_utils,_validators\nfrom._migration import getattr_migration\nfrom.annotated_handlers import GetCoreSchemaHandler,GetJsonSchemaHandler\nfrom.errors import PydanticUserError\nfrom.json_schema import JsonSchemaValue\nfrom.warnings import PydanticDeprecatedSince20\n\n__all__=(\n'Strict',\n'StrictStr',\n'SocketPath',\n'conbytes',\n'conlist',\n'conset',\n'confrozenset',\n'constr',\n'ImportString',\n'conint',\n'PositiveInt',\n'NegativeInt',\n'NonNegativeInt',\n'NonPositiveInt',\n'confloat',\n'PositiveFloat',\n'NegativeFloat',\n'NonNegativeFloat',\n'NonPositiveFloat',\n'FiniteFloat',\n'condecimal',\n'UUID1',\n'UUID3',\n'UUID4',\n'UUID5',\n'FilePath',\n'DirectoryPath',\n'NewPath',\n'Json',\n'Secret',\n'SecretStr',\n'SecretBytes',\n'StrictBool',\n'StrictBytes',\n'StrictInt',\n'StrictFloat',\n'PaymentCardNumber',\n'ByteSize',\n'PastDate',\n'FutureDate',\n'PastDatetime',\n'FutureDatetime',\n'condate',\n'AwareDatetime',\n'NaiveDatetime',\n'AllowInfNan',\n'EncoderProtocol',\n'EncodedBytes',\n'EncodedStr',\n'Base64Encoder',\n'Base64Bytes',\n'Base64Str',\n'Base64UrlBytes',\n'Base64UrlStr',\n'GetPydanticSchema',\n'StringConstraints',\n'Tag',\n'Discriminator',\n'JsonValue',\n'OnErrorOmit',\n'FailFast',\n)\n\n\nT=TypeVar('T')\n\n\n@_dataclasses.dataclass\nclass Strict(_fields.PydanticMetadata,BaseMetadata):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n strict:bool=True\n \n def __hash__(self)->int:\n  return hash(self.strict)\n  \n  \n  \n  \nStrictBool=Annotated[bool,Strict()]\n''\n\n\n\n\ndef conint(\n*,\nstrict:bool |None=None,\ngt:int |None=None,\nge:int |None=None,\nlt:int |None=None,\nle:int |None=None,\nmultiple_of:int |None=None,\n)->type[int]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return Annotated[\n int,\n Strict(strict)if strict is not None else None,\n annotated_types.Interval(gt=gt,ge=ge,lt=lt,le=le),\n annotated_types.MultipleOf(multiple_of)if multiple_of is not None else None,\n ]\n \n \nPositiveInt=Annotated[int,annotated_types.Gt(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegativeInt=Annotated[int,annotated_types.Lt(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonPositiveInt=Annotated[int,annotated_types.Le(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonNegativeInt=Annotated[int,annotated_types.Ge(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrictInt=Annotated[int,Strict()]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@_dataclasses.dataclass\nclass AllowInfNan(_fields.PydanticMetadata):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n allow_inf_nan:bool=True\n \n def __hash__(self)->int:\n  return hash(self.allow_inf_nan)\n  \n  \ndef confloat(\n*,\nstrict:bool |None=None,\ngt:float |None=None,\nge:float |None=None,\nlt:float |None=None,\nle:float |None=None,\nmultiple_of:float |None=None,\nallow_inf_nan:bool |None=None,\n)->type[float]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return Annotated[\n float,\n Strict(strict)if strict is not None else None,\n annotated_types.Interval(gt=gt,ge=ge,lt=lt,le=le),\n annotated_types.MultipleOf(multiple_of)if multiple_of is not None else None,\n AllowInfNan(allow_inf_nan)if allow_inf_nan is not None else None,\n ]\n \n \nPositiveFloat=Annotated[float,annotated_types.Gt(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegativeFloat=Annotated[float,annotated_types.Lt(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonPositiveFloat=Annotated[float,annotated_types.Le(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonNegativeFloat=Annotated[float,annotated_types.Ge(0)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrictFloat=Annotated[float,Strict(True)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFiniteFloat=Annotated[float,AllowInfNan(False)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef conbytes(\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nstrict:bool |None=None,\n)->type[bytes]:\n ''\n\n\n\n\n\n\n\n\n \n return Annotated[\n bytes,\n Strict(strict)if strict is not None else None,\n annotated_types.Len(min_length or 0,max_length),\n ]\n \n \nStrictBytes=Annotated[bytes,Strict()]\n''\n\n\n\n\n\n@_dataclasses.dataclass(frozen=True)\nclass StringConstraints(annotated_types.GroupedMetadata):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n strip_whitespace:bool |None=None\n to_upper:bool |None=None\n to_lower:bool |None=None\n strict:bool |None=None\n min_length:int |None=None\n max_length:int |None=None\n pattern:str |Pattern[str]|None=None\n \n def __iter__(self)->Iterator[BaseMetadata]:\n  if self.min_length is not None:\n   yield MinLen(self.min_length)\n  if self.max_length is not None:\n   yield MaxLen(self.max_length)\n  if self.strict is not None:\n   yield Strict(self.strict)\n  if(\n  self.strip_whitespace is not None\n  or self.pattern is not None\n  or self.to_lower is not None\n  or self.to_upper is not None\n  ):\n   yield _fields.pydantic_general_metadata(\n   strip_whitespace=self.strip_whitespace,\n   to_upper=self.to_upper,\n   to_lower=self.to_lower,\n   pattern=self.pattern,\n   )\n   \n   \ndef constr(\n*,\nstrip_whitespace:bool |None=None,\nto_upper:bool |None=None,\nto_lower:bool |None=None,\nstrict:bool |None=None,\nmin_length:int |None=None,\nmax_length:int |None=None,\npattern:str |Pattern[str]|None=None,\n)->type[str]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return Annotated[\n str,\n StringConstraints(\n strip_whitespace=strip_whitespace,\n to_upper=to_upper,\n to_lower=to_lower,\n strict=strict,\n min_length=min_length,\n max_length=max_length,\n pattern=pattern,\n ),\n ]\n \n \nStrictStr=Annotated[str,Strict()]\n''\n\n\n\nHashableItemType=TypeVar('HashableItemType',bound=Hashable)\n\n\ndef conset(\nitem_type:type[HashableItemType],*,min_length:int |None=None,max_length:int |None=None\n)->type[set[HashableItemType]]:\n ''\n\n\n\n\n\n\n\n\n \n return Annotated[Set[item_type],annotated_types.Len(min_length or 0,max_length)]\n \n \ndef confrozenset(\nitem_type:type[HashableItemType],*,min_length:int |None=None,max_length:int |None=None\n)->type[frozenset[HashableItemType]]:\n ''\n\n\n\n\n\n\n\n\n \n return Annotated[FrozenSet[item_type],annotated_types.Len(min_length or 0,max_length)]\n \n \nAnyItemType=TypeVar('AnyItemType')\n\n\ndef conlist(\nitem_type:type[AnyItemType],\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nunique_items:bool |None=None,\n)->type[list[AnyItemType]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n if unique_items is not None:\n  raise PydanticUserError(\n  (\n  '`unique_items` is removed, use `Set` instead'\n  '(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'\n  ),\n  code='removed-kwargs',\n  )\n return Annotated[List[item_type],annotated_types.Len(min_length or 0,max_length)]\n \n \n \n \nAnyType=TypeVar('AnyType')\nif TYPE_CHECKING:\n ImportString=Annotated[AnyType,...]\nelse:\n\n class ImportString:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  @classmethod\n  def __class_getitem__(cls,item:AnyType)->AnyType:\n   return Annotated[item,cls()]\n   \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   serializer=core_schema.plain_serializer_function_ser_schema(cls._serialize,when_used='json')\n   if cls is source:\n   \n    return core_schema.no_info_plain_validator_function(\n    function=_validators.import_string,serialization=serializer\n    )\n   else:\n    return core_schema.no_info_before_validator_function(\n    function=_validators.import_string,schema=handler(source),serialization=serializer\n    )\n    \n  @classmethod\n  def __get_pydantic_json_schema__(cls,cs:CoreSchema,handler:GetJsonSchemaHandler)->JsonSchemaValue:\n   return handler(core_schema.str_schema())\n   \n  @staticmethod\n  def _serialize(v:Any)->str:\n   if isinstance(v,ModuleType):\n    return v.__name__\n   elif hasattr(v,'__module__')and hasattr(v,'__name__'):\n    return f'{v.__module__}.{v.__name__}'\n    \n    \n   elif hasattr(v,'name'):\n    if v.name =='<stdout>':\n     return 'sys.stdout'\n    elif v.name =='<stdin>':\n     return 'sys.stdin'\n    elif v.name =='<stderr>':\n     return 'sys.stderr'\n   else:\n    return v\n    \n  def __repr__(self)->str:\n   return 'ImportString'\n   \n   \n   \n   \n   \ndef condecimal(\n*,\nstrict:bool |None=None,\ngt:int |Decimal |None=None,\nge:int |Decimal |None=None,\nlt:int |Decimal |None=None,\nle:int |Decimal |None=None,\nmultiple_of:int |Decimal |None=None,\nmax_digits:int |None=None,\ndecimal_places:int |None=None,\nallow_inf_nan:bool |None=None,\n)->type[Decimal]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return Annotated[\n Decimal,\n Strict(strict)if strict is not None else None,\n annotated_types.Interval(gt=gt,ge=ge,lt=lt,le=le),\n annotated_types.MultipleOf(multiple_of)if multiple_of is not None else None,\n _fields.pydantic_general_metadata(max_digits=max_digits,decimal_places=decimal_places),\n AllowInfNan(allow_inf_nan)if allow_inf_nan is not None else None,\n ]\n \n \n \n \n \n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass UuidVersion:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n uuid_version:Literal[1,3,4,5]\n \n def __get_pydantic_json_schema__(\n self,core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler\n )->JsonSchemaValue:\n  field_schema=handler(core_schema)\n  field_schema.pop('anyOf',None)\n  field_schema.update(type='string',format=f'uuid{self.uuid_version}')\n  return field_schema\n  \n def __get_pydantic_core_schema__(self,source:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  if isinstance(self,source):\n  \n   return core_schema.uuid_schema(version=self.uuid_version)\n  else:\n  \n   schema=handler(source)\n   _check_annotated_type(schema['type'],'uuid',self.__class__.__name__)\n   schema['version']=self.uuid_version\n   return schema\n   \n def __hash__(self)->int:\n  return hash(type(self.uuid_version))\n  \n  \nUUID1=Annotated[UUID,UuidVersion(1)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\nUUID3=Annotated[UUID,UuidVersion(3)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\nUUID4=Annotated[UUID,UuidVersion(4)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\nUUID5=Annotated[UUID,UuidVersion(5)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n@_dataclasses.dataclass\nclass PathType:\n path_type:Literal['file','dir','new','socket']\n \n def __get_pydantic_json_schema__(\n self,core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler\n )->JsonSchemaValue:\n  field_schema=handler(core_schema)\n  format_conversion={'file':'file-path','dir':'directory-path'}\n  field_schema.update(format=format_conversion.get(self.path_type,'path'),type='string')\n  return field_schema\n  \n def __get_pydantic_core_schema__(self,source:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  function_lookup={\n  'file':cast(core_schema.WithInfoValidatorFunction,self.validate_file),\n  'dir':cast(core_schema.WithInfoValidatorFunction,self.validate_directory),\n  'new':cast(core_schema.WithInfoValidatorFunction,self.validate_new),\n  'socket':cast(core_schema.WithInfoValidatorFunction,self.validate_socket),\n  }\n  \n  return core_schema.with_info_after_validator_function(\n  function_lookup[self.path_type],\n  handler(source),\n  )\n  \n @staticmethod\n def validate_file(path:Path,_:core_schema.ValidationInfo)->Path:\n  if path.is_file():\n   return path\n  else:\n   raise PydanticCustomError('path_not_file','Path does not point to a file')\n   \n @staticmethod\n def validate_socket(path:Path,_:core_schema.ValidationInfo)->Path:\n  if path.is_socket():\n   return path\n  else:\n   raise PydanticCustomError('path_not_socket','Path does not point to a socket')\n   \n @staticmethod\n def validate_directory(path:Path,_:core_schema.ValidationInfo)->Path:\n  if path.is_dir():\n   return path\n  else:\n   raise PydanticCustomError('path_not_directory','Path does not point to a directory')\n   \n @staticmethod\n def validate_new(path:Path,_:core_schema.ValidationInfo)->Path:\n  if path.exists():\n   raise PydanticCustomError('path_exists','Path already exists')\n  elif not path.parent.exists():\n   raise PydanticCustomError('parent_does_not_exist','Parent directory does not exist')\n  else:\n   return path\n   \n def __hash__(self)->int:\n  return hash(type(self.path_type))\n  \n  \nFilePath=Annotated[Path,PathType('file')]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirectoryPath=Annotated[Path,PathType('dir')]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNewPath=Annotated[Path,PathType('new')]\n''\n\nSocketPath=Annotated[Path,PathType('socket')]\n''\n\n\n\nif TYPE_CHECKING:\n\n Json=Annotated[AnyType,...]\n \nelse:\n\n class Json:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  @classmethod\n  def __class_getitem__(cls,item:AnyType)->AnyType:\n   return Annotated[item,cls()]\n   \n  @classmethod\n  def __get_pydantic_core_schema__(cls,source:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n   if cls is source:\n    return core_schema.json_schema(None)\n   else:\n    return core_schema.json_schema(handler(source))\n    \n  def __repr__(self)->str:\n   return 'Json'\n   \n  def __hash__(self)->int:\n   return hash(type(self))\n   \n  def __eq__(self,other:Any)->bool:\n   return type(other)is type(self)\n   \n   \n   \n   \nSecretType=TypeVar('SecretType')\n\n\nclass _SecretBase(Generic[SecretType]):\n def __init__(self,secret_value:SecretType)->None:\n  self._secret_value:SecretType=secret_value\n  \n def get_secret_value(self)->SecretType:\n  ''\n\n\n\n  \n  return self._secret_value\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,self.__class__)and self.get_secret_value()==other.get_secret_value()\n  \n def __hash__(self)->int:\n  return hash(self.get_secret_value())\n  \n def __str__(self)->str:\n  return str(self._display())\n  \n def __repr__(self)->str:\n  return f'{self.__class__.__name__}({self._display()!r})'\n  \n def _display(self)->str |bytes:\n  raise NotImplementedError\n  \n  \ndef _serialize_secret(value:Secret[SecretType],info:core_schema.SerializationInfo)->str |Secret[SecretType]:\n if info.mode =='json':\n  return str(value)\n else:\n  return value\n  \n  \nclass Secret(_SecretBase[SecretType]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def _display(self)->str |bytes:\n  return '**********'if self.get_secret_value()else ''\n  \n @classmethod\n def __get_pydantic_core_schema__(cls,source:type[Any],handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  inner_type=None\n  \n  origin_type=get_origin(source)\n  if origin_type is not None:\n   inner_type=get_args(source)[0]\n   \n  else:\n   bases=getattr(cls,'__orig_bases__',getattr(cls,'__bases__',[]))\n   for base in bases:\n    if get_origin(base)is Secret:\n     inner_type=get_args(base)[0]\n   if bases ==[]or inner_type is None:\n    raise TypeError(\n    f\"Can't get secret type from {cls.__name__}. \"\n    'Please use Secret[<type>], or subclass from Secret[<type>] instead.'\n    )\n    \n  inner_schema=handler.generate_schema(inner_type)\n  \n  def validate_secret_value(value,handler)->Secret[SecretType]:\n   if isinstance(value,Secret):\n    value=value.get_secret_value()\n   validated_inner=handler(value)\n   return cls(validated_inner)\n   \n  return core_schema.json_or_python_schema(\n  python_schema=core_schema.no_info_wrap_validator_function(\n  validate_secret_value,\n  inner_schema,\n  ),\n  json_schema=core_schema.no_info_after_validator_function(lambda x:cls(x),inner_schema),\n  serialization=core_schema.plain_serializer_function_ser_schema(\n  _serialize_secret,\n  info_arg=True,\n  when_used='always',\n  ),\n  )\n  \n __pydantic_serializer__=SchemaSerializer(\n core_schema.any_schema(\n serialization=core_schema.plain_serializer_function_ser_schema(\n _serialize_secret,\n info_arg=True,\n when_used='always',\n )\n )\n )\n \n \ndef _secret_display(value:SecretType)->str:\n return '**********'if value else ''\n \n \ndef _serialize_secret_field(\nvalue:_SecretField[SecretType],info:core_schema.SerializationInfo\n)->str |_SecretField[SecretType]:\n if info.mode =='json':\n \n \n  return _secret_display(value.get_secret_value())\n else:\n  return value\n  \n  \nclass _SecretField(_SecretBase[SecretType]):\n _inner_schema:ClassVar[CoreSchema]\n _error_kind:ClassVar[str]\n \n @classmethod\n def __get_pydantic_core_schema__(cls,source:type[Any],handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  def get_json_schema(_core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler)->JsonSchemaValue:\n   json_schema=handler(cls._inner_schema)\n   _utils.update_not_none(\n   json_schema,\n   type='string',\n   writeOnly=True,\n   format='password',\n   )\n   return json_schema\n   \n  json_schema=core_schema.no_info_after_validator_function(\n  source,\n  cls._inner_schema,\n  )\n  \n  def get_secret_schema(strict:bool)->CoreSchema:\n   return core_schema.json_or_python_schema(\n   python_schema=core_schema.union_schema(\n   [\n   core_schema.is_instance_schema(source),\n   json_schema,\n   ],\n   custom_error_type=cls._error_kind,\n   strict=strict,\n   ),\n   json_schema=json_schema,\n   serialization=core_schema.plain_serializer_function_ser_schema(\n   _serialize_secret_field,\n   info_arg=True,\n   when_used='always',\n   ),\n   )\n   \n  return core_schema.lax_or_strict_schema(\n  lax_schema=get_secret_schema(strict=False),\n  strict_schema=get_secret_schema(strict=True),\n  metadata={'pydantic_js_functions':[get_json_schema]},\n  )\n  \n __pydantic_serializer__=SchemaSerializer(\n core_schema.any_schema(\n serialization=core_schema.plain_serializer_function_ser_schema(\n _serialize_secret_field,\n info_arg=True,\n when_used='always',\n )\n )\n )\n \n \nclass SecretStr(_SecretField[str]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n _inner_schema:ClassVar[CoreSchema]=core_schema.str_schema()\n _error_kind:ClassVar[str]='string_type'\n \n def __len__(self)->int:\n  return len(self._secret_value)\n  \n def _display(self)->str:\n  return _secret_display(self._secret_value)\n  \n  \nclass SecretBytes(_SecretField[bytes]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n _inner_schema:ClassVar[CoreSchema]=core_schema.bytes_schema()\n _error_kind:ClassVar[str]='bytes_type'\n \n def __len__(self)->int:\n  return len(self._secret_value)\n  \n def _display(self)->bytes:\n  return _secret_display(self._secret_value).encode()\n  \n  \n  \n  \n  \nclass PaymentCardBrand(str,Enum):\n amex='American Express'\n mastercard='Mastercard'\n visa='Visa'\n other='other'\n \n def __str__(self)->str:\n  return self.value\n  \n  \n@deprecated(\n'The `PaymentCardNumber` class is deprecated, use `pydantic_extra_types` instead. '\n'See https://docs.pydantic.dev/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.',\ncategory=PydanticDeprecatedSince20,\n)\nclass PaymentCardNumber(str):\n ''\n \n strip_whitespace:ClassVar[bool]=True\n min_length:ClassVar[int]=12\n max_length:ClassVar[int]=19\n bin:str\n last4:str\n brand:PaymentCardBrand\n \n def __init__(self,card_number:str):\n  self.validate_digits(card_number)\n  \n  card_number=self.validate_luhn_check_digit(card_number)\n  \n  self.bin=card_number[:6]\n  self.last4=card_number[-4:]\n  self.brand=self.validate_brand(card_number)\n  \n @classmethod\n def __get_pydantic_core_schema__(cls,source:type[Any],handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  return core_schema.with_info_after_validator_function(\n  cls.validate,\n  core_schema.str_schema(\n  min_length=cls.min_length,max_length=cls.max_length,strip_whitespace=cls.strip_whitespace\n  ),\n  )\n  \n @classmethod\n def validate(cls,input_value:str,/,_:core_schema.ValidationInfo)->PaymentCardNumber:\n  ''\n  return cls(input_value)\n  \n @property\n def masked(self)->str:\n  ''\n\n\n\n  \n  num_masked=len(self)-10\n  return f'{self.bin}{\"*\"*num_masked}{self.last4}'\n  \n @classmethod\n def validate_digits(cls,card_number:str)->None:\n  ''\n  if not card_number.isdigit():\n   raise PydanticCustomError('payment_card_number_digits','Card number is not all digits')\n   \n @classmethod\n def validate_luhn_check_digit(cls,card_number:str)->str:\n  ''\n  sum_=int(card_number[-1])\n  length=len(card_number)\n  parity=length %2\n  for i in range(length -1):\n   digit=int(card_number[i])\n   if i %2 ==parity:\n    digit *=2\n   if digit >9:\n    digit -=9\n   sum_ +=digit\n  valid=sum_ %10 ==0\n  if not valid:\n   raise PydanticCustomError('payment_card_number_luhn','Card number is not luhn valid')\n  return card_number\n  \n @staticmethod\n def validate_brand(card_number:str)->PaymentCardBrand:\n  ''\n\n  \n  if card_number[0]=='4':\n   brand=PaymentCardBrand.visa\n  elif 51 <=int(card_number[:2])<=55:\n   brand=PaymentCardBrand.mastercard\n  elif card_number[:2]in{'34','37'}:\n   brand=PaymentCardBrand.amex\n  else:\n   brand=PaymentCardBrand.other\n   \n  required_length:None |int |str=None\n  if brand in PaymentCardBrand.mastercard:\n   required_length=16\n   valid=len(card_number)==required_length\n  elif brand ==PaymentCardBrand.visa:\n   required_length='13, 16 or 19'\n   valid=len(card_number)in{13,16,19}\n  elif brand ==PaymentCardBrand.amex:\n   required_length=15\n   valid=len(card_number)==required_length\n  else:\n   valid=True\n   \n  if not valid:\n   raise PydanticCustomError(\n   'payment_card_number_brand',\n   'Length for a {brand} card must be {required_length}',\n   {'brand':brand,'required_length':required_length},\n   )\n  return brand\n  \n  \n  \n  \n  \nclass ByteSize(int):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n byte_sizes={\n 'b':1,\n 'kb':10 **3,\n 'mb':10 **6,\n 'gb':10 **9,\n 'tb':10 **12,\n 'pb':10 **15,\n 'eb':10 **18,\n 'kib':2 **10,\n 'mib':2 **20,\n 'gib':2 **30,\n 'tib':2 **40,\n 'pib':2 **50,\n 'eib':2 **60,\n 'bit':1 /8,\n 'kbit':10 **3 /8,\n 'mbit':10 **6 /8,\n 'gbit':10 **9 /8,\n 'tbit':10 **12 /8,\n 'pbit':10 **15 /8,\n 'ebit':10 **18 /8,\n 'kibit':2 **10 /8,\n 'mibit':2 **20 /8,\n 'gibit':2 **30 /8,\n 'tibit':2 **40 /8,\n 'pibit':2 **50 /8,\n 'eibit':2 **60 /8,\n }\n byte_sizes.update({k.lower()[0]:v for k,v in byte_sizes.items()if 'i'not in k})\n \n byte_string_pattern=r'^\\s*(\\d*\\.?\\d+)\\s*(\\w+)?'\n byte_string_re=re.compile(byte_string_pattern,re.IGNORECASE)\n \n @classmethod\n def __get_pydantic_core_schema__(cls,source:type[Any],handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  return core_schema.with_info_after_validator_function(\n  function=cls._validate,\n  schema=core_schema.union_schema(\n  [\n  core_schema.str_schema(pattern=cls.byte_string_pattern),\n  core_schema.int_schema(ge=0),\n  ],\n  custom_error_type='byte_size',\n  custom_error_message='could not parse value and unit from byte string',\n  ),\n  serialization=core_schema.plain_serializer_function_ser_schema(\n  int,return_schema=core_schema.int_schema(ge=0)\n  ),\n  )\n  \n @classmethod\n def _validate(cls,input_value:Any,/,_:core_schema.ValidationInfo)->ByteSize:\n  try:\n   return cls(int(input_value))\n  except ValueError:\n   pass\n   \n  str_match=cls.byte_string_re.match(str(input_value))\n  if str_match is None:\n   raise PydanticCustomError('byte_size','could not parse value and unit from byte string')\n   \n  scalar,unit=str_match.groups()\n  if unit is None:\n   unit='b'\n   \n  try:\n   unit_mult=cls.byte_sizes[unit.lower()]\n  except KeyError:\n   raise PydanticCustomError('byte_size_unit','could not interpret byte unit: {unit}',{'unit':unit})\n   \n  return cls(int(float(scalar)*unit_mult))\n  \n def human_readable(self,decimal:bool=False,separator:str='')->str:\n  ''\n\n\n\n\n\n\n\n\n  \n  if decimal:\n   divisor=1000\n   units='B','KB','MB','GB','TB','PB'\n   final_unit='EB'\n  else:\n   divisor=1024\n   units='B','KiB','MiB','GiB','TiB','PiB'\n   final_unit='EiB'\n   \n  num=float(self)\n  for unit in units:\n   if abs(num)<divisor:\n    if unit =='B':\n     return f'{num:0.0f}{separator}{unit}'\n    else:\n     return f'{num:0.1f}{separator}{unit}'\n   num /=divisor\n   \n  return f'{num:0.1f}{separator}{final_unit}'\n  \n def to(self,unit:str)->float:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  try:\n   unit_div=self.byte_sizes[unit.lower()]\n  except KeyError:\n   raise PydanticCustomError('byte_size_unit','Could not interpret byte unit: {unit}',{'unit':unit})\n   \n  return self /unit_div\n  \n  \n  \n  \n  \ndef _check_annotated_type(annotated_type:str,expected_type:str,annotation:str)->None:\n if annotated_type !=expected_type:\n  raise PydanticUserError(f\"'{annotation}' cannot annotate '{annotated_type}'.\",code='invalid-annotated-type')\n  \n  \nif TYPE_CHECKING:\n PastDate=Annotated[date,...]\n FutureDate=Annotated[date,...]\nelse:\n\n class PastDate:\n  ''\n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   if cls is source:\n   \n    return core_schema.date_schema(now_op='past')\n   else:\n    schema=handler(source)\n    _check_annotated_type(schema['type'],'date',cls.__name__)\n    schema['now_op']='past'\n    return schema\n    \n  def __repr__(self)->str:\n   return 'PastDate'\n   \n class FutureDate:\n  ''\n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   if cls is source:\n   \n    return core_schema.date_schema(now_op='future')\n   else:\n    schema=handler(source)\n    _check_annotated_type(schema['type'],'date',cls.__name__)\n    schema['now_op']='future'\n    return schema\n    \n  def __repr__(self)->str:\n   return 'FutureDate'\n   \n   \ndef condate(\n*,\nstrict:bool |None=None,\ngt:date |None=None,\nge:date |None=None,\nlt:date |None=None,\nle:date |None=None,\n)->type[date]:\n ''\n\n\n\n\n\n\n\n\n\n\n \n return Annotated[\n date,\n Strict(strict)if strict is not None else None,\n annotated_types.Interval(gt=gt,ge=ge,lt=lt,le=le),\n ]\n \n \n \n \nif TYPE_CHECKING:\n AwareDatetime=Annotated[datetime,...]\n NaiveDatetime=Annotated[datetime,...]\n PastDatetime=Annotated[datetime,...]\n FutureDatetime=Annotated[datetime,...]\n \nelse:\n\n class AwareDatetime:\n  ''\n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   if cls is source:\n   \n    return core_schema.datetime_schema(tz_constraint='aware')\n   else:\n    schema=handler(source)\n    _check_annotated_type(schema['type'],'datetime',cls.__name__)\n    schema['tz_constraint']='aware'\n    return schema\n    \n  def __repr__(self)->str:\n   return 'AwareDatetime'\n   \n class NaiveDatetime:\n  ''\n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   if cls is source:\n   \n    return core_schema.datetime_schema(tz_constraint='naive')\n   else:\n    schema=handler(source)\n    _check_annotated_type(schema['type'],'datetime',cls.__name__)\n    schema['tz_constraint']='naive'\n    return schema\n    \n  def __repr__(self)->str:\n   return 'NaiveDatetime'\n   \n class PastDatetime:\n  ''\n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   if cls is source:\n   \n    return core_schema.datetime_schema(now_op='past')\n   else:\n    schema=handler(source)\n    _check_annotated_type(schema['type'],'datetime',cls.__name__)\n    schema['now_op']='past'\n    return schema\n    \n  def __repr__(self)->str:\n   return 'PastDatetime'\n   \n class FutureDatetime:\n  ''\n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,source:type[Any],handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   if cls is source:\n   \n    return core_schema.datetime_schema(now_op='future')\n   else:\n    schema=handler(source)\n    _check_annotated_type(schema['type'],'datetime',cls.__name__)\n    schema['now_op']='future'\n    return schema\n    \n  def __repr__(self)->str:\n   return 'FutureDatetime'\n   \n   \n   \n   \n   \nclass EncoderProtocol(Protocol):\n ''\n \n @classmethod\n def decode(cls,data:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  ...\n  \n @classmethod\n def encode(cls,value:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  ...\n  \n @classmethod\n def get_json_format(cls)->str:\n  ''\n\n\n\n  \n  ...\n  \n  \nclass Base64Encoder(EncoderProtocol):\n ''\n \n @classmethod\n def decode(cls,data:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  try:\n   return base64.b64decode(data)\n  except ValueError as e:\n   raise PydanticCustomError('base64_decode',\"Base64 decoding error: '{error}'\",{'error':str(e)})\n   \n @classmethod\n def encode(cls,value:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  return base64.b64encode(value)\n  \n @classmethod\n def get_json_format(cls)->Literal['base64']:\n  ''\n\n\n\n  \n  return 'base64'\n  \n  \nclass Base64UrlEncoder(EncoderProtocol):\n ''\n \n @classmethod\n def decode(cls,data:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  try:\n   return base64.urlsafe_b64decode(data)\n  except ValueError as e:\n   raise PydanticCustomError('base64_decode',\"Base64 decoding error: '{error}'\",{'error':str(e)})\n   \n @classmethod\n def encode(cls,value:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  return base64.urlsafe_b64encode(value)\n  \n @classmethod\n def get_json_format(cls)->Literal['base64url']:\n  ''\n\n\n\n  \n  return 'base64url'\n  \n  \n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass EncodedBytes:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n encoder:type[EncoderProtocol]\n \n def __get_pydantic_json_schema__(\n self,core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler\n )->JsonSchemaValue:\n  field_schema=handler(core_schema)\n  field_schema.update(type='string',format=self.encoder.get_json_format())\n  return field_schema\n  \n def __get_pydantic_core_schema__(self,source:type[Any],handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  schema=handler(source)\n  _check_annotated_type(schema['type'],'bytes',self.__class__.__name__)\n  return core_schema.with_info_after_validator_function(\n  function=self.decode,\n  schema=schema,\n  serialization=core_schema.plain_serializer_function_ser_schema(function=self.encode),\n  )\n  \n def decode(self,data:bytes,_:core_schema.ValidationInfo)->bytes:\n  ''\n\n\n\n\n\n\n  \n  return self.encoder.decode(data)\n  \n def encode(self,value:bytes)->bytes:\n  ''\n\n\n\n\n\n\n  \n  return self.encoder.encode(value)\n  \n def __hash__(self)->int:\n  return hash(self.encoder)\n  \n  \n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass EncodedStr:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n encoder:type[EncoderProtocol]\n \n def __get_pydantic_json_schema__(\n self,core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler\n )->JsonSchemaValue:\n  field_schema=handler(core_schema)\n  field_schema.update(type='string',format=self.encoder.get_json_format())\n  return field_schema\n  \n def __get_pydantic_core_schema__(self,source:type[Any],handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  schema=handler(source)\n  _check_annotated_type(schema['type'],'str',self.__class__.__name__)\n  return core_schema.with_info_after_validator_function(\n  function=self.decode_str,\n  schema=schema,\n  serialization=core_schema.plain_serializer_function_ser_schema(function=self.encode_str),\n  )\n  \n def decode_str(self,data:str,_:core_schema.ValidationInfo)->str:\n  ''\n\n\n\n\n\n\n  \n  return self.encoder.decode(data.encode()).decode()\n  \n def encode_str(self,value:str)->str:\n  ''\n\n\n\n\n\n\n  \n  return self.encoder.encode(value.encode()).decode()\n  \n def __hash__(self)->int:\n  return hash(self.encoder)\n  \n  \nBase64Bytes=Annotated[bytes,EncodedBytes(encoder=Base64Encoder)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase64Str=Annotated[str,EncodedStr(encoder=Base64Encoder)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase64UrlBytes=Annotated[bytes,EncodedBytes(encoder=Base64UrlEncoder)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase64UrlStr=Annotated[str,EncodedStr(encoder=Base64UrlEncoder)]\n''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__getattr__=getattr_migration(__name__)\n\n\n@_dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass GetPydanticSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n get_pydantic_core_schema:Callable[[Any,GetCoreSchemaHandler],CoreSchema]|None=None\n get_pydantic_json_schema:Callable[[Any,GetJsonSchemaHandler],JsonSchemaValue]|None=None\n \n \n \n \n if not TYPE_CHECKING:\n \n \n  def __getattr__(self,item:str)->Any:\n   ''\n   if item =='__get_pydantic_core_schema__'and self.get_pydantic_core_schema:\n    return self.get_pydantic_core_schema\n   elif item =='__get_pydantic_json_schema__'and self.get_pydantic_json_schema:\n    return self.get_pydantic_json_schema\n   else:\n    return object.__getattribute__(self,item)\n    \n __hash__=object.__hash__\n \n \n@_dataclasses.dataclass(**_internal_dataclass.slots_true,frozen=True)\nclass Tag:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n tag:str\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->CoreSchema:\n  schema=handler(source_type)\n  metadata=schema.setdefault('metadata',{})\n  assert isinstance(metadata,dict)\n  metadata[_core_utils.TAGGED_UNION_TAG_KEY]=self.tag\n  return schema\n  \n  \n@_dataclasses.dataclass(**_internal_dataclass.slots_true,frozen=True)\nclass Discriminator:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n discriminator:str |Callable[[Any],Hashable]\n ''\n\n\n\n \n custom_error_type:str |None=None\n ''\n\n \n custom_error_message:str |None=None\n '' \n custom_error_context:dict[str,int |str |float]|None=None\n '' \n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->CoreSchema:\n  origin=_typing_extra.get_origin(source_type)\n  if not origin or not _typing_extra.origin_is_union(origin):\n   raise TypeError(f'{type(self).__name__} must be used with a Union type, not {source_type}')\n   \n  if isinstance(self.discriminator,str):\n   from pydantic import Field\n   \n   return handler(Annotated[source_type,Field(discriminator=self.discriminator)])\n  else:\n   original_schema=handler(source_type)\n   return self._convert_schema(original_schema)\n   \n def _convert_schema(self,original_schema:core_schema.CoreSchema)->core_schema.TaggedUnionSchema:\n  if original_schema['type']!='union':\n  \n  \n  \n  \n   original_schema=core_schema.union_schema([original_schema])\n   \n  tagged_union_choices={}\n  for choice in original_schema['choices']:\n   tag=None\n   if isinstance(choice,tuple):\n    choice,tag=choice\n   metadata=choice.get('metadata')\n   if metadata is not None:\n    metadata_tag=metadata.get(_core_utils.TAGGED_UNION_TAG_KEY)\n    if metadata_tag is not None:\n     tag=metadata_tag\n   if tag is None:\n    raise PydanticUserError(\n    f'`Tag` not provided for choice {choice} used with `Discriminator`',\n    code='callable-discriminator-no-tag',\n    )\n   tagged_union_choices[tag]=choice\n   \n   \n  custom_error_type=self.custom_error_type\n  if custom_error_type is None:\n   custom_error_type=original_schema.get('custom_error_type')\n   \n  custom_error_message=self.custom_error_message\n  if custom_error_message is None:\n   custom_error_message=original_schema.get('custom_error_message')\n   \n  custom_error_context=self.custom_error_context\n  if custom_error_context is None:\n   custom_error_context=original_schema.get('custom_error_context')\n   \n  custom_error_type=original_schema.get('custom_error_type')if custom_error_type is None else custom_error_type\n  return core_schema.tagged_union_schema(\n  tagged_union_choices,\n  self.discriminator,\n  custom_error_type=custom_error_type,\n  custom_error_message=custom_error_message,\n  custom_error_context=custom_error_context,\n  strict=original_schema.get('strict'),\n  ref=original_schema.get('ref'),\n  metadata=original_schema.get('metadata'),\n  serialization=original_schema.get('serialization'),\n  )\n  \n  \n_JSON_TYPES={int,float,str,bool,list,dict,type(None)}\n\n\ndef _get_type_name(x:Any)->str:\n type_=type(x)\n if type_ in _JSON_TYPES:\n  return type_.__name__\n  \n  \n if isinstance(x,int):\n  return 'int'\n if isinstance(x,float):\n  return 'float'\n if isinstance(x,str):\n  return 'str'\n if isinstance(x,list):\n  return 'list'\n if isinstance(x,dict):\n  return 'dict'\n  \n  \n return getattr(type_,'__name__','<no type name>')\n \n \nclass _AllowAnyJson:\n @classmethod\n def __get_pydantic_core_schema__(cls,source_type:Any,handler:GetCoreSchemaHandler)->CoreSchema:\n  python_schema=handler(source_type)\n  return core_schema.json_or_python_schema(json_schema=core_schema.any_schema(),python_schema=python_schema)\n  \n  \nif TYPE_CHECKING:\n\n JsonValue:TypeAlias=Union[\n List['JsonValue'],\n Dict[str,'JsonValue'],\n str,\n bool,\n int,\n float,\n None,\n ]\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \nelse:\n JsonValue=TypeAliasType(\n 'JsonValue',\n Annotated[\n Union[\n Annotated[List['JsonValue'],Tag('list')],\n Annotated[Dict[str,'JsonValue'],Tag('dict')],\n Annotated[str,Tag('str')],\n Annotated[bool,Tag('bool')],\n Annotated[int,Tag('int')],\n Annotated[float,Tag('float')],\n Annotated[None,Tag('NoneType')],\n ],\n Discriminator(\n _get_type_name,\n custom_error_type='invalid-json-value',\n custom_error_message='input was not a valid JSON value',\n ),\n _AllowAnyJson,\n ],\n )\n \n \nclass _OnErrorOmit:\n @classmethod\n def __get_pydantic_core_schema__(cls,source_type:Any,handler:GetCoreSchemaHandler)->CoreSchema:\n \n \n \n  return core_schema.with_default_schema(schema=handler(source_type),on_error='omit')\n  \n  \nOnErrorOmit=Annotated[T,_OnErrorOmit]\n''\n\n\n\n\n\n\n\n@_dataclasses.dataclass\nclass FailFast(_fields.PydanticMetadata,BaseMetadata):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n fail_fast:bool=True\n", ["__future__", "annotated_types", "base64", "dataclasses", "datetime", "decimal", "enum", "pathlib", "pydantic", "pydantic._internal", "pydantic._internal._core_utils", "pydantic._internal._fields", "pydantic._internal._internal_dataclass", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic._internal._validators", "pydantic._migration", "pydantic.annotated_handlers", "pydantic.errors", "pydantic.json_schema", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "re", "types", "typing", "typing_extensions", "uuid"]], "pydantic.schema": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.fields": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport inspect\nimport sys\nimport typing\nfrom copy import copy\nfrom dataclasses import Field as DataclassField\nfrom functools import cached_property\nfrom typing import Any,Callable,ClassVar,TypeVar,cast,overload\nfrom warnings import warn\n\nimport annotated_types\nimport typing_extensions\nfrom pydantic_core import PydanticUndefined\nfrom typing_extensions import Literal,TypeAlias,Unpack,deprecated\n\nfrom. import types\nfrom._internal import _decorators,_fields,_generics,_internal_dataclass,_repr,_typing_extra,_utils\nfrom._internal._namespace_utils import GlobalsNamespace,MappingNamespace\nfrom.aliases import AliasChoices,AliasPath\nfrom.config import JsonDict\nfrom.errors import PydanticUserError\nfrom.json_schema import PydanticJsonSchemaWarning\nfrom.warnings import PydanticDeprecatedSince20\n\nif typing.TYPE_CHECKING:\n from._internal._repr import ReprArgs\nelse:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n__all__='Field','PrivateAttr','computed_field'\n\n\n_Unset:Any=PydanticUndefined\n\nif sys.version_info >=(3,13):\n import warnings\n \n Deprecated:TypeAlias=warnings.deprecated |deprecated\nelse:\n Deprecated:TypeAlias=deprecated\n \n \nclass _FromFieldInfoInputs(typing_extensions.TypedDict,total=False):\n ''\n \n annotation:type[Any]|None\n default_factory:Callable[[],Any]|Callable[[dict[str,Any]],Any]|None\n alias:str |None\n alias_priority:int |None\n validation_alias:str |AliasPath |AliasChoices |None\n serialization_alias:str |None\n title:str |None\n field_title_generator:Callable[[str,FieldInfo],str]|None\n description:str |None\n examples:list[Any]|None\n exclude:bool |None\n gt:annotated_types.SupportsGt |None\n ge:annotated_types.SupportsGe |None\n lt:annotated_types.SupportsLt |None\n le:annotated_types.SupportsLe |None\n multiple_of:float |None\n strict:bool |None\n min_length:int |None\n max_length:int |None\n pattern:str |typing.Pattern[str]|None\n allow_inf_nan:bool |None\n max_digits:int |None\n decimal_places:int |None\n union_mode:Literal['smart','left_to_right']|None\n discriminator:str |types.Discriminator |None\n deprecated:Deprecated |str |bool |None\n json_schema_extra:JsonDict |Callable[[JsonDict],None]|None\n frozen:bool |None\n validate_default:bool |None\n repr:bool\n init:bool |None\n init_var:bool |None\n kw_only:bool |None\n coerce_numbers_to_str:bool |None\n fail_fast:bool |None\n \n \nclass _FieldInfoInputs(_FromFieldInfoInputs,total=False):\n ''\n \n default:Any\n \n \nclass FieldInfo(_repr.Representation):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n annotation:type[Any]|None\n default:Any\n default_factory:Callable[[],Any]|Callable[[dict[str,Any]],Any]|None\n alias:str |None\n alias_priority:int |None\n validation_alias:str |AliasPath |AliasChoices |None\n serialization_alias:str |None\n title:str |None\n field_title_generator:Callable[[str,FieldInfo],str]|None\n description:str |None\n examples:list[Any]|None\n exclude:bool |None\n discriminator:str |types.Discriminator |None\n deprecated:Deprecated |str |bool |None\n json_schema_extra:JsonDict |Callable[[JsonDict],None]|None\n frozen:bool |None\n validate_default:bool |None\n repr:bool\n init:bool |None\n init_var:bool |None\n kw_only:bool |None\n metadata:list[Any]\n \n __slots__=(\n 'annotation',\n 'evaluated',\n 'default',\n 'default_factory',\n 'alias',\n 'alias_priority',\n 'validation_alias',\n 'serialization_alias',\n 'title',\n 'field_title_generator',\n 'description',\n 'examples',\n 'exclude',\n 'discriminator',\n 'deprecated',\n 'json_schema_extra',\n 'frozen',\n 'validate_default',\n 'repr',\n 'init',\n 'init_var',\n 'kw_only',\n 'metadata',\n '_attributes_set',\n )\n \n \n \n metadata_lookup:ClassVar[dict[str,typing.Callable[[Any],Any]|None]]={\n 'strict':types.Strict,\n 'gt':annotated_types.Gt,\n 'ge':annotated_types.Ge,\n 'lt':annotated_types.Lt,\n 'le':annotated_types.Le,\n 'multiple_of':annotated_types.MultipleOf,\n 'min_length':annotated_types.MinLen,\n 'max_length':annotated_types.MaxLen,\n 'pattern':None,\n 'allow_inf_nan':None,\n 'max_digits':None,\n 'decimal_places':None,\n 'union_mode':None,\n 'coerce_numbers_to_str':None,\n 'fail_fast':types.FailFast,\n }\n \n def __init__(self,**kwargs:Unpack[_FieldInfoInputs])->None:\n  ''\n\n\n\n  \n  self._attributes_set={k:v for k,v in kwargs.items()if v is not _Unset}\n  kwargs={k:_DefaultValues.get(k)if v is _Unset else v for k,v in kwargs.items()}\n  self.annotation,annotation_metadata=self._extract_metadata(kwargs.get('annotation'))\n  self.evaluated=False\n  \n  default=kwargs.pop('default',PydanticUndefined)\n  if default is Ellipsis:\n   self.default=PydanticUndefined\n   \n   \n   \n   self._attributes_set.pop('default',None)\n  else:\n   self.default=default\n   \n  self.default_factory=kwargs.pop('default_factory',None)\n  \n  if self.default is not PydanticUndefined and self.default_factory is not None:\n   raise TypeError('cannot specify both default and default_factory')\n   \n  self.alias=kwargs.pop('alias',None)\n  self.validation_alias=kwargs.pop('validation_alias',None)\n  self.serialization_alias=kwargs.pop('serialization_alias',None)\n  alias_is_set=any(alias is not None for alias in(self.alias,self.validation_alias,self.serialization_alias))\n  self.alias_priority=kwargs.pop('alias_priority',None)or 2 if alias_is_set else None\n  self.title=kwargs.pop('title',None)\n  self.field_title_generator=kwargs.pop('field_title_generator',None)\n  self.description=kwargs.pop('description',None)\n  self.examples=kwargs.pop('examples',None)\n  self.exclude=kwargs.pop('exclude',None)\n  self.discriminator=kwargs.pop('discriminator',None)\n  \n  self.deprecated=kwargs.pop('deprecated',getattr(self,'deprecated',None))\n  self.repr=kwargs.pop('repr',True)\n  self.json_schema_extra=kwargs.pop('json_schema_extra',None)\n  self.validate_default=kwargs.pop('validate_default',None)\n  self.frozen=kwargs.pop('frozen',None)\n  \n  self.init=kwargs.pop('init',None)\n  self.init_var=kwargs.pop('init_var',None)\n  self.kw_only=kwargs.pop('kw_only',None)\n  \n  self.metadata=self._collect_metadata(kwargs)+annotation_metadata\n  \n @staticmethod\n def from_field(default:Any=PydanticUndefined,**kwargs:Unpack[_FromFieldInfoInputs])->FieldInfo:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if 'annotation'in kwargs:\n   raise TypeError('\"annotation\" is not permitted as a Field keyword argument')\n  return FieldInfo(default=default,**kwargs)\n  \n @staticmethod\n def from_annotation(annotation:type[Any])->FieldInfo:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  final=False\n  if _typing_extra.is_finalvar(annotation):\n   final=True\n   if annotation is not typing_extensions.Final:\n    annotation=typing_extensions.get_args(annotation)[0]\n    \n  if _typing_extra.is_annotated(annotation):\n   first_arg,*extra_args=typing_extensions.get_args(annotation)\n   if _typing_extra.is_finalvar(first_arg):\n    final=True\n   field_info_annotations=[a for a in extra_args if isinstance(a,FieldInfo)]\n   field_info=FieldInfo.merge_field_infos(*field_info_annotations,annotation=first_arg)\n   if field_info:\n    new_field_info=copy(field_info)\n    new_field_info.annotation=first_arg\n    new_field_info.frozen=final or field_info.frozen\n    metadata:list[Any]=[]\n    for a in extra_args:\n     if _typing_extra.is_deprecated_instance(a):\n      new_field_info.deprecated=a.message\n     elif not isinstance(a,FieldInfo):\n      metadata.append(a)\n     else:\n      metadata.extend(a.metadata)\n    new_field_info.metadata=metadata\n    return new_field_info\n    \n  return FieldInfo(annotation=annotation,frozen=final or None)\n  \n @staticmethod\n def from_annotated_attribute(annotation:type[Any],default:Any)->FieldInfo:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if annotation is default:\n   raise PydanticUserError(\n   'Error when building FieldInfo from annotated attribute. '\n   \"Make sure you don't have any field name clashing with a type annotation \",\n   code='unevaluable-type-annotation',\n   )\n   \n  final=_typing_extra.is_finalvar(annotation)\n  if final and annotation is not typing_extensions.Final:\n   annotation=typing_extensions.get_args(annotation)[0]\n   \n  if isinstance(default,FieldInfo):\n   default.annotation,annotation_metadata=FieldInfo._extract_metadata(annotation)\n   default.metadata +=annotation_metadata\n   default=default.merge_field_infos(\n   *[x for x in annotation_metadata if isinstance(x,FieldInfo)],default,annotation=default.annotation\n   )\n   default.frozen=final or default.frozen\n   return default\n   \n  if isinstance(default,dataclasses.Field):\n   init_var=False\n   if annotation is dataclasses.InitVar:\n    init_var=True\n    annotation=typing.cast(Any,Any)\n   elif isinstance(annotation,dataclasses.InitVar):\n    init_var=True\n    annotation=annotation.type\n    \n   pydantic_field=FieldInfo._from_dataclass_field(default)\n   pydantic_field.annotation,annotation_metadata=FieldInfo._extract_metadata(annotation)\n   pydantic_field.metadata +=annotation_metadata\n   pydantic_field=pydantic_field.merge_field_infos(\n   *[x for x in annotation_metadata if isinstance(x,FieldInfo)],\n   pydantic_field,\n   annotation=pydantic_field.annotation,\n   )\n   pydantic_field.frozen=final or pydantic_field.frozen\n   pydantic_field.init_var=init_var\n   pydantic_field.init=getattr(default,'init',None)\n   pydantic_field.kw_only=getattr(default,'kw_only',None)\n   return pydantic_field\n   \n  if _typing_extra.is_annotated(annotation):\n   first_arg,*extra_args=typing_extensions.get_args(annotation)\n   field_infos=[a for a in extra_args if isinstance(a,FieldInfo)]\n   field_info=FieldInfo.merge_field_infos(*field_infos,annotation=first_arg,default=default)\n   metadata:list[Any]=[]\n   for a in extra_args:\n    if _typing_extra.is_deprecated_instance(a):\n     field_info.deprecated=a.message\n    elif not isinstance(a,FieldInfo):\n     metadata.append(a)\n    else:\n     metadata.extend(a.metadata)\n   field_info.metadata=metadata\n   return field_info\n   \n  return FieldInfo(annotation=annotation,default=default,frozen=final or None)\n  \n @staticmethod\n def merge_field_infos(*field_infos:FieldInfo,**overrides:Any)->FieldInfo:\n  ''\n\n\n\n\n\n  \n  if len(field_infos)==1:\n  \n   field_info=copy(field_infos[0])\n   field_info._attributes_set.update(overrides)\n   \n   default_override=overrides.pop('default',PydanticUndefined)\n   if default_override is Ellipsis:\n    default_override=PydanticUndefined\n   if default_override is not PydanticUndefined:\n    field_info.default=default_override\n    \n   for k,v in overrides.items():\n    setattr(field_info,k,v)\n   return field_info\n   \n  merged_field_info_kwargs:dict[str,Any]={}\n  metadata={}\n  for field_info in field_infos:\n   attributes_set=field_info._attributes_set.copy()\n   \n   try:\n    json_schema_extra=attributes_set.pop('json_schema_extra')\n    existing_json_schema_extra=merged_field_info_kwargs.get('json_schema_extra')\n    \n    if existing_json_schema_extra is None:\n     merged_field_info_kwargs['json_schema_extra']=json_schema_extra\n    if isinstance(existing_json_schema_extra,dict):\n     if isinstance(json_schema_extra,dict):\n      merged_field_info_kwargs['json_schema_extra']={\n      **existing_json_schema_extra,\n      **json_schema_extra,\n      }\n     if callable(json_schema_extra):\n      warn(\n      'Composing `dict` and `callable` type `json_schema_extra` is not supported.'\n      'The `callable` type is being ignored.'\n      \"If you'd like support for this behavior, please open an issue on pydantic.\",\n      PydanticJsonSchemaWarning,\n      )\n    elif callable(json_schema_extra):\n    \n     merged_field_info_kwargs['json_schema_extra']=json_schema_extra\n   except KeyError:\n    pass\n    \n    \n   merged_field_info_kwargs.update(attributes_set)\n   \n   for x in field_info.metadata:\n    if not isinstance(x,FieldInfo):\n     metadata[type(x)]=x\n     \n  merged_field_info_kwargs.update(overrides)\n  field_info=FieldInfo(**merged_field_info_kwargs)\n  field_info.metadata=list(metadata.values())\n  return field_info\n  \n @staticmethod\n def _from_dataclass_field(dc_field:DataclassField[Any])->FieldInfo:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  default=dc_field.default\n  if default is dataclasses.MISSING:\n   default=_Unset\n   \n  if dc_field.default_factory is dataclasses.MISSING:\n   default_factory=_Unset\n  else:\n   default_factory=dc_field.default_factory\n   \n   \n  dc_field_metadata={k:v for k,v in dc_field.metadata.items()if k in _FIELD_ARG_NAMES}\n  return Field(default=default,default_factory=default_factory,repr=dc_field.repr,**dc_field_metadata)\n  \n @staticmethod\n def _extract_metadata(annotation:type[Any]|None)->tuple[type[Any]|None,list[Any]]:\n  ''\n\n\n\n\n\n\n  \n  if annotation is not None:\n   if _typing_extra.is_annotated(annotation):\n    first_arg,*extra_args=typing_extensions.get_args(annotation)\n    return first_arg,list(extra_args)\n    \n  return annotation,[]\n  \n @staticmethod\n def _collect_metadata(kwargs:dict[str,Any])->list[Any]:\n  ''\n\n\n\n\n\n\n\n  \n  metadata:list[Any]=[]\n  general_metadata={}\n  for key,value in list(kwargs.items()):\n   try:\n    marker=FieldInfo.metadata_lookup[key]\n   except KeyError:\n    continue\n    \n   del kwargs[key]\n   if value is not None:\n    if marker is None:\n     general_metadata[key]=value\n    else:\n     metadata.append(marker(value))\n  if general_metadata:\n   metadata.append(_fields.pydantic_general_metadata(**general_metadata))\n  return metadata\n  \n @property\n def deprecation_message(self)->str |None:\n  ''\n  if self.deprecated is None:\n   return None\n  if isinstance(self.deprecated,bool):\n   return 'deprecated'if self.deprecated else None\n  return self.deprecated if isinstance(self.deprecated,str)else self.deprecated.message\n  \n @property\n def default_factory_takes_validated_data(self)->bool |None:\n  ''\n\n\n  \n  if self.default_factory is not None:\n   return _fields.takes_validated_data_argument(self.default_factory)\n   \n @overload\n def get_default(\n self,*,call_default_factory:Literal[True],validated_data:dict[str,Any]|None=None\n )->Any:...\n \n @overload\n def get_default(self,*,call_default_factory:Literal[False]=...)->Any:...\n \n def get_default(self,*,call_default_factory:bool=False,validated_data:dict[str,Any]|None=None)->Any:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  if self.default_factory is None:\n   return _utils.smart_deepcopy(self.default)\n  elif call_default_factory:\n   if self.default_factory_takes_validated_data:\n    fac=cast('Callable[[dict[str, Any]], Any]',self.default_factory)\n    if validated_data is None:\n     raise ValueError(\n     \"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'.\"\n     )\n    return fac(validated_data)\n   else:\n    fac=cast('Callable[[], Any]',self.default_factory)\n    return fac()\n  else:\n   return None\n   \n def is_required(self)->bool:\n  ''\n\n\n\n  \n  return self.default is PydanticUndefined and self.default_factory is None\n  \n def rebuild_annotation(self)->Any:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if not self.metadata:\n   return self.annotation\n  else:\n  \n   return typing_extensions.Annotated[(self.annotation,*self.metadata)]\n   \n def apply_typevars_map(\n self,\n typevars_map:dict[Any,Any]|None,\n globalns:GlobalsNamespace |None=None,\n localns:MappingNamespace |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  annotation,_=_typing_extra.try_eval_type(self.annotation,globalns,localns)\n  self.annotation=_generics.replace_types(annotation,typevars_map)\n  \n def __repr_args__(self)->ReprArgs:\n  yield 'annotation',_repr.PlainRepr(_repr.display_as_type(self.annotation))\n  yield 'required',self.is_required()\n  \n  for s in self.__slots__:\n  \n  \n   if s in('_attributes_set','annotation','evaluated'):\n    continue\n   elif s =='metadata'and not self.metadata:\n    continue\n   elif s =='repr'and self.repr is True:\n    continue\n   if s =='frozen'and self.frozen is False:\n    continue\n   if s =='validation_alias'and self.validation_alias ==self.alias:\n    continue\n   if s =='serialization_alias'and self.serialization_alias ==self.alias:\n    continue\n   if s =='default'and self.default is not PydanticUndefined:\n    yield 'default',self.default\n   elif s =='default_factory'and self.default_factory is not None:\n    yield 'default_factory',_repr.PlainRepr(_repr.display_as_type(self.default_factory))\n   else:\n    value=getattr(self,s)\n    if value is not None and value is not PydanticUndefined:\n     yield s,value\n     \n     \nclass _EmptyKwargs(typing_extensions.TypedDict):\n ''\n \n \n_DefaultValues={\n'default':...,\n'default_factory':None,\n'alias':None,\n'alias_priority':None,\n'validation_alias':None,\n'serialization_alias':None,\n'title':None,\n'description':None,\n'examples':None,\n'exclude':None,\n'discriminator':None,\n'json_schema_extra':None,\n'frozen':None,\n'validate_default':None,\n'repr':True,\n'init':None,\n'init_var':None,\n'kw_only':None,\n'pattern':None,\n'strict':None,\n'gt':None,\n'ge':None,\n'lt':None,\n'le':None,\n'multiple_of':None,\n'allow_inf_nan':None,\n'max_digits':None,\n'decimal_places':None,\n'min_length':None,\n'max_length':None,\n'coerce_numbers_to_str':None,\n}\n\n\n_T=TypeVar('_T')\n\n\n\n\n@overload\ndef Field(\ndefault:ellipsis,\n*,\nalias:str |None=_Unset,\nalias_priority:int |None=_Unset,\nvalidation_alias:str |AliasPath |AliasChoices |None=_Unset,\nserialization_alias:str |None=_Unset,\ntitle:str |None=_Unset,\nfield_title_generator:Callable[[str,FieldInfo],str]|None=_Unset,\ndescription:str |None=_Unset,\nexamples:list[Any]|None=_Unset,\nexclude:bool |None=_Unset,\ndiscriminator:str |types.Discriminator |None=_Unset,\ndeprecated:Deprecated |str |bool |None=_Unset,\njson_schema_extra:JsonDict |Callable[[JsonDict],None]|None=_Unset,\nfrozen:bool |None=_Unset,\nvalidate_default:bool |None=_Unset,\nrepr:bool=_Unset,\ninit:bool |None=_Unset,\ninit_var:bool |None=_Unset,\nkw_only:bool |None=_Unset,\npattern:str |typing.Pattern[str]|None=_Unset,\nstrict:bool |None=_Unset,\ncoerce_numbers_to_str:bool |None=_Unset,\ngt:annotated_types.SupportsGt |None=_Unset,\nge:annotated_types.SupportsGe |None=_Unset,\nlt:annotated_types.SupportsLt |None=_Unset,\nle:annotated_types.SupportsLe |None=_Unset,\nmultiple_of:float |None=_Unset,\nallow_inf_nan:bool |None=_Unset,\nmax_digits:int |None=_Unset,\ndecimal_places:int |None=_Unset,\nmin_length:int |None=_Unset,\nmax_length:int |None=_Unset,\nunion_mode:Literal['smart','left_to_right']=_Unset,\nfail_fast:bool |None=_Unset,\n**extra:Unpack[_EmptyKwargs],\n)->Any:...\n@overload\ndef Field(\ndefault:_T,\n*,\nalias:str |None=_Unset,\nalias_priority:int |None=_Unset,\nvalidation_alias:str |AliasPath |AliasChoices |None=_Unset,\nserialization_alias:str |None=_Unset,\ntitle:str |None=_Unset,\nfield_title_generator:Callable[[str,FieldInfo],str]|None=_Unset,\ndescription:str |None=_Unset,\nexamples:list[Any]|None=_Unset,\nexclude:bool |None=_Unset,\ndiscriminator:str |types.Discriminator |None=_Unset,\ndeprecated:Deprecated |str |bool |None=_Unset,\njson_schema_extra:JsonDict |Callable[[JsonDict],None]|None=_Unset,\nfrozen:bool |None=_Unset,\nvalidate_default:bool |None=_Unset,\nrepr:bool=_Unset,\ninit:bool |None=_Unset,\ninit_var:bool |None=_Unset,\nkw_only:bool |None=_Unset,\npattern:str |typing.Pattern[str]|None=_Unset,\nstrict:bool |None=_Unset,\ncoerce_numbers_to_str:bool |None=_Unset,\ngt:annotated_types.SupportsGt |None=_Unset,\nge:annotated_types.SupportsGe |None=_Unset,\nlt:annotated_types.SupportsLt |None=_Unset,\nle:annotated_types.SupportsLe |None=_Unset,\nmultiple_of:float |None=_Unset,\nallow_inf_nan:bool |None=_Unset,\nmax_digits:int |None=_Unset,\ndecimal_places:int |None=_Unset,\nmin_length:int |None=_Unset,\nmax_length:int |None=_Unset,\nunion_mode:Literal['smart','left_to_right']=_Unset,\nfail_fast:bool |None=_Unset,\n**extra:Unpack[_EmptyKwargs],\n)->_T:...\n@overload\ndef Field(\n*,\ndefault_factory:Callable[[],_T]|Callable[[dict[str,Any]],_T],\nalias:str |None=_Unset,\nalias_priority:int |None=_Unset,\nvalidation_alias:str |AliasPath |AliasChoices |None=_Unset,\nserialization_alias:str |None=_Unset,\ntitle:str |None=_Unset,\nfield_title_generator:Callable[[str,FieldInfo],str]|None=_Unset,\ndescription:str |None=_Unset,\nexamples:list[Any]|None=_Unset,\nexclude:bool |None=_Unset,\ndiscriminator:str |types.Discriminator |None=_Unset,\ndeprecated:Deprecated |str |bool |None=_Unset,\njson_schema_extra:JsonDict |Callable[[JsonDict],None]|None=_Unset,\nfrozen:bool |None=_Unset,\nvalidate_default:bool |None=_Unset,\nrepr:bool=_Unset,\ninit:bool |None=_Unset,\ninit_var:bool |None=_Unset,\nkw_only:bool |None=_Unset,\npattern:str |typing.Pattern[str]|None=_Unset,\nstrict:bool |None=_Unset,\ncoerce_numbers_to_str:bool |None=_Unset,\ngt:annotated_types.SupportsGt |None=_Unset,\nge:annotated_types.SupportsGe |None=_Unset,\nlt:annotated_types.SupportsLt |None=_Unset,\nle:annotated_types.SupportsLe |None=_Unset,\nmultiple_of:float |None=_Unset,\nallow_inf_nan:bool |None=_Unset,\nmax_digits:int |None=_Unset,\ndecimal_places:int |None=_Unset,\nmin_length:int |None=_Unset,\nmax_length:int |None=_Unset,\nunion_mode:Literal['smart','left_to_right']=_Unset,\nfail_fast:bool |None=_Unset,\n**extra:Unpack[_EmptyKwargs],\n)->_T:...\n@overload\ndef Field(\n*,\nalias:str |None=_Unset,\nalias_priority:int |None=_Unset,\nvalidation_alias:str |AliasPath |AliasChoices |None=_Unset,\nserialization_alias:str |None=_Unset,\ntitle:str |None=_Unset,\nfield_title_generator:Callable[[str,FieldInfo],str]|None=_Unset,\ndescription:str |None=_Unset,\nexamples:list[Any]|None=_Unset,\nexclude:bool |None=_Unset,\ndiscriminator:str |types.Discriminator |None=_Unset,\ndeprecated:Deprecated |str |bool |None=_Unset,\njson_schema_extra:JsonDict |Callable[[JsonDict],None]|None=_Unset,\nfrozen:bool |None=_Unset,\nvalidate_default:bool |None=_Unset,\nrepr:bool=_Unset,\ninit:bool |None=_Unset,\ninit_var:bool |None=_Unset,\nkw_only:bool |None=_Unset,\npattern:str |typing.Pattern[str]|None=_Unset,\nstrict:bool |None=_Unset,\ncoerce_numbers_to_str:bool |None=_Unset,\ngt:annotated_types.SupportsGt |None=_Unset,\nge:annotated_types.SupportsGe |None=_Unset,\nlt:annotated_types.SupportsLt |None=_Unset,\nle:annotated_types.SupportsLe |None=_Unset,\nmultiple_of:float |None=_Unset,\nallow_inf_nan:bool |None=_Unset,\nmax_digits:int |None=_Unset,\ndecimal_places:int |None=_Unset,\nmin_length:int |None=_Unset,\nmax_length:int |None=_Unset,\nunion_mode:Literal['smart','left_to_right']=_Unset,\nfail_fast:bool |None=_Unset,\n**extra:Unpack[_EmptyKwargs],\n)->Any:...\ndef Field(\ndefault:Any=PydanticUndefined,\n*,\ndefault_factory:Callable[[],Any]|Callable[[dict[str,Any]],Any]|None=_Unset,\nalias:str |None=_Unset,\nalias_priority:int |None=_Unset,\nvalidation_alias:str |AliasPath |AliasChoices |None=_Unset,\nserialization_alias:str |None=_Unset,\ntitle:str |None=_Unset,\nfield_title_generator:Callable[[str,FieldInfo],str]|None=_Unset,\ndescription:str |None=_Unset,\nexamples:list[Any]|None=_Unset,\nexclude:bool |None=_Unset,\ndiscriminator:str |types.Discriminator |None=_Unset,\ndeprecated:Deprecated |str |bool |None=_Unset,\njson_schema_extra:JsonDict |Callable[[JsonDict],None]|None=_Unset,\nfrozen:bool |None=_Unset,\nvalidate_default:bool |None=_Unset,\nrepr:bool=_Unset,\ninit:bool |None=_Unset,\ninit_var:bool |None=_Unset,\nkw_only:bool |None=_Unset,\npattern:str |typing.Pattern[str]|None=_Unset,\nstrict:bool |None=_Unset,\ncoerce_numbers_to_str:bool |None=_Unset,\ngt:annotated_types.SupportsGt |None=_Unset,\nge:annotated_types.SupportsGe |None=_Unset,\nlt:annotated_types.SupportsLt |None=_Unset,\nle:annotated_types.SupportsLe |None=_Unset,\nmultiple_of:float |None=_Unset,\nallow_inf_nan:bool |None=_Unset,\nmax_digits:int |None=_Unset,\ndecimal_places:int |None=_Unset,\nmin_length:int |None=_Unset,\nmax_length:int |None=_Unset,\nunion_mode:Literal['smart','left_to_right']=_Unset,\nfail_fast:bool |None=_Unset,\n**extra:Unpack[_EmptyKwargs],\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n const=extra.pop('const',None)\n if const is not None:\n  raise PydanticUserError('`const` is removed, use `Literal` instead',code='removed-kwargs')\n  \n min_items=extra.pop('min_items',None)\n if min_items is not None:\n  warn('`min_items` is deprecated and will be removed, use `min_length` instead',DeprecationWarning)\n  if min_length in(None,_Unset):\n   min_length=min_items\n   \n max_items=extra.pop('max_items',None)\n if max_items is not None:\n  warn('`max_items` is deprecated and will be removed, use `max_length` instead',DeprecationWarning)\n  if max_length in(None,_Unset):\n   max_length=max_items\n   \n unique_items=extra.pop('unique_items',None)\n if unique_items is not None:\n  raise PydanticUserError(\n  (\n  '`unique_items` is removed, use `Set` instead'\n  '(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'\n  ),\n  code='removed-kwargs',\n  )\n  \n allow_mutation=extra.pop('allow_mutation',None)\n if allow_mutation is not None:\n  warn('`allow_mutation` is deprecated and will be removed. use `frozen` instead',DeprecationWarning)\n  if allow_mutation is False:\n   frozen=True\n   \n regex=extra.pop('regex',None)\n if regex is not None:\n  raise PydanticUserError('`regex` is removed. use `pattern` instead',code='removed-kwargs')\n  \n if extra:\n  warn(\n  'Using extra keyword arguments on `Field` is deprecated and will be removed.'\n  ' Use `json_schema_extra` instead.'\n  f' (Extra keys: {\", \".join(k.__repr__()for k in extra.keys())})',\n  DeprecationWarning,\n  )\n  if not json_schema_extra or json_schema_extra is _Unset:\n   json_schema_extra=extra\n   \n if(\n validation_alias\n and validation_alias is not _Unset\n and not isinstance(validation_alias,(str,AliasChoices,AliasPath))\n ):\n  raise TypeError('Invalid `validation_alias` type. it should be `str`, `AliasChoices`, or `AliasPath`')\n  \n if serialization_alias in(_Unset,None)and isinstance(alias,str):\n  serialization_alias=alias\n  \n if validation_alias in(_Unset,None):\n  validation_alias=alias\n  \n include=extra.pop('include',None)\n if include is not None:\n  warn('`include` is deprecated and does nothing. It will be removed, use `exclude` instead',DeprecationWarning)\n  \n return FieldInfo.from_field(\n default,\n default_factory=default_factory,\n alias=alias,\n alias_priority=alias_priority,\n validation_alias=validation_alias,\n serialization_alias=serialization_alias,\n title=title,\n field_title_generator=field_title_generator,\n description=description,\n examples=examples,\n exclude=exclude,\n discriminator=discriminator,\n deprecated=deprecated,\n json_schema_extra=json_schema_extra,\n frozen=frozen,\n pattern=pattern,\n validate_default=validate_default,\n repr=repr,\n init=init,\n init_var=init_var,\n kw_only=kw_only,\n coerce_numbers_to_str=coerce_numbers_to_str,\n strict=strict,\n gt=gt,\n ge=ge,\n lt=lt,\n le=le,\n multiple_of=multiple_of,\n min_length=min_length,\n max_length=max_length,\n allow_inf_nan=allow_inf_nan,\n max_digits=max_digits,\n decimal_places=decimal_places,\n union_mode=union_mode,\n fail_fast=fail_fast,\n )\n \n \n_FIELD_ARG_NAMES=set(inspect.signature(Field).parameters)\n_FIELD_ARG_NAMES.remove('extra')\n\n\nclass ModelPrivateAttr(_repr.Representation):\n ''\n\n\n\n\n\n\n\n\n\n \n \n __slots__=('default','default_factory')\n \n def __init__(\n self,default:Any=PydanticUndefined,*,default_factory:typing.Callable[[],Any]|None=None\n )->None:\n  if default is Ellipsis:\n   self.default=PydanticUndefined\n  else:\n   self.default=default\n  self.default_factory=default_factory\n  \n if not typing.TYPE_CHECKING:\n \n \n  def __getattr__(self,item:str)->Any:\n   ''\n\n   \n   if item in{'__get__','__set__','__delete__'}:\n    if hasattr(self.default,item):\n     return getattr(self.default,item)\n   raise AttributeError(f'{type(self).__name__ !r} object has no attribute {item !r}')\n   \n def __set_name__(self,cls:type[Any],name:str)->None:\n  ''\n  default=self.default\n  if default is PydanticUndefined:\n   return\n  set_name=getattr(default,'__set_name__',None)\n  if callable(set_name):\n   set_name(cls,name)\n   \n def get_default(self)->Any:\n  ''\n\n\n\n\n\n\n\n  \n  return _utils.smart_deepcopy(self.default)if self.default_factory is None else self.default_factory()\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,self.__class__)and(self.default,self.default_factory)==(\n  other.default,\n  other.default_factory,\n  )\n  \n  \n  \n  \n@overload\ndef PrivateAttr(\ndefault:_T,\n*,\ninit:Literal[False]=False,\n)->_T:...\n@overload\ndef PrivateAttr(\n*,\ndefault_factory:Callable[[],_T],\ninit:Literal[False]=False,\n)->_T:...\n@overload\ndef PrivateAttr(\n*,\ninit:Literal[False]=False,\n)->Any:...\ndef PrivateAttr(\ndefault:Any=PydanticUndefined,\n*,\ndefault_factory:Callable[[],Any]|None=None,\ninit:Literal[False]=False,\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if default is not PydanticUndefined and default_factory is not None:\n  raise TypeError('cannot specify both default and default_factory')\n  \n return ModelPrivateAttr(\n default,\n default_factory=default_factory,\n )\n \n \n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass ComputedFieldInfo:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@computed_field'\n wrapped_property:property\n return_type:Any\n alias:str |None\n alias_priority:int |None\n title:str |None\n field_title_generator:typing.Callable[[str,ComputedFieldInfo],str]|None\n description:str |None\n deprecated:Deprecated |str |bool |None\n examples:list[Any]|None\n json_schema_extra:JsonDict |typing.Callable[[JsonDict],None]|None\n repr:bool\n \n @property\n def deprecation_message(self)->str |None:\n  ''\n  if self.deprecated is None:\n   return None\n  if isinstance(self.deprecated,bool):\n   return 'deprecated'if self.deprecated else None\n  return self.deprecated if isinstance(self.deprecated,str)else self.deprecated.message\n  \n  \ndef _wrapped_property_is_private(property_:cached_property |property)->bool:\n ''\n wrapped_name:str=''\n \n if isinstance(property_,property):\n  wrapped_name=getattr(property_.fget,'__name__','')\n elif isinstance(property_,cached_property):\n  wrapped_name=getattr(property_.func,'__name__','')\n  \n return wrapped_name.startswith('_')and not wrapped_name.startswith('__')\n \n \n \n \nPropertyT=typing.TypeVar('PropertyT')\n\n\n@typing.overload\ndef computed_field(\n*,\nalias:str |None=None,\nalias_priority:int |None=None,\ntitle:str |None=None,\nfield_title_generator:typing.Callable[[str,ComputedFieldInfo],str]|None=None,\ndescription:str |None=None,\ndeprecated:Deprecated |str |bool |None=None,\nexamples:list[Any]|None=None,\njson_schema_extra:JsonDict |typing.Callable[[JsonDict],None]|None=None,\nrepr:bool=True,\nreturn_type:Any=PydanticUndefined,\n)->typing.Callable[[PropertyT],PropertyT]:...\n\n\n@typing.overload\ndef computed_field(__func:PropertyT)->PropertyT:...\n\n\ndef computed_field(\nfunc:PropertyT |None=None,\n/,\n*,\nalias:str |None=None,\nalias_priority:int |None=None,\ntitle:str |None=None,\nfield_title_generator:typing.Callable[[str,ComputedFieldInfo],str]|None=None,\ndescription:str |None=None,\ndeprecated:Deprecated |str |bool |None=None,\nexamples:list[Any]|None=None,\njson_schema_extra:JsonDict |typing.Callable[[JsonDict],None]|None=None,\nrepr:bool |None=None,\nreturn_type:Any=PydanticUndefined,\n)->PropertyT |typing.Callable[[PropertyT],PropertyT]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def dec(f:Any)->Any:\n  nonlocal description,deprecated,return_type,alias_priority\n  unwrapped=_decorators.unwrap_wrapped_function(f)\n  \n  if description is None and unwrapped.__doc__:\n   description=inspect.cleandoc(unwrapped.__doc__)\n   \n  if deprecated is None and hasattr(unwrapped,'__deprecated__'):\n   deprecated=unwrapped.__deprecated__\n   \n   \n  f=_decorators.ensure_property(f)\n  alias_priority=(alias_priority or 2)if alias is not None else None\n  \n  if repr is None:\n   repr_:bool=not _wrapped_property_is_private(property_=f)\n  else:\n   repr_=repr\n   \n  dec_info=ComputedFieldInfo(\n  f,\n  return_type,\n  alias,\n  alias_priority,\n  title,\n  field_title_generator,\n  description,\n  deprecated,\n  examples,\n  json_schema_extra,\n  repr_,\n  )\n  return _decorators.PydanticDescriptorProxy(f,dec_info)\n  \n if func is None:\n  return dec\n else:\n  return dec(func)\n", ["__future__", "annotated_types", "copy", "dataclasses", "functools", "inspect", "pydantic", "pydantic._internal", "pydantic._internal._decorators", "pydantic._internal._fields", "pydantic._internal._generics", "pydantic._internal._internal_dataclass", "pydantic._internal._namespace_utils", "pydantic._internal._repr", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic.aliases", "pydantic.config", "pydantic.errors", "pydantic.json_schema", "pydantic.types", "pydantic.warnings", "pydantic_core", "sys", "typing", "typing_extensions", "warnings"]], "pydantic.dataclasses": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport sys\nimport types\nfrom typing import TYPE_CHECKING,Any,Callable,Generic,NoReturn,TypeVar,overload\nfrom warnings import warn\n\nfrom typing_extensions import Literal,TypeGuard,dataclass_transform\n\nfrom._internal import _config,_decorators,_namespace_utils,_typing_extra\nfrom._internal import _dataclasses as _pydantic_dataclasses\nfrom._migration import getattr_migration\nfrom.config import ConfigDict\nfrom.errors import PydanticUserError\nfrom.fields import Field,FieldInfo,PrivateAttr\n\nif TYPE_CHECKING:\n from._internal._dataclasses import PydanticDataclass\n from._internal._namespace_utils import MappingNamespace\n \n__all__='dataclass','rebuild_dataclass'\n\n_T=TypeVar('_T')\n\nif sys.version_info >=(3,10):\n\n @dataclass_transform(field_specifiers=(dataclasses.field,Field,PrivateAttr))\n @overload\n def dataclass(\n *,\n init:Literal[False]=False,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool=False,\n config:ConfigDict |type[object]|None=None,\n validate_on_init:bool |None=None,\n kw_only:bool=...,\n slots:bool=...,\n )->Callable[[type[_T]],type[PydanticDataclass]]:\n  ...\n  \n @dataclass_transform(field_specifiers=(dataclasses.field,Field,PrivateAttr))\n @overload\n def dataclass(\n _cls:type[_T],\n *,\n init:Literal[False]=False,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool |None=None,\n config:ConfigDict |type[object]|None=None,\n validate_on_init:bool |None=None,\n kw_only:bool=...,\n slots:bool=...,\n )->type[PydanticDataclass]:...\n \nelse:\n\n @dataclass_transform(field_specifiers=(dataclasses.field,Field,PrivateAttr))\n @overload\n def dataclass(\n *,\n init:Literal[False]=False,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool |None=None,\n config:ConfigDict |type[object]|None=None,\n validate_on_init:bool |None=None,\n )->Callable[[type[_T]],type[PydanticDataclass]]:\n  ...\n  \n @dataclass_transform(field_specifiers=(dataclasses.field,Field,PrivateAttr))\n @overload\n def dataclass(\n _cls:type[_T],\n *,\n init:Literal[False]=False,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool |None=None,\n config:ConfigDict |type[object]|None=None,\n validate_on_init:bool |None=None,\n )->type[PydanticDataclass]:...\n \n \n@dataclass_transform(field_specifiers=(dataclasses.field,Field,PrivateAttr))\ndef dataclass(\n_cls:type[_T]|None=None,\n*,\ninit:Literal[False]=False,\nrepr:bool=True,\neq:bool=True,\norder:bool=False,\nunsafe_hash:bool=False,\nfrozen:bool |None=None,\nconfig:ConfigDict |type[object]|None=None,\nvalidate_on_init:bool |None=None,\nkw_only:bool=False,\nslots:bool=False,\n)->Callable[[type[_T]],type[PydanticDataclass]]|type[PydanticDataclass]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n assert init is False,'pydantic.dataclasses.dataclass only supports init=False'\n assert validate_on_init is not False,'validate_on_init=False is no longer supported'\n \n if sys.version_info >=(3,10):\n  kwargs={'kw_only':kw_only,'slots':slots}\n else:\n  kwargs={}\n  \n def make_pydantic_fields_compatible(cls:type[Any])->None:\n  ''\n\n\n\n\n  \n  for annotation_cls in cls.__mro__:\n  \n  \n   annotations=getattr(annotation_cls,'__annotations__',[])\n   for field_name in annotations:\n    field_value=getattr(cls,field_name,None)\n    \n    if not isinstance(field_value,FieldInfo):\n     continue\n     \n     \n    field_args:dict={'default':field_value}\n    \n    \n    if sys.version_info >=(3,10)and field_value.kw_only:\n     field_args['kw_only']=True\n     \n     \n    if field_value.repr is not True:\n     field_args['repr']=field_value.repr\n     \n    setattr(cls,field_name,dataclasses.field(**field_args))\n    \n    \n    if cls.__dict__.get('__annotations__')is None:\n     cls.__annotations__={}\n    cls.__annotations__[field_name]=annotations[field_name]\n    \n def create_dataclass(cls:type[Any])->type[PydanticDataclass]:\n  ''\n\n\n\n\n\n\n  \n  from._internal._utils import is_model_class\n  \n  if is_model_class(cls):\n   raise PydanticUserError(\n   f'Cannot create a Pydantic dataclass from {cls.__name__} as it is already a Pydantic model',\n   code='dataclass-on-model',\n   )\n   \n  original_cls=cls\n  \n  \n  \n  has_dataclass_base=any(dataclasses.is_dataclass(base)for base in cls.__bases__)\n  if not has_dataclass_base and config is not None and hasattr(cls,'__pydantic_config__'):\n   warn(\n   f'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass {cls.__name__}. '\n   f'The `config` specification from `dataclass` decorator will take priority.',\n   category=UserWarning,\n   stacklevel=2,\n   )\n   \n   \n  config_dict=config if config is not None else getattr(cls,'__pydantic_config__',None)\n  config_wrapper=_config.ConfigWrapper(config_dict)\n  decorators=_decorators.DecoratorInfos.build(cls)\n  \n  \n  \n  \n  original_doc=cls.__doc__\n  \n  if _pydantic_dataclasses.is_builtin_dataclass(cls):\n  \n  \n   original_doc=None\n   \n   \n   \n   \n   bases=(cls,)\n   if issubclass(cls,Generic):\n    generic_base=Generic[cls.__parameters__]\n    bases=bases+(generic_base,)\n   cls=types.new_class(cls.__name__,bases)\n   \n  make_pydantic_fields_compatible(cls)\n  \n  \n  if frozen is not None:\n   frozen_=frozen\n   if config_wrapper.frozen:\n   \n    warn(\n    f'`frozen` is set via both the `dataclass` decorator and `config` for dataclass {cls.__name__ !r}.'\n    'This is not recommended. The `frozen` specification on `dataclass` will take priority.',\n    category=UserWarning,\n    stacklevel=2,\n    )\n  else:\n   frozen_=config_wrapper.frozen or False\n   \n  cls=dataclasses.dataclass(\n  cls,\n  \n  init=True,\n  repr=repr,\n  eq=eq,\n  order=order,\n  unsafe_hash=unsafe_hash,\n  frozen=frozen_,\n  **kwargs,\n  )\n  \n  cls.__pydantic_decorators__=decorators\n  cls.__doc__=original_doc\n  cls.__module__=original_cls.__module__\n  cls.__qualname__=original_cls.__qualname__\n  cls.__pydantic_complete__=False\n  \n  \n  \n  _pydantic_dataclasses.complete_dataclass(cls,config_wrapper,raise_errors=False)\n  return cls\n  \n return create_dataclass if _cls is None else create_dataclass(_cls)\n \n \n__getattr__=getattr_migration(__name__)\n\nif(3,8)<=sys.version_info <(3,11):\n\n\n\n def _call_initvar(*args:Any,**kwargs:Any)->NoReturn:\n  ''\n\n\n  \n  raise TypeError(\"'InitVar' object is not callable\")\n  \n dataclasses.InitVar.__call__=_call_initvar\n \n \ndef rebuild_dataclass(\ncls:type[PydanticDataclass],\n*,\nforce:bool=False,\nraise_errors:bool=True,\n_parent_namespace_depth:int=2,\n_types_namespace:MappingNamespace |None=None,\n)->bool |None:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if not force and cls.__pydantic_complete__:\n  return None\n  \n if '__pydantic_core_schema__'in cls.__dict__:\n  delattr(cls,'__pydantic_core_schema__')\n  \n if _types_namespace is not None:\n  rebuild_ns=_types_namespace\n elif _parent_namespace_depth >0:\n  rebuild_ns=_typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth,force=True)or{}\n else:\n  rebuild_ns={}\n  \n ns_resolver=_namespace_utils.NsResolver(\n parent_namespace=rebuild_ns,\n )\n \n return _pydantic_dataclasses.complete_dataclass(\n cls,\n _config.ConfigWrapper(cls.__pydantic_config__,check=False),\n raise_errors=raise_errors,\n ns_resolver=ns_resolver,\n \n \n \n \n _force_build=True,\n )\n \n \ndef is_pydantic_dataclass(class_:type[Any],/)->TypeGuard[type[PydanticDataclass]]:\n ''\n\n\n\n\n\n\n \n try:\n  return '__pydantic_validator__'in class_.__dict__ and dataclasses.is_dataclass(class_)\n except AttributeError:\n  return False\n", ["__future__", "dataclasses", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._dataclasses", "pydantic._internal._decorators", "pydantic._internal._namespace_utils", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic._migration", "pydantic.config", "pydantic.errors", "pydantic.fields", "sys", "types", "typing", "typing_extensions", "warnings"]], "pydantic.json_schema": [".py", "''\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport inspect\nimport math\nimport os\nimport re\nimport warnings\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom enum import Enum\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nCounter,\nDict,\nHashable,\nIterable,\nNewType,\nPattern,\nSequence,\nTuple,\nTypeVar,\nUnion,\ncast,\noverload,\n)\n\nimport pydantic_core\nfrom pydantic_core import CoreSchema,PydanticOmit,core_schema,to_jsonable_python\nfrom pydantic_core.core_schema import ComputedField\nfrom typing_extensions import Annotated,Literal,TypeAlias,assert_never,deprecated,final\n\nfrom pydantic.warnings import PydanticDeprecatedSince26,PydanticDeprecatedSince29\n\nfrom._internal import(\n_config,\n_core_metadata,\n_core_utils,\n_decorators,\n_internal_dataclass,\n_mock_val_ser,\n_schema_generation_shared,\n_typing_extra,\n)\nfrom.annotated_handlers import GetJsonSchemaHandler\nfrom.config import JsonDict,JsonValue\nfrom.errors import PydanticInvalidForJsonSchema,PydanticSchemaGenerationError,PydanticUserError\n\nif TYPE_CHECKING:\n from. import ConfigDict\n from._internal._core_utils import CoreSchemaField,CoreSchemaOrField\n from._internal._dataclasses import PydanticDataclass\n from._internal._schema_generation_shared import GetJsonSchemaFunction\n from.main import BaseModel\n \n \nCoreSchemaOrFieldType=Literal[core_schema.CoreSchemaType,core_schema.CoreSchemaFieldType]\n''\n\n\n\n\n\nJsonSchemaValue=Dict[str,Any]\n''\n\n\n\nJsonSchemaMode=Literal['validation','serialization']\n''\n\n\n\n\n\n\n\n\n_MODE_TITLE_MAPPING:dict[JsonSchemaMode,str]={'validation':'Input','serialization':'Output'}\n\n\nJsonSchemaWarningKind=Literal['skipped-choice','non-serializable-default','skipped-discriminator']\n''\n\n\n\n\n\n\n\nclass PydanticJsonSchemaWarning(UserWarning):\n ''\n\n\n\n \n \n \n \nDEFAULT_REF_TEMPLATE='#/$defs/{model}'\n''\n\n\n\n\nCoreRef=NewType('CoreRef',str)\n\n\n\nDefsRef=NewType('DefsRef',str)\n\n\nJsonRef=NewType('JsonRef',str)\n\nCoreModeRef=Tuple[CoreRef,JsonSchemaMode]\nJsonSchemaKeyT=TypeVar('JsonSchemaKeyT',bound=Hashable)\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass _DefinitionsRemapping:\n defs_remapping:dict[DefsRef,DefsRef]\n json_remapping:dict[JsonRef,JsonRef]\n \n @staticmethod\n def from_prioritized_choices(\n prioritized_choices:dict[DefsRef,list[DefsRef]],\n defs_to_json:dict[DefsRef,JsonRef],\n definitions:dict[DefsRef,JsonSchemaValue],\n )->_DefinitionsRemapping:\n  ''\n\n\n  \n  \n  \n  \n  \n  copied_definitions=deepcopy(definitions)\n  definitions_schema={'$defs':copied_definitions}\n  for _iter in range(100):\n  \n   schemas_for_alternatives:dict[DefsRef,list[JsonSchemaValue]]=defaultdict(list)\n   for defs_ref in copied_definitions:\n    alternatives=prioritized_choices[defs_ref]\n    for alternative in alternatives:\n     schemas_for_alternatives[alternative].append(copied_definitions[defs_ref])\n     \n     \n     \n   for defs_ref in schemas_for_alternatives:\n    schemas_for_alternatives[defs_ref]=_deduplicate_schemas(schemas_for_alternatives[defs_ref])\n    \n    \n   defs_remapping:dict[DefsRef,DefsRef]={}\n   json_remapping:dict[JsonRef,JsonRef]={}\n   for original_defs_ref in definitions:\n    alternatives=prioritized_choices[original_defs_ref]\n    \n    remapped_defs_ref=next(x for x in alternatives if len(schemas_for_alternatives[x])==1)\n    defs_remapping[original_defs_ref]=remapped_defs_ref\n    json_remapping[defs_to_json[original_defs_ref]]=defs_to_json[remapped_defs_ref]\n   remapping=_DefinitionsRemapping(defs_remapping,json_remapping)\n   new_definitions_schema=remapping.remap_json_schema({'$defs':copied_definitions})\n   if definitions_schema ==new_definitions_schema:\n   \n    return remapping\n   definitions_schema=new_definitions_schema\n   \n  raise PydanticInvalidForJsonSchema('Failed to simplify the JSON schema definitions')\n  \n def remap_defs_ref(self,ref:DefsRef)->DefsRef:\n  return self.defs_remapping.get(ref,ref)\n  \n def remap_json_ref(self,ref:JsonRef)->JsonRef:\n  return self.json_remapping.get(ref,ref)\n  \n def remap_json_schema(self,schema:Any)->Any:\n  ''\n\n  \n  if isinstance(schema,str):\n  \n   return self.remap_json_ref(JsonRef(schema))\n  elif isinstance(schema,list):\n   return[self.remap_json_schema(item)for item in schema]\n  elif isinstance(schema,dict):\n   for key,value in schema.items():\n    if key =='$ref'and isinstance(value,str):\n     schema['$ref']=self.remap_json_ref(JsonRef(value))\n    elif key =='$defs':\n     schema['$defs']={\n     self.remap_defs_ref(DefsRef(key)):self.remap_json_schema(value)\n     for key,value in schema['$defs'].items()\n     }\n    else:\n     schema[key]=self.remap_json_schema(value)\n  return schema\n  \n  \nclass GenerateJsonSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n schema_dialect='https://json-schema.org/draft/2020-12/schema'\n \n \n \n ignored_warning_kinds:set[JsonSchemaWarningKind]={'skipped-choice'}\n \n def __init__(self,by_alias:bool=True,ref_template:str=DEFAULT_REF_TEMPLATE):\n  self.by_alias=by_alias\n  self.ref_template=ref_template\n  \n  self.core_to_json_refs:dict[CoreModeRef,JsonRef]={}\n  self.core_to_defs_refs:dict[CoreModeRef,DefsRef]={}\n  self.defs_to_core_refs:dict[DefsRef,CoreModeRef]={}\n  self.json_to_defs_refs:dict[JsonRef,DefsRef]={}\n  \n  self.definitions:dict[DefsRef,JsonSchemaValue]={}\n  self._config_wrapper_stack=_config.ConfigWrapperStack(_config.ConfigWrapper({}))\n  \n  self._mode:JsonSchemaMode='validation'\n  \n  \n  \n  \n  \n  self._prioritized_defsref_choices:dict[DefsRef,list[DefsRef]]={}\n  self._collision_counter:dict[str,int]=defaultdict(int)\n  self._collision_index:dict[str,int]={}\n  \n  self._schema_type_to_method=self.build_schema_type_to_method()\n  \n  \n  \n  \n  \n  \n  \n  self._core_defs_invalid_for_json_schema:dict[DefsRef,PydanticInvalidForJsonSchema]={}\n  \n  \n  \n  self._used=False\n  \n @property\n def _config(self)->_config.ConfigWrapper:\n  return self._config_wrapper_stack.tail\n  \n @property\n def mode(self)->JsonSchemaMode:\n  if self._config.json_schema_mode_override is not None:\n   return self._config.json_schema_mode_override\n  else:\n   return self._mode\n   \n def build_schema_type_to_method(\n self,\n )->dict[CoreSchemaOrFieldType,Callable[[CoreSchemaOrField],JsonSchemaValue]]:\n  ''\n\n\n\n\n\n\n  \n  mapping:dict[CoreSchemaOrFieldType,Callable[[CoreSchemaOrField],JsonSchemaValue]]={}\n  core_schema_types:list[CoreSchemaOrFieldType]=_typing_extra.literal_values(\n  CoreSchemaOrFieldType\n  )\n  for key in core_schema_types:\n   method_name=f\"{key.replace('-','_')}_schema\"\n   try:\n    mapping[key]=getattr(self,method_name)\n   except AttributeError as e:\n    if os.environ['PYDANTIC_PRIVATE_ALLOW_UNHANDLED_SCHEMA_TYPES']=='1':\n     continue\n    raise TypeError(\n    f'No method for generating JsonSchema for core_schema.type={key !r} '\n    f'(expected: {type(self).__name__}.{method_name})'\n    )from e\n  return mapping\n  \n def generate_definitions(\n self,inputs:Sequence[tuple[JsonSchemaKeyT,JsonSchemaMode,core_schema.CoreSchema]]\n )->tuple[dict[tuple[JsonSchemaKeyT,JsonSchemaMode],JsonSchemaValue],dict[DefsRef,JsonSchemaValue]]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if self._used:\n   raise PydanticUserError(\n   'This JSON schema generator has already been used to generate a JSON schema. '\n   f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n   code='json-schema-already-used',\n   )\n   \n  for _,mode,schema in inputs:\n   self._mode=mode\n   self.generate_inner(schema)\n   \n  definitions_remapping=self._build_definitions_remapping()\n  \n  json_schemas_map:dict[tuple[JsonSchemaKeyT,JsonSchemaMode],DefsRef]={}\n  for key,mode,schema in inputs:\n   self._mode=mode\n   json_schema=self.generate_inner(schema)\n   json_schemas_map[(key,mode)]=definitions_remapping.remap_json_schema(json_schema)\n   \n  json_schema={'$defs':self.definitions}\n  json_schema=definitions_remapping.remap_json_schema(json_schema)\n  self._used=True\n  return json_schemas_map,self.sort(json_schema['$defs'])\n  \n def generate(self,schema:CoreSchema,mode:JsonSchemaMode='validation')->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  self._mode=mode\n  if self._used:\n   raise PydanticUserError(\n   'This JSON schema generator has already been used to generate a JSON schema. '\n   f'You must create a new instance of {type(self).__name__} to generate a new JSON schema.',\n   code='json-schema-already-used',\n   )\n   \n  json_schema:JsonSchemaValue=self.generate_inner(schema)\n  json_ref_counts=self.get_json_ref_counts(json_schema)\n  \n  ref=cast(JsonRef,json_schema.get('$ref'))\n  while ref is not None:\n   ref_json_schema=self.get_schema_from_definitions(ref)\n   if json_ref_counts[ref]==1 and ref_json_schema is not None and len(json_schema)==1:\n   \n    json_schema=ref_json_schema.copy()\n    json_ref_counts[ref]-=1\n    ref=cast(JsonRef,json_schema.get('$ref'))\n   ref=None\n   \n  self._garbage_collect_definitions(json_schema)\n  definitions_remapping=self._build_definitions_remapping()\n  \n  if self.definitions:\n   json_schema['$defs']=self.definitions\n   \n  json_schema=definitions_remapping.remap_json_schema(json_schema)\n  \n  \n  \n  \n  \n  self._used=True\n  return self.sort(json_schema)\n  \n def generate_inner(self,schema:CoreSchemaOrField)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  if 'ref'in schema:\n   core_ref=CoreRef(schema['ref'])\n   core_mode_ref=(core_ref,self.mode)\n   if core_mode_ref in self.core_to_defs_refs and self.core_to_defs_refs[core_mode_ref]in self.definitions:\n    return{'$ref':self.core_to_json_refs[core_mode_ref]}\n    \n  def populate_defs(core_schema:CoreSchema,json_schema:JsonSchemaValue)->JsonSchemaValue:\n   if 'ref'in core_schema:\n    core_ref=CoreRef(core_schema['ref'])\n    defs_ref,ref_json_schema=self.get_cache_defs_ref_schema(core_ref)\n    json_ref=JsonRef(ref_json_schema['$ref'])\n    \n    \n    \n    if json_schema.get('$ref',None)!=json_ref:\n     self.definitions[defs_ref]=json_schema\n     self._core_defs_invalid_for_json_schema.pop(defs_ref,None)\n    json_schema=ref_json_schema\n   return json_schema\n   \n  def handler_func(schema_or_field:CoreSchemaOrField)->JsonSchemaValue:\n   ''\n\n\n\n\n\n\n\n\n\n   \n   \n   json_schema:JsonSchemaValue |None=None\n   if self.mode =='serialization'and 'serialization'in schema_or_field:\n   \n   \n   \n    ser_schema=schema_or_field['serialization']\n    json_schema=self.ser_schema(ser_schema)\n    \n    \n    \n    if(\n    json_schema is not None\n    and ser_schema.get('when_used')in('unless-none','json-unless-none')\n    and schema_or_field['type']=='nullable'\n    ):\n     json_schema=self.get_flattened_anyof([{'type':'null'},json_schema])\n   if json_schema is None:\n    if _core_utils.is_core_schema(schema_or_field)or _core_utils.is_core_schema_field(schema_or_field):\n     generate_for_schema_type=self._schema_type_to_method[schema_or_field['type']]\n     json_schema=generate_for_schema_type(schema_or_field)\n    else:\n     raise TypeError(f'Unexpected schema type: schema={schema_or_field}')\n   if _core_utils.is_core_schema(schema_or_field):\n    json_schema=populate_defs(schema_or_field,json_schema)\n   return json_schema\n   \n  current_handler=_schema_generation_shared.GenerateJsonSchemaHandler(self,handler_func)\n  \n  metadata=cast(_core_metadata.CoreMetadata,schema.get('metadata',{}))\n  \n  \n  \n  if js_updates :=metadata.get('pydantic_js_updates'):\n  \n   def js_updates_handler_func(\n   schema_or_field:CoreSchemaOrField,\n   current_handler:GetJsonSchemaHandler=current_handler,\n   )->JsonSchemaValue:\n    json_schema={**current_handler(schema_or_field),**js_updates}\n    return json_schema\n    \n   current_handler=_schema_generation_shared.GenerateJsonSchemaHandler(self,js_updates_handler_func)\n   \n  if js_extra :=metadata.get('pydantic_js_extra'):\n  \n   def js_extra_handler_func(\n   schema_or_field:CoreSchemaOrField,\n   current_handler:GetJsonSchemaHandler=current_handler,\n   )->JsonSchemaValue:\n    json_schema=current_handler(schema_or_field)\n    if isinstance(js_extra,dict):\n     json_schema.update(to_jsonable_python(js_extra))\n    elif callable(js_extra):\n    \n     js_extra(json_schema)\n    return json_schema\n    \n   current_handler=_schema_generation_shared.GenerateJsonSchemaHandler(self,js_extra_handler_func)\n   \n  for js_modify_function in metadata.get('pydantic_js_functions',()):\n  \n   def new_handler_func(\n   schema_or_field:CoreSchemaOrField,\n   current_handler:GetJsonSchemaHandler=current_handler,\n   js_modify_function:GetJsonSchemaFunction=js_modify_function,\n   )->JsonSchemaValue:\n    json_schema=js_modify_function(schema_or_field,current_handler)\n    if _core_utils.is_core_schema(schema_or_field):\n     json_schema=populate_defs(schema_or_field,json_schema)\n    original_schema=current_handler.resolve_ref_schema(json_schema)\n    ref=json_schema.pop('$ref',None)\n    if ref and json_schema:\n     original_schema.update(json_schema)\n    return original_schema\n    \n   current_handler=_schema_generation_shared.GenerateJsonSchemaHandler(self,new_handler_func)\n   \n  for js_modify_function in metadata.get('pydantic_js_annotation_functions',()):\n  \n   def new_handler_func(\n   schema_or_field:CoreSchemaOrField,\n   current_handler:GetJsonSchemaHandler=current_handler,\n   js_modify_function:GetJsonSchemaFunction=js_modify_function,\n   )->JsonSchemaValue:\n    json_schema=js_modify_function(schema_or_field,current_handler)\n    if _core_utils.is_core_schema(schema_or_field):\n     json_schema=populate_defs(schema_or_field,json_schema)\n    return json_schema\n    \n   current_handler=_schema_generation_shared.GenerateJsonSchemaHandler(self,new_handler_func)\n   \n  json_schema=current_handler(schema)\n  if _core_utils.is_core_schema(schema):\n   json_schema=populate_defs(schema,json_schema)\n  return json_schema\n  \n def sort(self,value:JsonSchemaValue,parent_key:str |None=None)->JsonSchemaValue:\n  ''\n\n\n\n  \n  sorted_dict:dict[str,JsonSchemaValue]={}\n  keys=value.keys()\n  if parent_key not in('properties','default'):\n   keys=sorted(keys)\n  for key in keys:\n   sorted_dict[key]=self._sort_recursive(value[key],parent_key=key)\n  return sorted_dict\n  \n def _sort_recursive(self,value:Any,parent_key:str |None=None)->Any:\n  ''\n  if isinstance(value,dict):\n   sorted_dict:dict[str,JsonSchemaValue]={}\n   keys=value.keys()\n   if parent_key not in('properties','default'):\n    keys=sorted(keys)\n   for key in keys:\n    sorted_dict[key]=self._sort_recursive(value[key],parent_key=key)\n   return sorted_dict\n  elif isinstance(value,list):\n   sorted_list:list[JsonSchemaValue]=[]\n   for item in value:\n    sorted_list.append(self._sort_recursive(item,parent_key))\n   return sorted_list\n  else:\n   return value\n   \n   \n   \n def invalid_schema(self,schema:core_schema.InvalidSchema)->JsonSchemaValue:\n  ''\n  \n  raise RuntimeError('Cannot generate schema for invalid_schema. This is a bug! Please report it.')\n  \n def any_schema(self,schema:core_schema.AnySchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{}\n  \n def none_schema(self,schema:core_schema.NoneSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{'type':'null'}\n  \n def bool_schema(self,schema:core_schema.BoolSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{'type':'boolean'}\n  \n def int_schema(self,schema:core_schema.IntSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema:dict[str,Any]={'type':'integer'}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.numeric)\n  json_schema={k:v for k,v in json_schema.items()if v not in{math.inf,-math.inf}}\n  return json_schema\n  \n def float_schema(self,schema:core_schema.FloatSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema:dict[str,Any]={'type':'number'}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.numeric)\n  json_schema={k:v for k,v in json_schema.items()if v not in{math.inf,-math.inf}}\n  return json_schema\n  \n def decimal_schema(self,schema:core_schema.DecimalSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema=self.str_schema(core_schema.str_schema())\n  if self.mode =='validation':\n   multiple_of=schema.get('multiple_of')\n   le=schema.get('le')\n   ge=schema.get('ge')\n   lt=schema.get('lt')\n   gt=schema.get('gt')\n   json_schema={\n   'anyOf':[\n   self.float_schema(\n   core_schema.float_schema(\n   allow_inf_nan=schema.get('allow_inf_nan'),\n   multiple_of=None if multiple_of is None else float(multiple_of),\n   le=None if le is None else float(le),\n   ge=None if ge is None else float(ge),\n   lt=None if lt is None else float(lt),\n   gt=None if gt is None else float(gt),\n   )\n   ),\n   json_schema,\n   ],\n   }\n  return json_schema\n  \n def str_schema(self,schema:core_schema.StringSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema={'type':'string'}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.string)\n  if isinstance(json_schema.get('pattern'),Pattern):\n  \n   json_schema['pattern']=json_schema.get('pattern').pattern\n  return json_schema\n  \n def bytes_schema(self,schema:core_schema.BytesSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema={'type':'string','format':'base64url'if self._config.ser_json_bytes =='base64'else 'binary'}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.bytes)\n  return json_schema\n  \n def date_schema(self,schema:core_schema.DateSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{'type':'string','format':'date'}\n  \n def time_schema(self,schema:core_schema.TimeSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{'type':'string','format':'time'}\n  \n def datetime_schema(self,schema:core_schema.DatetimeSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{'type':'string','format':'date-time'}\n  \n def timedelta_schema(self,schema:core_schema.TimedeltaSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  if self._config.ser_json_timedelta =='float':\n   return{'type':'number'}\n  return{'type':'string','format':'duration'}\n  \n def literal_schema(self,schema:core_schema.LiteralSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  expected=[v.value if isinstance(v,Enum)else v for v in schema['expected']]\n  \n  expected=[to_jsonable_python(v)for v in expected]\n  \n  result:dict[str,Any]={}\n  if len(expected)==1:\n   result['const']=expected[0]\n  else:\n   result['enum']=expected\n   \n  types={type(e)for e in expected}\n  if types =={str}:\n   result['type']='string'\n  elif types =={int}:\n   result['type']='integer'\n  elif types =={float}:\n   result['type']='number'\n  elif types =={bool}:\n   result['type']='boolean'\n  elif types =={list}:\n   result['type']='array'\n  elif types =={type(None)}:\n   result['type']='null'\n  return result\n  \n def enum_schema(self,schema:core_schema.EnumSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  enum_type=schema['cls']\n  description=None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n  if(\n  description =='An enumeration.'\n  ):\n   description=None\n  result:dict[str,Any]={'title':enum_type.__name__,'description':description}\n  result={k:v for k,v in result.items()if v is not None}\n  \n  expected=[to_jsonable_python(v.value)for v in schema['members']]\n  \n  result['enum']=expected\n  \n  types={type(e)for e in expected}\n  if isinstance(enum_type,str)or types =={str}:\n   result['type']='string'\n  elif isinstance(enum_type,int)or types =={int}:\n   result['type']='integer'\n  elif isinstance(enum_type,float)or types =={float}:\n   result['type']='number'\n  elif types =={bool}:\n   result['type']='boolean'\n  elif types =={list}:\n   result['type']='array'\n   \n  return result\n  \n def is_instance_schema(self,schema:core_schema.IsInstanceSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n  \n  return self.handle_invalid_for_json_schema(schema,f'core_schema.IsInstanceSchema ({schema[\"cls\"]})')\n  \n def is_subclass_schema(self,schema:core_schema.IsSubclassSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  return{}\n  \n def callable_schema(self,schema:core_schema.CallableSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n  \n  return self.handle_invalid_for_json_schema(schema,'core_schema.CallableSchema')\n  \n def list_schema(self,schema:core_schema.ListSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  items_schema={}if 'items_schema'not in schema else self.generate_inner(schema['items_schema'])\n  json_schema={'type':'array','items':items_schema}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.array)\n  return json_schema\n  \n @deprecated('`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',category=None)\n @final\n def tuple_positional_schema(self,schema:core_schema.TupleSchema)->JsonSchemaValue:\n  ''\n  warnings.warn(\n  '`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.',\n  PydanticDeprecatedSince26,\n  stacklevel=2,\n  )\n  return self.tuple_schema(schema)\n  \n @deprecated('`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.',category=None)\n @final\n def tuple_variable_schema(self,schema:core_schema.TupleSchema)->JsonSchemaValue:\n  ''\n  warnings.warn(\n  '`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.',\n  PydanticDeprecatedSince26,\n  stacklevel=2,\n  )\n  return self.tuple_schema(schema)\n  \n def tuple_schema(self,schema:core_schema.TupleSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n  \n  json_schema:JsonSchemaValue={'type':'array'}\n  if 'variadic_item_index'in schema:\n   variadic_item_index=schema['variadic_item_index']\n   if variadic_item_index >0:\n    json_schema['minItems']=variadic_item_index\n    json_schema['prefixItems']=[\n    self.generate_inner(item)for item in schema['items_schema'][:variadic_item_index]\n    ]\n   if variadic_item_index+1 ==len(schema['items_schema']):\n   \n    json_schema['items']=self.generate_inner(schema['items_schema'][variadic_item_index])\n   else:\n   \n   \n   \n    json_schema['items']=True\n  else:\n   prefixItems=[self.generate_inner(item)for item in schema['items_schema']]\n   if prefixItems:\n    json_schema['prefixItems']=prefixItems\n   json_schema['minItems']=len(prefixItems)\n   json_schema['maxItems']=len(prefixItems)\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.array)\n  return json_schema\n  \n def set_schema(self,schema:core_schema.SetSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self._common_set_schema(schema)\n  \n def frozenset_schema(self,schema:core_schema.FrozenSetSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self._common_set_schema(schema)\n  \n def _common_set_schema(self,schema:core_schema.SetSchema |core_schema.FrozenSetSchema)->JsonSchemaValue:\n  items_schema={}if 'items_schema'not in schema else self.generate_inner(schema['items_schema'])\n  json_schema={'type':'array','uniqueItems':True,'items':items_schema}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.array)\n  return json_schema\n  \n def generator_schema(self,schema:core_schema.GeneratorSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  items_schema={}if 'items_schema'not in schema else self.generate_inner(schema['items_schema'])\n  json_schema={'type':'array','items':items_schema}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.array)\n  return json_schema\n  \n def dict_schema(self,schema:core_schema.DictSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema:JsonSchemaValue={'type':'object'}\n  \n  keys_schema=self.generate_inner(schema['keys_schema']).copy()if 'keys_schema'in schema else{}\n  if '$ref'not in keys_schema:\n   keys_pattern=keys_schema.pop('pattern',None)\n   \n   keys_schema.pop('title',None)\n  else:\n  \n  \n  \n  \n  \n   keys_pattern=None\n   \n  values_schema=self.generate_inner(schema['values_schema']).copy()if 'values_schema'in schema else{}\n  \n  values_schema.pop('title',None)\n  \n  if values_schema or keys_pattern is not None:\n   if keys_pattern is None:\n    json_schema['additionalProperties']=values_schema\n   else:\n    json_schema['patternProperties']={keys_pattern:values_schema}\n    \n  if(\n  \n  (keys_schema.get('type')=='string'and len(keys_schema)>1)\n  \n  or '$ref'in keys_schema\n  ):\n   keys_schema.pop('type',None)\n   json_schema['propertyNames']=keys_schema\n   \n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.object)\n  return json_schema\n  \n def function_before_schema(self,schema:core_schema.BeforeValidatorFunctionSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  if self._mode =='validation'and(input_schema :=schema.get('json_schema_input_schema')):\n   return self.generate_inner(input_schema)\n   \n  return self.generate_inner(schema['schema'])\n  \n def function_after_schema(self,schema:core_schema.AfterValidatorFunctionSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['schema'])\n  \n def function_plain_schema(self,schema:core_schema.PlainValidatorFunctionSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  if self._mode =='validation'and(input_schema :=schema.get('json_schema_input_schema')):\n   return self.generate_inner(input_schema)\n   \n  return self.handle_invalid_for_json_schema(\n  schema,f'core_schema.PlainValidatorFunctionSchema ({schema[\"function\"]})'\n  )\n  \n def function_wrap_schema(self,schema:core_schema.WrapValidatorFunctionSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  if self._mode =='validation'and(input_schema :=schema.get('json_schema_input_schema')):\n   return self.generate_inner(input_schema)\n   \n  return self.generate_inner(schema['schema'])\n  \n def default_schema(self,schema:core_schema.WithDefaultSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema=self.generate_inner(schema['schema'])\n  \n  if 'default'not in schema:\n   return json_schema\n  default=schema['default']\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  if(\n  self.mode =='serialization'\n  and(ser_schema :=schema['schema'].get('serialization'))\n  and(ser_func :=ser_schema.get('function'))\n  and ser_schema.get('type')=='function-plain'\n  and not ser_schema.get('info_arg')\n  and not(default is None and ser_schema.get('when_used')in('unless-none','json-unless-none'))\n  ):\n   try:\n    default=ser_func(default)\n   except Exception:\n   \n   \n   \n   \n   \n   \n    self.emit_warning(\n    'non-serializable-default',\n    f'Unable to serialize value {default !r} with the plain serializer; excluding default from JSON schema',\n    )\n    return json_schema\n    \n  try:\n   encoded_default=self.encode_default(default)\n  except pydantic_core.PydanticSerializationError:\n   self.emit_warning(\n   'non-serializable-default',\n   f'Default value {default} is not JSON serializable; excluding default from JSON schema',\n   )\n   \n   return json_schema\n   \n  json_schema['default']=encoded_default\n  return json_schema\n  \n def nullable_schema(self,schema:core_schema.NullableSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  null_schema={'type':'null'}\n  inner_json_schema=self.generate_inner(schema['schema'])\n  \n  if inner_json_schema ==null_schema:\n   return null_schema\n  else:\n  \n  \n   return self.get_flattened_anyof([inner_json_schema,null_schema])\n   \n def union_schema(self,schema:core_schema.UnionSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  generated:list[JsonSchemaValue]=[]\n  \n  choices=schema['choices']\n  for choice in choices:\n  \n   choice_schema=choice[0]if isinstance(choice,tuple)else choice\n   try:\n    generated.append(self.generate_inner(choice_schema))\n   except PydanticOmit:\n    continue\n   except PydanticInvalidForJsonSchema as exc:\n    self.emit_warning('skipped-choice',exc.message)\n  if len(generated)==1:\n   return generated[0]\n  return self.get_flattened_anyof(generated)\n  \n def tagged_union_schema(self,schema:core_schema.TaggedUnionSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n  \n  generated:dict[str,JsonSchemaValue]={}\n  for k,v in schema['choices'].items():\n   if isinstance(k,Enum):\n    k=k.value\n   try:\n   \n   \n    generated[str(k)]=self.generate_inner(v).copy()\n   except PydanticOmit:\n    continue\n   except PydanticInvalidForJsonSchema as exc:\n    self.emit_warning('skipped-choice',exc.message)\n    \n  one_of_choices=_deduplicate_schemas(generated.values())\n  json_schema:JsonSchemaValue={'oneOf':one_of_choices}\n  \n  \n  openapi_discriminator=self._extract_discriminator(schema,one_of_choices)\n  if openapi_discriminator is not None:\n   json_schema['discriminator']={\n   'propertyName':openapi_discriminator,\n   'mapping':{k:v.get('$ref',v)for k,v in generated.items()},\n   }\n   \n  return json_schema\n  \n def _extract_discriminator(\n self,schema:core_schema.TaggedUnionSchema,one_of_choices:list[JsonDict]\n )->str |None:\n  ''\n  \n  openapi_discriminator:str |None=None\n  \n  if isinstance(schema['discriminator'],str):\n   return schema['discriminator']\n   \n  if isinstance(schema['discriminator'],list):\n  \n   if len(schema['discriminator'])==1 and isinstance(schema['discriminator'][0],str):\n    return schema['discriminator'][0]\n    \n    \n    \n    \n   for alias_path in schema['discriminator']:\n    if not isinstance(alias_path,list):\n     break\n    if len(alias_path)!=1:\n     continue\n    alias=alias_path[0]\n    if not isinstance(alias,str):\n     continue\n    alias_is_present_on_all_choices=True\n    for choice in one_of_choices:\n     try:\n      choice=self.resolve_ref_schema(choice)\n     except RuntimeError as exc:\n     \n     \n     \n      self.emit_warning('skipped-discriminator',str(exc))\n      choice={}\n     properties=choice.get('properties',{})\n     if not isinstance(properties,dict)or alias not in properties:\n      alias_is_present_on_all_choices=False\n      break\n    if alias_is_present_on_all_choices:\n     openapi_discriminator=alias\n     break\n  return openapi_discriminator\n  \n def chain_schema(self,schema:core_schema.ChainSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  step_index=0 if self.mode =='validation'else -1\n  return self.generate_inner(schema['steps'][step_index])\n  \n def lax_or_strict_schema(self,schema:core_schema.LaxOrStrictSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n  \n  \n  use_strict=schema.get('strict',False)\n  \n  \n  if use_strict:\n   return self.generate_inner(schema['strict_schema'])\n  else:\n   return self.generate_inner(schema['lax_schema'])\n   \n def json_or_python_schema(self,schema:core_schema.JsonOrPythonSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['json_schema'])\n  \n def typed_dict_schema(self,schema:core_schema.TypedDictSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  total=schema.get('total',True)\n  named_required_fields:list[tuple[str,bool,CoreSchemaField]]=[\n  (name,self.field_is_required(field,total),field)\n  for name,field in schema['fields'].items()\n  if self.field_is_present(field)\n  ]\n  if self.mode =='serialization':\n   named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields',[])))\n  cls=schema.get('cls')\n  config=_get_typed_dict_config(cls)\n  with self._config_wrapper_stack.push(config):\n   json_schema=self._named_required_fields_schema(named_required_fields)\n   \n  if cls is not None:\n   self._update_class_schema(json_schema,cls,config)\n  else:\n   extra=config.get('extra')\n   if extra =='forbid':\n    json_schema['additionalProperties']=False\n   elif extra =='allow':\n    json_schema['additionalProperties']=True\n    \n  return json_schema\n  \n @staticmethod\n def _name_required_computed_fields(\n computed_fields:list[ComputedField],\n )->list[tuple[str,bool,core_schema.ComputedField]]:\n  return[(field['property_name'],True,field)for field in computed_fields]\n  \n def _named_required_fields_schema(\n self,named_required_fields:Sequence[tuple[str,bool,CoreSchemaField]]\n )->JsonSchemaValue:\n  properties:dict[str,JsonSchemaValue]={}\n  required_fields:list[str]=[]\n  for name,required,field in named_required_fields:\n   if self.by_alias:\n    name=self._get_alias_name(field,name)\n   try:\n    field_json_schema=self.generate_inner(field).copy()\n   except PydanticOmit:\n    continue\n   if 'title'not in field_json_schema and self.field_title_should_be_set(field):\n    title=self.get_title_from_name(name)\n    field_json_schema['title']=title\n   field_json_schema=self.handle_ref_overrides(field_json_schema)\n   properties[name]=field_json_schema\n   if required:\n    required_fields.append(name)\n    \n  json_schema={'type':'object','properties':properties}\n  if required_fields:\n   json_schema['required']=required_fields\n  return json_schema\n  \n def _get_alias_name(self,field:CoreSchemaField,name:str)->str:\n  if field['type']=='computed-field':\n   alias:Any=field.get('alias',name)\n  elif self.mode =='validation':\n   alias=field.get('validation_alias',name)\n  else:\n   alias=field.get('serialization_alias',name)\n  if isinstance(alias,str):\n   name=alias\n  elif isinstance(alias,list):\n   alias=cast('list[str] | str',alias)\n   for path in alias:\n    if isinstance(path,list)and len(path)==1 and isinstance(path[0],str):\n    \n    \n     name=path[0]\n     break\n  else:\n   assert_never(alias)\n  return name\n  \n def typed_dict_field_schema(self,schema:core_schema.TypedDictField)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['schema'])\n  \n def dataclass_field_schema(self,schema:core_schema.DataclassField)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['schema'])\n  \n def model_field_schema(self,schema:core_schema.ModelField)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['schema'])\n  \n def computed_field_schema(self,schema:core_schema.ComputedField)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['return_schema'])\n  \n def model_schema(self,schema:core_schema.ModelSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  \n  \n  cls=cast('type[BaseModel]',schema['cls'])\n  config=cls.model_config\n  \n  with self._config_wrapper_stack.push(config):\n   json_schema=self.generate_inner(schema['schema'])\n   \n  self._update_class_schema(json_schema,cls,config)\n  \n  return json_schema\n  \n def _update_class_schema(self,json_schema:JsonSchemaValue,cls:type[Any],config:ConfigDict)->None:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  from.main import BaseModel\n  from.root_model import RootModel\n  \n  if(config_title :=config.get('title'))is not None:\n   json_schema.setdefault('title',config_title)\n  elif model_title_generator :=config.get('model_title_generator'):\n   title=model_title_generator(cls)\n   if not isinstance(title,str):\n    raise TypeError(f'model_title_generator {model_title_generator} must return str, not {title.__class__}')\n   json_schema.setdefault('title',title)\n  if 'title'not in json_schema:\n   json_schema['title']=cls.__name__\n   \n   \n  docstring=None if cls is BaseModel or dataclasses.is_dataclass(cls)else cls.__doc__\n  \n  if docstring:\n   json_schema.setdefault('description',inspect.cleandoc(docstring))\n  elif issubclass(cls,RootModel)and(root_description :=cls.__pydantic_fields__['root'].description):\n   json_schema.setdefault('description',root_description)\n   \n  extra=config.get('extra')\n  if 'additionalProperties'not in json_schema:\n   if extra =='allow':\n    json_schema['additionalProperties']=True\n   elif extra =='forbid':\n    json_schema['additionalProperties']=False\n    \n  json_schema_extra=config.get('json_schema_extra')\n  if issubclass(cls,BaseModel)and cls.__pydantic_root_model__:\n   root_json_schema_extra=cls.model_fields['root'].json_schema_extra\n   if json_schema_extra and root_json_schema_extra:\n    raise ValueError(\n    '\"model_config[\\'json_schema_extra\\']\" and \"Field.json_schema_extra\" on \"RootModel.root\"'\n    ' field must not be set simultaneously'\n    )\n   if root_json_schema_extra:\n    json_schema_extra=root_json_schema_extra\n    \n  if isinstance(json_schema_extra,(staticmethod,classmethod)):\n  \n   json_schema_extra=json_schema_extra.__get__(cls)\n   \n  if isinstance(json_schema_extra,dict):\n   json_schema.update(json_schema_extra)\n  elif callable(json_schema_extra):\n  \n   if len(inspect.signature(json_schema_extra).parameters)>1:\n    json_schema_extra(json_schema,cls)\n   else:\n    json_schema_extra(json_schema)\n  elif json_schema_extra is not None:\n   raise ValueError(\n   f\"model_config['json_schema_extra']={json_schema_extra} should be a dict, callable, or None\"\n   )\n   \n  if hasattr(cls,'__deprecated__'):\n   json_schema['deprecated']=True\n   \n def resolve_ref_schema(self,json_schema:JsonSchemaValue)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  if '$ref'not in json_schema:\n   return json_schema\n   \n  ref=json_schema['$ref']\n  schema_to_update=self.get_schema_from_definitions(JsonRef(ref))\n  if schema_to_update is None:\n   raise RuntimeError(f'Cannot update undefined schema for $ref={ref}')\n  return self.resolve_ref_schema(schema_to_update)\n  \n def model_fields_schema(self,schema:core_schema.ModelFieldsSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  named_required_fields:list[tuple[str,bool,CoreSchemaField]]=[\n  (name,self.field_is_required(field,total=True),field)\n  for name,field in schema['fields'].items()\n  if self.field_is_present(field)\n  ]\n  if self.mode =='serialization':\n   named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields',[])))\n  json_schema=self._named_required_fields_schema(named_required_fields)\n  extras_schema=schema.get('extras_schema',None)\n  if extras_schema is not None:\n   schema_to_update=self.resolve_ref_schema(json_schema)\n   schema_to_update['additionalProperties']=self.generate_inner(extras_schema)\n  return json_schema\n  \n def field_is_present(self,field:CoreSchemaField)->bool:\n  ''\n\n\n\n\n\n\n  \n  if self.mode =='serialization':\n  \n  \n   return not field.get('serialization_exclude')\n  elif self.mode =='validation':\n   return True\n  else:\n   assert_never(self.mode)\n   \n def field_is_required(\n self,\n field:core_schema.ModelField |core_schema.DataclassField |core_schema.TypedDictField,\n total:bool,\n )->bool:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if self.mode =='serialization'and self._config.json_schema_serialization_defaults_required:\n   return not field.get('serialization_exclude')\n  else:\n   if field['type']=='typed-dict-field':\n    return field.get('required',total)\n   else:\n    return field['schema']['type']!='default'\n    \n def dataclass_args_schema(self,schema:core_schema.DataclassArgsSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  named_required_fields:list[tuple[str,bool,CoreSchemaField]]=[\n  (field['name'],self.field_is_required(field,total=True),field)\n  for field in schema['fields']\n  if self.field_is_present(field)\n  ]\n  if self.mode =='serialization':\n   named_required_fields.extend(self._name_required_computed_fields(schema.get('computed_fields',[])))\n  return self._named_required_fields_schema(named_required_fields)\n  \n def dataclass_schema(self,schema:core_schema.DataclassSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  from._internal._dataclasses import is_builtin_dataclass\n  \n  cls=schema['cls']\n  config:ConfigDict=getattr(cls,'__pydantic_config__',cast('ConfigDict',{}))\n  \n  with self._config_wrapper_stack.push(config):\n   json_schema=self.generate_inner(schema['schema']).copy()\n   \n  self._update_class_schema(json_schema,cls,config)\n  \n  \n  if is_builtin_dataclass(cls):\n  \n   description=None\n  else:\n   description=None if cls.__doc__ is None else inspect.cleandoc(cls.__doc__)\n  if description:\n   json_schema['description']=description\n   \n  return json_schema\n  \n def arguments_schema(self,schema:core_schema.ArgumentsSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  prefer_positional=schema.get('metadata',{}).get('pydantic_js_prefer_positional_arguments')\n  \n  arguments=schema['arguments_schema']\n  kw_only_arguments=[a for a in arguments if a.get('mode')=='keyword_only']\n  kw_or_p_arguments=[a for a in arguments if a.get('mode')in{'positional_or_keyword',None}]\n  p_only_arguments=[a for a in arguments if a.get('mode')=='positional_only']\n  var_args_schema=schema.get('var_args_schema')\n  var_kwargs_schema=schema.get('var_kwargs_schema')\n  \n  if prefer_positional:\n   positional_possible=not kw_only_arguments and not var_kwargs_schema\n   if positional_possible:\n    return self.p_arguments_schema(p_only_arguments+kw_or_p_arguments,var_args_schema)\n    \n  keyword_possible=not p_only_arguments and not var_args_schema\n  if keyword_possible:\n   return self.kw_arguments_schema(kw_or_p_arguments+kw_only_arguments,var_kwargs_schema)\n   \n  if not prefer_positional:\n   positional_possible=not kw_only_arguments and not var_kwargs_schema\n   if positional_possible:\n    return self.p_arguments_schema(p_only_arguments+kw_or_p_arguments,var_args_schema)\n    \n  raise PydanticInvalidForJsonSchema(\n  'Unable to generate JSON schema for arguments validator with positional-only and keyword-only arguments'\n  )\n  \n def kw_arguments_schema(\n self,arguments:list[core_schema.ArgumentsParameter],var_kwargs_schema:CoreSchema |None\n )->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  properties:dict[str,JsonSchemaValue]={}\n  required:list[str]=[]\n  for argument in arguments:\n   name=self.get_argument_name(argument)\n   argument_schema=self.generate_inner(argument['schema']).copy()\n   argument_schema['title']=self.get_title_from_name(name)\n   properties[name]=argument_schema\n   \n   if argument['schema']['type']!='default':\n   \n   \n   \n    required.append(name)\n    \n  json_schema:JsonSchemaValue={'type':'object','properties':properties}\n  if required:\n   json_schema['required']=required\n   \n  if var_kwargs_schema:\n   additional_properties_schema=self.generate_inner(var_kwargs_schema)\n   if additional_properties_schema:\n    json_schema['additionalProperties']=additional_properties_schema\n  else:\n   json_schema['additionalProperties']=False\n  return json_schema\n  \n def p_arguments_schema(\n self,arguments:list[core_schema.ArgumentsParameter],var_args_schema:CoreSchema |None\n )->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  prefix_items:list[JsonSchemaValue]=[]\n  min_items=0\n  \n  for argument in arguments:\n   name=self.get_argument_name(argument)\n   \n   argument_schema=self.generate_inner(argument['schema']).copy()\n   argument_schema['title']=self.get_title_from_name(name)\n   prefix_items.append(argument_schema)\n   \n   if argument['schema']['type']!='default':\n   \n   \n   \n    min_items +=1\n    \n  json_schema:JsonSchemaValue={'type':'array'}\n  if prefix_items:\n   json_schema['prefixItems']=prefix_items\n  if min_items:\n   json_schema['minItems']=min_items\n   \n  if var_args_schema:\n   items_schema=self.generate_inner(var_args_schema)\n   if items_schema:\n    json_schema['items']=items_schema\n  else:\n   json_schema['maxItems']=len(prefix_items)\n   \n  return json_schema\n  \n def get_argument_name(self,argument:core_schema.ArgumentsParameter)->str:\n  ''\n\n\n\n\n\n\n  \n  name=argument['name']\n  if self.by_alias:\n   alias=argument.get('alias')\n   if isinstance(alias,str):\n    name=alias\n   else:\n    pass\n  return name\n  \n def call_schema(self,schema:core_schema.CallSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['arguments_schema'])\n  \n def custom_error_schema(self,schema:core_schema.CustomErrorSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return self.generate_inner(schema['schema'])\n  \n def json_schema(self,schema:core_schema.JsonSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  content_core_schema=schema.get('schema')or core_schema.any_schema()\n  content_json_schema=self.generate_inner(content_core_schema)\n  if self.mode =='validation':\n   return{'type':'string','contentMediaType':'application/json','contentSchema':content_json_schema}\n  else:\n  \n   return content_json_schema\n   \n def url_schema(self,schema:core_schema.UrlSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  json_schema={'type':'string','format':'uri','minLength':1}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.string)\n  return json_schema\n  \n def multi_host_url_schema(self,schema:core_schema.MultiHostUrlSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  \n  json_schema={'type':'string','format':'multi-host-uri','minLength':1}\n  self.update_with_validations(json_schema,schema,self.ValidationsMapping.string)\n  return json_schema\n  \n def uuid_schema(self,schema:core_schema.UuidSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  return{'type':'string','format':'uuid'}\n  \n def definitions_schema(self,schema:core_schema.DefinitionsSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  for definition in schema['definitions']:\n   try:\n    self.generate_inner(definition)\n   except PydanticInvalidForJsonSchema as e:\n    core_ref:CoreRef=CoreRef(definition['ref'])\n    self._core_defs_invalid_for_json_schema[self.get_defs_ref((core_ref,self.mode))]=e\n    continue\n  return self.generate_inner(schema['schema'])\n  \n def definition_ref_schema(self,schema:core_schema.DefinitionReferenceSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  core_ref=CoreRef(schema['schema_ref'])\n  _,ref_json_schema=self.get_cache_defs_ref_schema(core_ref)\n  return ref_json_schema\n  \n def ser_schema(\n self,schema:core_schema.SerSchema |core_schema.IncExSeqSerSchema |core_schema.IncExDictSerSchema\n )->JsonSchemaValue |None:\n  ''\n\n\n\n\n\n\n  \n  schema_type=schema['type']\n  if schema_type =='function-plain'or schema_type =='function-wrap':\n  \n   return_schema=schema.get('return_schema')\n   if return_schema is not None:\n    return self.generate_inner(return_schema)\n  elif schema_type =='format'or schema_type =='to-string':\n  \n   return self.str_schema(core_schema.str_schema())\n  elif schema['type']=='model':\n  \n   return self.generate_inner(schema['schema'])\n  return None\n  \n def complex_schema(self,schema:core_schema.ComplexSchema)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  return{'type':'string'}\n  \n  \n  \n def get_title_from_name(self,name:str)->str:\n  ''\n\n\n\n\n\n\n  \n  return name.title().replace('_',' ').strip()\n  \n def field_title_should_be_set(self,schema:CoreSchemaOrField)->bool:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  if _core_utils.is_core_schema_field(schema):\n   if schema['type']=='computed-field':\n    field_schema=schema['return_schema']\n   else:\n    field_schema=schema['schema']\n   return self.field_title_should_be_set(field_schema)\n   \n  elif _core_utils.is_core_schema(schema):\n   if schema.get('ref'):\n    return False\n   if schema['type']in{'default','nullable','definitions'}:\n    return self.field_title_should_be_set(schema['schema'])\n   if _core_utils.is_function_with_inner_schema(schema):\n    return self.field_title_should_be_set(schema['schema'])\n   if schema['type']=='definition-ref':\n   \n   \n    return False\n   return True\n   \n  else:\n   raise PydanticInvalidForJsonSchema(f'Unexpected schema type: schema={schema}')\n   \n def normalize_name(self,name:str)->str:\n  ''\n\n\n\n\n\n\n  \n  return re.sub(r'[^a-zA-Z0-9.\\-_]','_',name).replace('.','__')\n  \n def get_defs_ref(self,core_mode_ref:CoreModeRef)->DefsRef:\n  ''\n\n\n\n\n\n\n  \n  \n  core_ref,mode=core_mode_ref\n  components=re.split(r'([\\][,])',core_ref)\n  \n  components=[x.rsplit(':',1)[0]for x in components]\n  core_ref_no_id=''.join(components)\n  \n  components=[re.sub(r'(?:[^.[\\]]+\\.)+((?:[^.[\\]]+))',r'\\1',x)for x in components]\n  short_ref=''.join(components)\n  \n  mode_title=_MODE_TITLE_MAPPING[mode]\n  \n  \n  \n  \n  name=DefsRef(self.normalize_name(short_ref))\n  name_mode=DefsRef(self.normalize_name(short_ref)+f'-{mode_title}')\n  module_qualname=DefsRef(self.normalize_name(core_ref_no_id))\n  module_qualname_mode=DefsRef(f'{module_qualname}-{mode_title}')\n  module_qualname_id=DefsRef(self.normalize_name(core_ref))\n  occurrence_index=self._collision_index.get(module_qualname_id)\n  if occurrence_index is None:\n   self._collision_counter[module_qualname]+=1\n   occurrence_index=self._collision_index[module_qualname_id]=self._collision_counter[module_qualname]\n   \n  module_qualname_occurrence=DefsRef(f'{module_qualname}__{occurrence_index}')\n  module_qualname_occurrence_mode=DefsRef(f'{module_qualname_mode}__{occurrence_index}')\n  \n  self._prioritized_defsref_choices[module_qualname_occurrence_mode]=[\n  name,\n  name_mode,\n  module_qualname,\n  module_qualname_mode,\n  module_qualname_occurrence,\n  module_qualname_occurrence_mode,\n  ]\n  \n  return module_qualname_occurrence_mode\n  \n def get_cache_defs_ref_schema(self,core_ref:CoreRef)->tuple[DefsRef,JsonSchemaValue]:\n  ''\n\n\n\n\n\n\n\n  \n  core_mode_ref=(core_ref,self.mode)\n  maybe_defs_ref=self.core_to_defs_refs.get(core_mode_ref)\n  if maybe_defs_ref is not None:\n   json_ref=self.core_to_json_refs[core_mode_ref]\n   return maybe_defs_ref,{'$ref':json_ref}\n   \n  defs_ref=self.get_defs_ref(core_mode_ref)\n  \n  \n  self.core_to_defs_refs[core_mode_ref]=defs_ref\n  self.defs_to_core_refs[defs_ref]=core_mode_ref\n  \n  json_ref=JsonRef(self.ref_template.format(model=defs_ref))\n  self.core_to_json_refs[core_mode_ref]=json_ref\n  self.json_to_defs_refs[json_ref]=defs_ref\n  ref_json_schema={'$ref':json_ref}\n  return defs_ref,ref_json_schema\n  \n def handle_ref_overrides(self,json_schema:JsonSchemaValue)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n  \n  if '$ref'in json_schema:\n  \n   json_schema=json_schema.copy()\n   \n   referenced_json_schema=self.get_schema_from_definitions(JsonRef(json_schema['$ref']))\n   if referenced_json_schema is None:\n   \n   \n   \n    return json_schema\n   for k,v in list(json_schema.items()):\n    if k =='$ref':\n     continue\n    if k in referenced_json_schema and referenced_json_schema[k]==v:\n     del json_schema[k]\n     \n  return json_schema\n  \n def get_schema_from_definitions(self,json_ref:JsonRef)->JsonSchemaValue |None:\n  try:\n   def_ref=self.json_to_defs_refs[json_ref]\n   if def_ref in self._core_defs_invalid_for_json_schema:\n    raise self._core_defs_invalid_for_json_schema[def_ref]\n   return self.definitions.get(def_ref,None)\n  except KeyError:\n   if json_ref.startswith(('http://','https://')):\n    return None\n   raise\n   \n def encode_default(self,dft:Any)->Any:\n  ''\n\n\n\n\n\n\n\n\n  \n  from.type_adapter import TypeAdapter,_type_has_config\n  \n  config=self._config\n  try:\n   default=(\n   dft\n   if _type_has_config(type(dft))\n   else TypeAdapter(type(dft),config=config.config_dict).dump_python(dft,mode='json')\n   )\n  except PydanticSchemaGenerationError:\n   raise pydantic_core.PydanticSerializationError(f'Unable to encode default value {dft}')\n   \n  return pydantic_core.to_jsonable_python(\n  default,\n  timedelta_mode=config.ser_json_timedelta,\n  bytes_mode=config.ser_json_bytes,\n  )\n  \n def update_with_validations(\n self,json_schema:JsonSchemaValue,core_schema:CoreSchema,mapping:dict[str,str]\n )->None:\n  ''\n\n\n\n\n\n\n  \n  for core_key,json_schema_key in mapping.items():\n   if core_key in core_schema:\n    json_schema[json_schema_key]=core_schema[core_key]\n    \n class ValidationsMapping:\n  ''\n\n\n\n  \n  \n  numeric={\n  'multiple_of':'multipleOf',\n  'le':'maximum',\n  'ge':'minimum',\n  'lt':'exclusiveMaximum',\n  'gt':'exclusiveMinimum',\n  }\n  bytes={\n  'min_length':'minLength',\n  'max_length':'maxLength',\n  }\n  string={\n  'min_length':'minLength',\n  'max_length':'maxLength',\n  'pattern':'pattern',\n  }\n  array={\n  'min_length':'minItems',\n  'max_length':'maxItems',\n  }\n  object={\n  'min_length':'minProperties',\n  'max_length':'maxProperties',\n  }\n  \n def get_flattened_anyof(self,schemas:list[JsonSchemaValue])->JsonSchemaValue:\n  members=[]\n  for schema in schemas:\n   if len(schema)==1 and 'anyOf'in schema:\n    members.extend(schema['anyOf'])\n   else:\n    members.append(schema)\n  members=_deduplicate_schemas(members)\n  if len(members)==1:\n   return members[0]\n  return{'anyOf':members}\n  \n def get_json_ref_counts(self,json_schema:JsonSchemaValue)->dict[JsonRef,int]:\n  ''\n  json_refs:dict[JsonRef,int]=Counter()\n  \n  def _add_json_refs(schema:Any)->None:\n   if isinstance(schema,dict):\n    if '$ref'in schema:\n     json_ref=JsonRef(schema['$ref'])\n     if not isinstance(json_ref,str):\n      return\n     already_visited=json_ref in json_refs\n     json_refs[json_ref]+=1\n     if already_visited:\n      return\n     try:\n      defs_ref=self.json_to_defs_refs[json_ref]\n      if defs_ref in self._core_defs_invalid_for_json_schema:\n       raise self._core_defs_invalid_for_json_schema[defs_ref]\n      _add_json_refs(self.definitions[defs_ref])\n     except KeyError:\n      if not json_ref.startswith(('http://','https://')):\n       raise\n       \n    for k,v in schema.items():\n     if k =='examples':\n      continue\n     _add_json_refs(v)\n   elif isinstance(schema,list):\n    for v in schema:\n     _add_json_refs(v)\n     \n  _add_json_refs(json_schema)\n  return json_refs\n  \n def handle_invalid_for_json_schema(self,schema:CoreSchemaOrField,error_info:str)->JsonSchemaValue:\n  raise PydanticInvalidForJsonSchema(f'Cannot generate a JsonSchema for {error_info}')\n  \n def emit_warning(self,kind:JsonSchemaWarningKind,detail:str)->None:\n  ''\n  message=self.render_warning_message(kind,detail)\n  if message is not None:\n   warnings.warn(message,PydanticJsonSchemaWarning)\n   \n def render_warning_message(self,kind:JsonSchemaWarningKind,detail:str)->str |None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if kind in self.ignored_warning_kinds:\n   return None\n  return f'{detail} [{kind}]'\n  \n def _build_definitions_remapping(self)->_DefinitionsRemapping:\n  defs_to_json:dict[DefsRef,JsonRef]={}\n  for defs_refs in self._prioritized_defsref_choices.values():\n   for defs_ref in defs_refs:\n    json_ref=JsonRef(self.ref_template.format(model=defs_ref))\n    defs_to_json[defs_ref]=json_ref\n    \n  return _DefinitionsRemapping.from_prioritized_choices(\n  self._prioritized_defsref_choices,defs_to_json,self.definitions\n  )\n  \n def _garbage_collect_definitions(self,schema:JsonSchemaValue)->None:\n  visited_defs_refs:set[DefsRef]=set()\n  unvisited_json_refs=_get_all_json_refs(schema)\n  while unvisited_json_refs:\n   next_json_ref=unvisited_json_refs.pop()\n   try:\n    next_defs_ref=self.json_to_defs_refs[next_json_ref]\n    if next_defs_ref in visited_defs_refs:\n     continue\n    visited_defs_refs.add(next_defs_ref)\n    unvisited_json_refs.update(_get_all_json_refs(self.definitions[next_defs_ref]))\n   except KeyError:\n    if not next_json_ref.startswith(('http://','https://')):\n     raise\n     \n  self.definitions={k:v for k,v in self.definitions.items()if k in visited_defs_refs}\n  \n  \n  \n  \n  \ndef model_json_schema(\ncls:type[BaseModel]|type[PydanticDataclass],\nby_alias:bool=True,\nref_template:str=DEFAULT_REF_TEMPLATE,\nschema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\nmode:JsonSchemaMode='validation',\n)->dict[str,Any]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n from.main import BaseModel\n \n schema_generator_instance=schema_generator(by_alias=by_alias,ref_template=ref_template)\n \n if isinstance(cls.__pydantic_core_schema__,_mock_val_ser.MockCoreSchema):\n  cls.__pydantic_core_schema__.rebuild()\n  \n if cls is BaseModel:\n  raise AttributeError('model_json_schema() must be called on a subclass of BaseModel, not BaseModel itself.')\n  \n assert not isinstance(cls.__pydantic_core_schema__,_mock_val_ser.MockCoreSchema),'this is a bug! please report it'\n return schema_generator_instance.generate(cls.__pydantic_core_schema__,mode=mode)\n \n \ndef models_json_schema(\nmodels:Sequence[tuple[type[BaseModel]|type[PydanticDataclass],JsonSchemaMode]],\n*,\nby_alias:bool=True,\ntitle:str |None=None,\ndescription:str |None=None,\nref_template:str=DEFAULT_REF_TEMPLATE,\nschema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\n)->tuple[dict[tuple[type[BaseModel]|type[PydanticDataclass],JsonSchemaMode],JsonSchemaValue],JsonSchemaValue]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n for cls,_ in models:\n  if isinstance(cls.__pydantic_core_schema__,_mock_val_ser.MockCoreSchema):\n   cls.__pydantic_core_schema__.rebuild()\n   \n instance=schema_generator(by_alias=by_alias,ref_template=ref_template)\n inputs:list[tuple[type[BaseModel]|type[PydanticDataclass],JsonSchemaMode,CoreSchema]]=[\n (m,mode,m.__pydantic_core_schema__)for m,mode in models\n ]\n json_schemas_map,definitions=instance.generate_definitions(inputs)\n \n json_schema:dict[str,Any]={}\n if definitions:\n  json_schema['$defs']=definitions\n if title:\n  json_schema['title']=title\n if description:\n  json_schema['description']=description\n  \n return json_schemas_map,json_schema\n \n \n \n \n \n_HashableJsonValue:TypeAlias=Union[\nint,float,str,bool,None,Tuple['_HashableJsonValue',...],Tuple[Tuple[str,'_HashableJsonValue'],...]\n]\n\n\ndef _deduplicate_schemas(schemas:Iterable[JsonDict])->list[JsonDict]:\n return list({_make_json_hashable(schema):schema for schema in schemas}.values())\n \n \ndef _make_json_hashable(value:JsonValue)->_HashableJsonValue:\n if isinstance(value,dict):\n  return tuple(sorted((k,_make_json_hashable(v))for k,v in value.items()))\n elif isinstance(value,list):\n  return tuple(_make_json_hashable(v)for v in value)\n else:\n  return value\n  \n  \n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass WithJsonSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n json_schema:JsonSchemaValue |None\n mode:Literal['validation','serialization']|None=None\n \n def __get_pydantic_json_schema__(\n self,core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler\n )->JsonSchemaValue:\n  mode=self.mode or handler.mode\n  if mode !=handler.mode:\n   return handler(core_schema)\n  if self.json_schema is None:\n  \n   raise PydanticOmit\n  else:\n   return self.json_schema\n   \n def __hash__(self)->int:\n  return hash(type(self.mode))\n  \n  \nclass Examples:\n ''\n\n\n\n\n\n\n \n \n @overload\n @deprecated('Using a dict for `examples` is deprecated since v2.9 and will be removed in v3.0. Use a list instead.')\n def __init__(\n self,examples:dict[str,Any],mode:Literal['validation','serialization']|None=None\n )->None:...\n \n @overload\n def __init__(self,examples:list[Any],mode:Literal['validation','serialization']|None=None)->None:...\n \n def __init__(\n self,examples:dict[str,Any]|list[Any],mode:Literal['validation','serialization']|None=None\n )->None:\n  if isinstance(examples,dict):\n   warnings.warn(\n   'Using a dict for `examples` is deprecated, use a list instead.',\n   PydanticDeprecatedSince29,\n   stacklevel=2,\n   )\n  self.examples=examples\n  self.mode=mode\n  \n def __get_pydantic_json_schema__(\n self,core_schema:core_schema.CoreSchema,handler:GetJsonSchemaHandler\n )->JsonSchemaValue:\n  mode=self.mode or handler.mode\n  json_schema=handler(core_schema)\n  if mode !=handler.mode:\n   return json_schema\n  examples=json_schema.get('examples')\n  if examples is None:\n   json_schema['examples']=to_jsonable_python(self.examples)\n  if isinstance(examples,dict):\n   if isinstance(self.examples,list):\n    warnings.warn(\n    'Updating existing JSON Schema examples of type dict with examples of type list. '\n    'Only the existing examples values will be retained. Note that dict support for '\n    'examples is deprecated and will be removed in v3.0.',\n    UserWarning,\n    )\n    json_schema['examples']=to_jsonable_python(\n    [ex for value in examples.values()for ex in value]+self.examples\n    )\n   else:\n    json_schema['examples']=to_jsonable_python({**examples,**self.examples})\n  if isinstance(examples,list):\n   if isinstance(self.examples,list):\n    json_schema['examples']=to_jsonable_python(examples+self.examples)\n   elif isinstance(self.examples,dict):\n    warnings.warn(\n    'Updating existing JSON Schema examples of type list with examples of type dict. '\n    'Only the examples values will be retained. Note that dict support for '\n    'examples is deprecated and will be removed in v3.0.',\n    UserWarning,\n    )\n    json_schema['examples']=to_jsonable_python(\n    examples+[ex for value in self.examples.values()for ex in value]\n    )\n    \n  return json_schema\n  \n def __hash__(self)->int:\n  return hash(type(self.mode))\n  \n  \ndef _get_all_json_refs(item:Any)->set[JsonRef]:\n ''\n refs:set[JsonRef]=set()\n stack=[item]\n \n while stack:\n  current=stack.pop()\n  if isinstance(current,dict):\n   for key,value in current.items():\n    if key =='examples':\n     continue\n    if key =='$ref'and isinstance(value,str):\n     refs.add(JsonRef(value))\n    elif isinstance(value,dict):\n     stack.append(value)\n    elif isinstance(value,list):\n     stack.extend(value)\n  elif isinstance(current,list):\n   stack.extend(current)\n   \n return refs\n \n \nAnyType=TypeVar('AnyType')\n\nif TYPE_CHECKING:\n SkipJsonSchema=Annotated[AnyType,...]\nelse:\n\n @dataclasses.dataclass(**_internal_dataclass.slots_true)\n class SkipJsonSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  def __class_getitem__(cls,item:AnyType)->AnyType:\n   return Annotated[item,cls()]\n   \n  def __get_pydantic_json_schema__(\n  self,core_schema:CoreSchema,handler:GetJsonSchemaHandler\n  )->JsonSchemaValue:\n   raise PydanticOmit\n   \n  def __hash__(self)->int:\n   return hash(type(self))\n   \n   \ndef _get_typed_dict_config(cls:type[Any]|None)->ConfigDict:\n if cls is not None:\n  try:\n   return _decorators.get_attribute_from_bases(cls,'__pydantic_config__')\n  except AttributeError:\n   pass\n return{}\n", ["__future__", "collections", "copy", "dataclasses", "enum", "inspect", "math", "os", "pydantic", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._core_metadata", "pydantic._internal._core_utils", "pydantic._internal._dataclasses", "pydantic._internal._decorators", "pydantic._internal._internal_dataclass", "pydantic._internal._mock_val_ser", "pydantic._internal._schema_generation_shared", "pydantic._internal._typing_extra", "pydantic.annotated_handlers", "pydantic.config", "pydantic.errors", "pydantic.main", "pydantic.root_model", "pydantic.type_adapter", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "re", "typing", "typing_extensions", "warnings"]], "pydantic.annotated_handlers": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nfrom typing import TYPE_CHECKING,Any,Union\n\nfrom pydantic_core import core_schema\n\nif TYPE_CHECKING:\n from._internal._namespace_utils import NamespacesTuple\n from.json_schema import JsonSchemaMode,JsonSchemaValue\n \n CoreSchemaOrField=Union[\n core_schema.CoreSchema,\n core_schema.ModelField,\n core_schema.DataclassField,\n core_schema.TypedDictField,\n core_schema.ComputedField,\n ]\n \n__all__='GetJsonSchemaHandler','GetCoreSchemaHandler'\n\n\nclass GetJsonSchemaHandler:\n ''\n\n\n\n \n \n mode:JsonSchemaMode\n \n def __call__(self,core_schema:CoreSchemaOrField,/)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def resolve_ref_schema(self,maybe_ref_json_schema:JsonSchemaValue,/)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n  \nclass GetCoreSchemaHandler:\n ''\n \n def __call__(self,source_type:Any,/)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def generate_schema(self,source_type:Any,/)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def resolve_ref_schema(self,maybe_ref_schema:core_schema.CoreSchema,/)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n @property\n def field_name(self)->str |None:\n  ''\n  raise NotImplementedError\n  \n def _get_types_namespace(self)->NamespacesTuple:\n  ''\n  raise NotImplementedError\n", ["__future__", "pydantic._internal._namespace_utils", "pydantic.json_schema", "pydantic_core", "pydantic_core.core_schema", "typing"]], "pydantic.functional_serializers": [".py", "''\n\nfrom __future__ import annotations\n\nimport dataclasses\nfrom functools import partial,partialmethod\nfrom typing import TYPE_CHECKING,Any,Callable,TypeVar,overload\n\nfrom pydantic_core import PydanticUndefined,core_schema\nfrom pydantic_core.core_schema import SerializationInfo,SerializerFunctionWrapHandler,WhenUsed\nfrom typing_extensions import Annotated,Literal,TypeAlias\n\nfrom. import PydanticUndefinedAnnotation\nfrom._internal import _decorators,_internal_dataclass\nfrom.annotated_handlers import GetCoreSchemaHandler\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true,frozen=True)\nclass PlainSerializer:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:core_schema.SerializerFunction\n return_type:Any=PydanticUndefined\n when_used:WhenUsed='always'\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n  \n  schema=handler(source_type)\n  try:\n  \n  \n  \n   return_type=_decorators.get_function_return_type(\n   self.func,\n   self.return_type,\n   localns=handler._get_types_namespace().locals,\n   )\n  except NameError as e:\n   raise PydanticUndefinedAnnotation.from_name_error(e)from e\n  return_schema=None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n  schema['serialization']=core_schema.plain_serializer_function_ser_schema(\n  function=self.func,\n  info_arg=_decorators.inspect_annotated_serializer(self.func,'plain'),\n  return_schema=return_schema,\n  when_used=self.when_used,\n  )\n  return schema\n  \n  \n@dataclasses.dataclass(**_internal_dataclass.slots_true,frozen=True)\nclass WrapSerializer:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:core_schema.WrapSerializerFunction\n return_type:Any=PydanticUndefined\n when_used:WhenUsed='always'\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n  \n  schema=handler(source_type)\n  globalns,localns=handler._get_types_namespace()\n  try:\n  \n  \n  \n   return_type=_decorators.get_function_return_type(\n   self.func,\n   self.return_type,\n   localns=handler._get_types_namespace().locals,\n   )\n  except NameError as e:\n   raise PydanticUndefinedAnnotation.from_name_error(e)from e\n  return_schema=None if return_type is PydanticUndefined else handler.generate_schema(return_type)\n  schema['serialization']=core_schema.wrap_serializer_function_ser_schema(\n  function=self.func,\n  info_arg=_decorators.inspect_annotated_serializer(self.func,'wrap'),\n  return_schema=return_schema,\n  when_used=self.when_used,\n  )\n  return schema\n  \n  \nif TYPE_CHECKING:\n _Partial:TypeAlias='partial[Any] | partialmethod[Any]'\n \n FieldPlainSerializer:TypeAlias='core_schema.SerializerFunction | _Partial'\n '' \n \n FieldWrapSerializer:TypeAlias='core_schema.WrapSerializerFunction | _Partial'\n '' \n \n FieldSerializer:TypeAlias='FieldPlainSerializer | FieldWrapSerializer'\n '' \n \n _FieldPlainSerializerT=TypeVar('_FieldPlainSerializerT',bound=FieldPlainSerializer)\n _FieldWrapSerializerT=TypeVar('_FieldWrapSerializerT',bound=FieldWrapSerializer)\n \n \n@overload\ndef field_serializer(\nfield:str,\n/,\n*fields:str,\nmode:Literal['wrap'],\nreturn_type:Any=...,\nwhen_used:WhenUsed=...,\ncheck_fields:bool |None=...,\n)->Callable[[_FieldWrapSerializerT],_FieldWrapSerializerT]:...\n\n\n@overload\ndef field_serializer(\nfield:str,\n/,\n*fields:str,\nmode:Literal['plain']=...,\nreturn_type:Any=...,\nwhen_used:WhenUsed=...,\ncheck_fields:bool |None=...,\n)->Callable[[_FieldPlainSerializerT],_FieldPlainSerializerT]:...\n\n\ndef field_serializer(\n*fields:str,\nmode:Literal['plain','wrap']='plain',\nreturn_type:Any=PydanticUndefined,\nwhen_used:WhenUsed='always',\ncheck_fields:bool |None=None,\n)->(\nCallable[[_FieldWrapSerializerT],_FieldWrapSerializerT]\n|Callable[[_FieldPlainSerializerT],_FieldPlainSerializerT]\n):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def dec(f:FieldSerializer)->_decorators.PydanticDescriptorProxy[Any]:\n  dec_info=_decorators.FieldSerializerDecoratorInfo(\n  fields=fields,\n  mode=mode,\n  return_type=return_type,\n  when_used=when_used,\n  check_fields=check_fields,\n  )\n  return _decorators.PydanticDescriptorProxy(f,dec_info)\n  \n return dec\n \n \nif TYPE_CHECKING:\n\n\n ModelPlainSerializerWithInfo:TypeAlias=Callable[[Any,SerializationInfo],Any]\n '' \n \n ModelPlainSerializerWithoutInfo:TypeAlias=Callable[[Any],Any]\n '' \n \n ModelPlainSerializer:TypeAlias='ModelPlainSerializerWithInfo | ModelPlainSerializerWithoutInfo'\n '' \n \n ModelWrapSerializerWithInfo:TypeAlias=Callable[[Any,SerializerFunctionWrapHandler,SerializationInfo],Any]\n '' \n \n ModelWrapSerializerWithoutInfo:TypeAlias=Callable[[Any,SerializerFunctionWrapHandler],Any]\n '' \n \n ModelWrapSerializer:TypeAlias='ModelWrapSerializerWithInfo | ModelWrapSerializerWithoutInfo'\n '' \n \n ModelSerializer:TypeAlias='ModelPlainSerializer | ModelWrapSerializer'\n \n _ModelPlainSerializerT=TypeVar('_ModelPlainSerializerT',bound=ModelPlainSerializer)\n _ModelWrapSerializerT=TypeVar('_ModelWrapSerializerT',bound=ModelWrapSerializer)\n \n \n@overload\ndef model_serializer(f:_ModelPlainSerializerT,/)->_ModelPlainSerializerT:...\n\n\n@overload\ndef model_serializer(\n*,mode:Literal['wrap'],when_used:WhenUsed='always',return_type:Any=...\n)->Callable[[_ModelWrapSerializerT],_ModelWrapSerializerT]:...\n\n\n@overload\ndef model_serializer(\n*,\nmode:Literal['plain']=...,\nwhen_used:WhenUsed='always',\nreturn_type:Any=...,\n)->Callable[[_ModelPlainSerializerT],_ModelPlainSerializerT]:...\n\n\ndef model_serializer(\nf:_ModelPlainSerializerT |_ModelWrapSerializerT |None=None,\n/,\n*,\nmode:Literal['plain','wrap']='plain',\nwhen_used:WhenUsed='always',\nreturn_type:Any=PydanticUndefined,\n)->(\n_ModelPlainSerializerT\n|Callable[[_ModelWrapSerializerT],_ModelWrapSerializerT]\n|Callable[[_ModelPlainSerializerT],_ModelPlainSerializerT]\n):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def dec(f:ModelSerializer)->_decorators.PydanticDescriptorProxy[Any]:\n  dec_info=_decorators.ModelSerializerDecoratorInfo(mode=mode,return_type=return_type,when_used=when_used)\n  return _decorators.PydanticDescriptorProxy(f,dec_info)\n  \n if f is None:\n  return dec\n else:\n  return dec(f)\n  \n  \nAnyType=TypeVar('AnyType')\n\n\nif TYPE_CHECKING:\n SerializeAsAny=Annotated[AnyType,...]\n ''\n\n\n\n \nelse:\n\n @dataclasses.dataclass(**_internal_dataclass.slots_true)\n class SerializeAsAny:\n  def __class_getitem__(cls,item:Any)->Any:\n   return Annotated[item,SerializeAsAny()]\n   \n  def __get_pydantic_core_schema__(\n  self,source_type:Any,handler:GetCoreSchemaHandler\n  )->core_schema.CoreSchema:\n   schema=handler(source_type)\n   schema_to_update=schema\n   while schema_to_update['type']=='definitions':\n    schema_to_update=schema_to_update.copy()\n    schema_to_update=schema_to_update['schema']\n   schema_to_update['serialization']=core_schema.wrap_serializer_function_ser_schema(\n   lambda x,h:h(x),schema=core_schema.any_schema()\n   )\n   return schema\n   \n  __hash__=object.__hash__\n", ["__future__", "dataclasses", "functools", "pydantic", "pydantic._internal", "pydantic._internal._decorators", "pydantic._internal._internal_dataclass", "pydantic.annotated_handlers", "pydantic_core", "pydantic_core.core_schema", "typing", "typing_extensions"]], "pydantic.errors": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport re\n\nfrom typing_extensions import Literal,Self\n\nfrom._migration import getattr_migration\nfrom.version import version_short\n\n__all__=(\n'PydanticUserError',\n'PydanticUndefinedAnnotation',\n'PydanticImportError',\n'PydanticSchemaGenerationError',\n'PydanticInvalidForJsonSchema',\n'PydanticErrorCodes',\n)\n\n\n\n\nDEV_ERROR_DOCS_URL=f'https://errors.pydantic.dev/{version_short()}/u/'\nPydanticErrorCodes=Literal[\n'class-not-fully-defined',\n'custom-json-schema',\n'decorator-missing-field',\n'discriminator-no-field',\n'discriminator-alias-type',\n'discriminator-needs-literal',\n'discriminator-alias',\n'discriminator-validator',\n'callable-discriminator-no-tag',\n'typed-dict-version',\n'model-field-overridden',\n'model-field-missing-annotation',\n'config-both',\n'removed-kwargs',\n'circular-reference-schema',\n'invalid-for-json-schema',\n'json-schema-already-used',\n'base-model-instantiated',\n'undefined-annotation',\n'schema-for-unknown-type',\n'import-error',\n'create-model-field-definitions',\n'create-model-config-base',\n'validator-no-fields',\n'validator-invalid-fields',\n'validator-instance-method',\n'validator-input-type',\n'root-validator-pre-skip',\n'model-serializer-instance-method',\n'validator-field-config-info',\n'validator-v1-signature',\n'validator-signature',\n'field-serializer-signature',\n'model-serializer-signature',\n'multiple-field-serializers',\n'invalid-annotated-type',\n'type-adapter-config-unused',\n'root-model-extra',\n'unevaluable-type-annotation',\n'dataclass-init-false-extra-allow',\n'clashing-init-and-init-var',\n'model-config-invalid-field-name',\n'with-config-on-model',\n'dataclass-on-model',\n'validate-call-type',\n'unpack-typed-dict',\n'overlapping-unpack-typed-dict',\n'invalid-self-type',\n]\n\n\nclass PydanticErrorMixin:\n ''\n\n\n\n\n \n \n def __init__(self,message:str,*,code:PydanticErrorCodes |None)->None:\n  self.message=message\n  self.code=code\n  \n def __str__(self)->str:\n  if self.code is None:\n   return self.message\n  else:\n   return f'{self.message}\\n\\nFor further information visit {DEV_ERROR_DOCS_URL}{self.code}'\n   \n   \nclass PydanticUserError(PydanticErrorMixin,TypeError):\n ''\n \n \nclass PydanticUndefinedAnnotation(PydanticErrorMixin,NameError):\n ''\n\n\n\n\n \n \n def __init__(self,name:str,message:str)->None:\n  self.name=name\n  super().__init__(message=message,code='undefined-annotation')\n  \n @classmethod\n def from_name_error(cls,name_error:NameError)->Self:\n  ''\n\n\n\n\n\n\n  \n  try:\n   name=name_error.name\n  except AttributeError:\n   name=re.search(r\".*'(.+?)'\",str(name_error)).group(1)\n  return cls(name=name,message=str(name_error))\n  \n  \nclass PydanticImportError(PydanticErrorMixin,ImportError):\n ''\n\n\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message,code='import-error')\n  \n  \nclass PydanticSchemaGenerationError(PydanticUserError):\n ''\n\n\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message,code='schema-for-unknown-type')\n  \n  \nclass PydanticInvalidForJsonSchema(PydanticUserError):\n ''\n\n\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message,code='invalid-for-json-schema')\n  \n  \n__getattr__=getattr_migration(__name__)\n", ["__future__", "pydantic._migration", "pydantic.version", "re", "typing_extensions"]], "pydantic.type_adapter": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport sys\nfrom dataclasses import is_dataclass\nfrom types import FrameType\nfrom typing import(\nAny,\nGeneric,\nIterable,\nLiteral,\nTypeVar,\ncast,\nfinal,\noverload,\n)\n\nfrom pydantic_core import CoreSchema,SchemaSerializer,SchemaValidator,Some\nfrom typing_extensions import ParamSpec,is_typeddict\n\nfrom pydantic.errors import PydanticUserError\nfrom pydantic.main import BaseModel,IncEx\n\nfrom._internal import _config,_generate_schema,_mock_val_ser,_namespace_utils,_repr,_typing_extra,_utils\nfrom.config import ConfigDict\nfrom.errors import PydanticUndefinedAnnotation\nfrom.json_schema import(\nDEFAULT_REF_TEMPLATE,\nGenerateJsonSchema,\nJsonSchemaKeyT,\nJsonSchemaMode,\nJsonSchemaValue,\n)\nfrom.plugin._schema_validator import PluggableSchemaValidator,create_schema_validator\n\nT=TypeVar('T')\nR=TypeVar('R')\nP=ParamSpec('P')\nTypeAdapterT=TypeVar('TypeAdapterT',bound='TypeAdapter')\n\n\ndef _getattr_no_parents(obj:Any,attribute:str)->Any:\n ''\n if hasattr(obj,'__dict__'):\n  try:\n   return obj.__dict__[attribute]\n  except KeyError:\n   pass\n   \n slots=getattr(obj,'__slots__',None)\n if slots is not None and attribute in slots:\n  return getattr(obj,attribute)\n else:\n  raise AttributeError(attribute)\n  \n  \ndef _type_has_config(type_:Any)->bool:\n ''\n type_=_typing_extra.annotated_type(type_)or type_\n try:\n  return issubclass(type_,BaseModel)or is_dataclass(type_)or is_typeddict(type_)\n except TypeError:\n \n  return False\n  \n  \n@final\nclass TypeAdapter(Generic[T]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n core_schema:CoreSchema\n validator:SchemaValidator |PluggableSchemaValidator\n serializer:SchemaSerializer\n pydantic_complete:bool\n \n @overload\n def __init__(\n self,\n type:type[T],\n *,\n config:ConfigDict |None=...,\n _parent_depth:int=...,\n module:str |None=...,\n )->None:...\n \n \n \n \n @overload\n def __init__(\n self,\n type:Any,\n *,\n config:ConfigDict |None=...,\n _parent_depth:int=...,\n module:str |None=...,\n )->None:...\n \n def __init__(\n self,\n type:Any,\n *,\n config:ConfigDict |None=None,\n _parent_depth:int=2,\n module:str |None=None,\n )->None:\n  if _type_has_config(type)and config is not None:\n   raise PydanticUserError(\n   'Cannot use `config` when the type is a BaseModel, dataclass or TypedDict.'\n   ' These types can have their own config and setting the config via the `config`'\n   ' parameter to TypeAdapter will not override it, thus the `config` you passed to'\n   ' TypeAdapter becomes meaningless, which is probably not what you want.',\n   code='type-adapter-config-unused',\n   )\n   \n  self._type=type\n  self._config=config\n  self._parent_depth=_parent_depth\n  self.pydantic_complete=False\n  \n  parent_frame=self._fetch_parent_frame()\n  if parent_frame is not None:\n   globalns=parent_frame.f_globals\n   \n   localns=parent_frame.f_locals if parent_frame.f_locals is not globalns else{}\n  else:\n   globalns={}\n   localns={}\n   \n  self._module_name=module or cast(str,globalns.get('__name__',''))\n  self._init_core_attrs(\n  ns_resolver=_namespace_utils.NsResolver(\n  namespaces_tuple=_namespace_utils.NamespacesTuple(locals=localns,globals=globalns),\n  parent_namespace=localns,\n  ),\n  force=False,\n  )\n  \n def _fetch_parent_frame(self)->FrameType |None:\n  frame=sys._getframe(self._parent_depth)\n  if frame.f_globals.get('__name__')=='typing':\n  \n  \n  \n  \n   return frame.f_back\n   \n  return frame\n  \n def _init_core_attrs(\n self,ns_resolver:_namespace_utils.NsResolver,force:bool,raise_errors:bool=False\n )->bool:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if not force and self._defer_build:\n   _mock_val_ser.set_type_adapter_mocks(self,str(self._type))\n   self.pydantic_complete=False\n   return False\n   \n  try:\n   self.core_schema=_getattr_no_parents(self._type,'__pydantic_core_schema__')\n   self.validator=_getattr_no_parents(self._type,'__pydantic_validator__')\n   self.serializer=_getattr_no_parents(self._type,'__pydantic_serializer__')\n   \n   \n   \n   \n   if(\n   isinstance(self.core_schema,_mock_val_ser.MockCoreSchema)\n   or isinstance(self.validator,_mock_val_ser.MockValSer)\n   or isinstance(self.serializer,_mock_val_ser.MockValSer)\n   ):\n    raise AttributeError()\n  except AttributeError:\n   config_wrapper=_config.ConfigWrapper(self._config)\n   \n   schema_generator=_generate_schema.GenerateSchema(config_wrapper,ns_resolver=ns_resolver)\n   \n   try:\n    core_schema=schema_generator.generate_schema(self._type)\n   except PydanticUndefinedAnnotation:\n    if raise_errors:\n     raise\n    _mock_val_ser.set_type_adapter_mocks(self,str(self._type))\n    return False\n    \n   try:\n    self.core_schema=schema_generator.clean_schema(core_schema)\n   except schema_generator.CollectedInvalid:\n    _mock_val_ser.set_type_adapter_mocks(self,str(self._type))\n    return False\n    \n   core_config=config_wrapper.core_config(None)\n   \n   self.validator=create_schema_validator(\n   schema=self.core_schema,\n   schema_type=self._type,\n   schema_type_module=self._module_name,\n   schema_type_name=str(self._type),\n   schema_kind='TypeAdapter',\n   config=core_config,\n   plugin_settings=config_wrapper.plugin_settings,\n   )\n   self.serializer=SchemaSerializer(self.core_schema,core_config)\n   \n  self.pydantic_complete=True\n  return True\n  \n @property\n def _defer_build(self)->bool:\n  config=self._config if self._config is not None else self._model_config\n  if config:\n   return config.get('defer_build')is True\n  return False\n  \n @property\n def _model_config(self)->ConfigDict |None:\n  type_:Any=_typing_extra.annotated_type(self._type)or self._type\n  if _utils.lenient_issubclass(type_,BaseModel):\n   return type_.model_config\n  return getattr(type_,'__pydantic_config__',None)\n  \n def __repr__(self)->str:\n  return f'TypeAdapter({_repr.display_as_type(self._type)})'\n  \n def rebuild(\n self,\n *,\n force:bool=False,\n raise_errors:bool=True,\n _parent_namespace_depth:int=2,\n _types_namespace:_namespace_utils.MappingNamespace |None=None,\n )->bool |None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if not force and self.pydantic_complete:\n   return None\n   \n  if _types_namespace is not None:\n   rebuild_ns=_types_namespace\n  elif _parent_namespace_depth >0:\n   rebuild_ns=_typing_extra.parent_frame_namespace(parent_depth=_parent_namespace_depth,force=True)or{}\n  else:\n   rebuild_ns={}\n   \n   \n   \n  globalns=sys._getframe(max(_parent_namespace_depth -1,1)).f_globals\n  ns_resolver=_namespace_utils.NsResolver(\n  namespaces_tuple=_namespace_utils.NamespacesTuple(locals=rebuild_ns,globals=globalns),\n  parent_namespace=rebuild_ns,\n  )\n  return self._init_core_attrs(ns_resolver=ns_resolver,force=True,raise_errors=raise_errors)\n  \n def validate_python(\n self,\n object:Any,\n /,\n *,\n strict:bool |None=None,\n from_attributes:bool |None=None,\n context:dict[str,Any]|None=None,\n experimental_allow_partial:bool |Literal['off','on','trailing-strings']=False,\n )->T:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.validator.validate_python(\n  object,\n  strict=strict,\n  from_attributes=from_attributes,\n  context=context,\n  allow_partial=experimental_allow_partial,\n  )\n  \n def validate_json(\n self,\n data:str |bytes |bytearray,\n /,\n *,\n strict:bool |None=None,\n context:dict[str,Any]|None=None,\n experimental_allow_partial:bool |Literal['off','on','trailing-strings']=False,\n )->T:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.validator.validate_json(\n  data,strict=strict,context=context,allow_partial=experimental_allow_partial\n  )\n  \n def validate_strings(\n self,\n obj:Any,\n /,\n *,\n strict:bool |None=None,\n context:dict[str,Any]|None=None,\n experimental_allow_partial:bool |Literal['off','on','trailing-strings']=False,\n )->T:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.validator.validate_strings(\n  obj,strict=strict,context=context,allow_partial=experimental_allow_partial\n  )\n  \n def get_default_value(self,*,strict:bool |None=None,context:dict[str,Any]|None=None)->Some[T]|None:\n  ''\n\n\n\n\n\n\n\n  \n  return self.validator.get_default_value(strict=strict,context=context)\n  \n def dump_python(\n self,\n instance:T,\n /,\n *,\n mode:Literal['json','python']='python',\n include:IncEx |None=None,\n exclude:IncEx |None=None,\n by_alias:bool=False,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n round_trip:bool=False,\n warnings:bool |Literal['none','warn','error']=True,\n serialize_as_any:bool=False,\n context:dict[str,Any]|None=None,\n )->Any:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.serializer.to_python(\n  instance,\n  mode=mode,\n  by_alias=by_alias,\n  include=include,\n  exclude=exclude,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  round_trip=round_trip,\n  warnings=warnings,\n  serialize_as_any=serialize_as_any,\n  context=context,\n  )\n  \n def dump_json(\n self,\n instance:T,\n /,\n *,\n indent:int |None=None,\n include:IncEx |None=None,\n exclude:IncEx |None=None,\n by_alias:bool=False,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n round_trip:bool=False,\n warnings:bool |Literal['none','warn','error']=True,\n serialize_as_any:bool=False,\n context:dict[str,Any]|None=None,\n )->bytes:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.serializer.to_json(\n  instance,\n  indent=indent,\n  include=include,\n  exclude=exclude,\n  by_alias=by_alias,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  round_trip=round_trip,\n  warnings=warnings,\n  serialize_as_any=serialize_as_any,\n  context=context,\n  )\n  \n def json_schema(\n self,\n *,\n by_alias:bool=True,\n ref_template:str=DEFAULT_REF_TEMPLATE,\n schema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\n mode:JsonSchemaMode='validation',\n )->dict[str,Any]:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  schema_generator_instance=schema_generator(by_alias=by_alias,ref_template=ref_template)\n  if isinstance(self.core_schema,_mock_val_ser.MockCoreSchema):\n   self.core_schema.rebuild()\n   assert not isinstance(self.core_schema,_mock_val_ser.MockCoreSchema),'this is a bug! please report it'\n  return schema_generator_instance.generate(self.core_schema,mode=mode)\n  \n @staticmethod\n def json_schemas(\n inputs:Iterable[tuple[JsonSchemaKeyT,JsonSchemaMode,TypeAdapter[Any]]],\n /,\n *,\n by_alias:bool=True,\n title:str |None=None,\n description:str |None=None,\n ref_template:str=DEFAULT_REF_TEMPLATE,\n schema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\n )->tuple[dict[tuple[JsonSchemaKeyT,JsonSchemaMode],JsonSchemaValue],JsonSchemaValue]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  schema_generator_instance=schema_generator(by_alias=by_alias,ref_template=ref_template)\n  \n  inputs_=[]\n  for key,mode,adapter in inputs:\n  \n   if isinstance(adapter.core_schema,_mock_val_ser.MockCoreSchema):\n    adapter.core_schema.rebuild()\n    assert not isinstance(\n    adapter.core_schema,_mock_val_ser.MockCoreSchema\n    ),'this is a bug! please report it'\n   inputs_.append((key,mode,adapter.core_schema))\n   \n  json_schemas_map,definitions=schema_generator_instance.generate_definitions(inputs_)\n  \n  json_schema:dict[str,Any]={}\n  if definitions:\n   json_schema['$defs']=definitions\n  if title:\n   json_schema['title']=title\n  if description:\n   json_schema['description']=description\n   \n  return json_schemas_map,json_schema\n", ["__future__", "dataclasses", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._generate_schema", "pydantic._internal._mock_val_ser", "pydantic._internal._namespace_utils", "pydantic._internal._repr", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic.config", "pydantic.errors", "pydantic.json_schema", "pydantic.main", "pydantic.plugin._schema_validator", "pydantic_core", "sys", "types", "typing", "typing_extensions"]], "pydantic.typing": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.warnings": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nfrom.version import version_short\n\n__all__=(\n'PydanticDeprecatedSince20',\n'PydanticDeprecationWarning',\n'PydanticDeprecatedSince26',\n'PydanticExperimentalWarning',\n)\n\n\nclass PydanticDeprecationWarning(DeprecationWarning):\n ''\n\n\n\n\n\n\n\n\n \n \n message:str\n since:tuple[int,int]\n expected_removal:tuple[int,int]\n \n def __init__(\n self,message:str,*args:object,since:tuple[int,int],expected_removal:tuple[int,int]|None=None\n )->None:\n  super().__init__(message,*args)\n  self.message=message.rstrip('.')\n  self.since=since\n  self.expected_removal=expected_removal if expected_removal is not None else(since[0]+1,0)\n  \n def __str__(self)->str:\n  message=(\n  f'{self.message}. Deprecated in Pydantic V{self.since[0]}.{self.since[1]}'\n  f' to be removed in V{self.expected_removal[0]}.{self.expected_removal[1]}.'\n  )\n  if self.since ==(2,0):\n   message +=f' See Pydantic V2 Migration Guide at https://errors.pydantic.dev/{version_short()}/migration/'\n  return message\n  \n  \nclass PydanticDeprecatedSince20(PydanticDeprecationWarning):\n ''\n \n def __init__(self,message:str,*args:object)->None:\n  super().__init__(message,*args,since=(2,0),expected_removal=(3,0))\n  \n  \nclass PydanticDeprecatedSince26(PydanticDeprecationWarning):\n ''\n \n def __init__(self,message:str,*args:object)->None:\n  super().__init__(message,*args,since=(2,6),expected_removal=(3,0))\n  \n  \nclass PydanticDeprecatedSince29(PydanticDeprecationWarning):\n ''\n \n def __init__(self,message:str,*args:object)->None:\n  super().__init__(message,*args,since=(2,9),expected_removal=(3,0))\n  \n  \nclass PydanticDeprecatedSince210(PydanticDeprecationWarning):\n ''\n \n def __init__(self,message:str,*args:object)->None:\n  super().__init__(message,*args,since=(2,10),expected_removal=(3,0))\n  \n  \nclass GenericBeforeBaseModelWarning(Warning):\n pass\n \n \nclass PydanticExperimentalWarning(Warning):\n ''\n\n\n\n \n", ["__future__", "pydantic.version"]], "pydantic.color": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport math\nimport re\nfrom colorsys import hls_to_rgb,rgb_to_hls\nfrom typing import Any,Callable,Optional,Tuple,Type,Union,cast\n\nfrom pydantic_core import CoreSchema,PydanticCustomError,core_schema\nfrom typing_extensions import deprecated\n\nfrom._internal import _repr\nfrom._internal._schema_generation_shared import GetJsonSchemaHandler as _GetJsonSchemaHandler\nfrom.json_schema import JsonSchemaValue\nfrom.warnings import PydanticDeprecatedSince20\n\nColorTuple=Union[Tuple[int,int,int],Tuple[int,int,int,float]]\nColorType=Union[ColorTuple,str]\nHslColorTuple=Union[Tuple[float,float,float],Tuple[float,float,float,float]]\n\n\nclass RGBA:\n ''\n \n __slots__='r','g','b','alpha','_tuple'\n \n def __init__(self,r:float,g:float,b:float,alpha:Optional[float]):\n  self.r=r\n  self.g=g\n  self.b=b\n  self.alpha=alpha\n  \n  self._tuple:Tuple[float,float,float,Optional[float]]=(r,g,b,alpha)\n  \n def __getitem__(self,item:Any)->Any:\n  return self._tuple[item]\n  \n  \n  \n_r_255=r'(\\d{1,3}(?:\\.\\d+)?)'\n_r_comma=r'\\s*,\\s*'\n_r_alpha=r'(\\d(?:\\.\\d+)?|\\.\\d+|\\d{1,2}%)'\n_r_h=r'(-?\\d+(?:\\.\\d+)?|-?\\.\\d+)(deg|rad|turn)?'\n_r_sl=r'(\\d{1,3}(?:\\.\\d+)?)%'\nr_hex_short=r'\\s*(?:#|0x)?([0-9a-f])([0-9a-f])([0-9a-f])([0-9a-f])?\\s*'\nr_hex_long=r'\\s*(?:#|0x)?([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})?\\s*'\n\nr_rgb=rf'\\s*rgba?\\(\\s*{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_255}(?:{_r_comma}{_r_alpha})?\\s*\\)\\s*'\n\nr_hsl=rf'\\s*hsla?\\(\\s*{_r_h}{_r_comma}{_r_sl}{_r_comma}{_r_sl}(?:{_r_comma}{_r_alpha})?\\s*\\)\\s*'\n\nr_rgb_v4_style=rf'\\s*rgba?\\(\\s*{_r_255}\\s+{_r_255}\\s+{_r_255}(?:\\s*/\\s*{_r_alpha})?\\s*\\)\\s*'\n\nr_hsl_v4_style=rf'\\s*hsla?\\(\\s*{_r_h}\\s+{_r_sl}\\s+{_r_sl}(?:\\s*/\\s*{_r_alpha})?\\s*\\)\\s*'\n\n\nrepeat_colors={int(c *2,16)for c in '0123456789abcdef'}\nrads=2 *math.pi\n\n\n@deprecated(\n'The `Color` class is deprecated, use `pydantic_extra_types` instead. '\n'See https://docs.pydantic.dev/latest/api/pydantic_extra_types_color/.',\ncategory=PydanticDeprecatedSince20,\n)\nclass Color(_repr.Representation):\n ''\n \n __slots__='_original','_rgba'\n \n def __init__(self,value:ColorType)->None:\n  self._rgba:RGBA\n  self._original:ColorType\n  if isinstance(value,(tuple,list)):\n   self._rgba=parse_tuple(value)\n  elif isinstance(value,str):\n   self._rgba=parse_str(value)\n  elif isinstance(value,Color):\n   self._rgba=value._rgba\n   value=value._original\n  else:\n   raise PydanticCustomError(\n   'color_error','value is not a valid color: value must be a tuple, list or string'\n   )\n   \n   \n  self._original=value\n  \n @classmethod\n def __get_pydantic_json_schema__(\n cls,core_schema:core_schema.CoreSchema,handler:_GetJsonSchemaHandler\n )->JsonSchemaValue:\n  field_schema={}\n  field_schema.update(type='string',format='color')\n  return field_schema\n  \n def original(self)->ColorType:\n  ''\n  return self._original\n  \n def as_named(self,*,fallback:bool=False)->str:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  if self._rgba.alpha is None:\n   rgb=cast(Tuple[int,int,int],self.as_rgb_tuple())\n   try:\n    return COLORS_BY_VALUE[rgb]\n   except KeyError as e:\n    if fallback:\n     return self.as_hex()\n    else:\n     raise ValueError('no named color found, use fallback=True, as_hex() or as_rgb()')from e\n  else:\n   return self.as_hex()\n   \n def as_hex(self)->str:\n  ''\n\n\n\n\n\n\n  \n  values=[float_to_255(c)for c in self._rgba[:3]]\n  if self._rgba.alpha is not None:\n   values.append(float_to_255(self._rgba.alpha))\n   \n  as_hex=''.join(f'{v:02x}'for v in values)\n  if all(c in repeat_colors for c in values):\n   as_hex=''.join(as_hex[c]for c in range(0,len(as_hex),2))\n  return '#'+as_hex\n  \n def as_rgb(self)->str:\n  ''\n  if self._rgba.alpha is None:\n   return f'rgb({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)})'\n  else:\n   return(\n   f'rgba({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)}, '\n   f'{round(self._alpha_float(),2)})'\n   )\n   \n def as_rgb_tuple(self,*,alpha:Optional[bool]=None)->ColorTuple:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  r,g,b=(float_to_255(c)for c in self._rgba[:3])\n  if alpha is None:\n   if self._rgba.alpha is None:\n    return r,g,b\n   else:\n    return r,g,b,self._alpha_float()\n  elif alpha:\n   return r,g,b,self._alpha_float()\n  else:\n  \n   return r,g,b\n   \n def as_hsl(self)->str:\n  ''\n  if self._rgba.alpha is None:\n   h,s,li=self.as_hsl_tuple(alpha=False)\n   return f'hsl({h *360:0.0f}, {s:0.0%}, {li:0.0%})'\n  else:\n   h,s,li,a=self.as_hsl_tuple(alpha=True)\n   return f'hsl({h *360:0.0f}, {s:0.0%}, {li:0.0%}, {round(a,2)})'\n   \n def as_hsl_tuple(self,*,alpha:Optional[bool]=None)->HslColorTuple:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  h,l,s=rgb_to_hls(self._rgba.r,self._rgba.g,self._rgba.b)\n  if alpha is None:\n   if self._rgba.alpha is None:\n    return h,s,l\n   else:\n    return h,s,l,self._alpha_float()\n  if alpha:\n   return h,s,l,self._alpha_float()\n  else:\n  \n   return h,s,l\n   \n def _alpha_float(self)->float:\n  return 1 if self._rgba.alpha is None else self._rgba.alpha\n  \n @classmethod\n def __get_pydantic_core_schema__(\n cls,source:Type[Any],handler:Callable[[Any],CoreSchema]\n )->core_schema.CoreSchema:\n  return core_schema.with_info_plain_validator_function(\n  cls._validate,serialization=core_schema.to_string_ser_schema()\n  )\n  \n @classmethod\n def _validate(cls,__input_value:Any,_:Any)->'Color':\n  return cls(__input_value)\n  \n def __str__(self)->str:\n  return self.as_named(fallback=True)\n  \n def __repr_args__(self)->'_repr.ReprArgs':\n  return[(None,self.as_named(fallback=True))]+[('rgb',self.as_rgb_tuple())]\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,Color)and self.as_rgb_tuple()==other.as_rgb_tuple()\n  \n def __hash__(self)->int:\n  return hash(self.as_rgb_tuple())\n  \n  \ndef parse_tuple(value:Tuple[Any,...])->RGBA:\n ''\n\n\n\n\n\n\n\n\n\n \n if len(value)==3:\n  r,g,b=(parse_color_value(v)for v in value)\n  return RGBA(r,g,b,None)\n elif len(value)==4:\n  r,g,b=(parse_color_value(v)for v in value[:3])\n  return RGBA(r,g,b,parse_float_alpha(value[3]))\n else:\n  raise PydanticCustomError('color_error','value is not a valid color: tuples must have length 3 or 4')\n  \n  \ndef parse_str(value:str)->RGBA:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n value_lower=value.lower()\n try:\n  r,g,b=COLORS_BY_NAME[value_lower]\n except KeyError:\n  pass\n else:\n  return ints_to_rgba(r,g,b,None)\n  \n m=re.fullmatch(r_hex_short,value_lower)\n if m:\n  *rgb,a=m.groups()\n  r,g,b=(int(v *2,16)for v in rgb)\n  if a:\n   alpha:Optional[float]=int(a *2,16)/255\n  else:\n   alpha=None\n  return ints_to_rgba(r,g,b,alpha)\n  \n m=re.fullmatch(r_hex_long,value_lower)\n if m:\n  *rgb,a=m.groups()\n  r,g,b=(int(v,16)for v in rgb)\n  if a:\n   alpha=int(a,16)/255\n  else:\n   alpha=None\n  return ints_to_rgba(r,g,b,alpha)\n  \n m=re.fullmatch(r_rgb,value_lower)or re.fullmatch(r_rgb_v4_style,value_lower)\n if m:\n  return ints_to_rgba(*m.groups())\n  \n m=re.fullmatch(r_hsl,value_lower)or re.fullmatch(r_hsl_v4_style,value_lower)\n if m:\n  return parse_hsl(*m.groups())\n  \n raise PydanticCustomError('color_error','value is not a valid color: string not recognised as a valid color')\n \n \ndef ints_to_rgba(r:Union[int,str],g:Union[int,str],b:Union[int,str],alpha:Optional[float]=None)->RGBA:\n ''\n\n\n\n\n\n\n\n\n\n \n return RGBA(parse_color_value(r),parse_color_value(g),parse_color_value(b),parse_float_alpha(alpha))\n \n \ndef parse_color_value(value:Union[int,str],max_val:int=255)->float:\n ''\n\n\n\n\n\n\n\n\n\n\n \n try:\n  color=float(value)\n except ValueError:\n  raise PydanticCustomError('color_error','value is not a valid color: color values must be a valid number')\n if 0 <=color <=max_val:\n  return color /max_val\n else:\n  raise PydanticCustomError(\n  'color_error',\n  'value is not a valid color: color values must be in the range 0 to {max_val}',\n  {'max_val':max_val},\n  )\n  \n  \ndef parse_float_alpha(value:Union[None,str,float,int])->Optional[float]:\n ''\n\n\n\n\n\n\n\n\n\n \n if value is None:\n  return None\n try:\n  if isinstance(value,str)and value.endswith('%'):\n   alpha=float(value[:-1])/100\n  else:\n   alpha=float(value)\n except ValueError:\n  raise PydanticCustomError('color_error','value is not a valid color: alpha values must be a valid float')\n  \n if math.isclose(alpha,1):\n  return None\n elif 0 <=alpha <=1:\n  return alpha\n else:\n  raise PydanticCustomError('color_error','value is not a valid color: alpha values must be in the range 0 to 1')\n  \n  \ndef parse_hsl(h:str,h_units:str,sat:str,light:str,alpha:Optional[float]=None)->RGBA:\n ''\n\n\n\n\n\n\n\n\n\n\n \n s_value,l_value=parse_color_value(sat,100),parse_color_value(light,100)\n \n h_value=float(h)\n if h_units in{None,'deg'}:\n  h_value=h_value %360 /360\n elif h_units =='rad':\n  h_value=h_value %rads /rads\n else:\n \n  h_value=h_value %1\n  \n r,g,b=hls_to_rgb(h_value,l_value,s_value)\n return RGBA(r,g,b,parse_float_alpha(alpha))\n \n \ndef float_to_255(c:float)->int:\n ''\n\n\n\n\n\n\n\n\n\n \n return int(round(c *255))\n \n \nCOLORS_BY_NAME={\n'aliceblue':(240,248,255),\n'antiquewhite':(250,235,215),\n'aqua':(0,255,255),\n'aquamarine':(127,255,212),\n'azure':(240,255,255),\n'beige':(245,245,220),\n'bisque':(255,228,196),\n'black':(0,0,0),\n'blanchedalmond':(255,235,205),\n'blue':(0,0,255),\n'blueviolet':(138,43,226),\n'brown':(165,42,42),\n'burlywood':(222,184,135),\n'cadetblue':(95,158,160),\n'chartreuse':(127,255,0),\n'chocolate':(210,105,30),\n'coral':(255,127,80),\n'cornflowerblue':(100,149,237),\n'cornsilk':(255,248,220),\n'crimson':(220,20,60),\n'cyan':(0,255,255),\n'darkblue':(0,0,139),\n'darkcyan':(0,139,139),\n'darkgoldenrod':(184,134,11),\n'darkgray':(169,169,169),\n'darkgreen':(0,100,0),\n'darkgrey':(169,169,169),\n'darkkhaki':(189,183,107),\n'darkmagenta':(139,0,139),\n'darkolivegreen':(85,107,47),\n'darkorange':(255,140,0),\n'darkorchid':(153,50,204),\n'darkred':(139,0,0),\n'darksalmon':(233,150,122),\n'darkseagreen':(143,188,143),\n'darkslateblue':(72,61,139),\n'darkslategray':(47,79,79),\n'darkslategrey':(47,79,79),\n'darkturquoise':(0,206,209),\n'darkviolet':(148,0,211),\n'deeppink':(255,20,147),\n'deepskyblue':(0,191,255),\n'dimgray':(105,105,105),\n'dimgrey':(105,105,105),\n'dodgerblue':(30,144,255),\n'firebrick':(178,34,34),\n'floralwhite':(255,250,240),\n'forestgreen':(34,139,34),\n'fuchsia':(255,0,255),\n'gainsboro':(220,220,220),\n'ghostwhite':(248,248,255),\n'gold':(255,215,0),\n'goldenrod':(218,165,32),\n'gray':(128,128,128),\n'green':(0,128,0),\n'greenyellow':(173,255,47),\n'grey':(128,128,128),\n'honeydew':(240,255,240),\n'hotpink':(255,105,180),\n'indianred':(205,92,92),\n'indigo':(75,0,130),\n'ivory':(255,255,240),\n'khaki':(240,230,140),\n'lavender':(230,230,250),\n'lavenderblush':(255,240,245),\n'lawngreen':(124,252,0),\n'lemonchiffon':(255,250,205),\n'lightblue':(173,216,230),\n'lightcoral':(240,128,128),\n'lightcyan':(224,255,255),\n'lightgoldenrodyellow':(250,250,210),\n'lightgray':(211,211,211),\n'lightgreen':(144,238,144),\n'lightgrey':(211,211,211),\n'lightpink':(255,182,193),\n'lightsalmon':(255,160,122),\n'lightseagreen':(32,178,170),\n'lightskyblue':(135,206,250),\n'lightslategray':(119,136,153),\n'lightslategrey':(119,136,153),\n'lightsteelblue':(176,196,222),\n'lightyellow':(255,255,224),\n'lime':(0,255,0),\n'limegreen':(50,205,50),\n'linen':(250,240,230),\n'magenta':(255,0,255),\n'maroon':(128,0,0),\n'mediumaquamarine':(102,205,170),\n'mediumblue':(0,0,205),\n'mediumorchid':(186,85,211),\n'mediumpurple':(147,112,219),\n'mediumseagreen':(60,179,113),\n'mediumslateblue':(123,104,238),\n'mediumspringgreen':(0,250,154),\n'mediumturquoise':(72,209,204),\n'mediumvioletred':(199,21,133),\n'midnightblue':(25,25,112),\n'mintcream':(245,255,250),\n'mistyrose':(255,228,225),\n'moccasin':(255,228,181),\n'navajowhite':(255,222,173),\n'navy':(0,0,128),\n'oldlace':(253,245,230),\n'olive':(128,128,0),\n'olivedrab':(107,142,35),\n'orange':(255,165,0),\n'orangered':(255,69,0),\n'orchid':(218,112,214),\n'palegoldenrod':(238,232,170),\n'palegreen':(152,251,152),\n'paleturquoise':(175,238,238),\n'palevioletred':(219,112,147),\n'papayawhip':(255,239,213),\n'peachpuff':(255,218,185),\n'peru':(205,133,63),\n'pink':(255,192,203),\n'plum':(221,160,221),\n'powderblue':(176,224,230),\n'purple':(128,0,128),\n'red':(255,0,0),\n'rosybrown':(188,143,143),\n'royalblue':(65,105,225),\n'saddlebrown':(139,69,19),\n'salmon':(250,128,114),\n'sandybrown':(244,164,96),\n'seagreen':(46,139,87),\n'seashell':(255,245,238),\n'sienna':(160,82,45),\n'silver':(192,192,192),\n'skyblue':(135,206,235),\n'slateblue':(106,90,205),\n'slategray':(112,128,144),\n'slategrey':(112,128,144),\n'snow':(255,250,250),\n'springgreen':(0,255,127),\n'steelblue':(70,130,180),\n'tan':(210,180,140),\n'teal':(0,128,128),\n'thistle':(216,191,216),\n'tomato':(255,99,71),\n'turquoise':(64,224,208),\n'violet':(238,130,238),\n'wheat':(245,222,179),\n'white':(255,255,255),\n'whitesmoke':(245,245,245),\n'yellow':(255,255,0),\n'yellowgreen':(154,205,50),\n}\n\nCOLORS_BY_VALUE={v:k for k,v in COLORS_BY_NAME.items()}\n", ["colorsys", "math", "pydantic._internal", "pydantic._internal._repr", "pydantic._internal._schema_generation_shared", "pydantic.json_schema", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "re", "typing", "typing_extensions"]], "pydantic.aliases": [".py", "''\n\nfrom __future__ import annotations\n\nimport dataclasses\nfrom typing import Any,Callable,Literal\n\nfrom pydantic_core import PydanticUndefined\n\nfrom._internal import _internal_dataclass\n\n__all__=('AliasGenerator','AliasPath','AliasChoices')\n\n\n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass AliasPath:\n ''\n\n\n\n\n\n \n \n path:list[int |str]\n \n def __init__(self,first_arg:str,*args:str |int)->None:\n  self.path=[first_arg]+list(args)\n  \n def convert_to_aliases(self)->list[str |int]:\n  ''\n\n\n\n  \n  return self.path\n  \n def search_dict_for_path(self,d:dict)->Any:\n  ''\n\n\n\n  \n  v=d\n  for k in self.path:\n   if isinstance(v,str):\n   \n    return PydanticUndefined\n   try:\n    v=v[k]\n   except(KeyError,IndexError,TypeError):\n    return PydanticUndefined\n  return v\n  \n  \n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass AliasChoices:\n ''\n\n\n\n\n\n \n \n choices:list[str |AliasPath]\n \n def __init__(self,first_choice:str |AliasPath,*choices:str |AliasPath)->None:\n  self.choices=[first_choice]+list(choices)\n  \n def convert_to_aliases(self)->list[list[str |int]]:\n  ''\n\n\n\n  \n  aliases:list[list[str |int]]=[]\n  for c in self.choices:\n   if isinstance(c,AliasPath):\n    aliases.append(c.convert_to_aliases())\n   else:\n    aliases.append([c])\n  return aliases\n  \n  \n@dataclasses.dataclass(**_internal_dataclass.slots_true)\nclass AliasGenerator:\n ''\n\n\n\n\n\n\n\n \n \n alias:Callable[[str],str]|None=None\n validation_alias:Callable[[str],str |AliasPath |AliasChoices]|None=None\n serialization_alias:Callable[[str],str]|None=None\n \n def _generate_alias(\n self,\n alias_kind:Literal['alias','validation_alias','serialization_alias'],\n allowed_types:tuple[type[str]|type[AliasPath]|type[AliasChoices],...],\n field_name:str,\n )->str |AliasPath |AliasChoices |None:\n  ''\n\n\n\n  \n  alias=None\n  if alias_generator :=getattr(self,alias_kind):\n   alias=alias_generator(field_name)\n   if alias and not isinstance(alias,allowed_types):\n    raise TypeError(\n    f'Invalid `{alias_kind}` type. `{alias_kind}` generator must produce one of `{allowed_types}`'\n    )\n  return alias\n  \n def generate_aliases(self,field_name:str)->tuple[str |None,str |AliasPath |AliasChoices |None,str |None]:\n  ''\n\n\n\n  \n  alias=self._generate_alias('alias',(str,),field_name)\n  validation_alias=self._generate_alias('validation_alias',(str,AliasChoices,AliasPath),field_name)\n  serialization_alias=self._generate_alias('serialization_alias',(str,),field_name)\n  \n  return alias,validation_alias,serialization_alias\n", ["__future__", "dataclasses", "pydantic._internal", "pydantic._internal._internal_dataclass", "pydantic_core", "typing"]], "pydantic.alias_generators": [".py", "''\n\nimport re\n\n__all__=('to_pascal','to_camel','to_snake')\n\n\n\n\n\n\ndef to_pascal(snake:str)->str:\n ''\n\n\n\n\n\n\n \n camel=snake.title()\n return re.sub('([0-9A-Za-z])_(?=[0-9A-Z])',lambda m:m.group(1),camel)\n \n \ndef to_camel(snake:str)->str:\n ''\n\n\n\n\n\n\n \n \n \n if re.match('^[a-z]+[A-Za-z0-9]*$',snake)and not re.search(r'\\d[a-z]',snake):\n  return snake\n  \n camel=to_pascal(snake)\n return re.sub('(^_*[A-Z])',lambda m:m.group(1).lower(),camel)\n \n \ndef to_snake(camel:str)->str:\n ''\n\n\n\n\n\n\n \n \n snake=re.sub(r'([A-Z]+)([A-Z][a-z])',lambda m:f'{m.group(1)}_{m.group(2)}',camel)\n \n snake=re.sub(r'([a-z])([A-Z])',lambda m:f'{m.group(1)}_{m.group(2)}',snake)\n \n snake=re.sub(r'([0-9])([A-Z])',lambda m:f'{m.group(1)}_{m.group(2)}',snake)\n \n snake=re.sub(r'([a-z])([0-9])',lambda m:f'{m.group(1)}_{m.group(2)}',snake)\n \n snake=snake.replace('-','_')\n return snake.lower()\n", ["re"]], "pydantic.tools": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic._migration": [".py", "import sys\nfrom typing import Any,Callable,Dict\n\nfrom.version import version_short\n\nMOVED_IN_V2={\n'pydantic.utils:version_info':'pydantic.version:version_info',\n'pydantic.error_wrappers:ValidationError':'pydantic:ValidationError',\n'pydantic.utils:to_camel':'pydantic.alias_generators:to_pascal',\n'pydantic.utils:to_lower_camel':'pydantic.alias_generators:to_camel',\n'pydantic:PyObject':'pydantic.types:ImportString',\n'pydantic.types:PyObject':'pydantic.types:ImportString',\n'pydantic.generics:GenericModel':'pydantic.BaseModel',\n}\n\nDEPRECATED_MOVED_IN_V2={\n'pydantic.tools:schema_of':'pydantic.deprecated.tools:schema_of',\n'pydantic.tools:parse_obj_as':'pydantic.deprecated.tools:parse_obj_as',\n'pydantic.tools:schema_json_of':'pydantic.deprecated.tools:schema_json_of',\n'pydantic.json:pydantic_encoder':'pydantic.deprecated.json:pydantic_encoder',\n'pydantic:validate_arguments':'pydantic.deprecated.decorator:validate_arguments',\n'pydantic.json:custom_pydantic_encoder':'pydantic.deprecated.json:custom_pydantic_encoder',\n'pydantic.json:timedelta_isoformat':'pydantic.deprecated.json:timedelta_isoformat',\n'pydantic.decorator:validate_arguments':'pydantic.deprecated.decorator:validate_arguments',\n'pydantic.class_validators:validator':'pydantic.deprecated.class_validators:validator',\n'pydantic.class_validators:root_validator':'pydantic.deprecated.class_validators:root_validator',\n'pydantic.config:BaseConfig':'pydantic.deprecated.config:BaseConfig',\n'pydantic.config:Extra':'pydantic.deprecated.config:Extra',\n}\n\nREDIRECT_TO_V1={\nf'pydantic.utils:{obj}':f'pydantic.v1.utils:{obj}'\nfor obj in(\n'deep_update',\n'GetterDict',\n'lenient_issubclass',\n'lenient_isinstance',\n'is_valid_field',\n'update_not_none',\n'import_string',\n'Representation',\n'ROOT_KEY',\n'smart_deepcopy',\n'sequence_like',\n)\n}\n\n\nREMOVED_IN_V2={\n'pydantic:ConstrainedBytes',\n'pydantic:ConstrainedDate',\n'pydantic:ConstrainedDecimal',\n'pydantic:ConstrainedFloat',\n'pydantic:ConstrainedFrozenSet',\n'pydantic:ConstrainedInt',\n'pydantic:ConstrainedList',\n'pydantic:ConstrainedSet',\n'pydantic:ConstrainedStr',\n'pydantic:JsonWrapper',\n'pydantic:NoneBytes',\n'pydantic:NoneStr',\n'pydantic:NoneStrBytes',\n'pydantic:Protocol',\n'pydantic:Required',\n'pydantic:StrBytes',\n'pydantic:compiled',\n'pydantic.config:get_config',\n'pydantic.config:inherit_config',\n'pydantic.config:prepare_config',\n'pydantic:create_model_from_namedtuple',\n'pydantic:create_model_from_typeddict',\n'pydantic.dataclasses:create_pydantic_model_from_dataclass',\n'pydantic.dataclasses:make_dataclass_validator',\n'pydantic.dataclasses:set_validation',\n'pydantic.datetime_parse:parse_date',\n'pydantic.datetime_parse:parse_time',\n'pydantic.datetime_parse:parse_datetime',\n'pydantic.datetime_parse:parse_duration',\n'pydantic.error_wrappers:ErrorWrapper',\n'pydantic.errors:AnyStrMaxLengthError',\n'pydantic.errors:AnyStrMinLengthError',\n'pydantic.errors:ArbitraryTypeError',\n'pydantic.errors:BoolError',\n'pydantic.errors:BytesError',\n'pydantic.errors:CallableError',\n'pydantic.errors:ClassError',\n'pydantic.errors:ColorError',\n'pydantic.errors:ConfigError',\n'pydantic.errors:DataclassTypeError',\n'pydantic.errors:DateError',\n'pydantic.errors:DateNotInTheFutureError',\n'pydantic.errors:DateNotInThePastError',\n'pydantic.errors:DateTimeError',\n'pydantic.errors:DecimalError',\n'pydantic.errors:DecimalIsNotFiniteError',\n'pydantic.errors:DecimalMaxDigitsError',\n'pydantic.errors:DecimalMaxPlacesError',\n'pydantic.errors:DecimalWholeDigitsError',\n'pydantic.errors:DictError',\n'pydantic.errors:DurationError',\n'pydantic.errors:EmailError',\n'pydantic.errors:EnumError',\n'pydantic.errors:EnumMemberError',\n'pydantic.errors:ExtraError',\n'pydantic.errors:FloatError',\n'pydantic.errors:FrozenSetError',\n'pydantic.errors:FrozenSetMaxLengthError',\n'pydantic.errors:FrozenSetMinLengthError',\n'pydantic.errors:HashableError',\n'pydantic.errors:IPv4AddressError',\n'pydantic.errors:IPv4InterfaceError',\n'pydantic.errors:IPv4NetworkError',\n'pydantic.errors:IPv6AddressError',\n'pydantic.errors:IPv6InterfaceError',\n'pydantic.errors:IPv6NetworkError',\n'pydantic.errors:IPvAnyAddressError',\n'pydantic.errors:IPvAnyInterfaceError',\n'pydantic.errors:IPvAnyNetworkError',\n'pydantic.errors:IntEnumError',\n'pydantic.errors:IntegerError',\n'pydantic.errors:InvalidByteSize',\n'pydantic.errors:InvalidByteSizeUnit',\n'pydantic.errors:InvalidDiscriminator',\n'pydantic.errors:InvalidLengthForBrand',\n'pydantic.errors:JsonError',\n'pydantic.errors:JsonTypeError',\n'pydantic.errors:ListError',\n'pydantic.errors:ListMaxLengthError',\n'pydantic.errors:ListMinLengthError',\n'pydantic.errors:ListUniqueItemsError',\n'pydantic.errors:LuhnValidationError',\n'pydantic.errors:MissingDiscriminator',\n'pydantic.errors:MissingError',\n'pydantic.errors:NoneIsAllowedError',\n'pydantic.errors:NoneIsNotAllowedError',\n'pydantic.errors:NotDigitError',\n'pydantic.errors:NotNoneError',\n'pydantic.errors:NumberNotGeError',\n'pydantic.errors:NumberNotGtError',\n'pydantic.errors:NumberNotLeError',\n'pydantic.errors:NumberNotLtError',\n'pydantic.errors:NumberNotMultipleError',\n'pydantic.errors:PathError',\n'pydantic.errors:PathNotADirectoryError',\n'pydantic.errors:PathNotAFileError',\n'pydantic.errors:PathNotExistsError',\n'pydantic.errors:PatternError',\n'pydantic.errors:PyObjectError',\n'pydantic.errors:PydanticTypeError',\n'pydantic.errors:PydanticValueError',\n'pydantic.errors:SequenceError',\n'pydantic.errors:SetError',\n'pydantic.errors:SetMaxLengthError',\n'pydantic.errors:SetMinLengthError',\n'pydantic.errors:StrError',\n'pydantic.errors:StrRegexError',\n'pydantic.errors:StrictBoolError',\n'pydantic.errors:SubclassError',\n'pydantic.errors:TimeError',\n'pydantic.errors:TupleError',\n'pydantic.errors:TupleLengthError',\n'pydantic.errors:UUIDError',\n'pydantic.errors:UUIDVersionError',\n'pydantic.errors:UrlError',\n'pydantic.errors:UrlExtraError',\n'pydantic.errors:UrlHostError',\n'pydantic.errors:UrlHostTldError',\n'pydantic.errors:UrlPortError',\n'pydantic.errors:UrlSchemeError',\n'pydantic.errors:UrlSchemePermittedError',\n'pydantic.errors:UrlUserInfoError',\n'pydantic.errors:WrongConstantError',\n'pydantic.main:validate_model',\n'pydantic.networks:stricturl',\n'pydantic:parse_file_as',\n'pydantic:parse_raw_as',\n'pydantic:stricturl',\n'pydantic.tools:parse_file_as',\n'pydantic.tools:parse_raw_as',\n'pydantic.types:ConstrainedBytes',\n'pydantic.types:ConstrainedDate',\n'pydantic.types:ConstrainedDecimal',\n'pydantic.types:ConstrainedFloat',\n'pydantic.types:ConstrainedFrozenSet',\n'pydantic.types:ConstrainedInt',\n'pydantic.types:ConstrainedList',\n'pydantic.types:ConstrainedSet',\n'pydantic.types:ConstrainedStr',\n'pydantic.types:JsonWrapper',\n'pydantic.types:NoneBytes',\n'pydantic.types:NoneStr',\n'pydantic.types:NoneStrBytes',\n'pydantic.types:StrBytes',\n'pydantic.typing:evaluate_forwardref',\n'pydantic.typing:AbstractSetIntStr',\n'pydantic.typing:AnyCallable',\n'pydantic.typing:AnyClassMethod',\n'pydantic.typing:CallableGenerator',\n'pydantic.typing:DictAny',\n'pydantic.typing:DictIntStrAny',\n'pydantic.typing:DictStrAny',\n'pydantic.typing:IntStr',\n'pydantic.typing:ListStr',\n'pydantic.typing:MappingIntStrAny',\n'pydantic.typing:NoArgAnyCallable',\n'pydantic.typing:NoneType',\n'pydantic.typing:ReprArgs',\n'pydantic.typing:SetStr',\n'pydantic.typing:StrPath',\n'pydantic.typing:TupleGenerator',\n'pydantic.typing:WithArgsTypes',\n'pydantic.typing:all_literal_values',\n'pydantic.typing:display_as_type',\n'pydantic.typing:get_all_type_hints',\n'pydantic.typing:get_args',\n'pydantic.typing:get_origin',\n'pydantic.typing:get_sub_types',\n'pydantic.typing:is_callable_type',\n'pydantic.typing:is_classvar',\n'pydantic.typing:is_finalvar',\n'pydantic.typing:is_literal_type',\n'pydantic.typing:is_namedtuple',\n'pydantic.typing:is_new_type',\n'pydantic.typing:is_none_type',\n'pydantic.typing:is_typeddict',\n'pydantic.typing:is_typeddict_special',\n'pydantic.typing:is_union',\n'pydantic.typing:new_type_supertype',\n'pydantic.typing:resolve_annotations',\n'pydantic.typing:typing_base',\n'pydantic.typing:update_field_forward_refs',\n'pydantic.typing:update_model_forward_refs',\n'pydantic.utils:ClassAttribute',\n'pydantic.utils:DUNDER_ATTRIBUTES',\n'pydantic.utils:PyObjectStr',\n'pydantic.utils:ValueItems',\n'pydantic.utils:almost_equal_floats',\n'pydantic.utils:get_discriminator_alias_and_values',\n'pydantic.utils:get_model',\n'pydantic.utils:get_unique_discriminator_alias',\n'pydantic.utils:in_ipython',\n'pydantic.utils:is_valid_identifier',\n'pydantic.utils:path_type',\n'pydantic.utils:validate_field_name',\n'pydantic:validate_model',\n}\n\n\ndef getattr_migration(module:str)->Callable[[str],Any]:\n ''\n\n\n\n\n\n\n\n \n \n from.errors import PydanticImportError\n \n def wrapper(name:str)->object:\n  ''\n\n\n\n\n\n\n\n\n  \n  if name =='__path__':\n   raise AttributeError(f'module {module !r} has no attribute {name !r}')\n   \n  import warnings\n  \n  from._internal._validators import import_string\n  \n  import_path=f'{module}:{name}'\n  if import_path in MOVED_IN_V2.keys():\n   new_location=MOVED_IN_V2[import_path]\n   warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n   return import_string(MOVED_IN_V2[import_path])\n  if import_path in DEPRECATED_MOVED_IN_V2:\n  \n   return import_string(DEPRECATED_MOVED_IN_V2[import_path])\n  if import_path in REDIRECT_TO_V1:\n   new_location=REDIRECT_TO_V1[import_path]\n   warnings.warn(\n   f'`{import_path}` has been removed. We are importing from `{new_location}` instead.'\n   'See the migration guide for more details: https://docs.pydantic.dev/latest/migration/'\n   )\n   return import_string(REDIRECT_TO_V1[import_path])\n  if import_path =='pydantic:BaseSettings':\n   raise PydanticImportError(\n   '`BaseSettings` has been moved to the `pydantic-settings` package. '\n   f'See https://docs.pydantic.dev/{version_short()}/migration/#basesettings-has-moved-to-pydantic-settings '\n   'for more details.'\n   )\n  if import_path in REMOVED_IN_V2:\n   raise PydanticImportError(f'`{import_path}` has been removed in V2.')\n  globals:Dict[str,Any]=sys.modules[module].__dict__\n  if name in globals:\n   return globals[name]\n  raise AttributeError(f'module {module !r} has no attribute {name !r}')\n  \n return wrapper\n", ["pydantic._internal._validators", "pydantic.errors", "pydantic.version", "sys", "typing", "warnings"]], "pydantic.functional_validators": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport sys\nfrom functools import partialmethod\nfrom types import FunctionType\nfrom typing import TYPE_CHECKING,Any,Callable,TypeVar,Union,cast,overload\n\nfrom pydantic_core import PydanticUndefined,core_schema\nfrom pydantic_core import core_schema as _core_schema\nfrom typing_extensions import Annotated,Literal,Self,TypeAlias\n\nfrom._internal import _decorators,_generics,_internal_dataclass\nfrom.annotated_handlers import GetCoreSchemaHandler\nfrom.errors import PydanticUserError\n\nif sys.version_info <(3,11):\n from typing_extensions import Protocol\nelse:\n from typing import Protocol\n \n_inspect_validator=_decorators.inspect_validator\n\n\n@dataclasses.dataclass(frozen=True,**_internal_dataclass.slots_true)\nclass AfterValidator:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:core_schema.NoInfoValidatorFunction |core_schema.WithInfoValidatorFunction\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  schema=handler(source_type)\n  info_arg=_inspect_validator(self.func,'after')\n  if info_arg:\n   func=cast(core_schema.WithInfoValidatorFunction,self.func)\n   return core_schema.with_info_after_validator_function(func,schema=schema,field_name=handler.field_name)\n  else:\n   func=cast(core_schema.NoInfoValidatorFunction,self.func)\n   return core_schema.no_info_after_validator_function(func,schema=schema)\n   \n @classmethod\n def _from_decorator(cls,decorator:_decorators.Decorator[_decorators.FieldValidatorDecoratorInfo])->Self:\n  return cls(func=decorator.func)\n  \n  \n@dataclasses.dataclass(frozen=True,**_internal_dataclass.slots_true)\nclass BeforeValidator:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:core_schema.NoInfoValidatorFunction |core_schema.WithInfoValidatorFunction\n json_schema_input_type:Any=PydanticUndefined\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  schema=handler(source_type)\n  input_schema=(\n  None\n  if self.json_schema_input_type is PydanticUndefined\n  else handler.generate_schema(self.json_schema_input_type)\n  )\n  \n  info_arg=_inspect_validator(self.func,'before')\n  if info_arg:\n   func=cast(core_schema.WithInfoValidatorFunction,self.func)\n   return core_schema.with_info_before_validator_function(\n   func,\n   schema=schema,\n   field_name=handler.field_name,\n   json_schema_input_schema=input_schema,\n   )\n  else:\n   func=cast(core_schema.NoInfoValidatorFunction,self.func)\n   return core_schema.no_info_before_validator_function(\n   func,schema=schema,json_schema_input_schema=input_schema\n   )\n   \n @classmethod\n def _from_decorator(cls,decorator:_decorators.Decorator[_decorators.FieldValidatorDecoratorInfo])->Self:\n  return cls(\n  func=decorator.func,\n  json_schema_input_type=decorator.info.json_schema_input_type,\n  )\n  \n  \n@dataclasses.dataclass(frozen=True,**_internal_dataclass.slots_true)\nclass PlainValidator:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:core_schema.NoInfoValidatorFunction |core_schema.WithInfoValidatorFunction\n json_schema_input_type:Any=Any\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n \n \n \n \n \n  from pydantic import PydanticSchemaGenerationError\n  \n  try:\n   schema=handler(source_type)\n   \n   \n   serialization=schema.get(\n   'serialization',\n   core_schema.wrap_serializer_function_ser_schema(\n   function=lambda v,h:h(v),\n   schema=schema,\n   return_schema=handler.generate_schema(source_type),\n   ),\n   )\n  except PydanticSchemaGenerationError:\n   serialization=None\n   \n  input_schema=handler.generate_schema(self.json_schema_input_type)\n  \n  info_arg=_inspect_validator(self.func,'plain')\n  if info_arg:\n   func=cast(core_schema.WithInfoValidatorFunction,self.func)\n   return core_schema.with_info_plain_validator_function(\n   func,\n   field_name=handler.field_name,\n   serialization=serialization,\n   json_schema_input_schema=input_schema,\n   )\n  else:\n   func=cast(core_schema.NoInfoValidatorFunction,self.func)\n   return core_schema.no_info_plain_validator_function(\n   func,\n   serialization=serialization,\n   json_schema_input_schema=input_schema,\n   )\n   \n @classmethod\n def _from_decorator(cls,decorator:_decorators.Decorator[_decorators.FieldValidatorDecoratorInfo])->Self:\n  return cls(\n  func=decorator.func,\n  json_schema_input_type=decorator.info.json_schema_input_type,\n  )\n  \n  \n@dataclasses.dataclass(frozen=True,**_internal_dataclass.slots_true)\nclass WrapValidator:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:core_schema.NoInfoWrapValidatorFunction |core_schema.WithInfoWrapValidatorFunction\n json_schema_input_type:Any=PydanticUndefined\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  schema=handler(source_type)\n  input_schema=(\n  None\n  if self.json_schema_input_type is PydanticUndefined\n  else handler.generate_schema(self.json_schema_input_type)\n  )\n  \n  info_arg=_inspect_validator(self.func,'wrap')\n  if info_arg:\n   func=cast(core_schema.WithInfoWrapValidatorFunction,self.func)\n   return core_schema.with_info_wrap_validator_function(\n   func,\n   schema=schema,\n   field_name=handler.field_name,\n   json_schema_input_schema=input_schema,\n   )\n  else:\n   func=cast(core_schema.NoInfoWrapValidatorFunction,self.func)\n   return core_schema.no_info_wrap_validator_function(\n   func,\n   schema=schema,\n   json_schema_input_schema=input_schema,\n   )\n   \n @classmethod\n def _from_decorator(cls,decorator:_decorators.Decorator[_decorators.FieldValidatorDecoratorInfo])->Self:\n  return cls(\n  func=decorator.func,\n  json_schema_input_type=decorator.info.json_schema_input_type,\n  )\n  \n  \nif TYPE_CHECKING:\n\n class _OnlyValueValidatorClsMethod(Protocol):\n  def __call__(self,cls:Any,value:Any,/)->Any:...\n  \n class _V2ValidatorClsMethod(Protocol):\n  def __call__(self,cls:Any,value:Any,info:_core_schema.ValidationInfo,/)->Any:...\n  \n class _OnlyValueWrapValidatorClsMethod(Protocol):\n  def __call__(self,cls:Any,value:Any,handler:_core_schema.ValidatorFunctionWrapHandler,/)->Any:...\n  \n class _V2WrapValidatorClsMethod(Protocol):\n  def __call__(\n  self,\n  cls:Any,\n  value:Any,\n  handler:_core_schema.ValidatorFunctionWrapHandler,\n  info:_core_schema.ValidationInfo,\n  /,\n  )->Any:...\n  \n _V2Validator=Union[\n _V2ValidatorClsMethod,\n _core_schema.WithInfoValidatorFunction,\n _OnlyValueValidatorClsMethod,\n _core_schema.NoInfoValidatorFunction,\n ]\n \n _V2WrapValidator=Union[\n _V2WrapValidatorClsMethod,\n _core_schema.WithInfoWrapValidatorFunction,\n _OnlyValueWrapValidatorClsMethod,\n _core_schema.NoInfoWrapValidatorFunction,\n ]\n \n _PartialClsOrStaticMethod:TypeAlias=Union[classmethod[Any,Any,Any],staticmethod[Any,Any],partialmethod[Any]]\n \n _V2BeforeAfterOrPlainValidatorType=TypeVar(\n '_V2BeforeAfterOrPlainValidatorType',\n bound=Union[_V2Validator,_PartialClsOrStaticMethod],\n )\n _V2WrapValidatorType=TypeVar('_V2WrapValidatorType',bound=Union[_V2WrapValidator,_PartialClsOrStaticMethod])\n \nFieldValidatorModes:TypeAlias=Literal['before','after','wrap','plain']\n\n\n@overload\ndef field_validator(\nfield:str,\n/,\n*fields:str,\nmode:Literal['wrap'],\ncheck_fields:bool |None=...,\njson_schema_input_type:Any=...,\n)->Callable[[_V2WrapValidatorType],_V2WrapValidatorType]:...\n\n\n@overload\ndef field_validator(\nfield:str,\n/,\n*fields:str,\nmode:Literal['before','plain'],\ncheck_fields:bool |None=...,\njson_schema_input_type:Any=...,\n)->Callable[[_V2BeforeAfterOrPlainValidatorType],_V2BeforeAfterOrPlainValidatorType]:...\n\n\n@overload\ndef field_validator(\nfield:str,\n/,\n*fields:str,\nmode:Literal['after']=...,\ncheck_fields:bool |None=...,\n)->Callable[[_V2BeforeAfterOrPlainValidatorType],_V2BeforeAfterOrPlainValidatorType]:...\n\n\ndef field_validator(\nfield:str,\n/,\n*fields:str,\nmode:FieldValidatorModes='after',\ncheck_fields:bool |None=None,\njson_schema_input_type:Any=PydanticUndefined,\n)->Callable[[Any],Any]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if isinstance(field,FunctionType):\n  raise PydanticUserError(\n  '`@field_validator` should be used with fields and keyword arguments, not bare. '\n  \"E.g. usage should be `@validator('<field_name>', ...)`\",\n  code='validator-no-fields',\n  )\n  \n if mode not in('before','plain','wrap')and json_schema_input_type is not PydanticUndefined:\n  raise PydanticUserError(\n  f\"`json_schema_input_type` can't be used when mode is set to {mode !r}\",\n  code='validator-input-type',\n  )\n  \n if json_schema_input_type is PydanticUndefined and mode =='plain':\n  json_schema_input_type=Any\n  \n fields=field,*fields\n if not all(isinstance(field,str)for field in fields):\n  raise PydanticUserError(\n  '`@field_validator` fields should be passed as separate string args. '\n  \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\",\n  code='validator-invalid-fields',\n  )\n  \n def dec(\n f:Callable[...,Any]|staticmethod[Any,Any]|classmethod[Any,Any,Any],\n )->_decorators.PydanticDescriptorProxy[Any]:\n  if _decorators.is_instance_method_from_sig(f):\n   raise PydanticUserError(\n   '`@field_validator` cannot be applied to instance methods',code='validator-instance-method'\n   )\n   \n   \n  f=_decorators.ensure_classmethod_based_on_signature(f)\n  \n  dec_info=_decorators.FieldValidatorDecoratorInfo(\n  fields=fields,mode=mode,check_fields=check_fields,json_schema_input_type=json_schema_input_type\n  )\n  return _decorators.PydanticDescriptorProxy(f,dec_info)\n  \n return dec\n \n \n_ModelType=TypeVar('_ModelType')\n_ModelTypeCo=TypeVar('_ModelTypeCo',covariant=True)\n\n\nclass ModelWrapValidatorHandler(_core_schema.ValidatorFunctionWrapHandler,Protocol[_ModelTypeCo]):\n ''\n \n def __call__(\n self,\n value:Any,\n outer_location:str |int |None=None,\n /,\n )->_ModelTypeCo:\n  ...\n  \n  \nclass ModelWrapValidatorWithoutInfo(Protocol[_ModelType]):\n ''\n\n \n \n def __call__(\n self,\n cls:type[_ModelType],\n \n \n \n value:Any,\n handler:ModelWrapValidatorHandler[_ModelType],\n /,\n )->_ModelType:...\n \n \nclass ModelWrapValidator(Protocol[_ModelType]):\n ''\n \n def __call__(\n self,\n cls:type[_ModelType],\n \n \n \n value:Any,\n handler:ModelWrapValidatorHandler[_ModelType],\n info:_core_schema.ValidationInfo,\n /,\n )->_ModelType:...\n \n \nclass FreeModelBeforeValidatorWithoutInfo(Protocol):\n ''\n\n \n \n def __call__(\n self,\n \n \n \n value:Any,\n /,\n )->Any:...\n \n \nclass ModelBeforeValidatorWithoutInfo(Protocol):\n ''\n\n \n \n def __call__(\n self,\n cls:Any,\n \n \n \n value:Any,\n /,\n )->Any:...\n \n \nclass FreeModelBeforeValidator(Protocol):\n ''\n \n def __call__(\n self,\n \n \n \n value:Any,\n info:_core_schema.ValidationInfo,\n /,\n )->Any:...\n \n \nclass ModelBeforeValidator(Protocol):\n ''\n \n def __call__(\n self,\n cls:Any,\n \n \n \n value:Any,\n info:_core_schema.ValidationInfo,\n /,\n )->Any:...\n \n \nModelAfterValidatorWithoutInfo=Callable[[_ModelType],_ModelType]\n''\n\n\n\nModelAfterValidator=Callable[[_ModelType,_core_schema.ValidationInfo],_ModelType]\n''\n\n_AnyModelWrapValidator=Union[ModelWrapValidator[_ModelType],ModelWrapValidatorWithoutInfo[_ModelType]]\n_AnyModelBeforeValidator=Union[\nFreeModelBeforeValidator,ModelBeforeValidator,FreeModelBeforeValidatorWithoutInfo,ModelBeforeValidatorWithoutInfo\n]\n_AnyModelAfterValidator=Union[ModelAfterValidator[_ModelType],ModelAfterValidatorWithoutInfo[_ModelType]]\n\n\n@overload\ndef model_validator(\n*,\nmode:Literal['wrap'],\n)->Callable[\n[_AnyModelWrapValidator[_ModelType]],_decorators.PydanticDescriptorProxy[_decorators.ModelValidatorDecoratorInfo]\n]:...\n\n\n@overload\ndef model_validator(\n*,\nmode:Literal['before'],\n)->Callable[\n[_AnyModelBeforeValidator],_decorators.PydanticDescriptorProxy[_decorators.ModelValidatorDecoratorInfo]\n]:...\n\n\n@overload\ndef model_validator(\n*,\nmode:Literal['after'],\n)->Callable[\n[_AnyModelAfterValidator[_ModelType]],_decorators.PydanticDescriptorProxy[_decorators.ModelValidatorDecoratorInfo]\n]:...\n\n\ndef model_validator(\n*,\nmode:Literal['wrap','before','after'],\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def dec(f:Any)->_decorators.PydanticDescriptorProxy[Any]:\n \n  f=_decorators.ensure_classmethod_based_on_signature(f)\n  dec_info=_decorators.ModelValidatorDecoratorInfo(mode=mode)\n  return _decorators.PydanticDescriptorProxy(f,dec_info)\n  \n return dec\n \n \nAnyType=TypeVar('AnyType')\n\n\nif TYPE_CHECKING:\n\n InstanceOf=Annotated[AnyType,...]\n \nelse:\n\n @dataclasses.dataclass(**_internal_dataclass.slots_true)\n class InstanceOf:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  @classmethod\n  def __class_getitem__(cls,item:AnyType)->AnyType:\n   return Annotated[item,cls()]\n   \n  @classmethod\n  def __get_pydantic_core_schema__(cls,source:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n   from pydantic import PydanticSchemaGenerationError\n   \n   \n   instance_of_schema=core_schema.is_instance_schema(_generics.get_origin(source)or source)\n   \n   try:\n   \n    original_schema=handler(source)\n   except PydanticSchemaGenerationError:\n   \n    return instance_of_schema\n   else:\n   \n    instance_of_schema['serialization']=core_schema.wrap_serializer_function_ser_schema(\n    function=lambda v,h:h(v),schema=original_schema\n    )\n    return core_schema.json_or_python_schema(python_schema=instance_of_schema,json_schema=original_schema)\n    \n  __hash__=object.__hash__\n  \n  \nif TYPE_CHECKING:\n SkipValidation=Annotated[AnyType,...]\nelse:\n\n @dataclasses.dataclass(**_internal_dataclass.slots_true)\n class SkipValidation:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  def __class_getitem__(cls,item:Any)->Any:\n   return Annotated[item,SkipValidation()]\n   \n  @classmethod\n  def __get_pydantic_core_schema__(cls,source:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n   original_schema=handler(source)\n   metadata={'pydantic_js_annotation_functions':[lambda _c,h:h(original_schema)]}\n   return core_schema.any_schema(\n   metadata=metadata,\n   serialization=core_schema.wrap_serializer_function_ser_schema(\n   function=lambda v,h:h(v),schema=original_schema\n   ),\n   )\n   \n  __hash__=object.__hash__\n", ["__future__", "dataclasses", "functools", "pydantic", "pydantic._internal", "pydantic._internal._decorators", "pydantic._internal._generics", "pydantic._internal._internal_dataclass", "pydantic.annotated_handlers", "pydantic.errors", "pydantic_core", "pydantic_core.core_schema", "sys", "types", "typing", "typing_extensions"]], "pydantic.error_wrappers": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.config": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nfrom re import Pattern\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,List,Type,TypeVar,Union\n\nfrom typing_extensions import Literal,TypeAlias,TypedDict\n\nfrom._migration import getattr_migration\nfrom.aliases import AliasGenerator\nfrom.errors import PydanticUserError\n\nif TYPE_CHECKING:\n from._internal._generate_schema import GenerateSchema as _GenerateSchema\n from.fields import ComputedFieldInfo,FieldInfo\n \n__all__=('ConfigDict','with_config')\n\n\nJsonValue:TypeAlias=Union[int,float,str,bool,None,List['JsonValue'],'JsonDict']\nJsonDict:TypeAlias=Dict[str,JsonValue]\n\nJsonEncoder=Callable[[Any],Any]\n\nJsonSchemaExtraCallable:TypeAlias=Union[\nCallable[[JsonDict],None],\nCallable[[JsonDict,Type[Any]],None],\n]\n\nExtraValues=Literal['allow','ignore','forbid']\n\n\nclass ConfigDict(TypedDict,total=False):\n ''\n \n title:str |None\n '' \n \n model_title_generator:Callable[[type],str]|None\n '' \n \n field_title_generator:Callable[[str,FieldInfo |ComputedFieldInfo],str]|None\n '' \n \n str_to_lower:bool\n '' \n \n str_to_upper:bool\n '' \n \n str_strip_whitespace:bool\n '' \n \n str_min_length:int\n '' \n \n str_max_length:int |None\n '' \n \n extra:ExtraValues |None\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n frozen:bool\n ''\n\n\n\n\n\n\n \n \n populate_by_name:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n use_enum_values:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n validate_assignment:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n arbitrary_types_allowed:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n from_attributes:bool\n ''\n\n \n \n loc_by_alias:bool\n '' \n \n alias_generator:Callable[[str],str]|AliasGenerator |None\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n ignored_types:tuple[type,...]\n ''\n\n\n\n \n \n allow_inf_nan:bool\n '' \n \n json_schema_extra:JsonDict |JsonSchemaExtraCallable |None\n '' \n \n json_encoders:dict[type[object],JsonEncoder]|None\n ''\n\n\n\n\n\n\n \n \n \n strict:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n revalidate_instances:Literal['always','never','subclass-instances']\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n ser_json_timedelta:Literal['iso8601','float']\n ''\n\n\n\n\n\n \n \n ser_json_bytes:Literal['utf8','base64','hex']\n ''\n\n\n\n\n\n\n \n \n val_json_bytes:Literal['utf8','base64','hex']\n ''\n\n\n\n\n\n\n \n \n ser_json_inf_nan:Literal['null','constants','strings']\n ''\n\n\n\n\n\n \n \n \n validate_default:bool\n '' \n \n validate_return:bool\n '' \n \n protected_namespaces:tuple[str |Pattern[str],...]\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n hide_input_in_errors:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n defer_build:bool\n ''\n\n\n\n\n\n\n\n \n \n plugin_settings:dict[str,object]|None\n '' \n \n schema_generator:type[_GenerateSchema]|None\n ''\n\n\n\n\n\n\n \n \n json_schema_serialization_defaults_required:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n json_schema_mode_override:Literal['validation','serialization',None]\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n coerce_numbers_to_str:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n regex_engine:Literal['rust-regex','python-re']\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n validation_error_cause:bool\n ''\n\n\n\n\n\n\n\n \n \n use_attribute_docstrings:bool\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n cache_strings:bool |Literal['all','keys','none']\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n_TypeT=TypeVar('_TypeT',bound=type)\n\n\ndef with_config(config:ConfigDict)->Callable[[_TypeT],_TypeT]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def inner(class_:_TypeT,/)->_TypeT:\n \n \n \n  from._internal._utils import is_model_class\n  \n  if is_model_class(class_):\n   raise PydanticUserError(\n   f'Cannot use `with_config` on {class_.__name__} as it is a Pydantic model',\n   code='with-config-on-model',\n   )\n  class_.__pydantic_config__=config\n  return class_\n  \n return inner\n \n \n__getattr__=getattr_migration(__name__)\n", ["__future__", "pydantic._internal._generate_schema", "pydantic._internal._utils", "pydantic._migration", "pydantic.aliases", "pydantic.errors", "pydantic.fields", "re", "typing", "typing_extensions"]], "pydantic.datetime_parse": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.utils": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.validators": [".py", "''\n\nfrom._migration import getattr_migration\n\n__getattr__=getattr_migration(__name__)\n", ["pydantic._migration"]], "pydantic.root_model": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport typing\nfrom copy import copy,deepcopy\n\nfrom pydantic_core import PydanticUndefined\n\nfrom. import PydanticUserError\nfrom._internal import _model_construction,_repr\nfrom.main import BaseModel,_object_setattr\n\nif typing.TYPE_CHECKING:\n from typing import Any\n \n from typing_extensions import Literal,Self,dataclass_transform\n \n from.fields import Field as PydanticModelField\n from.fields import PrivateAttr as PydanticModelPrivateAttr\n \n \n \n \n @dataclass_transform(kw_only_default=False,field_specifiers=(PydanticModelField,PydanticModelPrivateAttr))\n class _RootModelMetaclass(_model_construction.ModelMetaclass):...\nelse:\n _RootModelMetaclass=_model_construction.ModelMetaclass\n \n__all__=('RootModel',)\n\nRootModelRootType=typing.TypeVar('RootModelRootType')\n\n\nclass RootModel(BaseModel,typing.Generic[RootModelRootType],metaclass=_RootModelMetaclass):\n ''\n\n\n\n\n\n\n\n\n\n \n \n __pydantic_root_model__=True\n __pydantic_private__=None\n __pydantic_extra__=None\n \n root:RootModelRootType\n \n def __init_subclass__(cls,**kwargs):\n  extra=cls.model_config.get('extra')\n  if extra is not None:\n   raise PydanticUserError(\n   \"`RootModel` does not support setting `model_config['extra']`\",code='root-model-extra'\n   )\n  super().__init_subclass__(**kwargs)\n  \n def __init__(self,/,root:RootModelRootType=PydanticUndefined,**data)->None:\n  __tracebackhide__=True\n  if data:\n   if root is not PydanticUndefined:\n    raise ValueError(\n    '\"RootModel.__init__\" accepts either a single positional argument or arbitrary keyword arguments'\n    )\n   root=data\n  self.__pydantic_validator__.validate_python(root,self_instance=self)\n  \n __init__.__pydantic_base_init__=True\n \n @classmethod\n def model_construct(cls,root:RootModelRootType,_fields_set:set[str]|None=None)->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return super().model_construct(root=root,_fields_set=_fields_set)\n  \n def __getstate__(self)->dict[Any,Any]:\n  return{\n  '__dict__':self.__dict__,\n  '__pydantic_fields_set__':self.__pydantic_fields_set__,\n  }\n  \n def __setstate__(self,state:dict[Any,Any])->None:\n  _object_setattr(self,'__pydantic_fields_set__',state['__pydantic_fields_set__'])\n  _object_setattr(self,'__dict__',state['__dict__'])\n  \n def __copy__(self)->Self:\n  ''\n  cls=type(self)\n  m=cls.__new__(cls)\n  _object_setattr(m,'__dict__',copy(self.__dict__))\n  _object_setattr(m,'__pydantic_fields_set__',copy(self.__pydantic_fields_set__))\n  return m\n  \n def __deepcopy__(self,memo:dict[int,Any]|None=None)->Self:\n  ''\n  cls=type(self)\n  m=cls.__new__(cls)\n  _object_setattr(m,'__dict__',deepcopy(self.__dict__,memo=memo))\n  \n  \n  _object_setattr(m,'__pydantic_fields_set__',copy(self.__pydantic_fields_set__))\n  return m\n  \n if typing.TYPE_CHECKING:\n \n  def model_dump(\n  self,\n  *,\n  mode:Literal['json','python']|str='python',\n  include:Any=None,\n  exclude:Any=None,\n  context:dict[str,Any]|None=None,\n  by_alias:bool=False,\n  exclude_unset:bool=False,\n  exclude_defaults:bool=False,\n  exclude_none:bool=False,\n  round_trip:bool=False,\n  warnings:bool |Literal['none','warn','error']=True,\n  serialize_as_any:bool=False,\n  )->Any:\n   ''\n\n\n\n\n\n\n\n\n\n   \n   ...\n   \n def __eq__(self,other:Any)->bool:\n  if not isinstance(other,RootModel):\n   return NotImplemented\n  return self.__pydantic_fields__['root'].annotation ==other.__pydantic_fields__[\n  'root'\n  ].annotation and super().__eq__(other)\n  \n def __repr_args__(self)->_repr.ReprArgs:\n  yield 'root',self.root\n", ["__future__", "copy", "pydantic", "pydantic._internal", "pydantic._internal._model_construction", "pydantic._internal._repr", "pydantic.fields", "pydantic.main", "pydantic_core", "typing", "typing_extensions"]], "pydantic.mypy": [".py", "''\n\nfrom __future__ import annotations\n\nimport sys\nfrom configparser import ConfigParser\nfrom typing import Any,Callable,Iterator\n\nfrom mypy.errorcodes import ErrorCode\nfrom mypy.expandtype import expand_type,expand_type_by_instance\nfrom mypy.nodes import(\nARG_NAMED,\nARG_NAMED_OPT,\nARG_OPT,\nARG_POS,\nARG_STAR2,\nINVARIANT,\nMDEF,\nArgument,\nAssignmentStmt,\nBlock,\nCallExpr,\nClassDef,\nContext,\nDecorator,\nDictExpr,\nEllipsisExpr,\nExpression,\nFuncDef,\nIfStmt,\nJsonDict,\nMemberExpr,\nNameExpr,\nPassStmt,\nPlaceholderNode,\nRefExpr,\nStatement,\nStrExpr,\nSymbolTableNode,\nTempNode,\nTypeAlias,\nTypeInfo,\nVar,\n)\nfrom mypy.options import Options\nfrom mypy.plugin import(\nCheckerPluginInterface,\nClassDefContext,\nMethodContext,\nPlugin,\nReportConfigContext,\nSemanticAnalyzerPluginInterface,\n)\nfrom mypy.plugins.common import(\ndeserialize_and_fixup_type,\n)\nfrom mypy.semanal import set_callable_name\nfrom mypy.server.trigger import make_wildcard_trigger\nfrom mypy.state import state\nfrom mypy.typeops import map_type_from_supertype\nfrom mypy.types import(\nAnyType,\nCallableType,\nInstance,\nNoneType,\nType,\nTypeOfAny,\nTypeType,\nTypeVarType,\nUnionType,\nget_proper_type,\n)\nfrom mypy.typevars import fill_typevars\nfrom mypy.util import get_unique_redefinition_name\nfrom mypy.version import __version__ as mypy_version\n\nfrom pydantic._internal import _fields\nfrom pydantic.version import parse_mypy_version\n\nCONFIGFILE_KEY='pydantic-mypy'\nMETADATA_KEY='pydantic-mypy-metadata'\nBASEMODEL_FULLNAME='pydantic.main.BaseModel'\nBASESETTINGS_FULLNAME='pydantic_settings.main.BaseSettings'\nROOT_MODEL_FULLNAME='pydantic.root_model.RootModel'\nMODEL_METACLASS_FULLNAME='pydantic._internal._model_construction.ModelMetaclass'\nFIELD_FULLNAME='pydantic.fields.Field'\nDATACLASS_FULLNAME='pydantic.dataclasses.dataclass'\nMODEL_VALIDATOR_FULLNAME='pydantic.functional_validators.model_validator'\nDECORATOR_FULLNAMES={\n'pydantic.functional_validators.field_validator',\n'pydantic.functional_validators.model_validator',\n'pydantic.functional_serializers.serializer',\n'pydantic.functional_serializers.model_serializer',\n'pydantic.deprecated.class_validators.validator',\n'pydantic.deprecated.class_validators.root_validator',\n}\n\n\nMYPY_VERSION_TUPLE=parse_mypy_version(mypy_version)\nBUILTINS_NAME='builtins'\n\n\n__version__=2\n\n\ndef plugin(version:str)->type[Plugin]:\n ''\n\n\n\n\n\n\n\n\n\n \n return PydanticPlugin\n \n \nclass PydanticPlugin(Plugin):\n ''\n \n def __init__(self,options:Options)->None:\n  self.plugin_config=PydanticPluginConfig(options)\n  self._plugin_data=self.plugin_config.to_data()\n  super().__init__(options)\n  \n def get_base_class_hook(self,fullname:str)->Callable[[ClassDefContext],None]|None:\n  ''\n  sym=self.lookup_fully_qualified(fullname)\n  if sym and isinstance(sym.node,TypeInfo):\n  \n   if any(base.fullname ==BASEMODEL_FULLNAME for base in sym.node.mro):\n    return self._pydantic_model_class_maker_callback\n  return None\n  \n def get_metaclass_hook(self,fullname:str)->Callable[[ClassDefContext],None]|None:\n  ''\n  if fullname ==MODEL_METACLASS_FULLNAME:\n   return self._pydantic_model_metaclass_marker_callback\n  return None\n  \n def get_method_hook(self,fullname:str)->Callable[[MethodContext],Type]|None:\n  ''\n  if fullname.endswith('.from_orm'):\n   return from_attributes_callback\n  return None\n  \n def report_config_data(self,ctx:ReportConfigContext)->dict[str,Any]:\n  ''\n\n\n  \n  return self._plugin_data\n  \n def _pydantic_model_class_maker_callback(self,ctx:ClassDefContext)->None:\n  transformer=PydanticModelTransformer(ctx.cls,ctx.reason,ctx.api,self.plugin_config)\n  transformer.transform()\n  \n def _pydantic_model_metaclass_marker_callback(self,ctx:ClassDefContext)->None:\n  ''\n\n\n\n  \n  if self.plugin_config.debug_dataclass_transform:\n   return\n  info_metaclass=ctx.cls.info.declared_metaclass\n  assert info_metaclass,\"callback not passed from 'get_metaclass_hook'\"\n  if getattr(info_metaclass.type,'dataclass_transform_spec',None):\n   info_metaclass.type.dataclass_transform_spec=None\n   \n   \nclass PydanticPluginConfig:\n ''\n\n\n\n\n\n\n\n \n \n __slots__=(\n 'init_forbid_extra',\n 'init_typed',\n 'warn_required_dynamic_aliases',\n 'debug_dataclass_transform',\n )\n init_forbid_extra:bool\n init_typed:bool\n warn_required_dynamic_aliases:bool\n debug_dataclass_transform:bool\n \n def __init__(self,options:Options)->None:\n  if options.config_file is None:\n   return\n   \n  toml_config=parse_toml(options.config_file)\n  if toml_config is not None:\n   config=toml_config.get('tool',{}).get('pydantic-mypy',{})\n   for key in self.__slots__:\n    setting=config.get(key,False)\n    if not isinstance(setting,bool):\n     raise ValueError(f'Configuration value must be a boolean for key: {key}')\n    setattr(self,key,setting)\n  else:\n   plugin_config=ConfigParser()\n   plugin_config.read(options.config_file)\n   for key in self.__slots__:\n    setting=plugin_config.getboolean(CONFIGFILE_KEY,key,fallback=False)\n    setattr(self,key,setting)\n    \n def to_data(self)->dict[str,Any]:\n  ''\n  return{key:getattr(self,key)for key in self.__slots__}\n  \n  \ndef from_attributes_callback(ctx:MethodContext)->Type:\n ''\n model_type:Instance\n ctx_type=ctx.type\n if isinstance(ctx_type,TypeType):\n  ctx_type=ctx_type.item\n if isinstance(ctx_type,CallableType)and isinstance(ctx_type.ret_type,Instance):\n  model_type=ctx_type.ret_type\n elif isinstance(ctx_type,Instance):\n  model_type=ctx_type\n else:\n  detail=f'ctx.type: {ctx_type} (of type {ctx_type.__class__.__name__})'\n  error_unexpected_behavior(detail,ctx.api,ctx.context)\n  return ctx.default_return_type\n pydantic_metadata=model_type.type.metadata.get(METADATA_KEY)\n if pydantic_metadata is None:\n  return ctx.default_return_type\n if not any(base.fullname ==BASEMODEL_FULLNAME for base in model_type.type.mro):\n \n  return ctx.default_return_type\n from_attributes=pydantic_metadata.get('config',{}).get('from_attributes')\n if from_attributes is not True:\n  error_from_attributes(model_type.type.name,ctx.api,ctx.context)\n return ctx.default_return_type\n \n \nclass PydanticModelField:\n ''\n \n def __init__(\n self,\n name:str,\n alias:str |None,\n is_frozen:bool,\n has_dynamic_alias:bool,\n has_default:bool,\n strict:bool |None,\n line:int,\n column:int,\n type:Type |None,\n info:TypeInfo,\n ):\n  self.name=name\n  self.alias=alias\n  self.is_frozen=is_frozen\n  self.has_dynamic_alias=has_dynamic_alias\n  self.has_default=has_default\n  self.strict=strict\n  self.line=line\n  self.column=column\n  self.type=type\n  self.info=info\n  \n def to_argument(\n self,\n current_info:TypeInfo,\n typed:bool,\n model_strict:bool,\n force_optional:bool,\n use_alias:bool,\n api:SemanticAnalyzerPluginInterface,\n force_typevars_invariant:bool,\n is_root_model_root:bool,\n )->Argument:\n  ''\n  variable=self.to_var(current_info,api,use_alias,force_typevars_invariant)\n  \n  strict=model_strict if self.strict is None else self.strict\n  if typed or strict:\n   type_annotation=self.expand_type(current_info,api)\n  else:\n   type_annotation=AnyType(TypeOfAny.explicit)\n   \n  return Argument(\n  variable=variable,\n  type_annotation=type_annotation,\n  initializer=None,\n  kind=ARG_OPT\n  if is_root_model_root\n  else(ARG_NAMED_OPT if force_optional or self.has_default else ARG_NAMED),\n  )\n  \n def expand_type(\n self,current_info:TypeInfo,api:SemanticAnalyzerPluginInterface,force_typevars_invariant:bool=False\n )->Type |None:\n  ''\n  if force_typevars_invariant:\n  \n  \n  \n  \n   if isinstance(self.type,TypeVarType):\n    modified_type=self.type.copy_modified()\n    modified_type.variance=INVARIANT\n    self.type=modified_type\n    \n  if self.type is not None and self.info.self_type is not None:\n  \n  \n  \n  \n   with state.strict_optional_set(api.options.strict_optional):\n    filled_with_typevars=fill_typevars(current_info)\n    \n    assert isinstance(filled_with_typevars,Instance)\n    if force_typevars_invariant:\n     for arg in filled_with_typevars.args:\n      if isinstance(arg,TypeVarType):\n       arg.variance=INVARIANT\n    return expand_type(self.type,{self.info.self_type.id:filled_with_typevars})\n  return self.type\n  \n def to_var(\n self,\n current_info:TypeInfo,\n api:SemanticAnalyzerPluginInterface,\n use_alias:bool,\n force_typevars_invariant:bool=False,\n )->Var:\n  ''\n  if use_alias and self.alias is not None:\n   name=self.alias\n  else:\n   name=self.name\n   \n  return Var(name,self.expand_type(current_info,api,force_typevars_invariant))\n  \n def serialize(self)->JsonDict:\n  ''\n  assert self.type\n  return{\n  'name':self.name,\n  'alias':self.alias,\n  'is_frozen':self.is_frozen,\n  'has_dynamic_alias':self.has_dynamic_alias,\n  'has_default':self.has_default,\n  'strict':self.strict,\n  'line':self.line,\n  'column':self.column,\n  'type':self.type.serialize(),\n  }\n  \n @classmethod\n def deserialize(cls,info:TypeInfo,data:JsonDict,api:SemanticAnalyzerPluginInterface)->PydanticModelField:\n  ''\n  data=data.copy()\n  typ=deserialize_and_fixup_type(data.pop('type'),api)\n  return cls(type=typ,info=info,**data)\n  \n def expand_typevar_from_subtype(self,sub_type:TypeInfo,api:SemanticAnalyzerPluginInterface)->None:\n  ''\n\n  \n  if self.type is not None:\n   with state.strict_optional_set(api.options.strict_optional):\n    self.type=map_type_from_supertype(self.type,sub_type,self.info)\n    \n    \nclass PydanticModelClassVar:\n ''\n\n\n\n\n\n \n \n def __init__(self,name):\n  self.name=name\n  \n @classmethod\n def deserialize(cls,data:JsonDict)->PydanticModelClassVar:\n  ''\n  data=data.copy()\n  return cls(**data)\n  \n def serialize(self)->JsonDict:\n  ''\n  return{\n  'name':self.name,\n  }\n  \n  \nclass PydanticModelTransformer:\n ''\n\n\n\n \n \n tracked_config_fields:set[str]={\n 'extra',\n 'frozen',\n 'from_attributes',\n 'populate_by_name',\n 'alias_generator',\n 'strict',\n }\n \n def __init__(\n self,\n cls:ClassDef,\n reason:Expression |Statement,\n api:SemanticAnalyzerPluginInterface,\n plugin_config:PydanticPluginConfig,\n )->None:\n  self._cls=cls\n  self._reason=reason\n  self._api=api\n  \n  self.plugin_config=plugin_config\n  \n def transform(self)->bool:\n  ''\n\n\n\n\n\n\n\n  \n  info=self._cls.info\n  is_root_model=any(ROOT_MODEL_FULLNAME in base.fullname for base in info.mro[:-1])\n  config=self.collect_config()\n  fields,class_vars=self.collect_fields_and_class_vars(config,is_root_model)\n  if fields is None or class_vars is None:\n  \n   return False\n  for field in fields:\n   if field.type is None:\n    return False\n    \n  is_settings=any(base.fullname ==BASESETTINGS_FULLNAME for base in info.mro[:-1])\n  self.add_initializer(fields,config,is_settings,is_root_model)\n  self.add_model_construct_method(fields,config,is_settings,is_root_model)\n  self.set_frozen(fields,self._api,frozen=config.frozen is True)\n  \n  self.adjust_decorator_signatures()\n  \n  info.metadata[METADATA_KEY]={\n  'fields':{field.name:field.serialize()for field in fields},\n  'class_vars':{class_var.name:class_var.serialize()for class_var in class_vars},\n  'config':config.get_values_dict(),\n  }\n  \n  return True\n  \n def adjust_decorator_signatures(self)->None:\n  ''\n\n\n\n\n\n  \n  for sym in self._cls.info.names.values():\n   if isinstance(sym.node,Decorator):\n    first_dec=sym.node.original_decorators[0]\n    if(\n    isinstance(first_dec,CallExpr)\n    and isinstance(first_dec.callee,NameExpr)\n    and first_dec.callee.fullname in DECORATOR_FULLNAMES\n    \n    and not(\n    first_dec.callee.fullname ==MODEL_VALIDATOR_FULLNAME\n    and any(\n    first_dec.arg_names[i]=='mode'and isinstance(arg,StrExpr)and arg.value =='after'\n    for i,arg in enumerate(first_dec.args)\n    )\n    )\n    ):\n    \n     sym.node.func.is_class=True\n     \n def collect_config(self)->ModelConfigData:\n  ''\n  cls=self._cls\n  config=ModelConfigData()\n  \n  has_config_kwargs=False\n  has_config_from_namespace=False\n  \n  \n  for name,expr in cls.keywords.items():\n   config_data=self.get_config_update(name,expr)\n   if config_data:\n    has_config_kwargs=True\n    config.update(config_data)\n    \n    \n  stmt:Statement |None=None\n  for stmt in cls.defs.body:\n   if not isinstance(stmt,(AssignmentStmt,ClassDef)):\n    continue\n    \n   if isinstance(stmt,AssignmentStmt):\n    lhs=stmt.lvalues[0]\n    if not isinstance(lhs,NameExpr)or lhs.name !='model_config':\n     continue\n     \n    if isinstance(stmt.rvalue,CallExpr):\n     for arg_name,arg in zip(stmt.rvalue.arg_names,stmt.rvalue.args):\n      if arg_name is None:\n       continue\n      config.update(self.get_config_update(arg_name,arg,lax_extra=True))\n    elif isinstance(stmt.rvalue,DictExpr):\n     for key_expr,value_expr in stmt.rvalue.items:\n      if not isinstance(key_expr,StrExpr):\n       continue\n      config.update(self.get_config_update(key_expr.value,value_expr))\n      \n   elif isinstance(stmt,ClassDef):\n    if stmt.name !='Config':\n     continue\n    for substmt in stmt.defs.body:\n     if not isinstance(substmt,AssignmentStmt):\n      continue\n     lhs=substmt.lvalues[0]\n     if not isinstance(lhs,NameExpr):\n      continue\n     config.update(self.get_config_update(lhs.name,substmt.rvalue))\n     \n   if has_config_kwargs:\n    self._api.fail(\n    'Specifying config in two places is ambiguous, use either Config attribute or class kwargs',\n    cls,\n    )\n    break\n    \n   has_config_from_namespace=True\n   \n  if has_config_kwargs or has_config_from_namespace:\n   if(\n   stmt\n   and config.has_alias_generator\n   and not config.populate_by_name\n   and self.plugin_config.warn_required_dynamic_aliases\n   ):\n    error_required_dynamic_aliases(self._api,stmt)\n    \n  for info in cls.info.mro[1:]:\n   if METADATA_KEY not in info.metadata:\n    continue\n    \n    \n   self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n   for name,value in info.metadata[METADATA_KEY]['config'].items():\n    config.setdefault(name,value)\n  return config\n  \n def collect_fields_and_class_vars(\n self,model_config:ModelConfigData,is_root_model:bool\n )->tuple[list[PydanticModelField]|None,list[PydanticModelClassVar]|None]:\n  ''\n  cls=self._cls\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  found_fields:dict[str,PydanticModelField]={}\n  found_class_vars:dict[str,PydanticModelClassVar]={}\n  for info in reversed(cls.info.mro[1:-1]):\n  \n  \n  \n   if METADATA_KEY not in info.metadata:\n    continue\n    \n    \n   self._api.add_plugin_dependency(make_wildcard_trigger(info.fullname))\n   \n   for name,data in info.metadata[METADATA_KEY]['fields'].items():\n    field=PydanticModelField.deserialize(info,data,self._api)\n    \n    \n    \n    \n    field.expand_typevar_from_subtype(cls.info,self._api)\n    found_fields[name]=field\n    \n    sym_node=cls.info.names.get(name)\n    if sym_node and sym_node.node and not isinstance(sym_node.node,Var):\n     self._api.fail(\n     'BaseModel field may only be overridden by another field',\n     sym_node.node,\n     )\n     \n   for name,data in info.metadata[METADATA_KEY]['class_vars'].items():\n    found_class_vars[name]=PydanticModelClassVar.deserialize(data)\n    \n    \n  current_field_names:set[str]=set()\n  current_class_vars_names:set[str]=set()\n  for stmt in self._get_assignment_statements_from_block(cls.defs):\n   maybe_field=self.collect_field_or_class_var_from_stmt(stmt,model_config,found_class_vars)\n   if maybe_field is None:\n    continue\n    \n   lhs=stmt.lvalues[0]\n   assert isinstance(lhs,NameExpr)\n   if isinstance(maybe_field,PydanticModelField):\n    if is_root_model and lhs.name !='root':\n     error_extra_fields_on_root_model(self._api,stmt)\n    else:\n     current_field_names.add(lhs.name)\n     found_fields[lhs.name]=maybe_field\n   elif isinstance(maybe_field,PydanticModelClassVar):\n    current_class_vars_names.add(lhs.name)\n    found_class_vars[lhs.name]=maybe_field\n    \n  return list(found_fields.values()),list(found_class_vars.values())\n  \n def _get_assignment_statements_from_if_statement(self,stmt:IfStmt)->Iterator[AssignmentStmt]:\n  for body in stmt.body:\n   if not body.is_unreachable:\n    yield from self._get_assignment_statements_from_block(body)\n  if stmt.else_body is not None and not stmt.else_body.is_unreachable:\n   yield from self._get_assignment_statements_from_block(stmt.else_body)\n   \n def _get_assignment_statements_from_block(self,block:Block)->Iterator[AssignmentStmt]:\n  for stmt in block.body:\n   if isinstance(stmt,AssignmentStmt):\n    yield stmt\n   elif isinstance(stmt,IfStmt):\n    yield from self._get_assignment_statements_from_if_statement(stmt)\n    \n def collect_field_or_class_var_from_stmt(\n self,stmt:AssignmentStmt,model_config:ModelConfigData,class_vars:dict[str,PydanticModelClassVar]\n )->PydanticModelField |PydanticModelClassVar |None:\n  ''\n\n\n\n\n\n\n\n\n  \n  cls=self._cls\n  \n  lhs=stmt.lvalues[0]\n  if not isinstance(lhs,NameExpr)or not _fields.is_valid_field_name(lhs.name)or lhs.name =='model_config':\n   return None\n   \n  if not stmt.new_syntax:\n   if(\n   isinstance(stmt.rvalue,CallExpr)\n   and isinstance(stmt.rvalue.callee,CallExpr)\n   and isinstance(stmt.rvalue.callee.callee,NameExpr)\n   and stmt.rvalue.callee.callee.fullname in DECORATOR_FULLNAMES\n   ):\n   \n   \n   \n    return None\n    \n   if lhs.name in class_vars:\n   \n    return None\n    \n    \n   error_untyped_fields(self._api,stmt)\n   return None\n   \n  lhs=stmt.lvalues[0]\n  if not isinstance(lhs,NameExpr):\n   return None\n   \n  if not _fields.is_valid_field_name(lhs.name)or lhs.name =='model_config':\n   return None\n   \n  sym=cls.info.names.get(lhs.name)\n  if sym is None:\n  \n  \n   return None\n   \n  node=sym.node\n  if isinstance(node,PlaceholderNode):\n  \n  \n  \n  \n   return None\n   \n  if isinstance(node,TypeAlias):\n   self._api.fail(\n   'Type aliases inside BaseModel definitions are not supported at runtime',\n   node,\n   )\n   \n   \n   \n   return None\n   \n  if not isinstance(node,Var):\n  \n  \n  \n  \n   return None\n   \n   \n  if node.is_classvar:\n   return PydanticModelClassVar(lhs.name)\n   \n   \n  node_type=get_proper_type(node.type)\n  if isinstance(node_type,Instance)and node_type.type.fullname =='dataclasses.InitVar':\n   self._api.fail(\n   'InitVar is not supported in BaseModel',\n   node,\n   )\n   \n  has_default=self.get_has_default(stmt)\n  strict=self.get_strict(stmt)\n  \n  if sym.type is None and node.is_final and node.is_inferred:\n  \n  \n  \n  \n  \n  \n  \n   typ=self._api.analyze_simple_literal_type(stmt.rvalue,is_final=True)\n   if typ:\n    node.type=typ\n   else:\n    self._api.fail(\n    'Need type argument for Final[...] with non-literal default in BaseModel',\n    stmt,\n    )\n    node.type=AnyType(TypeOfAny.from_error)\n    \n  alias,has_dynamic_alias=self.get_alias_info(stmt)\n  if has_dynamic_alias and not model_config.populate_by_name and self.plugin_config.warn_required_dynamic_aliases:\n   error_required_dynamic_aliases(self._api,stmt)\n  is_frozen=self.is_field_frozen(stmt)\n  \n  init_type=self._infer_dataclass_attr_init_type(sym,lhs.name,stmt)\n  return PydanticModelField(\n  name=lhs.name,\n  has_dynamic_alias=has_dynamic_alias,\n  has_default=has_default,\n  strict=strict,\n  alias=alias,\n  is_frozen=is_frozen,\n  line=stmt.line,\n  column=stmt.column,\n  type=init_type,\n  info=cls.info,\n  )\n  \n def _infer_dataclass_attr_init_type(self,sym:SymbolTableNode,name:str,context:Context)->Type |None:\n  ''\n\n\n  \n  default=sym.type\n  if sym.implicit:\n   return default\n  t=get_proper_type(sym.type)\n  \n  \n  \n  \n  \n  if not isinstance(t,Instance):\n   return default\n  setter=t.type.get('__set__')\n  if setter:\n   if isinstance(setter.node,FuncDef):\n    super_info=t.type.get_containing_type_info('__set__')\n    assert super_info\n    if setter.type:\n     setter_type=get_proper_type(map_type_from_supertype(setter.type,t.type,super_info))\n    else:\n     return AnyType(TypeOfAny.unannotated)\n    if isinstance(setter_type,CallableType)and setter_type.arg_kinds ==[\n    ARG_POS,\n    ARG_POS,\n    ARG_POS,\n    ]:\n     return expand_type_by_instance(setter_type.arg_types[2],t)\n    else:\n     self._api.fail(f'Unsupported signature for \"__set__\" in \"{t.type.name}\"',context)\n   else:\n    self._api.fail(f'Unsupported \"__set__\" in \"{t.type.name}\"',context)\n    \n  return default\n  \n def add_initializer(\n self,fields:list[PydanticModelField],config:ModelConfigData,is_settings:bool,is_root_model:bool\n )->None:\n  ''\n\n\n  \n  if '__init__'in self._cls.info.names and not self._cls.info.names['__init__'].plugin_generated:\n   return\n   \n  typed=self.plugin_config.init_typed\n  model_strict=bool(config.strict)\n  use_alias=config.populate_by_name is not True\n  requires_dynamic_aliases=bool(config.has_alias_generator and not config.populate_by_name)\n  args=self.get_field_arguments(\n  fields,\n  typed=typed,\n  model_strict=model_strict,\n  requires_dynamic_aliases=requires_dynamic_aliases,\n  use_alias=use_alias,\n  is_settings=is_settings,\n  is_root_model=is_root_model,\n  force_typevars_invariant=True,\n  )\n  \n  if is_settings:\n   base_settings_node=self._api.lookup_fully_qualified(BASESETTINGS_FULLNAME).node\n   assert isinstance(base_settings_node,TypeInfo)\n   if '__init__'in base_settings_node.names:\n    base_settings_init_node=base_settings_node.names['__init__'].node\n    assert isinstance(base_settings_init_node,FuncDef)\n    if base_settings_init_node is not None and base_settings_init_node.type is not None:\n     func_type=base_settings_init_node.type\n     assert isinstance(func_type,CallableType)\n     for arg_idx,arg_name in enumerate(func_type.arg_names):\n      if arg_name is None or arg_name.startswith('__')or not arg_name.startswith('_'):\n       continue\n      analyzed_variable_type=self._api.anal_type(func_type.arg_types[arg_idx])\n      variable=Var(arg_name,analyzed_variable_type)\n      args.append(Argument(variable,analyzed_variable_type,None,ARG_OPT))\n      \n  if not self.should_init_forbid_extra(fields,config):\n   var=Var('kwargs')\n   args.append(Argument(var,AnyType(TypeOfAny.explicit),None,ARG_STAR2))\n   \n  add_method(self._api,self._cls,'__init__',args=args,return_type=NoneType())\n  \n def add_model_construct_method(\n self,\n fields:list[PydanticModelField],\n config:ModelConfigData,\n is_settings:bool,\n is_root_model:bool,\n )->None:\n  ''\n\n\n\n  \n  set_str=self._api.named_type(f'{BUILTINS_NAME}.set',[self._api.named_type(f'{BUILTINS_NAME}.str')])\n  optional_set_str=UnionType([set_str,NoneType()])\n  fields_set_argument=Argument(Var('_fields_set',optional_set_str),optional_set_str,None,ARG_OPT)\n  with state.strict_optional_set(self._api.options.strict_optional):\n   args=self.get_field_arguments(\n   fields,\n   typed=True,\n   model_strict=bool(config.strict),\n   requires_dynamic_aliases=False,\n   use_alias=False,\n   is_settings=is_settings,\n   is_root_model=is_root_model,\n   )\n  if not self.should_init_forbid_extra(fields,config):\n   var=Var('kwargs')\n   args.append(Argument(var,AnyType(TypeOfAny.explicit),None,ARG_STAR2))\n   \n  args=args+[fields_set_argument]if is_root_model else[fields_set_argument]+args\n  \n  add_method(\n  self._api,\n  self._cls,\n  'model_construct',\n  args=args,\n  return_type=fill_typevars(self._cls.info),\n  is_classmethod=True,\n  )\n  \n def set_frozen(self,fields:list[PydanticModelField],api:SemanticAnalyzerPluginInterface,frozen:bool)->None:\n  ''\n\n\n  \n  info=self._cls.info\n  for field in fields:\n   sym_node=info.names.get(field.name)\n   if sym_node is not None:\n    var=sym_node.node\n    if isinstance(var,Var):\n     var.is_property=frozen or field.is_frozen\n    elif isinstance(var,PlaceholderNode)and not self._api.final_iteration:\n    \n     self._api.defer()\n    else:\n    \n     try:\n      var_str=str(var)\n     except TypeError:\n     \n      var_str=repr(var)\n     detail=f'sym_node.node: {var_str} (of type {var.__class__})'\n     error_unexpected_behavior(detail,self._api,self._cls)\n   else:\n    var=field.to_var(info,api,use_alias=False)\n    var.info=info\n    var.is_property=frozen\n    var._fullname=info.fullname+'.'+var.name\n    info.names[var.name]=SymbolTableNode(MDEF,var)\n    \n def get_config_update(self,name:str,arg:Expression,lax_extra:bool=False)->ModelConfigData |None:\n  ''\n\n\n  \n  if name not in self.tracked_config_fields:\n   return None\n  if name =='extra':\n   if isinstance(arg,StrExpr):\n    forbid_extra=arg.value =='forbid'\n   elif isinstance(arg,MemberExpr):\n    forbid_extra=arg.name =='forbid'\n   else:\n    if not lax_extra:\n    \n    \n    \n    \n    \n    \n    \n    \n     error_invalid_config_value(name,self._api,arg)\n    return None\n   return ModelConfigData(forbid_extra=forbid_extra)\n  if name =='alias_generator':\n   has_alias_generator=True\n   if isinstance(arg,NameExpr)and arg.fullname =='builtins.None':\n    has_alias_generator=False\n   return ModelConfigData(has_alias_generator=has_alias_generator)\n  if isinstance(arg,NameExpr)and arg.fullname in('builtins.True','builtins.False'):\n   return ModelConfigData(**{name:arg.fullname =='builtins.True'})\n  error_invalid_config_value(name,self._api,arg)\n  return None\n  \n @staticmethod\n def get_has_default(stmt:AssignmentStmt)->bool:\n  ''\n  expr=stmt.rvalue\n  if isinstance(expr,TempNode):\n  \n   return False\n  if isinstance(expr,CallExpr)and isinstance(expr.callee,RefExpr)and expr.callee.fullname ==FIELD_FULLNAME:\n  \n  \n  \n  \n   for arg,name in zip(expr.args,expr.arg_names):\n   \n    if name is None or name =='default':\n     return arg.__class__ is not EllipsisExpr\n    if name =='default_factory':\n     return not(isinstance(arg,NameExpr)and arg.fullname =='builtins.None')\n   return False\n   \n  return not isinstance(expr,EllipsisExpr)\n  \n @staticmethod\n def get_strict(stmt:AssignmentStmt)->bool |None:\n  ''\n  expr=stmt.rvalue\n  if isinstance(expr,CallExpr)and isinstance(expr.callee,RefExpr)and expr.callee.fullname ==FIELD_FULLNAME:\n   for arg,name in zip(expr.args,expr.arg_names):\n    if name !='strict':\n     continue\n    if isinstance(arg,NameExpr):\n     if arg.fullname =='builtins.True':\n      return True\n     elif arg.fullname =='builtins.False':\n      return False\n    return None\n  return None\n  \n @staticmethod\n def get_alias_info(stmt:AssignmentStmt)->tuple[str |None,bool]:\n  ''\n\n\n\n  \n  expr=stmt.rvalue\n  if isinstance(expr,TempNode):\n  \n   return None,False\n   \n  if not(\n  isinstance(expr,CallExpr)and isinstance(expr.callee,RefExpr)and expr.callee.fullname ==FIELD_FULLNAME\n  ):\n  \n   return None,False\n   \n  for i,arg_name in enumerate(expr.arg_names):\n   if arg_name !='alias':\n    continue\n   arg=expr.args[i]\n   if isinstance(arg,StrExpr):\n    return arg.value,False\n   else:\n    return None,True\n  return None,False\n  \n @staticmethod\n def is_field_frozen(stmt:AssignmentStmt)->bool:\n  ''\n\n\n\n\n  \n  expr=stmt.rvalue\n  if isinstance(expr,TempNode):\n  \n   return False\n   \n  if not(\n  isinstance(expr,CallExpr)and isinstance(expr.callee,RefExpr)and expr.callee.fullname ==FIELD_FULLNAME\n  ):\n  \n   return False\n   \n  for i,arg_name in enumerate(expr.arg_names):\n   if arg_name =='frozen':\n    arg=expr.args[i]\n    return isinstance(arg,NameExpr)and arg.fullname =='builtins.True'\n  return False\n  \n def get_field_arguments(\n self,\n fields:list[PydanticModelField],\n typed:bool,\n model_strict:bool,\n use_alias:bool,\n requires_dynamic_aliases:bool,\n is_settings:bool,\n is_root_model:bool,\n force_typevars_invariant:bool=False,\n )->list[Argument]:\n  ''\n\n\n  \n  info=self._cls.info\n  arguments=[\n  field.to_argument(\n  info,\n  typed=typed,\n  model_strict=model_strict,\n  force_optional=requires_dynamic_aliases or is_settings,\n  use_alias=use_alias,\n  api=self._api,\n  force_typevars_invariant=force_typevars_invariant,\n  is_root_model_root=is_root_model and field.name =='root',\n  )\n  for field in fields\n  if not(use_alias and field.has_dynamic_alias)\n  ]\n  return arguments\n  \n def should_init_forbid_extra(self,fields:list[PydanticModelField],config:ModelConfigData)->bool:\n  ''\n\n\n\n  \n  if not config.populate_by_name:\n   if self.is_dynamic_alias_present(fields,bool(config.has_alias_generator)):\n    return False\n  if config.forbid_extra:\n   return True\n  return self.plugin_config.init_forbid_extra\n  \n @staticmethod\n def is_dynamic_alias_present(fields:list[PydanticModelField],has_alias_generator:bool)->bool:\n  ''\n\n  \n  for field in fields:\n   if field.has_dynamic_alias:\n    return True\n  if has_alias_generator:\n   for field in fields:\n    if field.alias is None:\n     return True\n  return False\n  \n  \nclass ModelConfigData:\n ''\n \n def __init__(\n self,\n forbid_extra:bool |None=None,\n frozen:bool |None=None,\n from_attributes:bool |None=None,\n populate_by_name:bool |None=None,\n has_alias_generator:bool |None=None,\n strict:bool |None=None,\n ):\n  self.forbid_extra=forbid_extra\n  self.frozen=frozen\n  self.from_attributes=from_attributes\n  self.populate_by_name=populate_by_name\n  self.has_alias_generator=has_alias_generator\n  self.strict=strict\n  \n def get_values_dict(self)->dict[str,Any]:\n  ''\n\n\n  \n  return{k:v for k,v in self.__dict__.items()if v is not None}\n  \n def update(self,config:ModelConfigData |None)->None:\n  ''\n  if config is None:\n   return\n  for k,v in config.get_values_dict().items():\n   setattr(self,k,v)\n   \n def setdefault(self,key:str,value:Any)->None:\n  ''\n  if getattr(self,key)is None:\n   setattr(self,key,value)\n   \n   \nERROR_ORM=ErrorCode('pydantic-orm','Invalid from_attributes call','Pydantic')\nERROR_CONFIG=ErrorCode('pydantic-config','Invalid config value','Pydantic')\nERROR_ALIAS=ErrorCode('pydantic-alias','Dynamic alias disallowed','Pydantic')\nERROR_UNEXPECTED=ErrorCode('pydantic-unexpected','Unexpected behavior','Pydantic')\nERROR_UNTYPED=ErrorCode('pydantic-field','Untyped field disallowed','Pydantic')\nERROR_FIELD_DEFAULTS=ErrorCode('pydantic-field','Invalid Field defaults','Pydantic')\nERROR_EXTRA_FIELD_ROOT_MODEL=ErrorCode('pydantic-field','Extra field on RootModel subclass','Pydantic')\n\n\ndef error_from_attributes(model_name:str,api:CheckerPluginInterface,context:Context)->None:\n ''\n api.fail(f'\"{model_name}\" does not have from_attributes=True',context,code=ERROR_ORM)\n \n \ndef error_invalid_config_value(name:str,api:SemanticAnalyzerPluginInterface,context:Context)->None:\n ''\n api.fail(f'Invalid value for \"Config.{name}\"',context,code=ERROR_CONFIG)\n \n \ndef error_required_dynamic_aliases(api:SemanticAnalyzerPluginInterface,context:Context)->None:\n ''\n\n\n \n api.fail('Required dynamic aliases disallowed',context,code=ERROR_ALIAS)\n \n \ndef error_unexpected_behavior(\ndetail:str,api:CheckerPluginInterface |SemanticAnalyzerPluginInterface,context:Context\n)->None:\n ''\n \n link='https://github.com/pydantic/pydantic/issues/new/choose'\n full_message=f'The pydantic mypy plugin ran into unexpected behavior: {detail}\\n'\n full_message +=f'Please consider reporting this bug at {link} so we can try to fix it!'\n api.fail(full_message,context,code=ERROR_UNEXPECTED)\n \n \ndef error_untyped_fields(api:SemanticAnalyzerPluginInterface,context:Context)->None:\n ''\n api.fail('Untyped fields disallowed',context,code=ERROR_UNTYPED)\n \n \ndef error_extra_fields_on_root_model(api:CheckerPluginInterface,context:Context)->None:\n ''\n api.fail('Only `root` is allowed as a field of a `RootModel`',context,code=ERROR_EXTRA_FIELD_ROOT_MODEL)\n \n \ndef add_method(\napi:SemanticAnalyzerPluginInterface |CheckerPluginInterface,\ncls:ClassDef,\nname:str,\nargs:list[Argument],\nreturn_type:Type,\nself_type:Type |None=None,\ntvar_def:TypeVarType |None=None,\nis_classmethod:bool=False,\n)->None:\n ''\n info=cls.info\n \n \n \n if name in info.names:\n  sym=info.names[name]\n  if sym.plugin_generated and isinstance(sym.node,FuncDef):\n   cls.defs.body.remove(sym.node)\n   \n if isinstance(api,SemanticAnalyzerPluginInterface):\n  function_type=api.named_type('builtins.function')\n else:\n  function_type=api.named_generic_type('builtins.function',[])\n  \n if is_classmethod:\n  self_type=self_type or TypeType(fill_typevars(info))\n  first=[Argument(Var('_cls'),self_type,None,ARG_POS,True)]\n else:\n  self_type=self_type or fill_typevars(info)\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  first=[Argument(Var('__pydantic_self__'),self_type,None,ARG_POS)]\n args=first+args\n \n arg_types,arg_names,arg_kinds=[],[],[]\n for arg in args:\n  assert arg.type_annotation,'All arguments must be fully typed.'\n  arg_types.append(arg.type_annotation)\n  arg_names.append(arg.variable.name)\n  arg_kinds.append(arg.kind)\n  \n signature=CallableType(arg_types,arg_kinds,arg_names,return_type,function_type)\n if tvar_def:\n  signature.variables=[tvar_def]\n  \n func=FuncDef(name,args,Block([PassStmt()]))\n func.info=info\n func.type=set_callable_name(signature,func)\n func.is_class=is_classmethod\n func._fullname=info.fullname+'.'+name\n func.line=info.line\n \n \n \n if name in info.names:\n \n  r_name=get_unique_redefinition_name(name,info.names)\n  info.names[r_name]=info.names[name]\n  \n  \n  \n  \n if is_classmethod:\n  func.is_decorated=True\n  v=Var(name,func.type)\n  v.info=info\n  v._fullname=func._fullname\n  v.is_classmethod=True\n  dec=Decorator(func,[NameExpr('classmethod')],v)\n  dec.line=info.line\n  sym=SymbolTableNode(MDEF,dec)\n else:\n  sym=SymbolTableNode(MDEF,func)\n sym.plugin_generated=True\n info.names[name]=sym\n \n info.defn.defs.body.append(func)\n \n \ndef parse_toml(config_file:str)->dict[str,Any]|None:\n ''\n\n\n \n if not config_file.endswith('.toml'):\n  return None\n  \n if sys.version_info >=(3,11):\n  import tomllib as toml_\n else:\n  try:\n   import tomli as toml_\n  except ImportError:\n   import warnings\n   \n   warnings.warn('No TOML parser installed, cannot read configuration from `pyproject.toml`.')\n   return None\n   \n with open(config_file,'rb')as rf:\n  return toml_.load(rf)\n", ["__future__", "configparser", "mypy.errorcodes", "mypy.expandtype", "mypy.nodes", "mypy.options", "mypy.plugin", "mypy.plugins.common", "mypy.semanal", "mypy.server.trigger", "mypy.state", "mypy.typeops", "mypy.types", "mypy.typevars", "mypy.util", "mypy.version", "pydantic._internal", "pydantic._internal._fields", "pydantic.version", "sys", "tomli", "tomllib", "typing", "warnings"]], "pydantic.networks": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses as _dataclasses\nimport re\nfrom dataclasses import fields\nfrom functools import lru_cache\nfrom importlib.metadata import version\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom typing import TYPE_CHECKING,Any,ClassVar\n\nfrom pydantic_core import MultiHostHost,PydanticCustomError,SchemaSerializer,core_schema\nfrom pydantic_core import MultiHostUrl as _CoreMultiHostUrl\nfrom pydantic_core import Url as _CoreUrl\nfrom typing_extensions import Annotated,Self,TypeAlias\n\nfrom pydantic.errors import PydanticUserError\n\nfrom._internal import _repr,_schema_generation_shared\nfrom._migration import getattr_migration\nfrom.annotated_handlers import GetCoreSchemaHandler\nfrom.json_schema import JsonSchemaValue\nfrom.type_adapter import TypeAdapter\n\nif TYPE_CHECKING:\n import email_validator\n \n NetworkType:TypeAlias='str | bytes | int | tuple[str | bytes | int, str | int]'\n \nelse:\n email_validator=None\n \n \n__all__=[\n'AnyUrl',\n'AnyHttpUrl',\n'FileUrl',\n'FtpUrl',\n'HttpUrl',\n'WebsocketUrl',\n'AnyWebsocketUrl',\n'UrlConstraints',\n'EmailStr',\n'NameEmail',\n'IPvAnyAddress',\n'IPvAnyInterface',\n'IPvAnyNetwork',\n'PostgresDsn',\n'CockroachDsn',\n'AmqpDsn',\n'RedisDsn',\n'MongoDsn',\n'KafkaDsn',\n'NatsDsn',\n'validate_email',\n'MySQLDsn',\n'MariaDBDsn',\n'ClickHouseDsn',\n'SnowflakeDsn',\n]\n\n\n@_dataclasses.dataclass\nclass UrlConstraints:\n ''\n\n\n\n\n\n\n\n\n \n \n max_length:int |None=None\n allowed_schemes:list[str]|None=None\n host_required:bool |None=None\n default_host:str |None=None\n default_port:int |None=None\n default_path:str |None=None\n \n def __hash__(self)->int:\n  return hash(\n  (\n  self.max_length,\n  tuple(self.allowed_schemes)if self.allowed_schemes is not None else None,\n  self.host_required,\n  self.default_host,\n  self.default_port,\n  self.default_path,\n  )\n  )\n  \n @property\n def defined_constraints(self)->dict[str,Any]:\n  ''\n  return{field.name:value for field in fields(self)if(value :=getattr(self,field.name))is not None}\n  \n def __get_pydantic_core_schema__(self,source:Any,handler:GetCoreSchemaHandler)->core_schema.CoreSchema:\n  schema=handler(source)\n  \n  \n  \n  \n  schema_to_mutate=schema['schema']if schema['type']=='function-wrap'else schema\n  if annotated_type :=schema_to_mutate['type']not in('url','multi-host-url'):\n   raise PydanticUserError(\n   f\"'UrlConstraints' cannot annotate '{annotated_type}'.\",code='invalid-annotated-type'\n   )\n  for constraint_key,constraint_value in self.defined_constraints.items():\n   schema_to_mutate[constraint_key]=constraint_value\n  return schema\n  \n  \nclass _BaseUrl:\n _constraints:ClassVar[UrlConstraints]=UrlConstraints()\n _url:_CoreUrl\n \n def __init__(self,url:str |_CoreUrl |_BaseUrl)->None:\n  self._url=_build_type_adapter(self.__class__).validate_python(url)._url\n  \n @property\n def scheme(self)->str:\n  ''\n\n\n  \n  return self._url.scheme\n  \n @property\n def username(self)->str |None:\n  ''\n\n\n  \n  return self._url.username\n  \n @property\n def password(self)->str |None:\n  ''\n\n\n  \n  return self._url.password\n  \n @property\n def host(self)->str |None:\n  ''\n\n\n\n  \n  return self._url.host\n  \n def unicode_host(self)->str |None:\n  ''\n\n\n\n\n\n  \n  return self._url.unicode_host()\n  \n @property\n def port(self)->int |None:\n  ''\n\n\n  \n  return self._url.port\n  \n @property\n def path(self)->str |None:\n  ''\n\n\n  \n  return self._url.path\n  \n @property\n def query(self)->str |None:\n  ''\n\n\n  \n  return self._url.query\n  \n def query_params(self)->list[tuple[str,str]]:\n  ''\n\n\n  \n  return self._url.query_params()\n  \n @property\n def fragment(self)->str |None:\n  ''\n\n\n  \n  return self._url.fragment\n  \n def unicode_string(self)->str:\n  ''\n\n\n\n  \n  return self._url.unicode_string()\n  \n def __str__(self)->str:\n  ''\n  return str(self._url)\n  \n def __repr__(self)->str:\n  return f'{self.__class__.__name__}({str(self._url)!r})'\n  \n def __deepcopy__(self,memo:dict)->Self:\n  return self.__class__(self._url)\n  \n def __eq__(self,other:Any)->bool:\n  return self.__class__ is other.__class__ and self._url ==other._url\n  \n def __lt__(self,other:Any)->bool:\n  return self.__class__ is other.__class__ and self._url <other._url\n  \n def __gt__(self,other:Any)->bool:\n  return self.__class__ is other.__class__ and self._url >other._url\n  \n def __le__(self,other:Any)->bool:\n  return self.__class__ is other.__class__ and self._url <=other._url\n  \n def __ge__(self,other:Any)->bool:\n  return self.__class__ is other.__class__ and self._url >=other._url\n  \n def __hash__(self)->int:\n  return hash(self._url)\n  \n def __len__(self)->int:\n  return len(str(self._url))\n  \n @classmethod\n def build(\n cls,\n *,\n scheme:str,\n username:str |None=None,\n password:str |None=None,\n host:str,\n port:int |None=None,\n path:str |None=None,\n query:str |None=None,\n fragment:str |None=None,\n )->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return cls(\n  _CoreUrl.build(\n  scheme=scheme,\n  username=username,\n  password=password,\n  host=host,\n  port=port,\n  path=path,\n  query=query,\n  fragment=fragment,\n  )\n  )\n  \n @classmethod\n def __get_pydantic_core_schema__(\n cls,source:type[_BaseUrl],handler:GetCoreSchemaHandler\n )->core_schema.CoreSchema:\n  def wrap_val(v,h):\n   if isinstance(v,source):\n    return v\n   if isinstance(v,_BaseUrl):\n    v=str(v)\n   core_url=h(v)\n   instance=source.__new__(source)\n   instance._url=core_url\n   return instance\n   \n  return core_schema.no_info_wrap_validator_function(\n  wrap_val,\n  schema=core_schema.url_schema(**cls._constraints.defined_constraints),\n  serialization=core_schema.to_string_ser_schema(),\n  )\n  \n @classmethod\n def __get_pydantic_json_schema__(\n cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n )->JsonSchemaValue:\n \n \n  inner_schema=core_schema['schema']if core_schema['type']=='function-wrap'else core_schema\n  return handler(inner_schema)\n  \n __pydantic_serializer__=SchemaSerializer(core_schema.any_schema(serialization=core_schema.to_string_ser_schema()))\n \n \nclass _BaseMultiHostUrl:\n _constraints:ClassVar[UrlConstraints]=UrlConstraints()\n _url:_CoreMultiHostUrl\n \n def __init__(self,url:str |_CoreMultiHostUrl |_BaseMultiHostUrl)->None:\n  self._url=_build_type_adapter(self.__class__).validate_python(url)._url\n  \n @property\n def scheme(self)->str:\n  ''\n\n\n  \n  return self._url.scheme\n  \n @property\n def path(self)->str |None:\n  ''\n\n\n  \n  return self._url.path\n  \n @property\n def query(self)->str |None:\n  ''\n\n\n  \n  return self._url.query\n  \n def query_params(self)->list[tuple[str,str]]:\n  ''\n\n\n  \n  return self._url.query_params()\n  \n @property\n def fragment(self)->str |None:\n  ''\n\n\n  \n  return self._url.fragment\n  \n def hosts(self)->list[MultiHostHost]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self._url.hosts()\n  \n def unicode_string(self)->str:\n  ''\n  return self._url.unicode_string()\n  \n def __str__(self)->str:\n  ''\n  return str(self._url)\n  \n def __repr__(self)->str:\n  return f'{self.__class__.__name__}({str(self._url)!r})'\n  \n def __deepcopy__(self,memo:dict)->Self:\n  return self.__class__(self._url)\n  \n def __eq__(self,other:Any)->bool:\n  return self.__class__ is other.__class__ and self._url ==other._url\n  \n def __hash__(self)->int:\n  return hash(self._url)\n  \n def __len__(self)->int:\n  return len(str(self._url))\n  \n @classmethod\n def build(\n cls,\n *,\n scheme:str,\n hosts:list[MultiHostHost]|None=None,\n username:str |None=None,\n password:str |None=None,\n host:str |None=None,\n port:int |None=None,\n path:str |None=None,\n query:str |None=None,\n fragment:str |None=None,\n )->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return cls(\n  _CoreMultiHostUrl.build(\n  scheme=scheme,\n  hosts=hosts,\n  username=username,\n  password=password,\n  host=host,\n  port=port,\n  path=path,\n  query=query,\n  fragment=fragment,\n  )\n  )\n  \n @classmethod\n def __get_pydantic_core_schema__(\n cls,source:type[_BaseMultiHostUrl],handler:GetCoreSchemaHandler\n )->core_schema.CoreSchema:\n  def wrap_val(v,h):\n   if isinstance(v,source):\n    return v\n   if isinstance(v,_BaseMultiHostUrl):\n    v=str(v)\n   core_url=h(v)\n   instance=source.__new__(source)\n   instance._url=core_url\n   return instance\n   \n  return core_schema.no_info_wrap_validator_function(\n  wrap_val,\n  schema=core_schema.multi_host_url_schema(**cls._constraints.defined_constraints),\n  serialization=core_schema.to_string_ser_schema(),\n  )\n  \n @classmethod\n def __get_pydantic_json_schema__(\n cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n )->JsonSchemaValue:\n \n \n  inner_schema=core_schema['schema']if core_schema['type']=='function-wrap'else core_schema\n  return handler(inner_schema)\n  \n __pydantic_serializer__=SchemaSerializer(core_schema.any_schema(serialization=core_schema.to_string_ser_schema()))\n \n \n@lru_cache\ndef _build_type_adapter(cls:type[_BaseUrl |_BaseMultiHostUrl])->TypeAdapter:\n return TypeAdapter(cls)\n \n \nclass AnyUrl(_BaseUrl):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \n \n \n \n \nclass AnyHttpUrl(AnyUrl):\n ''\n\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['http','https'])\n \n \nclass HttpUrl(AnyUrl):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n _constraints=UrlConstraints(max_length=2083,allowed_schemes=['http','https'])\n \n \nclass AnyWebsocketUrl(AnyUrl):\n ''\n\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['ws','wss'])\n \n \nclass WebsocketUrl(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(max_length=2083,allowed_schemes=['ws','wss'])\n \n \nclass FileUrl(AnyUrl):\n ''\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['file'])\n \n \nclass FtpUrl(AnyUrl):\n ''\n\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['ftp'])\n \n \nclass PostgresDsn(_BaseMultiHostUrl):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n _constraints=UrlConstraints(\n host_required=True,\n allowed_schemes=[\n 'postgres',\n 'postgresql',\n 'postgresql+asyncpg',\n 'postgresql+pg8000',\n 'postgresql+psycopg',\n 'postgresql+psycopg2',\n 'postgresql+psycopg2cffi',\n 'postgresql+py-postgresql',\n 'postgresql+pygresql',\n ],\n )\n \n @property\n def host(self)->str:\n  ''\n  return self._url.host\n  \n  \nclass CockroachDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(\n host_required=True,\n allowed_schemes=[\n 'cockroachdb',\n 'cockroachdb+psycopg2',\n 'cockroachdb+asyncpg',\n ],\n )\n \n @property\n def host(self)->str:\n  ''\n  return self._url.host\n  \n  \nclass AmqpDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['amqp','amqps'])\n \n \nclass RedisDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(\n allowed_schemes=['redis','rediss'],\n default_host='localhost',\n default_port=6379,\n default_path='/0',\n host_required=True,\n )\n \n @property\n def host(self)->str:\n  ''\n  return self._url.host\n  \n  \nclass MongoDsn(_BaseMultiHostUrl):\n ''\n\n\n\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['mongodb','mongodb+srv'],default_port=27017)\n \n \nclass KafkaDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(allowed_schemes=['kafka'],default_host='localhost',default_port=9092)\n \n \nclass NatsDsn(_BaseMultiHostUrl):\n ''\n\n\n\n\n\n \n \n _constraints=UrlConstraints(\n allowed_schemes=['nats','tls','ws','wss'],default_host='localhost',default_port=4222\n )\n \n \nclass MySQLDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(\n allowed_schemes=[\n 'mysql',\n 'mysql+mysqlconnector',\n 'mysql+aiomysql',\n 'mysql+asyncmy',\n 'mysql+mysqldb',\n 'mysql+pymysql',\n 'mysql+cymysql',\n 'mysql+pyodbc',\n ],\n default_port=3306,\n host_required=True,\n )\n \n \nclass MariaDBDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(\n allowed_schemes=['mariadb','mariadb+mariadbconnector','mariadb+pymysql'],\n default_port=3306,\n )\n \n \nclass ClickHouseDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(\n allowed_schemes=['clickhouse+native','clickhouse+asynch'],\n default_host='localhost',\n default_port=9000,\n )\n \n \nclass SnowflakeDsn(AnyUrl):\n ''\n\n\n\n\n \n \n _constraints=UrlConstraints(\n allowed_schemes=['snowflake'],\n host_required=True,\n )\n \n @property\n def host(self)->str:\n  ''\n  return self._url.host\n  \n  \ndef import_email_validator()->None:\n global email_validator\n try:\n  import email_validator\n except ImportError as e:\n  raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')from e\n if not version('email-validator').partition('.')[0]=='2':\n  raise ImportError('email-validator version >= 2.0 required, run pip install -U email-validator')\n  \n  \nif TYPE_CHECKING:\n EmailStr=Annotated[str,...]\nelse:\n\n class EmailStr:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,\n  _source:type[Any],\n  _handler:GetCoreSchemaHandler,\n  )->core_schema.CoreSchema:\n   import_email_validator()\n   return core_schema.no_info_after_validator_function(cls._validate,core_schema.str_schema())\n   \n  @classmethod\n  def __get_pydantic_json_schema__(\n  cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n  )->JsonSchemaValue:\n   field_schema=handler(core_schema)\n   field_schema.update(type='string',format='email')\n   return field_schema\n   \n  @classmethod\n  def _validate(cls,input_value:str,/)->str:\n   return validate_email(input_value)[1]\n   \n   \nclass NameEmail(_repr.Representation):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__='name','email'\n \n def __init__(self,name:str,email:str):\n  self.name=name\n  self.email=email\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,NameEmail)and(self.name,self.email)==(other.name,other.email)\n  \n @classmethod\n def __get_pydantic_json_schema__(\n cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n )->JsonSchemaValue:\n  field_schema=handler(core_schema)\n  field_schema.update(type='string',format='name-email')\n  return field_schema\n  \n @classmethod\n def __get_pydantic_core_schema__(\n cls,\n _source:type[Any],\n _handler:GetCoreSchemaHandler,\n )->core_schema.CoreSchema:\n  import_email_validator()\n  \n  return core_schema.no_info_after_validator_function(\n  cls._validate,\n  core_schema.json_or_python_schema(\n  json_schema=core_schema.str_schema(),\n  python_schema=core_schema.union_schema(\n  [core_schema.is_instance_schema(cls),core_schema.str_schema()],\n  custom_error_type='name_email_type',\n  custom_error_message='Input is not a valid NameEmail',\n  ),\n  serialization=core_schema.to_string_ser_schema(),\n  ),\n  )\n  \n @classmethod\n def _validate(cls,input_value:Self |str,/)->Self:\n  if isinstance(input_value,str):\n   name,email=validate_email(input_value)\n   return cls(name,email)\n  else:\n   return input_value\n   \n def __str__(self)->str:\n  if '@'in self.name:\n   return f'\"{self.name}\" <{self.email}>'\n   \n  return f'{self.name} <{self.email}>'\n  \n  \nIPvAnyAddressType:TypeAlias='IPv4Address | IPv6Address'\nIPvAnyInterfaceType:TypeAlias='IPv4Interface | IPv6Interface'\nIPvAnyNetworkType:TypeAlias='IPv4Network | IPv6Network'\n\nif TYPE_CHECKING:\n IPvAnyAddress=IPvAnyAddressType\n IPvAnyInterface=IPvAnyInterfaceType\n IPvAnyNetwork=IPvAnyNetworkType\nelse:\n\n class IPvAnyAddress:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  __slots__=()\n  \n  def __new__(cls,value:Any)->IPvAnyAddressType:\n   ''\n   try:\n    return IPv4Address(value)\n   except ValueError:\n    pass\n    \n   try:\n    return IPv6Address(value)\n   except ValueError:\n    raise PydanticCustomError('ip_any_address','value is not a valid IPv4 or IPv6 address')\n    \n  @classmethod\n  def __get_pydantic_json_schema__(\n  cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n  )->JsonSchemaValue:\n   field_schema={}\n   field_schema.update(type='string',format='ipvanyaddress')\n   return field_schema\n   \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,\n  _source:type[Any],\n  _handler:GetCoreSchemaHandler,\n  )->core_schema.CoreSchema:\n   return core_schema.no_info_plain_validator_function(\n   cls._validate,serialization=core_schema.to_string_ser_schema()\n   )\n   \n  @classmethod\n  def _validate(cls,input_value:Any,/)->IPvAnyAddressType:\n   return cls(input_value)\n   \n class IPvAnyInterface:\n  ''\n  \n  __slots__=()\n  \n  def __new__(cls,value:NetworkType)->IPvAnyInterfaceType:\n   ''\n   try:\n    return IPv4Interface(value)\n   except ValueError:\n    pass\n    \n   try:\n    return IPv6Interface(value)\n   except ValueError:\n    raise PydanticCustomError('ip_any_interface','value is not a valid IPv4 or IPv6 interface')\n    \n  @classmethod\n  def __get_pydantic_json_schema__(\n  cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n  )->JsonSchemaValue:\n   field_schema={}\n   field_schema.update(type='string',format='ipvanyinterface')\n   return field_schema\n   \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,\n  _source:type[Any],\n  _handler:GetCoreSchemaHandler,\n  )->core_schema.CoreSchema:\n   return core_schema.no_info_plain_validator_function(\n   cls._validate,serialization=core_schema.to_string_ser_schema()\n   )\n   \n  @classmethod\n  def _validate(cls,input_value:NetworkType,/)->IPvAnyInterfaceType:\n   return cls(input_value)\n   \n class IPvAnyNetwork:\n  ''\n  \n  __slots__=()\n  \n  def __new__(cls,value:NetworkType)->IPvAnyNetworkType:\n   ''\n   \n   \n   try:\n    return IPv4Network(value)\n   except ValueError:\n    pass\n    \n   try:\n    return IPv6Network(value)\n   except ValueError:\n    raise PydanticCustomError('ip_any_network','value is not a valid IPv4 or IPv6 network')\n    \n  @classmethod\n  def __get_pydantic_json_schema__(\n  cls,core_schema:core_schema.CoreSchema,handler:_schema_generation_shared.GetJsonSchemaHandler\n  )->JsonSchemaValue:\n   field_schema={}\n   field_schema.update(type='string',format='ipvanynetwork')\n   return field_schema\n   \n  @classmethod\n  def __get_pydantic_core_schema__(\n  cls,\n  _source:type[Any],\n  _handler:GetCoreSchemaHandler,\n  )->core_schema.CoreSchema:\n   return core_schema.no_info_plain_validator_function(\n   cls._validate,serialization=core_schema.to_string_ser_schema()\n   )\n   \n  @classmethod\n  def _validate(cls,input_value:NetworkType,/)->IPvAnyNetworkType:\n   return cls(input_value)\n   \n   \ndef _build_pretty_email_regex()->re.Pattern[str]:\n name_chars=r'[\\w!#$%&\\'*+\\-/=?^_`{|}~]'\n unquoted_name_group=rf'((?:{name_chars}+\\s+)*{name_chars}+)'\n quoted_name_group=r'\"((?:[^\"]|\\\")+)\"'\n email_group=r'<(.+)>'\n return re.compile(rf'\\s*(?:{unquoted_name_group}|{quoted_name_group})?\\s*{email_group}\\s*')\n \n \npretty_email_regex=_build_pretty_email_regex()\n\nMAX_EMAIL_LENGTH=2048\n''\n\n\n\n\ndef validate_email(value:str)->tuple[str,str]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if email_validator is None:\n  import_email_validator()\n  \n if len(value)>MAX_EMAIL_LENGTH:\n  raise PydanticCustomError(\n  'value_error',\n  'value is not a valid email address: {reason}',\n  {'reason':f'Length must not exceed {MAX_EMAIL_LENGTH} characters'},\n  )\n  \n m=pretty_email_regex.fullmatch(value)\n name:str |None=None\n if m:\n  unquoted_name,quoted_name,value=m.groups()\n  name=unquoted_name or quoted_name\n  \n email=value.strip()\n \n try:\n  parts=email_validator.validate_email(email,check_deliverability=False)\n except email_validator.EmailNotValidError as e:\n  raise PydanticCustomError(\n  'value_error','value is not a valid email address: {reason}',{'reason':str(e.args[0])}\n  )from e\n  \n email=parts.normalized\n assert email is not None\n name=name or parts.local_part\n return name,email\n \n \n__getattr__=getattr_migration(__name__)\n", ["__future__", "dataclasses", "email_validator", "functools", "importlib.metadata", "ipaddress", "pydantic._internal", "pydantic._internal._repr", "pydantic._internal._schema_generation_shared", "pydantic._migration", "pydantic.annotated_handlers", "pydantic.errors", "pydantic.json_schema", "pydantic.type_adapter", "pydantic_core", "pydantic_core.core_schema", "re", "typing", "typing_extensions"]], "pydantic._internal": [".py", "", [], 1], "pydantic._internal._validate_call": [".py", "from __future__ import annotations as _annotations\n\nimport functools\nimport inspect\nfrom functools import partial\nfrom typing import Any,Awaitable,Callable\n\nimport pydantic_core\n\nfrom..config import ConfigDict\nfrom..plugin._schema_validator import create_schema_validator\nfrom._config import ConfigWrapper\nfrom._generate_schema import GenerateSchema,ValidateCallSupportedTypes\nfrom._namespace_utils import MappingNamespace,NsResolver,ns_for_function\n\n\ndef extract_function_name(func:ValidateCallSupportedTypes)->str:\n ''\n return f'partial({func.func.__name__})'if isinstance(func,functools.partial)else func.__name__\n \n \ndef extract_function_qualname(func:ValidateCallSupportedTypes)->str:\n ''\n return f'partial({func.func.__qualname__})'if isinstance(func,functools.partial)else func.__qualname__\n \n \ndef update_wrapper_attributes(wrapped:ValidateCallSupportedTypes,wrapper:Callable[...,Any]):\n ''\n if inspect.iscoroutinefunction(wrapped):\n \n  @functools.wraps(wrapped)\n  async def wrapper_function(*args,**kwargs):\n   return await wrapper(*args,**kwargs)\n else:\n \n  @functools.wraps(wrapped)\n  def wrapper_function(*args,**kwargs):\n   return wrapper(*args,**kwargs)\n   \n   \n wrapper_function.__name__=extract_function_name(wrapped)\n wrapper_function.__qualname__=extract_function_qualname(wrapped)\n wrapper_function.raw_function=wrapped\n \n return wrapper_function\n \n \nclass ValidateCallWrapper:\n ''\n \n __slots__=('__pydantic_validator__','__return_pydantic_validator__')\n \n def __init__(\n self,\n function:ValidateCallSupportedTypes,\n config:ConfigDict |None,\n validate_return:bool,\n parent_namespace:MappingNamespace |None,\n )->None:\n  if isinstance(function,partial):\n   schema_type=function.func\n   module=function.func.__module__\n  else:\n   schema_type=function\n   module=function.__module__\n  qualname=extract_function_qualname(function)\n  \n  ns_resolver=NsResolver(namespaces_tuple=ns_for_function(schema_type,parent_namespace=parent_namespace))\n  \n  config_wrapper=ConfigWrapper(config)\n  gen_schema=GenerateSchema(config_wrapper,ns_resolver)\n  schema=gen_schema.clean_schema(gen_schema.generate_schema(function))\n  core_config=config_wrapper.core_config(title=qualname)\n  \n  self.__pydantic_validator__=create_schema_validator(\n  schema,\n  schema_type,\n  module,\n  qualname,\n  'validate_call',\n  core_config,\n  config_wrapper.plugin_settings,\n  )\n  \n  if validate_return:\n   signature=inspect.signature(function)\n   return_type=signature.return_annotation if signature.return_annotation is not signature.empty else Any\n   gen_schema=GenerateSchema(config_wrapper,ns_resolver)\n   schema=gen_schema.clean_schema(gen_schema.generate_schema(return_type))\n   validator=create_schema_validator(\n   schema,\n   schema_type,\n   module,\n   qualname,\n   'validate_call',\n   core_config,\n   config_wrapper.plugin_settings,\n   )\n   if inspect.iscoroutinefunction(function):\n   \n    async def return_val_wrapper(aw:Awaitable[Any])->None:\n     return validator.validate_python(await aw)\n     \n    self.__return_pydantic_validator__=return_val_wrapper\n   else:\n    self.__return_pydantic_validator__=validator.validate_python\n  else:\n   self.__return_pydantic_validator__=None\n   \n def __call__(self,*args:Any,**kwargs:Any)->Any:\n  res=self.__pydantic_validator__.validate_python(pydantic_core.ArgsKwargs(args,kwargs))\n  if self.__return_pydantic_validator__:\n   return self.__return_pydantic_validator__(res)\n  else:\n   return res\n", ["__future__", "functools", "inspect", "pydantic._internal._config", "pydantic._internal._generate_schema", "pydantic._internal._namespace_utils", "pydantic.config", "pydantic.plugin._schema_validator", "pydantic_core", "typing"]], "pydantic._internal._utils": [".py", "''\n\n\n\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport keyword\nimport typing\nimport weakref\nfrom collections import OrderedDict,defaultdict,deque\nfrom copy import deepcopy\nfrom functools import cached_property\nfrom inspect import Parameter\nfrom itertools import zip_longest\nfrom types import BuiltinFunctionType,CodeType,FunctionType,GeneratorType,LambdaType,ModuleType\nfrom typing import Any,Callable,Mapping,TypeVar\n\nfrom typing_extensions import TypeAlias,TypeGuard\n\nfrom. import _repr,_typing_extra\nfrom._import_utils import import_cached_base_model\n\nif typing.TYPE_CHECKING:\n MappingIntStrAny:TypeAlias='typing.Mapping[int, Any] | typing.Mapping[str, Any]'\n AbstractSetIntStr:TypeAlias='typing.AbstractSet[int] | typing.AbstractSet[str]'\n from..main import BaseModel\n \n \n \nIMMUTABLE_NON_COLLECTIONS_TYPES:set[type[Any]]={\nint,\nfloat,\ncomplex,\nstr,\nbool,\nbytes,\ntype,\n_typing_extra.NoneType,\nFunctionType,\nBuiltinFunctionType,\nLambdaType,\nweakref.ref,\nCodeType,\n\n\n\nModuleType,\nNotImplemented.__class__,\nEllipsis.__class__,\n}\n\n\nBUILTIN_COLLECTIONS:set[type[Any]]={\nlist,\nset,\ntuple,\nfrozenset,\ndict,\nOrderedDict,\ndefaultdict,\ndeque,\n}\n\n\ndef can_be_positional(param:Parameter)->bool:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return param.kind in(Parameter.POSITIONAL_ONLY,Parameter.POSITIONAL_OR_KEYWORD)\n \n \ndef sequence_like(v:Any)->bool:\n return isinstance(v,(list,tuple,set,frozenset,GeneratorType,deque))\n \n \ndef lenient_isinstance(o:Any,class_or_tuple:type[Any]|tuple[type[Any],...]|None)->bool:\n try:\n  return isinstance(o,class_or_tuple)\n except TypeError:\n  return False\n  \n  \ndef lenient_issubclass(cls:Any,class_or_tuple:Any)->bool:\n try:\n  return isinstance(cls,type)and issubclass(cls,class_or_tuple)\n except TypeError:\n  if isinstance(cls,_typing_extra.WithArgsTypes):\n   return False\n  raise\n  \n  \ndef is_model_class(cls:Any)->TypeGuard[type[BaseModel]]:\n ''\n\n \n BaseModel=import_cached_base_model()\n \n return lenient_issubclass(cls,BaseModel)and cls is not BaseModel\n \n \ndef is_valid_identifier(identifier:str)->bool:\n ''\n\n\n \n return identifier.isidentifier()and not keyword.iskeyword(identifier)\n \n \nKeyType=TypeVar('KeyType')\n\n\ndef deep_update(mapping:dict[KeyType,Any],*updating_mappings:dict[KeyType,Any])->dict[KeyType,Any]:\n updated_mapping=mapping.copy()\n for updating_mapping in updating_mappings:\n  for k,v in updating_mapping.items():\n   if k in updated_mapping and isinstance(updated_mapping[k],dict)and isinstance(v,dict):\n    updated_mapping[k]=deep_update(updated_mapping[k],v)\n   else:\n    updated_mapping[k]=v\n return updated_mapping\n \n \ndef update_not_none(mapping:dict[Any,Any],**update:Any)->None:\n mapping.update({k:v for k,v in update.items()if v is not None})\n \n \nT=TypeVar('T')\n\n\ndef unique_list(\ninput_list:list[T]|tuple[T,...],\n*,\nname_factory:typing.Callable[[T],str]=str,\n)->list[T]:\n ''\n\n\n \n result:list[T]=[]\n result_names:list[str]=[]\n for v in input_list:\n  v_name=name_factory(v)\n  if v_name not in result_names:\n   result_names.append(v_name)\n   result.append(v)\n  else:\n   result[result_names.index(v_name)]=v\n   \n return result\n \n \nclass ValueItems(_repr.Representation):\n ''\n \n __slots__=('_items','_type')\n \n def __init__(self,value:Any,items:AbstractSetIntStr |MappingIntStrAny)->None:\n  items=self._coerce_items(items)\n  \n  if isinstance(value,(list,tuple)):\n   items=self._normalize_indexes(items,len(value))\n   \n  self._items:MappingIntStrAny=items\n  \n def is_excluded(self,item:Any)->bool:\n  ''\n\n\n  \n  return self.is_true(self._items.get(item))\n  \n def is_included(self,item:Any)->bool:\n  ''\n\n\n  \n  return item in self._items\n  \n def for_element(self,e:int |str)->AbstractSetIntStr |MappingIntStrAny |None:\n  ''\n\n  \n  item=self._items.get(e)\n  return item if not self.is_true(item)else None\n  \n def _normalize_indexes(self,items:MappingIntStrAny,v_length:int)->dict[int |str,Any]:\n  ''\n\n\n\n\n\n\n  \n  normalized_items:dict[int |str,Any]={}\n  all_items=None\n  for i,v in items.items():\n   if not(isinstance(v,typing.Mapping)or isinstance(v,typing.AbstractSet)or self.is_true(v)):\n    raise TypeError(f'Unexpected type of exclude value for index \"{i}\" {v.__class__}')\n   if i =='__all__':\n    all_items=self._coerce_value(v)\n    continue\n   if not isinstance(i,int):\n    raise TypeError(\n    'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n    'expected integer keys or keyword \"__all__\"'\n    )\n   normalized_i=v_length+i if i <0 else i\n   normalized_items[normalized_i]=self.merge(v,normalized_items.get(normalized_i))\n   \n  if not all_items:\n   return normalized_items\n  if self.is_true(all_items):\n   for i in range(v_length):\n    normalized_items.setdefault(i,...)\n   return normalized_items\n  for i in range(v_length):\n   normalized_item=normalized_items.setdefault(i,{})\n   if not self.is_true(normalized_item):\n    normalized_items[i]=self.merge(all_items,normalized_item)\n  return normalized_items\n  \n @classmethod\n def merge(cls,base:Any,override:Any,intersect:bool=False)->Any:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  override=cls._coerce_value(override)\n  base=cls._coerce_value(base)\n  if override is None:\n   return base\n  if cls.is_true(base)or base is None:\n   return override\n  if cls.is_true(override):\n   return base if intersect else override\n   \n   \n  if intersect:\n   merge_keys=[k for k in base if k in override]+[k for k in override if k in base]\n  else:\n   merge_keys=list(base)+[k for k in override if k not in base]\n   \n  merged:dict[int |str,Any]={}\n  for k in merge_keys:\n   merged_item=cls.merge(base.get(k),override.get(k),intersect=intersect)\n   if merged_item is not None:\n    merged[k]=merged_item\n    \n  return merged\n  \n @staticmethod\n def _coerce_items(items:AbstractSetIntStr |MappingIntStrAny)->MappingIntStrAny:\n  if isinstance(items,typing.Mapping):\n   pass\n  elif isinstance(items,typing.AbstractSet):\n   items=dict.fromkeys(items,...)\n  else:\n   class_name=getattr(items,'__class__','???')\n   raise TypeError(f'Unexpected type of exclude value {class_name}')\n  return items\n  \n @classmethod\n def _coerce_value(cls,value:Any)->Any:\n  if value is None or cls.is_true(value):\n   return value\n  return cls._coerce_items(value)\n  \n @staticmethod\n def is_true(v:Any)->bool:\n  return v is True or v is ...\n  \n def __repr_args__(self)->_repr.ReprArgs:\n  return[(None,self._items)]\n  \n  \nif typing.TYPE_CHECKING:\n\n def LazyClassAttribute(name:str,get_value:Callable[[],T])->T:...\n \nelse:\n\n class LazyClassAttribute:\n  ''\n\n\n  \n  \n  def __init__(self,name:str,get_value:Callable[[],Any])->None:\n   self.name=name\n   self.get_value=get_value\n   \n  @cached_property\n  def value(self)->Any:\n   return self.get_value()\n   \n  def __get__(self,instance:Any,owner:type[Any])->None:\n   if instance is None:\n    return self.value\n   raise AttributeError(f'{self.name !r} attribute of {owner.__name__ !r} is class-only')\n   \n   \nObj=TypeVar('Obj')\n\n\ndef smart_deepcopy(obj:Obj)->Obj:\n ''\n\n\n \n obj_type=obj.__class__\n if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n  return obj\n try:\n  if not obj and obj_type in BUILTIN_COLLECTIONS:\n  \n   return obj if obj_type is tuple else obj.copy()\n except(TypeError,ValueError,RuntimeError):\n \n  pass\n  \n return deepcopy(obj)\n \n \n_SENTINEL=object()\n\n\ndef all_identical(left:typing.Iterable[Any],right:typing.Iterable[Any])->bool:\n ''\n\n\n\n\n\n\n \n for left_item,right_item in zip_longest(left,right,fillvalue=_SENTINEL):\n  if left_item is not right_item:\n   return False\n return True\n \n \n@dataclasses.dataclass(frozen=True)\nclass SafeGetItemProxy:\n ''\n\n\n \n \n \n \n __slots__=('wrapped',)\n \n wrapped:Mapping[str,Any]\n \n def __getitem__(self,key:str,/)->Any:\n  return self.wrapped.get(key,_SENTINEL)\n  \n  \n  \n  \n  \n if typing.TYPE_CHECKING:\n \n  def __contains__(self,key:str,/)->bool:\n   return self.wrapped.__contains__(key)\n", ["__future__", "collections", "copy", "dataclasses", "functools", "inspect", "itertools", "keyword", "pydantic._internal", "pydantic._internal._import_utils", "pydantic._internal._repr", "pydantic._internal._typing_extra", "pydantic.main", "types", "typing", "typing_extensions", "weakref"]], "pydantic._internal._typing_extra": [".py", "''\n\nfrom __future__ import annotations\n\nimport collections.abc\nimport re\nimport sys\nimport types\nimport typing\nimport warnings\nfrom functools import lru_cache,partial\nfrom typing import TYPE_CHECKING,Any,Callable\n\nimport typing_extensions\nfrom typing_extensions import TypeIs,deprecated,get_args,get_origin\n\nfrom._namespace_utils import GlobalsNamespace,MappingNamespace,NsResolver,get_module_ns_of\n\nif sys.version_info <(3,10):\n NoneType=type(None)\n EllipsisType=type(Ellipsis)\nelse:\n from types import EllipsisType as EllipsisType\n from types import NoneType as NoneType\n \nif TYPE_CHECKING:\n from pydantic import BaseModel\n \n \n \n \n@lru_cache(maxsize=None)\ndef _get_typing_objects_by_name_of(name:str)->tuple[Any,...]:\n ''\n result=tuple(getattr(module,name)for module in(typing,typing_extensions)if hasattr(module,name))\n if not result:\n  raise ValueError(f'Neither `typing` nor `typing_extensions` has an object called {name !r}')\n return result\n \n \n \n \n \ndef _is_typing_name(obj:object,name:str)->bool:\n ''\n \n for thing in _get_typing_objects_by_name_of(name):\n  if obj is thing:\n   return True\n return False\n \n \ndef is_any(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(tp,name='Any')\n \n \ndef is_union(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n\n \n return _is_typing_name(get_origin(tp),name='Union')\n \n \ndef is_literal(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(get_origin(tp),name='Literal')\n \n \n \n \ndef literal_values(tp:Any,/)->list[Any]:\n ''\n if not is_literal(tp):\n  return[tp]\n  \n values=get_args(tp)\n return[x for value in values for x in literal_values(value)]\n \n \ndef is_annotated(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(get_origin(tp),name='Annotated')\n \n \ndef annotated_type(tp:Any,/)->Any |None:\n ''\n return get_args(tp)[0]if is_annotated(tp)else None\n \n \ndef is_unpack(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(get_origin(tp),name='Unpack')\n \n \ndef unpack_type(tp:Any,/)->Any |None:\n ''\n return get_args(tp)[0]if is_unpack(tp)else None\n \n \ndef is_self(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(tp,name='Self')\n \n \ndef is_new_type(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n if sys.version_info <(3,10):\n \n  return hasattr(tp,'__supertype__')\n else:\n  return _is_typing_name(type(tp),name='NewType')\n  \n  \ndef is_hashable(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n \n \n return tp is collections.abc.Hashable or get_origin(tp)is collections.abc.Hashable\n \n \ndef is_callable(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n\n\n\n \n \n \n return tp is collections.abc.Callable or get_origin(tp)is collections.abc.Callable\n \n \n_PARAMSPEC_TYPES:tuple[type[typing_extensions.ParamSpec],...]=(typing_extensions.ParamSpec,)\nif sys.version_info >=(3,10):\n _PARAMSPEC_TYPES=(*_PARAMSPEC_TYPES,typing.ParamSpec)\n \n \ndef is_paramspec(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n \n return isinstance(tp,_PARAMSPEC_TYPES)\n \n \n_TYPE_ALIAS_TYPES:tuple[type[typing_extensions.TypeAliasType],...]=(typing_extensions.TypeAliasType,)\nif sys.version_info >=(3,12):\n _TYPE_ALIAS_TYPES=(*_TYPE_ALIAS_TYPES,typing.TypeAliasType)\n \n \ndef is_type_alias_type(tp:Any,/)->TypeIs[typing_extensions.TypeAliasType]:\n ''\n\n\n\n\n\n\n\n\n\n \n return isinstance(tp,_TYPE_ALIAS_TYPES)\n \n \ndef is_classvar(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n return _is_typing_name(tp,name='ClassVar')or _is_typing_name(get_origin(tp),name='ClassVar')\n \n \n_classvar_re=re.compile(r'((\\w+\\.)?Annotated\\[)?(\\w+\\.)?ClassVar\\[')\n\n\ndef is_classvar_annotation(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n\n\n \n if is_classvar(tp)or(anntp :=annotated_type(tp))is not None and is_classvar(anntp):\n  return True\n  \n str_ann:str |None=None\n if isinstance(tp,typing.ForwardRef):\n  str_ann=tp.__forward_arg__\n if isinstance(tp,str):\n  str_ann=tp\n  \n if str_ann is not None and _classvar_re.match(str_ann):\n \n \n  return True\n  \n return False\n \n \n \ndef is_finalvar(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n \n \n return _is_typing_name(tp,name='Final')or _is_typing_name(get_origin(tp),name='Final')\n \n \ndef is_required(tp:Any,/)->bool:\n ''\n\n\n\n\n \n return _is_typing_name(get_origin(tp),name='Required')\n \n \ndef is_not_required(tp:Any,/)->bool:\n ''\n\n\n\n\n \n return _is_typing_name(get_origin(tp),name='NotRequired')\n \n \ndef is_no_return(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(tp,name='NoReturn')\n \n \ndef is_never(tp:Any,/)->bool:\n ''\n\n\n\n\n\n \n return _is_typing_name(tp,name='Never')\n \n \n_DEPRECATED_TYPES:tuple[type[typing_extensions.deprecated],...]=(typing_extensions.deprecated,)\nif hasattr(warnings,'deprecated'):\n _DEPRECATED_TYPES=(*_DEPRECATED_TYPES,warnings.deprecated)\n \n \ndef is_deprecated_instance(obj:Any,/)->TypeIs[deprecated]:\n ''\n return isinstance(obj,_DEPRECATED_TYPES)\n \n \n_NONE_TYPES:tuple[Any,...]=(None,NoneType,typing.Literal[None],typing_extensions.Literal[None])\n\n\ndef is_none_type(tp:Any,/)->bool:\n ''\n\n\n\n\n\n\n\n\n\n\n \n return tp in _NONE_TYPES\n \n \ndef is_namedtuple(tp:Any,/)->bool:\n ''\n\n\n\n \n from._utils import lenient_issubclass\n \n return lenient_issubclass(tp,tuple)and hasattr(tp,'_fields')\n \n \nif sys.version_info <(3,9):\n\n def is_zoneinfo_type(tp:Any,/)->bool:\n  ''\n  return False\n  \nelse:\n from zoneinfo import ZoneInfo\n \n def is_zoneinfo_type(tp:Any,/)->TypeIs[type[ZoneInfo]]:\n  ''\n  return tp is ZoneInfo\n  \n  \nif sys.version_info <(3,10):\n\n def origin_is_union(tp:Any,/)->bool:\n  ''\n  return _is_typing_name(tp,name='Union')\n  \n def is_generic_alias(type_:type[Any])->bool:\n  return isinstance(type_,typing._GenericAlias)\n  \nelse:\n\n def origin_is_union(tp:Any,/)->bool:\n  ''\n  return _is_typing_name(tp,name='Union')or tp is types.UnionType\n  \n def is_generic_alias(tp:Any,/)->bool:\n  return isinstance(tp,(types.GenericAlias,typing._GenericAlias))\n  \n  \n  \n  \nif sys.version_info <(3,9):\n WithArgsTypes:tuple[Any,...]=(typing._GenericAlias,)\nelif sys.version_info <(3,10):\n WithArgsTypes:tuple[Any,...]=(typing._GenericAlias,types.GenericAlias)\nelse:\n WithArgsTypes:tuple[Any,...]=(typing._GenericAlias,types.GenericAlias,types.UnionType)\n \n \n \ntyping_base:Any=typing._Final\n\n\n\n\n\ndef parent_frame_namespace(*,parent_depth:int=2,force:bool=False)->dict[str,Any]|None:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n frame=sys._getframe(parent_depth)\n \n \n \n if force:\n  return frame.f_locals\n  \n  \n  \n  \n if frame.f_back is None or frame.f_code.co_name =='<module>':\n  return None\n  \n return frame.f_locals\n \n \ndef _type_convert(arg:Any)->Any:\n ''\n\n\n\n\n\n\n \n if arg is None:\n  return NoneType\n if isinstance(arg,str):\n \n \n  return _make_forward_ref(arg,is_argument=False,is_class=True)\n return arg\n \n \ndef get_model_type_hints(\nobj:type[BaseModel],\n*,\nns_resolver:NsResolver |None=None,\n)->dict[str,tuple[Any,bool]]:\n ''\n\n\n\n\n\n\n\n\n\n \n hints:dict[str,Any]|dict[str,tuple[Any,bool]]={}\n ns_resolver=ns_resolver or NsResolver()\n \n for base in reversed(obj.__mro__):\n  ann:dict[str,Any]|None=base.__dict__.get('__annotations__')\n  if not ann or isinstance(ann,types.GetSetDescriptorType):\n   continue\n  with ns_resolver.push(base):\n   globalns,localns=ns_resolver.types_namespace\n   for name,value in ann.items():\n    if name.startswith('_'):\n    \n    \n    \n    \n    \n     try:\n      hints[name]=try_eval_type(value,globalns,localns)\n     except Exception:\n      hints[name]=(value,False)\n    else:\n     hints[name]=try_eval_type(value,globalns,localns)\n return hints\n \n \ndef get_cls_type_hints(\nobj:type[Any],\n*,\nns_resolver:NsResolver |None=None,\n)->dict[str,Any]:\n ''\n\n\n\n\n \n hints:dict[str,Any]|dict[str,tuple[Any,bool]]={}\n ns_resolver=ns_resolver or NsResolver()\n \n for base in reversed(obj.__mro__):\n  ann:dict[str,Any]|None=base.__dict__.get('__annotations__')\n  if not ann or isinstance(ann,types.GetSetDescriptorType):\n   continue\n  with ns_resolver.push(base):\n   globalns,localns=ns_resolver.types_namespace\n   for name,value in ann.items():\n    hints[name]=eval_type(value,globalns,localns)\n return hints\n \n \ndef try_eval_type(\nvalue:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\n)->tuple[Any,bool]:\n ''\n\n\n\n\n\n\n\n\n\n\n \n value=_type_convert(value)\n \n try:\n  return eval_type_backport(value,globalns,localns),True\n except NameError:\n  return value,False\n  \n  \ndef eval_type(\nvalue:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\n)->Any:\n ''\n\n\n\n\n\n\n \n value=_type_convert(value)\n return eval_type_backport(value,globalns,localns)\n \n \n@deprecated(\n'`eval_type_lenient` is deprecated, use `try_eval_type` instead.',\ncategory=None,\n)\ndef eval_type_lenient(\nvalue:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\n)->Any:\n ev,_=try_eval_type(value,globalns,localns)\n return ev\n \n \ndef eval_type_backport(\nvalue:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\ntype_params:tuple[Any,...]|None=None,\n)->Any:\n ''\n\n\n\n\n\n\n\n \n try:\n  return _eval_type_backport(value,globalns,localns,type_params)\n except TypeError as e:\n  if 'Unable to evaluate type annotation'in str(e):\n   raise\n   \n   \n   \n  assert isinstance(value,typing.ForwardRef)\n  \n  message=f'Unable to evaluate type annotation {value.__forward_arg__ !r}.'\n  if sys.version_info >=(3,11):\n   e.add_note(message)\n   raise\n  else:\n   raise TypeError(message)from e\n   \n   \ndef _eval_type_backport(\nvalue:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\ntype_params:tuple[Any,...]|None=None,\n)->Any:\n try:\n  return _eval_type(value,globalns,localns,type_params)\n except TypeError as e:\n  if not(isinstance(value,typing.ForwardRef)and is_backport_fixable_error(e)):\n   raise\n   \n  try:\n   from eval_type_backport import eval_type_backport\n  except ImportError:\n   raise TypeError(\n   f'Unable to evaluate type annotation {value.__forward_arg__ !r}. If you are making use '\n   'of the new typing syntax (unions using `|` since Python 3.10 or builtins subscripting '\n   'since Python 3.9), you should either replace the use of new syntax with the existing '\n   '`typing` constructs or install the `eval_type_backport` package.'\n   )from e\n   \n  return eval_type_backport(\n  value,\n  globalns,\n  localns,\n  try_default=False,\n  )\n  \n  \ndef _eval_type(\nvalue:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\ntype_params:tuple[Any,...]|None=None,\n)->Any:\n if sys.version_info >=(3,13):\n  return typing._eval_type(\n  value,globalns,localns,type_params=type_params\n  )\n else:\n  return typing._eval_type(\n  value,globalns,localns\n  )\n  \n  \ndef is_backport_fixable_error(e:TypeError)->bool:\n msg=str(e)\n \n return(\n sys.version_info <(3,10)\n and msg.startswith('unsupported operand type(s) for |: ')\n or sys.version_info <(3,9)\n and \"' object is not subscriptable\"in msg\n )\n \n \ndef get_function_type_hints(\nfunction:Callable[...,Any],\n*,\ninclude_keys:set[str]|None=None,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\n)->dict[str,Any]:\n ''\n\n\n\n\n\n\n\n \n try:\n  if isinstance(function,partial):\n   annotations=function.func.__annotations__\n  else:\n   annotations=function.__annotations__\n except AttributeError:\n  type_hints=get_type_hints(function)\n  if isinstance(function,type):\n  \n  \n  \n   type_hints.setdefault('return',function)\n  return type_hints\n  \n if globalns is None:\n  globalns=get_module_ns_of(function)\n type_params:tuple[Any,...]|None=None\n if localns is None:\n \n \n  type_params=getattr(function,'__type_params__',())\n  \n type_hints={}\n for name,value in annotations.items():\n  if include_keys is not None and name not in include_keys:\n   continue\n  if value is None:\n   value=NoneType\n  elif isinstance(value,str):\n   value=_make_forward_ref(value)\n   \n  type_hints[name]=eval_type_backport(value,globalns,localns,type_params)\n  \n return type_hints\n \n \nif sys.version_info <(3,9,8)or(3,10)<=sys.version_info <(3,10,1):\n\n def _make_forward_ref(\n arg:Any,\n is_argument:bool=True,\n *,\n is_class:bool=False,\n )->typing.ForwardRef:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  return typing.ForwardRef(arg,is_argument)\n  \nelse:\n _make_forward_ref=typing.ForwardRef\n \n \nif sys.version_info >=(3,10):\n get_type_hints=typing.get_type_hints\n \nelse:\n ''\n\n\n \n \n @typing.no_type_check\n def get_type_hints(\n obj:Any,\n globalns:dict[str,Any]|None=None,\n localns:dict[str,Any]|None=None,\n include_extras:bool=False,\n )->dict[str,Any]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if getattr(obj,'__no_type_check__',None):\n   return{}\n   \n  if isinstance(obj,type):\n   hints={}\n   for base in reversed(obj.__mro__):\n    if globalns is None:\n     base_globals=getattr(sys.modules.get(base.__module__,None),'__dict__',{})\n    else:\n     base_globals=globalns\n    ann=base.__dict__.get('__annotations__',{})\n    if isinstance(ann,types.GetSetDescriptorType):\n     ann={}\n    base_locals=dict(vars(base))if localns is None else localns\n    if localns is None and globalns is None:\n    \n    \n    \n    \n    \n    \n     base_globals,base_locals=base_locals,base_globals\n    for name,value in ann.items():\n     if value is None:\n      value=type(None)\n     if isinstance(value,str):\n      value=_make_forward_ref(value,is_argument=False,is_class=True)\n      \n     value=eval_type_backport(value,base_globals,base_locals)\n     hints[name]=value\n   if not include_extras and hasattr(typing,'_strip_annotations'):\n    return{\n    k:typing._strip_annotations(t)\n    for k,t in hints.items()\n    }\n   else:\n    return hints\n    \n  if globalns is None:\n   if isinstance(obj,types.ModuleType):\n    globalns=obj.__dict__\n   else:\n    nsobj=obj\n    \n    while hasattr(nsobj,'__wrapped__'):\n     nsobj=nsobj.__wrapped__\n    globalns=getattr(nsobj,'__globals__',{})\n   if localns is None:\n    localns=globalns\n  elif localns is None:\n   localns=globalns\n  hints=getattr(obj,'__annotations__',None)\n  if hints is None:\n  \n   if isinstance(obj,typing._allowed_types):\n    return{}\n   else:\n    raise TypeError(f'{obj !r} is not a module, class, method, ''or function.')\n  defaults=typing._get_defaults(obj)\n  hints=dict(hints)\n  for name,value in hints.items():\n   if value is None:\n    value=type(None)\n   if isinstance(value,str):\n   \n   \n   \n    value=_make_forward_ref(\n    value,\n    is_argument=not isinstance(obj,types.ModuleType),\n    is_class=False,\n    )\n   value=eval_type_backport(value,globalns,localns)\n   if name in defaults and defaults[name]is None:\n    value=typing.Optional[value]\n   hints[name]=value\n  return hints if include_extras else{k:typing._strip_annotations(t)for k,t in hints.items()}\n", ["__future__", "collections.abc", "eval_type_backport", "functools", "pydantic", "pydantic._internal._namespace_utils", "pydantic._internal._utils", "re", "sys", "types", "typing", "typing_extensions", "warnings", "zoneinfo"]], "pydantic._internal._internal_dataclass": [".py", "import sys\n\n\nif sys.version_info >=(3,10):\n slots_true={'slots':True}\nelse:\n slots_true={}\n", ["sys"]], "pydantic._internal._repr": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport types\nimport typing\nfrom typing import Any\n\nimport typing_extensions\n\nfrom. import _typing_extra\n\nif typing.TYPE_CHECKING:\n ReprArgs:typing_extensions.TypeAlias='typing.Iterable[tuple[str | None, Any]]'\n RichReprResult:typing_extensions.TypeAlias=(\n 'typing.Iterable[Any | tuple[Any] | tuple[str, Any] | tuple[str, Any, Any]]'\n )\n \n \nclass PlainRepr(str):\n ''\n\n \n \n def __repr__(self)->str:\n  return str(self)\n  \n  \nclass Representation:\n\n\n\n\n\n\n __slots__=()\n \n def __repr_args__(self)->ReprArgs:\n  ''\n\n\n\n\n  \n  attrs_names=self.__slots__\n  if not attrs_names and hasattr(self,'__dict__'):\n   attrs_names=self.__dict__.keys()\n  attrs=((s,getattr(self,s))for s in attrs_names)\n  return[(a,v if v is not self else self.__repr_recursion__(v))for a,v in attrs if v is not None]\n  \n def __repr_name__(self)->str:\n  ''\n  return self.__class__.__name__\n  \n def __repr_recursion__(self,object:Any)->str:\n  ''\n  \n  return f'<Recursion on {type(object).__name__} with id={id(object)}>'\n  \n def __repr_str__(self,join_str:str)->str:\n  return join_str.join(repr(v)if a is None else f'{a}={v !r}'for a,v in self.__repr_args__())\n  \n def __pretty__(self,fmt:typing.Callable[[Any],Any],**kwargs:Any)->typing.Generator[Any,None,None]:\n  ''\n  yield self.__repr_name__()+'('\n  yield 1\n  for name,value in self.__repr_args__():\n   if name is not None:\n    yield name+'='\n   yield fmt(value)\n   yield ','\n   yield 0\n  yield -1\n  yield ')'\n  \n def __rich_repr__(self)->RichReprResult:\n  ''\n  for name,field_repr in self.__repr_args__():\n   if name is None:\n    yield field_repr\n   else:\n    yield name,field_repr\n    \n def __str__(self)->str:\n  return self.__repr_str__(' ')\n  \n def __repr__(self)->str:\n  return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n  \n  \ndef display_as_type(obj:Any)->str:\n ''\n\n\n \n if isinstance(obj,(types.FunctionType,types.BuiltinFunctionType)):\n  return obj.__name__\n elif obj is ...:\n  return '...'\n elif isinstance(obj,Representation):\n  return repr(obj)\n elif isinstance(obj,typing.ForwardRef)or _typing_extra.is_type_alias_type(obj):\n  return str(obj)\n  \n if not isinstance(obj,(_typing_extra.typing_base,_typing_extra.WithArgsTypes,type)):\n  obj=obj.__class__\n  \n if _typing_extra.origin_is_union(typing_extensions.get_origin(obj)):\n  args=', '.join(map(display_as_type,typing_extensions.get_args(obj)))\n  return f'Union[{args}]'\n elif isinstance(obj,_typing_extra.WithArgsTypes):\n  if _typing_extra.is_literal(obj):\n   args=', '.join(map(repr,typing_extensions.get_args(obj)))\n  else:\n   args=', '.join(map(display_as_type,typing_extensions.get_args(obj)))\n  try:\n   return f'{obj.__qualname__}[{args}]'\n  except AttributeError:\n   return str(obj).replace('typing.','').replace('typing_extensions.','')\n elif isinstance(obj,type):\n  return obj.__qualname__\n else:\n  return repr(obj).replace('typing.','').replace('typing_extensions.','')\n", ["__future__", "pydantic._internal", "pydantic._internal._typing_extra", "types", "typing", "typing_extensions"]], "pydantic._internal._discriminated_union": [".py", "from __future__ import annotations as _annotations\n\nfrom typing import TYPE_CHECKING,Any,Hashable,Sequence\n\nfrom pydantic_core import CoreSchema,core_schema\n\nfrom..errors import PydanticUserError\nfrom. import _core_utils\nfrom._core_utils import(\nCoreSchemaField,\ncollect_definitions,\n)\n\nif TYPE_CHECKING:\n from..types import Discriminator\n \nCORE_SCHEMA_METADATA_DISCRIMINATOR_PLACEHOLDER_KEY='pydantic.internal.union_discriminator'\n\n\nclass MissingDefinitionForUnionRef(Exception):\n ''\n\n \n \n def __init__(self,ref:str)->None:\n  self.ref=ref\n  super().__init__(f'Missing definition for ref {self.ref !r}')\n  \n  \ndef set_discriminator_in_metadata(schema:CoreSchema,discriminator:Any)->None:\n schema.setdefault('metadata',{})\n metadata=schema.get('metadata')\n assert metadata is not None\n metadata[CORE_SCHEMA_METADATA_DISCRIMINATOR_PLACEHOLDER_KEY]=discriminator\n \n \ndef apply_discriminators(schema:core_schema.CoreSchema)->core_schema.CoreSchema:\n\n\n\n\n\n global_definitions:dict[str,CoreSchema]=collect_definitions(schema)\n \n def inner(s:core_schema.CoreSchema,recurse:_core_utils.Recurse)->core_schema.CoreSchema:\n  nonlocal global_definitions\n  \n  s=recurse(s,inner)\n  if s['type']=='tagged-union':\n   return s\n   \n  metadata=s.get('metadata',{})\n  discriminator=metadata.pop(CORE_SCHEMA_METADATA_DISCRIMINATOR_PLACEHOLDER_KEY,None)\n  if discriminator is not None:\n   s=apply_discriminator(s,discriminator,global_definitions)\n  return s\n  \n return _core_utils.walk_core_schema(schema,inner,copy=False)\n \n \ndef apply_discriminator(\nschema:core_schema.CoreSchema,\ndiscriminator:str |Discriminator,\ndefinitions:dict[str,core_schema.CoreSchema]|None=None,\n)->core_schema.CoreSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n from..types import Discriminator\n \n if isinstance(discriminator,Discriminator):\n  if isinstance(discriminator.discriminator,str):\n   discriminator=discriminator.discriminator\n  else:\n   return discriminator._convert_schema(schema)\n   \n return _ApplyInferredDiscriminator(discriminator,definitions or{}).apply(schema)\n \n \nclass _ApplyInferredDiscriminator:\n ''\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,discriminator:str,definitions:dict[str,core_schema.CoreSchema]):\n \n \n \n \n  self.discriminator=discriminator\n  \n  \n  \n  self.definitions=definitions\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  self._discriminator_alias:str |None=None\n  \n  \n  \n  \n  \n  \n  \n  self._should_be_nullable=False\n  \n  \n  \n  \n  \n  \n  \n  \n  self._is_nullable=False\n  \n  \n  \n  \n  self._choices_to_handle:list[core_schema.CoreSchema]=[]\n  \n  \n  \n  self._tagged_union_choices:dict[Hashable,core_schema.CoreSchema]={}\n  \n  \n  self._used=False\n  \n def apply(self,schema:core_schema.CoreSchema)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  assert not self._used\n  schema=self._apply_to_root(schema)\n  if self._should_be_nullable and not self._is_nullable:\n   schema=core_schema.nullable_schema(schema)\n  self._used=True\n  return schema\n  \n def _apply_to_root(self,schema:core_schema.CoreSchema)->core_schema.CoreSchema:\n  ''\n\n\n  \n  if schema['type']=='nullable':\n   self._is_nullable=True\n   wrapped=self._apply_to_root(schema['schema'])\n   nullable_wrapper=schema.copy()\n   nullable_wrapper['schema']=wrapped\n   return nullable_wrapper\n   \n  if schema['type']=='definitions':\n   wrapped=self._apply_to_root(schema['schema'])\n   definitions_wrapper=schema.copy()\n   definitions_wrapper['schema']=wrapped\n   return definitions_wrapper\n   \n  if schema['type']!='union':\n  \n  \n  \n  \n   schema=core_schema.union_schema([schema])\n   \n   \n  choices_schemas=[v[0]if isinstance(v,tuple)else v for v in schema['choices'][::-1]]\n  self._choices_to_handle.extend(choices_schemas)\n  while self._choices_to_handle:\n   choice=self._choices_to_handle.pop()\n   self._handle_choice(choice)\n   \n  if self._discriminator_alias is not None and self._discriminator_alias !=self.discriminator:\n  \n  \n  \n  \n  \n  \n  \n   discriminator:str |list[list[str |int]]=[[self.discriminator],[self._discriminator_alias]]\n  else:\n   discriminator=self.discriminator\n  return core_schema.tagged_union_schema(\n  choices=self._tagged_union_choices,\n  discriminator=discriminator,\n  custom_error_type=schema.get('custom_error_type'),\n  custom_error_message=schema.get('custom_error_message'),\n  custom_error_context=schema.get('custom_error_context'),\n  strict=False,\n  from_attributes=True,\n  ref=schema.get('ref'),\n  metadata=schema.get('metadata'),\n  serialization=schema.get('serialization'),\n  )\n  \n def _handle_choice(self,choice:core_schema.CoreSchema)->None:\n  ''\n\n\n\n\n\n\n\n\n  \n  if choice['type']=='definition-ref':\n   if choice['schema_ref']not in self.definitions:\n    raise MissingDefinitionForUnionRef(choice['schema_ref'])\n    \n  if choice['type']=='none':\n   self._should_be_nullable=True\n  elif choice['type']=='definitions':\n   self._handle_choice(choice['schema'])\n  elif choice['type']=='nullable':\n   self._should_be_nullable=True\n   self._handle_choice(choice['schema'])\n  elif choice['type']=='union':\n  \n   choices_schemas=[v[0]if isinstance(v,tuple)else v for v in choice['choices'][::-1]]\n   self._choices_to_handle.extend(choices_schemas)\n  elif choice['type']not in{\n  'model',\n  'typed-dict',\n  'tagged-union',\n  'lax-or-strict',\n  'dataclass',\n  'dataclass-args',\n  'definition-ref',\n  }and not _core_utils.is_function_with_inner_schema(choice):\n  \n   raise TypeError(\n   f'{choice[\"type\"]!r} is not a valid discriminated union variant;'\n   ' should be a `BaseModel` or `dataclass`'\n   )\n  else:\n   if choice['type']=='tagged-union'and self._is_discriminator_shared(choice):\n   \n   \n    subchoices=[x for x in choice['choices'].values()if not isinstance(x,(str,int))]\n    \n    self._choices_to_handle.extend(subchoices[::-1])\n    return\n    \n   inferred_discriminator_values=self._infer_discriminator_values_for_choice(choice,source_name=None)\n   self._set_unique_choice_for_values(choice,inferred_discriminator_values)\n   \n def _is_discriminator_shared(self,choice:core_schema.TaggedUnionSchema)->bool:\n  ''\n\n\n\n  \n  inner_discriminator=choice['discriminator']\n  return inner_discriminator ==self.discriminator or(\n  isinstance(inner_discriminator,list)\n  and(self.discriminator in inner_discriminator or[self.discriminator]in inner_discriminator)\n  )\n  \n def _infer_discriminator_values_for_choice(\n self,choice:core_schema.CoreSchema,source_name:str |None\n )->list[str |int]:\n  ''\n\n\n  \n  if choice['type']=='definitions':\n   return self._infer_discriminator_values_for_choice(choice['schema'],source_name=source_name)\n  elif choice['type']=='function-plain':\n   raise TypeError(\n   f'{choice[\"type\"]!r} is not a valid discriminated union variant;'\n   ' should be a `BaseModel` or `dataclass`'\n   )\n  elif _core_utils.is_function_with_inner_schema(choice):\n   return self._infer_discriminator_values_for_choice(choice['schema'],source_name=source_name)\n  elif choice['type']=='lax-or-strict':\n   return sorted(\n   set(\n   self._infer_discriminator_values_for_choice(choice['lax_schema'],source_name=None)\n   +self._infer_discriminator_values_for_choice(choice['strict_schema'],source_name=None)\n   )\n   )\n   \n  elif choice['type']=='tagged-union':\n   values:list[str |int]=[]\n   \n   subchoices=[x for x in choice['choices'].values()if not isinstance(x,(str,int))]\n   for subchoice in subchoices:\n    subchoice_values=self._infer_discriminator_values_for_choice(subchoice,source_name=None)\n    values.extend(subchoice_values)\n   return values\n   \n  elif choice['type']=='union':\n   values=[]\n   for subchoice in choice['choices']:\n    subchoice_schema=subchoice[0]if isinstance(subchoice,tuple)else subchoice\n    subchoice_values=self._infer_discriminator_values_for_choice(subchoice_schema,source_name=None)\n    values.extend(subchoice_values)\n   return values\n   \n  elif choice['type']=='nullable':\n   self._should_be_nullable=True\n   return self._infer_discriminator_values_for_choice(choice['schema'],source_name=None)\n   \n  elif choice['type']=='model':\n   return self._infer_discriminator_values_for_choice(choice['schema'],source_name=choice['cls'].__name__)\n   \n  elif choice['type']=='dataclass':\n   return self._infer_discriminator_values_for_choice(choice['schema'],source_name=choice['cls'].__name__)\n   \n  elif choice['type']=='model-fields':\n   return self._infer_discriminator_values_for_model_choice(choice,source_name=source_name)\n   \n  elif choice['type']=='dataclass-args':\n   return self._infer_discriminator_values_for_dataclass_choice(choice,source_name=source_name)\n   \n  elif choice['type']=='typed-dict':\n   return self._infer_discriminator_values_for_typed_dict_choice(choice,source_name=source_name)\n   \n  elif choice['type']=='definition-ref':\n   schema_ref=choice['schema_ref']\n   if schema_ref not in self.definitions:\n    raise MissingDefinitionForUnionRef(schema_ref)\n   return self._infer_discriminator_values_for_choice(self.definitions[schema_ref],source_name=source_name)\n  else:\n   raise TypeError(\n   f'{choice[\"type\"]!r} is not a valid discriminated union variant;'\n   ' should be a `BaseModel` or `dataclass`'\n   )\n   \n def _infer_discriminator_values_for_typed_dict_choice(\n self,choice:core_schema.TypedDictSchema,source_name:str |None=None\n )->list[str |int]:\n  ''\n\n  \n  source='TypedDict'if source_name is None else f'TypedDict {source_name !r}'\n  field=choice['fields'].get(self.discriminator)\n  if field is None:\n   raise PydanticUserError(\n   f'{source} needs a discriminator field for key {self.discriminator !r}',code='discriminator-no-field'\n   )\n  return self._infer_discriminator_values_for_field(field,source)\n  \n def _infer_discriminator_values_for_model_choice(\n self,choice:core_schema.ModelFieldsSchema,source_name:str |None=None\n )->list[str |int]:\n  source='ModelFields'if source_name is None else f'Model {source_name !r}'\n  field=choice['fields'].get(self.discriminator)\n  if field is None:\n   raise PydanticUserError(\n   f'{source} needs a discriminator field for key {self.discriminator !r}',code='discriminator-no-field'\n   )\n  return self._infer_discriminator_values_for_field(field,source)\n  \n def _infer_discriminator_values_for_dataclass_choice(\n self,choice:core_schema.DataclassArgsSchema,source_name:str |None=None\n )->list[str |int]:\n  source='DataclassArgs'if source_name is None else f'Dataclass {source_name !r}'\n  for field in choice['fields']:\n   if field['name']==self.discriminator:\n    break\n  else:\n   raise PydanticUserError(\n   f'{source} needs a discriminator field for key {self.discriminator !r}',code='discriminator-no-field'\n   )\n  return self._infer_discriminator_values_for_field(field,source)\n  \n def _infer_discriminator_values_for_field(self,field:CoreSchemaField,source:str)->list[str |int]:\n  if field['type']=='computed-field':\n  \n   return[]\n  alias=field.get('validation_alias',self.discriminator)\n  if not isinstance(alias,str):\n   raise PydanticUserError(\n   f'Alias {alias !r} is not supported in a discriminated union',code='discriminator-alias-type'\n   )\n  if self._discriminator_alias is None:\n   self._discriminator_alias=alias\n  elif self._discriminator_alias !=alias:\n   raise PydanticUserError(\n   f'Aliases for discriminator {self.discriminator !r} must be the same '\n   f'(got {alias}, {self._discriminator_alias})',\n   code='discriminator-alias',\n   )\n  return self._infer_discriminator_values_for_inner_schema(field['schema'],source)\n  \n def _infer_discriminator_values_for_inner_schema(\n self,schema:core_schema.CoreSchema,source:str\n )->list[str |int]:\n  ''\n\n  \n  if schema['type']=='literal':\n   return schema['expected']\n   \n  elif schema['type']=='union':\n  \n  \n  \n   values:list[Any]=[]\n   for choice in schema['choices']:\n    choice_schema=choice[0]if isinstance(choice,tuple)else choice\n    choice_values=self._infer_discriminator_values_for_inner_schema(choice_schema,source)\n    values.extend(choice_values)\n   return values\n   \n  elif schema['type']=='default':\n  \n   return self._infer_discriminator_values_for_inner_schema(schema['schema'],source)\n   \n  elif schema['type']=='function-after':\n  \n   return self._infer_discriminator_values_for_inner_schema(schema['schema'],source)\n   \n  elif schema['type']in{'function-before','function-wrap','function-plain'}:\n   validator_type=repr(schema['type'].split('-')[1])\n   raise PydanticUserError(\n   f'Cannot use a mode={validator_type} validator in the'\n   f' discriminator field {self.discriminator !r} of {source}',\n   code='discriminator-validator',\n   )\n   \n  else:\n   raise PydanticUserError(\n   f'{source} needs field {self.discriminator !r} to be of type `Literal`',\n   code='discriminator-needs-literal',\n   )\n   \n def _set_unique_choice_for_values(self,choice:core_schema.CoreSchema,values:Sequence[str |int])->None:\n  ''\n\n  \n  for discriminator_value in values:\n   if discriminator_value in self._tagged_union_choices:\n   \n   \n   \n    existing_choice=self._tagged_union_choices[discriminator_value]\n    if existing_choice !=choice:\n     raise TypeError(\n     f'Value {discriminator_value !r} for discriminator '\n     f'{self.discriminator !r} mapped to multiple choices'\n     )\n   else:\n    self._tagged_union_choices[discriminator_value]=choice\n", ["__future__", "pydantic._internal", "pydantic._internal._core_utils", "pydantic.errors", "pydantic.types", "pydantic_core", "pydantic_core.core_schema", "typing"]], "pydantic._internal._docs_extraction": [".py", "''\n\nfrom __future__ import annotations\n\nimport ast\nimport inspect\nimport textwrap\nfrom typing import Any\n\n\nclass DocstringVisitor(ast.NodeVisitor):\n def __init__(self)->None:\n  super().__init__()\n  \n  self.target:str |None=None\n  self.attrs:dict[str,str]={}\n  self.previous_node_type:type[ast.AST]|None=None\n  \n def visit(self,node:ast.AST)->Any:\n  node_result=super().visit(node)\n  self.previous_node_type=type(node)\n  return node_result\n  \n def visit_AnnAssign(self,node:ast.AnnAssign)->Any:\n  if isinstance(node.target,ast.Name):\n   self.target=node.target.id\n   \n def visit_Expr(self,node:ast.Expr)->Any:\n  if(\n  isinstance(node.value,ast.Constant)\n  and isinstance(node.value.value,str)\n  and self.previous_node_type is ast.AnnAssign\n  ):\n   docstring=inspect.cleandoc(node.value.value)\n   if self.target:\n    self.attrs[self.target]=docstring\n   self.target=None\n   \n   \ndef _dedent_source_lines(source:list[str])->str:\n\n dedent_source=textwrap.dedent(''.join(source))\n if dedent_source.startswith((' ','\\t')):\n \n \n \n  dedent_source=f'def dedent_workaround():\\n{dedent_source}'\n return dedent_source\n \n \ndef _extract_source_from_frame(cls:type[Any])->list[str]|None:\n frame=inspect.currentframe()\n \n while frame:\n  if inspect.getmodule(frame)is inspect.getmodule(cls):\n   lnum=frame.f_lineno\n   try:\n    lines,_=inspect.findsource(frame)\n   except OSError:\n   \n   \n    pass\n   else:\n    block_lines=inspect.getblock(lines[lnum -1:])\n    dedent_source=_dedent_source_lines(block_lines)\n    try:\n     block_tree=ast.parse(dedent_source)\n    except SyntaxError:\n     pass\n    else:\n     stmt=block_tree.body[0]\n     if isinstance(stmt,ast.FunctionDef)and stmt.name =='dedent_workaround':\n     \n      stmt=stmt.body[0]\n     if isinstance(stmt,ast.ClassDef)and stmt.name ==cls.__name__:\n      return block_lines\n      \n  frame=frame.f_back\n  \n  \ndef extract_docstrings_from_cls(cls:type[Any],use_inspect:bool=False)->dict[str,str]:\n ''\n\n\n\n\n\n\n\n\n \n if use_inspect:\n \n  try:\n   source,_=inspect.getsourcelines(cls)\n  except OSError:\n   return{}\n else:\n  source=_extract_source_from_frame(cls)\n  \n if not source:\n  return{}\n  \n dedent_source=_dedent_source_lines(source)\n \n visitor=DocstringVisitor()\n visitor.visit(ast.parse(dedent_source))\n return visitor.attrs\n", ["__future__", "ast", "inspect", "textwrap", "typing"]], "pydantic._internal._schema_generation_shared": [".py", "''\n\nfrom __future__ import annotations\n\nfrom typing import TYPE_CHECKING,Any,Callable\n\nfrom pydantic_core import core_schema\nfrom typing_extensions import Literal\n\nfrom..annotated_handlers import GetCoreSchemaHandler,GetJsonSchemaHandler\n\nif TYPE_CHECKING:\n from..json_schema import GenerateJsonSchema,JsonSchemaValue\n from._core_utils import CoreSchemaOrField\n from._generate_schema import GenerateSchema\n from._namespace_utils import NamespacesTuple\n \n GetJsonSchemaFunction=Callable[[CoreSchemaOrField,GetJsonSchemaHandler],JsonSchemaValue]\n HandlerOverride=Callable[[CoreSchemaOrField],JsonSchemaValue]\n \n \nclass GenerateJsonSchemaHandler(GetJsonSchemaHandler):\n ''\n\n\n\n\n\n\n \n \n def __init__(self,generate_json_schema:GenerateJsonSchema,handler_override:HandlerOverride |None)->None:\n  self.generate_json_schema=generate_json_schema\n  self.handler=handler_override or generate_json_schema.generate_inner\n  self.mode=generate_json_schema.mode\n  \n def __call__(self,core_schema:CoreSchemaOrField,/)->JsonSchemaValue:\n  return self.handler(core_schema)\n  \n def resolve_ref_schema(self,maybe_ref_json_schema:JsonSchemaValue)->JsonSchemaValue:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  if '$ref'not in maybe_ref_json_schema:\n   return maybe_ref_json_schema\n  ref=maybe_ref_json_schema['$ref']\n  json_schema=self.generate_json_schema.get_schema_from_definitions(ref)\n  if json_schema is None:\n   raise LookupError(\n   f'Could not find a ref for {ref}.'\n   ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n   )\n  return json_schema\n  \n  \nclass CallbackGetCoreSchemaHandler(GetCoreSchemaHandler):\n ''\n\n\n\n \n \n def __init__(\n self,\n handler:Callable[[Any],core_schema.CoreSchema],\n generate_schema:GenerateSchema,\n ref_mode:Literal['to-def','unpack']='to-def',\n )->None:\n  self._handler=handler\n  self._generate_schema=generate_schema\n  self._ref_mode=ref_mode\n  \n def __call__(self,source_type:Any,/)->core_schema.CoreSchema:\n  schema=self._handler(source_type)\n  ref=schema.get('ref')\n  if self._ref_mode =='to-def':\n   if ref is not None:\n    self._generate_schema.defs.definitions[ref]=schema\n    return core_schema.definition_reference_schema(ref)\n   return schema\n  else:\n   return self.resolve_ref_schema(schema)\n   \n def _get_types_namespace(self)->NamespacesTuple:\n  return self._generate_schema._types_namespace\n  \n def generate_schema(self,source_type:Any,/)->core_schema.CoreSchema:\n  return self._generate_schema.generate_schema(source_type)\n  \n @property\n def field_name(self)->str |None:\n  return self._generate_schema.field_name_stack.get()\n  \n def resolve_ref_schema(self,maybe_ref_schema:core_schema.CoreSchema)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  if maybe_ref_schema['type']=='definition-ref':\n   ref=maybe_ref_schema['schema_ref']\n   if ref not in self._generate_schema.defs.definitions:\n    raise LookupError(\n    f'Could not find a ref for {ref}.'\n    ' Maybe you tried to call resolve_ref_schema from within a recursive model?'\n    )\n   return self._generate_schema.defs.definitions[ref]\n  elif maybe_ref_schema['type']=='definitions':\n   return self.resolve_ref_schema(maybe_ref_schema['schema'])\n  return maybe_ref_schema\n", ["__future__", "pydantic._internal._core_utils", "pydantic._internal._generate_schema", "pydantic._internal._namespace_utils", "pydantic.annotated_handlers", "pydantic.json_schema", "pydantic_core", "pydantic_core.core_schema", "typing", "typing_extensions"]], "pydantic._internal._core_utils": [".py", "from __future__ import annotations\n\nimport os\nfrom collections import defaultdict\nfrom typing import Any,Callable,Hashable,TypeVar,Union\n\nfrom pydantic_core import CoreSchema,core_schema\nfrom pydantic_core import validate_core_schema as _validate_core_schema\nfrom typing_extensions import TypeGuard,get_args,get_origin\n\nfrom..errors import PydanticUserError\nfrom. import _repr\nfrom._core_metadata import CoreMetadata\nfrom._typing_extra import is_generic_alias,is_type_alias_type\n\nAnyFunctionSchema=Union[\ncore_schema.AfterValidatorFunctionSchema,\ncore_schema.BeforeValidatorFunctionSchema,\ncore_schema.WrapValidatorFunctionSchema,\ncore_schema.PlainValidatorFunctionSchema,\n]\n\n\nFunctionSchemaWithInnerSchema=Union[\ncore_schema.AfterValidatorFunctionSchema,\ncore_schema.BeforeValidatorFunctionSchema,\ncore_schema.WrapValidatorFunctionSchema,\n]\n\nCoreSchemaField=Union[\ncore_schema.ModelField,core_schema.DataclassField,core_schema.TypedDictField,core_schema.ComputedField\n]\nCoreSchemaOrField=Union[core_schema.CoreSchema,CoreSchemaField]\n\n_CORE_SCHEMA_FIELD_TYPES={'typed-dict-field','dataclass-field','model-field','computed-field'}\n_FUNCTION_WITH_INNER_SCHEMA_TYPES={'function-before','function-after','function-wrap'}\n_LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES={'list','set','frozenset'}\n\nTAGGED_UNION_TAG_KEY='pydantic.internal.tagged_union_tag'\n''\n\n\n\n\ndef is_core_schema(\nschema:CoreSchemaOrField,\n)->TypeGuard[CoreSchema]:\n return schema['type']not in _CORE_SCHEMA_FIELD_TYPES\n \n \ndef is_core_schema_field(\nschema:CoreSchemaOrField,\n)->TypeGuard[CoreSchemaField]:\n return schema['type']in _CORE_SCHEMA_FIELD_TYPES\n \n \ndef is_function_with_inner_schema(\nschema:CoreSchemaOrField,\n)->TypeGuard[FunctionSchemaWithInnerSchema]:\n return schema['type']in _FUNCTION_WITH_INNER_SCHEMA_TYPES\n \n \ndef is_list_like_schema_with_items_schema(\nschema:CoreSchema,\n)->TypeGuard[core_schema.ListSchema |core_schema.SetSchema |core_schema.FrozenSetSchema]:\n return schema['type']in _LIST_LIKE_SCHEMA_WITH_ITEMS_TYPES\n \n \ndef get_type_ref(type_:type[Any],args_override:tuple[type[Any],...]|None=None)->str:\n ''\n\n\n\n \n origin=get_origin(type_)or type_\n \n args=get_args(type_)if is_generic_alias(type_)else(args_override or())\n generic_metadata=getattr(type_,'__pydantic_generic_metadata__',None)\n if generic_metadata:\n  origin=generic_metadata['origin']or origin\n  args=generic_metadata['args']or args\n  \n module_name=getattr(origin,'__module__','<No __module__>')\n if is_type_alias_type(origin):\n  type_ref=f'{module_name}.{origin.__name__}:{id(origin)}'\n else:\n  try:\n   qualname=getattr(origin,'__qualname__',f'<No __qualname__: {origin}>')\n  except Exception:\n   qualname=getattr(origin,'__qualname__','<No __qualname__>')\n  type_ref=f'{module_name}.{qualname}:{id(origin)}'\n  \n arg_refs:list[str]=[]\n for arg in args:\n  if isinstance(arg,str):\n  \n  \n   arg_ref=f'{arg}:str-{id(arg)}'\n  else:\n   arg_ref=f'{_repr.display_as_type(arg)}:{id(arg)}'\n  arg_refs.append(arg_ref)\n if arg_refs:\n  type_ref=f'{type_ref}[{\",\".join(arg_refs)}]'\n return type_ref\n \n \ndef get_ref(s:core_schema.CoreSchema)->None |str:\n ''\n\n \n return s.get('ref',None)\n \n \ndef collect_definitions(schema:core_schema.CoreSchema)->dict[str,core_schema.CoreSchema]:\n defs:dict[str,CoreSchema]={}\n \n def _record_valid_refs(s:core_schema.CoreSchema,recurse:Recurse)->core_schema.CoreSchema:\n  ref=get_ref(s)\n  if ref:\n   defs[ref]=s\n  return recurse(s,_record_valid_refs)\n  \n walk_core_schema(schema,_record_valid_refs,copy=False)\n \n return defs\n \n \ndef define_expected_missing_refs(\nschema:core_schema.CoreSchema,allowed_missing_refs:set[str]\n)->core_schema.CoreSchema |None:\n if not allowed_missing_refs:\n \n \n  return None\n  \n refs=collect_definitions(schema).keys()\n \n expected_missing_refs=allowed_missing_refs.difference(refs)\n if expected_missing_refs:\n  definitions:list[core_schema.CoreSchema]=[\n  core_schema.invalid_schema(ref=ref)for ref in expected_missing_refs\n  ]\n  return core_schema.definitions_schema(schema,definitions)\n return None\n \n \ndef collect_invalid_schemas(schema:core_schema.CoreSchema)->bool:\n invalid=False\n \n def _is_schema_valid(s:core_schema.CoreSchema,recurse:Recurse)->core_schema.CoreSchema:\n  nonlocal invalid\n  \n  if s['type']=='invalid':\n   invalid=True\n   return s\n   \n  return recurse(s,_is_schema_valid)\n  \n walk_core_schema(schema,_is_schema_valid,copy=False)\n return invalid\n \n \nT=TypeVar('T')\n\n\nRecurse=Callable[[core_schema.CoreSchema,'Walk'],core_schema.CoreSchema]\nWalk=Callable[[core_schema.CoreSchema,Recurse],core_schema.CoreSchema]\n\n\n\n\nCoreSchemaT=TypeVar('CoreSchemaT')\n\n\nclass _WalkCoreSchema:\n def __init__(self,*,copy:bool=True):\n  self._schema_type_to_method=self._build_schema_type_to_method()\n  self._copy=copy\n  \n def _copy_schema(self,schema:CoreSchemaT)->CoreSchemaT:\n  return schema.copy()if self._copy else schema\n  \n def _build_schema_type_to_method(self)->dict[core_schema.CoreSchemaType,Recurse]:\n  mapping:dict[core_schema.CoreSchemaType,Recurse]={}\n  key:core_schema.CoreSchemaType\n  for key in get_args(core_schema.CoreSchemaType):\n   method_name=f\"handle_{key.replace('-','_')}_schema\"\n   mapping[key]=getattr(self,method_name,self._handle_other_schemas)\n  return mapping\n  \n def walk(self,schema:core_schema.CoreSchema,f:Walk)->core_schema.CoreSchema:\n  return f(schema,self._walk)\n  \n def _walk(self,schema:core_schema.CoreSchema,f:Walk)->core_schema.CoreSchema:\n  schema=self._schema_type_to_method[schema['type']](self._copy_schema(schema),f)\n  ser_schema:core_schema.SerSchema |None=schema.get('serialization')\n  if ser_schema:\n   schema['serialization']=self._handle_ser_schemas(ser_schema,f)\n  return schema\n  \n def _handle_other_schemas(self,schema:core_schema.CoreSchema,f:Walk)->core_schema.CoreSchema:\n  sub_schema=schema.get('schema',None)\n  if sub_schema is not None:\n   schema['schema']=self.walk(sub_schema,f)\n  return schema\n  \n def _handle_ser_schemas(self,ser_schema:core_schema.SerSchema,f:Walk)->core_schema.SerSchema:\n  schema:core_schema.CoreSchema |None=ser_schema.get('schema',None)\n  return_schema:core_schema.CoreSchema |None=ser_schema.get('return_schema',None)\n  if schema is not None or return_schema is not None:\n   ser_schema=self._copy_schema(ser_schema)\n   if schema is not None:\n    ser_schema['schema']=self.walk(schema,f)\n   if return_schema is not None:\n    ser_schema['return_schema']=self.walk(return_schema,f)\n  return ser_schema\n  \n def handle_definitions_schema(self,schema:core_schema.DefinitionsSchema,f:Walk)->core_schema.CoreSchema:\n  new_definitions:list[core_schema.CoreSchema]=[]\n  for definition in schema['definitions']:\n   if 'schema_ref'in definition and 'ref'in definition:\n   \n   \n    new_definitions.append(definition)\n    \n    self.walk(definition,f)\n    continue\n    \n   updated_definition=self.walk(definition,f)\n   if 'ref'in updated_definition:\n   \n   \n   \n    new_definitions.append(updated_definition)\n  new_inner_schema=self.walk(schema['schema'],f)\n  \n  if not new_definitions and len(schema)==3:\n  \n   return new_inner_schema\n   \n  new_schema=self._copy_schema(schema)\n  new_schema['schema']=new_inner_schema\n  new_schema['definitions']=new_definitions\n  return new_schema\n  \n def handle_list_schema(self,schema:core_schema.ListSchema,f:Walk)->core_schema.CoreSchema:\n  items_schema=schema.get('items_schema')\n  if items_schema is not None:\n   schema['items_schema']=self.walk(items_schema,f)\n  return schema\n  \n def handle_set_schema(self,schema:core_schema.SetSchema,f:Walk)->core_schema.CoreSchema:\n  items_schema=schema.get('items_schema')\n  if items_schema is not None:\n   schema['items_schema']=self.walk(items_schema,f)\n  return schema\n  \n def handle_frozenset_schema(self,schema:core_schema.FrozenSetSchema,f:Walk)->core_schema.CoreSchema:\n  items_schema=schema.get('items_schema')\n  if items_schema is not None:\n   schema['items_schema']=self.walk(items_schema,f)\n  return schema\n  \n def handle_generator_schema(self,schema:core_schema.GeneratorSchema,f:Walk)->core_schema.CoreSchema:\n  items_schema=schema.get('items_schema')\n  if items_schema is not None:\n   schema['items_schema']=self.walk(items_schema,f)\n  return schema\n  \n def handle_tuple_schema(self,schema:core_schema.TupleSchema,f:Walk)->core_schema.CoreSchema:\n  schema['items_schema']=[self.walk(v,f)for v in schema['items_schema']]\n  return schema\n  \n def handle_dict_schema(self,schema:core_schema.DictSchema,f:Walk)->core_schema.CoreSchema:\n  keys_schema=schema.get('keys_schema')\n  if keys_schema is not None:\n   schema['keys_schema']=self.walk(keys_schema,f)\n  values_schema=schema.get('values_schema')\n  if values_schema:\n   schema['values_schema']=self.walk(values_schema,f)\n  return schema\n  \n def handle_function_after_schema(\n self,schema:core_schema.AfterValidatorFunctionSchema,f:Walk\n )->core_schema.CoreSchema:\n  schema['schema']=self.walk(schema['schema'],f)\n  return schema\n  \n def handle_function_before_schema(\n self,schema:core_schema.BeforeValidatorFunctionSchema,f:Walk\n )->core_schema.CoreSchema:\n  schema['schema']=self.walk(schema['schema'],f)\n  if 'json_schema_input_schema'in schema:\n   schema['json_schema_input_schema']=self.walk(schema['json_schema_input_schema'],f)\n  return schema\n  \n  \n def handle_function_plain_schema(\n self,schema:core_schema.PlainValidatorFunctionSchema |core_schema.PlainSerializerFunctionSerSchema,f:Walk\n )->core_schema.CoreSchema:\n  if 'json_schema_input_schema'in schema:\n   schema['json_schema_input_schema']=self.walk(schema['json_schema_input_schema'],f)\n  return schema\n  \n  \n def handle_function_wrap_schema(\n self,schema:core_schema.WrapValidatorFunctionSchema |core_schema.WrapSerializerFunctionSerSchema,f:Walk\n )->core_schema.CoreSchema:\n  if 'schema'in schema:\n   schema['schema']=self.walk(schema['schema'],f)\n  if 'json_schema_input_schema'in schema:\n   schema['json_schema_input_schema']=self.walk(schema['json_schema_input_schema'],f)\n  return schema\n  \n def handle_union_schema(self,schema:core_schema.UnionSchema,f:Walk)->core_schema.CoreSchema:\n  new_choices:list[CoreSchema |tuple[CoreSchema,str]]=[]\n  for v in schema['choices']:\n   if isinstance(v,tuple):\n    new_choices.append((self.walk(v[0],f),v[1]))\n   else:\n    new_choices.append(self.walk(v,f))\n  schema['choices']=new_choices\n  return schema\n  \n def handle_tagged_union_schema(self,schema:core_schema.TaggedUnionSchema,f:Walk)->core_schema.CoreSchema:\n  new_choices:dict[Hashable,core_schema.CoreSchema]={}\n  for k,v in schema['choices'].items():\n   new_choices[k]=v if isinstance(v,(str,int))else self.walk(v,f)\n  schema['choices']=new_choices\n  return schema\n  \n def handle_chain_schema(self,schema:core_schema.ChainSchema,f:Walk)->core_schema.CoreSchema:\n  schema['steps']=[self.walk(v,f)for v in schema['steps']]\n  return schema\n  \n def handle_lax_or_strict_schema(self,schema:core_schema.LaxOrStrictSchema,f:Walk)->core_schema.CoreSchema:\n  schema['lax_schema']=self.walk(schema['lax_schema'],f)\n  schema['strict_schema']=self.walk(schema['strict_schema'],f)\n  return schema\n  \n def handle_json_or_python_schema(self,schema:core_schema.JsonOrPythonSchema,f:Walk)->core_schema.CoreSchema:\n  schema['json_schema']=self.walk(schema['json_schema'],f)\n  schema['python_schema']=self.walk(schema['python_schema'],f)\n  return schema\n  \n def handle_model_fields_schema(self,schema:core_schema.ModelFieldsSchema,f:Walk)->core_schema.CoreSchema:\n  extras_schema=schema.get('extras_schema')\n  if extras_schema is not None:\n   schema['extras_schema']=self.walk(extras_schema,f)\n  replaced_fields:dict[str,core_schema.ModelField]={}\n  replaced_computed_fields:list[core_schema.ComputedField]=[]\n  for computed_field in schema.get('computed_fields',()):\n   replaced_field=self._copy_schema(computed_field)\n   replaced_field['return_schema']=self.walk(computed_field['return_schema'],f)\n   replaced_computed_fields.append(replaced_field)\n  if replaced_computed_fields:\n   schema['computed_fields']=replaced_computed_fields\n  for k,v in schema['fields'].items():\n   replaced_field=self._copy_schema(v)\n   replaced_field['schema']=self.walk(v['schema'],f)\n   replaced_fields[k]=replaced_field\n  schema['fields']=replaced_fields\n  return schema\n  \n def handle_typed_dict_schema(self,schema:core_schema.TypedDictSchema,f:Walk)->core_schema.CoreSchema:\n  extras_schema=schema.get('extras_schema')\n  if extras_schema is not None:\n   schema['extras_schema']=self.walk(extras_schema,f)\n  replaced_computed_fields:list[core_schema.ComputedField]=[]\n  for computed_field in schema.get('computed_fields',()):\n   replaced_field=self._copy_schema(computed_field)\n   replaced_field['return_schema']=self.walk(computed_field['return_schema'],f)\n   replaced_computed_fields.append(replaced_field)\n  if replaced_computed_fields:\n   schema['computed_fields']=replaced_computed_fields\n  replaced_fields:dict[str,core_schema.TypedDictField]={}\n  for k,v in schema['fields'].items():\n   replaced_field=self._copy_schema(v)\n   replaced_field['schema']=self.walk(v['schema'],f)\n   replaced_fields[k]=replaced_field\n  schema['fields']=replaced_fields\n  return schema\n  \n def handle_dataclass_args_schema(self,schema:core_schema.DataclassArgsSchema,f:Walk)->core_schema.CoreSchema:\n  replaced_fields:list[core_schema.DataclassField]=[]\n  replaced_computed_fields:list[core_schema.ComputedField]=[]\n  for computed_field in schema.get('computed_fields',()):\n   replaced_field=self._copy_schema(computed_field)\n   replaced_field['return_schema']=self.walk(computed_field['return_schema'],f)\n   replaced_computed_fields.append(replaced_field)\n  if replaced_computed_fields:\n   schema['computed_fields']=replaced_computed_fields\n  for field in schema['fields']:\n   replaced_field=self._copy_schema(field)\n   replaced_field['schema']=self.walk(field['schema'],f)\n   replaced_fields.append(replaced_field)\n  schema['fields']=replaced_fields\n  return schema\n  \n def handle_arguments_schema(self,schema:core_schema.ArgumentsSchema,f:Walk)->core_schema.CoreSchema:\n  replaced_arguments_schema:list[core_schema.ArgumentsParameter]=[]\n  for param in schema['arguments_schema']:\n   replaced_param=self._copy_schema(param)\n   replaced_param['schema']=self.walk(param['schema'],f)\n   replaced_arguments_schema.append(replaced_param)\n  schema['arguments_schema']=replaced_arguments_schema\n  if 'var_args_schema'in schema:\n   schema['var_args_schema']=self.walk(schema['var_args_schema'],f)\n  if 'var_kwargs_schema'in schema:\n   schema['var_kwargs_schema']=self.walk(schema['var_kwargs_schema'],f)\n  return schema\n  \n def handle_call_schema(self,schema:core_schema.CallSchema,f:Walk)->core_schema.CoreSchema:\n  schema['arguments_schema']=self.walk(schema['arguments_schema'],f)\n  if 'return_schema'in schema:\n   schema['return_schema']=self.walk(schema['return_schema'],f)\n  return schema\n  \n  \n_dispatch=_WalkCoreSchema().walk\n_dispatch_no_copy=_WalkCoreSchema(copy=False).walk\n\n\ndef walk_core_schema(schema:core_schema.CoreSchema,f:Walk,*,copy:bool=True)->core_schema.CoreSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n return f(schema.copy()if copy else schema,_dispatch if copy else _dispatch_no_copy)\n \n \ndef simplify_schema_references(schema:core_schema.CoreSchema)->core_schema.CoreSchema:\n definitions:dict[str,core_schema.CoreSchema]={}\n ref_counts:dict[str,int]=defaultdict(int)\n involved_in_recursion:dict[str,bool]={}\n current_recursion_ref_count:dict[str,int]=defaultdict(int)\n \n def collect_refs(s:core_schema.CoreSchema,recurse:Recurse)->core_schema.CoreSchema:\n  if s['type']=='definitions':\n   for definition in s['definitions']:\n    ref=get_ref(definition)\n    assert ref is not None\n    if ref not in definitions:\n     definitions[ref]=definition\n    recurse(definition,collect_refs)\n   return recurse(s['schema'],collect_refs)\n  else:\n   ref=get_ref(s)\n   if ref is not None:\n    new=recurse(s,collect_refs)\n    new_ref=get_ref(new)\n    if new_ref:\n     definitions[new_ref]=new\n    return core_schema.definition_reference_schema(schema_ref=ref)\n   else:\n    return recurse(s,collect_refs)\n    \n schema=walk_core_schema(schema,collect_refs)\n \n def count_refs(s:core_schema.CoreSchema,recurse:Recurse)->core_schema.CoreSchema:\n  if s['type']!='definition-ref':\n   return recurse(s,count_refs)\n  ref=s['schema_ref']\n  ref_counts[ref]+=1\n  \n  if ref_counts[ref]>=2:\n  \n  \n   if current_recursion_ref_count[ref]!=0:\n    involved_in_recursion[ref]=True\n   return s\n   \n  current_recursion_ref_count[ref]+=1\n  if 'serialization'in s:\n  \n  \n   recurse(s,count_refs)\n   \n  next_s=definitions[ref]\n  visited:set[str]=set()\n  while next_s['type']=='definition-ref':\n   if next_s['schema_ref']in visited:\n    raise PydanticUserError(\n    f'{ref} contains a circular reference to itself.',code='circular-reference-schema'\n    )\n    \n   visited.add(next_s['schema_ref'])\n   ref_counts[next_s['schema_ref']]+=1\n   next_s=definitions[next_s['schema_ref']]\n   \n  recurse(next_s,count_refs)\n  current_recursion_ref_count[ref]-=1\n  return s\n  \n schema=walk_core_schema(schema,count_refs,copy=False)\n \n assert all(c ==0 for c in current_recursion_ref_count.values()),'this is a bug! please report it'\n \n def can_be_inlined(s:core_schema.DefinitionReferenceSchema,ref:str)->bool:\n  if ref_counts[ref]>1:\n   return False\n  if involved_in_recursion.get(ref,False):\n   return False\n  if 'serialization'in s:\n   return False\n  if 'metadata'in s:\n   metadata=s['metadata']\n   for k in[\n   *CoreMetadata.__annotations__.keys(),\n   'pydantic.internal.union_discriminator',\n   'pydantic.internal.tagged_union_tag',\n   ]:\n    if k in metadata:\n    \n     return False\n  return True\n  \n def inline_refs(s:core_schema.CoreSchema,recurse:Recurse)->core_schema.CoreSchema:\n \n  while s['type']=='definition-ref':\n   ref=s['schema_ref']\n   \n   \n   \n   if can_be_inlined(s,ref):\n   \n    new=definitions.pop(ref)\n    ref_counts[ref]-=1\n    \n    \n    if 'serialization'in s:\n     new['serialization']=s['serialization']\n    s=new\n   else:\n    break\n  return recurse(s,inline_refs)\n  \n schema=walk_core_schema(schema,inline_refs,copy=False)\n \n def_values=[v for v in definitions.values()if ref_counts[v['ref']]>0]\n \n if def_values:\n  schema=core_schema.definitions_schema(schema=schema,definitions=def_values)\n return schema\n \n \ndef _strip_metadata(schema:CoreSchema)->CoreSchema:\n def strip_metadata(s:CoreSchema,recurse:Recurse)->CoreSchema:\n  s=s.copy()\n  s.pop('metadata',None)\n  if s['type']=='model-fields':\n   s=s.copy()\n   s['fields']={k:v.copy()for k,v in s['fields'].items()}\n   for field_name,field_schema in s['fields'].items():\n    field_schema.pop('metadata',None)\n    s['fields'][field_name]=field_schema\n   computed_fields=s.get('computed_fields',None)\n   if computed_fields:\n    s['computed_fields']=[cf.copy()for cf in computed_fields]\n    for cf in computed_fields:\n     cf.pop('metadata',None)\n   else:\n    s.pop('computed_fields',None)\n  elif s['type']=='model':\n  \n   if s.get('custom_init',True)is False:\n    s.pop('custom_init')\n   if s.get('root_model',True)is False:\n    s.pop('root_model')\n   if{'title'}.issuperset(s.get('config',{}).keys()):\n    s.pop('config',None)\n    \n  return recurse(s,strip_metadata)\n  \n return walk_core_schema(schema,strip_metadata)\n \n \ndef pretty_print_core_schema(\nschema:CoreSchema,\ninclude_metadata:bool=False,\n)->None:\n ''\n\n\n\n\n\n \n from rich import print\n \n if not include_metadata:\n  schema=_strip_metadata(schema)\n  \n return print(schema)\n \n \ndef validate_core_schema(schema:CoreSchema)->CoreSchema:\n if 'PYDANTIC_SKIP_VALIDATING_CORE_SCHEMAS'in os.environ:\n  return schema\n return _validate_core_schema(schema)\n", ["__future__", "collections", "os", "pydantic._internal", "pydantic._internal._core_metadata", "pydantic._internal._repr", "pydantic._internal._typing_extra", "pydantic.errors", "pydantic_core", "pydantic_core.core_schema", "rich", "typing", "typing_extensions"]], "pydantic._internal._config": [".py", "from __future__ import annotations as _annotations\n\nimport warnings\nfrom contextlib import contextmanager\nfrom re import Pattern\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\ncast,\n)\n\nfrom pydantic_core import core_schema\nfrom typing_extensions import(\nLiteral,\nSelf,\n)\n\nfrom..aliases import AliasGenerator\nfrom..config import ConfigDict,ExtraValues,JsonDict,JsonEncoder,JsonSchemaExtraCallable\nfrom..errors import PydanticUserError\nfrom..warnings import PydanticDeprecatedSince20,PydanticDeprecatedSince210\n\nif not TYPE_CHECKING:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \nif TYPE_CHECKING:\n from.._internal._schema_generation_shared import GenerateSchema\n from..fields import ComputedFieldInfo,FieldInfo\n \nDEPRECATION_MESSAGE='Support for class-based `config` is deprecated, use ConfigDict instead.'\n\n\nclass ConfigWrapper:\n ''\n \n __slots__=('config_dict',)\n \n config_dict:ConfigDict\n \n \n \n title:str |None\n str_to_lower:bool\n str_to_upper:bool\n str_strip_whitespace:bool\n str_min_length:int\n str_max_length:int |None\n extra:ExtraValues |None\n frozen:bool\n populate_by_name:bool\n use_enum_values:bool\n validate_assignment:bool\n arbitrary_types_allowed:bool\n from_attributes:bool\n \n \n loc_by_alias:bool\n alias_generator:Callable[[str],str]|AliasGenerator |None\n model_title_generator:Callable[[type],str]|None\n field_title_generator:Callable[[str,FieldInfo |ComputedFieldInfo],str]|None\n ignored_types:tuple[type,...]\n allow_inf_nan:bool\n json_schema_extra:JsonDict |JsonSchemaExtraCallable |None\n json_encoders:dict[type[object],JsonEncoder]|None\n \n \n strict:bool\n \n revalidate_instances:Literal['always','never','subclass-instances']\n ser_json_timedelta:Literal['iso8601','float']\n ser_json_bytes:Literal['utf8','base64','hex']\n val_json_bytes:Literal['utf8','base64','hex']\n ser_json_inf_nan:Literal['null','constants','strings']\n \n validate_default:bool\n validate_return:bool\n protected_namespaces:tuple[str |Pattern[str],...]\n hide_input_in_errors:bool\n defer_build:bool\n plugin_settings:dict[str,object]|None\n schema_generator:type[GenerateSchema]|None\n json_schema_serialization_defaults_required:bool\n json_schema_mode_override:Literal['validation','serialization',None]\n coerce_numbers_to_str:bool\n regex_engine:Literal['rust-regex','python-re']\n validation_error_cause:bool\n use_attribute_docstrings:bool\n cache_strings:bool |Literal['all','keys','none']\n \n def __init__(self,config:ConfigDict |dict[str,Any]|type[Any]|None,*,check:bool=True):\n  if check:\n   self.config_dict=prepare_config(config)\n  else:\n   self.config_dict=cast(ConfigDict,config)\n   \n @classmethod\n def for_model(cls,bases:tuple[type[Any],...],namespace:dict[str,Any],kwargs:dict[str,Any])->Self:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  config_new=ConfigDict()\n  for base in bases:\n   config=getattr(base,'model_config',None)\n   if config:\n    config_new.update(config.copy())\n    \n  config_class_from_namespace=namespace.get('Config')\n  config_dict_from_namespace=namespace.get('model_config')\n  \n  raw_annotations=namespace.get('__annotations__',{})\n  if raw_annotations.get('model_config')and config_dict_from_namespace is None:\n   raise PydanticUserError(\n   '`model_config` cannot be used as a model field name. Use `model_config` for model configuration.',\n   code='model-config-invalid-field-name',\n   )\n   \n  if config_class_from_namespace and config_dict_from_namespace:\n   raise PydanticUserError('\"Config\" and \"model_config\" cannot be used together',code='config-both')\n   \n  config_from_namespace=config_dict_from_namespace or prepare_config(config_class_from_namespace)\n  \n  config_new.update(config_from_namespace)\n  \n  for k in list(kwargs.keys()):\n   if k in config_keys:\n    config_new[k]=kwargs.pop(k)\n    \n  return cls(config_new)\n  \n  \n if not TYPE_CHECKING:\n \n  def __getattr__(self,name:str)->Any:\n   try:\n    return self.config_dict[name]\n   except KeyError:\n    try:\n     return config_defaults[name]\n    except KeyError:\n     raise AttributeError(f'Config has no attribute {name !r}')from None\n     \n def core_config(self,title:str |None)->core_schema.CoreConfig:\n  ''\n\n\n\n\n\n\n\n\n  \n  config=self.config_dict\n  \n  if config.get('schema_generator')is not None:\n   warnings.warn(\n   'The `schema_generator` setting has been deprecated since v2.10. This setting no longer has any effect.',\n   PydanticDeprecatedSince210,\n   stacklevel=2,\n   )\n   \n  core_config_values={\n  'title':config.get('title')or title or None,\n  'extra_fields_behavior':config.get('extra'),\n  'allow_inf_nan':config.get('allow_inf_nan'),\n  'populate_by_name':config.get('populate_by_name'),\n  'str_strip_whitespace':config.get('str_strip_whitespace'),\n  'str_to_lower':config.get('str_to_lower'),\n  'str_to_upper':config.get('str_to_upper'),\n  'strict':config.get('strict'),\n  'ser_json_timedelta':config.get('ser_json_timedelta'),\n  'ser_json_bytes':config.get('ser_json_bytes'),\n  'val_json_bytes':config.get('val_json_bytes'),\n  'ser_json_inf_nan':config.get('ser_json_inf_nan'),\n  'from_attributes':config.get('from_attributes'),\n  'loc_by_alias':config.get('loc_by_alias'),\n  'revalidate_instances':config.get('revalidate_instances'),\n  'validate_default':config.get('validate_default'),\n  'str_max_length':config.get('str_max_length'),\n  'str_min_length':config.get('str_min_length'),\n  'hide_input_in_errors':config.get('hide_input_in_errors'),\n  'coerce_numbers_to_str':config.get('coerce_numbers_to_str'),\n  'regex_engine':config.get('regex_engine'),\n  'validation_error_cause':config.get('validation_error_cause'),\n  'cache_strings':config.get('cache_strings'),\n  }\n  \n  return core_schema.CoreConfig(**{k:v for k,v in core_config_values.items()if v is not None})\n  \n def __repr__(self):\n  c=', '.join(f'{k}={v !r}'for k,v in self.config_dict.items())\n  return f'ConfigWrapper({c})'\n  \n  \nclass ConfigWrapperStack:\n ''\n \n def __init__(self,config_wrapper:ConfigWrapper):\n  self._config_wrapper_stack:list[ConfigWrapper]=[config_wrapper]\n  \n @property\n def tail(self)->ConfigWrapper:\n  return self._config_wrapper_stack[-1]\n  \n @contextmanager\n def push(self,config_wrapper:ConfigWrapper |ConfigDict |None):\n  if config_wrapper is None:\n   yield\n   return\n   \n  if not isinstance(config_wrapper,ConfigWrapper):\n   config_wrapper=ConfigWrapper(config_wrapper,check=False)\n   \n  self._config_wrapper_stack.append(config_wrapper)\n  try:\n   yield\n  finally:\n   self._config_wrapper_stack.pop()\n   \n   \nconfig_defaults=ConfigDict(\ntitle=None,\nstr_to_lower=False,\nstr_to_upper=False,\nstr_strip_whitespace=False,\nstr_min_length=0,\nstr_max_length=None,\n\nextra=None,\nfrozen=False,\npopulate_by_name=False,\nuse_enum_values=False,\nvalidate_assignment=False,\narbitrary_types_allowed=False,\nfrom_attributes=False,\nloc_by_alias=True,\nalias_generator=None,\nmodel_title_generator=None,\nfield_title_generator=None,\nignored_types=(),\nallow_inf_nan=True,\njson_schema_extra=None,\nstrict=False,\nrevalidate_instances='never',\nser_json_timedelta='iso8601',\nser_json_bytes='utf8',\nval_json_bytes='utf8',\nser_json_inf_nan='null',\nvalidate_default=False,\nvalidate_return=False,\nprotected_namespaces=('model_validate','model_dump'),\nhide_input_in_errors=False,\njson_encoders=None,\ndefer_build=False,\nschema_generator=None,\nplugin_settings=None,\njson_schema_serialization_defaults_required=False,\njson_schema_mode_override=None,\ncoerce_numbers_to_str=False,\nregex_engine='rust-regex',\nvalidation_error_cause=False,\nuse_attribute_docstrings=False,\ncache_strings=True,\n)\n\n\ndef prepare_config(config:ConfigDict |dict[str,Any]|type[Any]|None)->ConfigDict:\n ''\n\n\n\n\n\n\n \n if config is None:\n  return ConfigDict()\n  \n if not isinstance(config,dict):\n  warnings.warn(DEPRECATION_MESSAGE,DeprecationWarning)\n  config={k:getattr(config,k)for k in dir(config)if not k.startswith('__')}\n  \n config_dict=cast(ConfigDict,config)\n check_deprecated(config_dict)\n return config_dict\n \n \nconfig_keys=set(ConfigDict.__annotations__.keys())\n\n\nV2_REMOVED_KEYS={\n'allow_mutation',\n'error_msg_templates',\n'fields',\n'getter_dict',\n'smart_union',\n'underscore_attrs_are_private',\n'json_loads',\n'json_dumps',\n'copy_on_model_validation',\n'post_init_call',\n}\nV2_RENAMED_KEYS={\n'allow_population_by_field_name':'populate_by_name',\n'anystr_lower':'str_to_lower',\n'anystr_strip_whitespace':'str_strip_whitespace',\n'anystr_upper':'str_to_upper',\n'keep_untouched':'ignored_types',\n'max_anystr_length':'str_max_length',\n'min_anystr_length':'str_min_length',\n'orm_mode':'from_attributes',\n'schema_extra':'json_schema_extra',\n'validate_all':'validate_default',\n}\n\n\ndef check_deprecated(config_dict:ConfigDict)->None:\n ''\n\n\n\n \n deprecated_removed_keys=V2_REMOVED_KEYS&config_dict.keys()\n deprecated_renamed_keys=V2_RENAMED_KEYS.keys()&config_dict.keys()\n if deprecated_removed_keys or deprecated_renamed_keys:\n  renamings={k:V2_RENAMED_KEYS[k]for k in sorted(deprecated_renamed_keys)}\n  renamed_bullets=[f'* {k !r} has been renamed to {v !r}'for k,v in renamings.items()]\n  removed_bullets=[f'* {k !r} has been removed'for k in sorted(deprecated_removed_keys)]\n  message='\\n'.join(['Valid config keys have changed in V2:']+renamed_bullets+removed_bullets)\n  warnings.warn(message,UserWarning)\n", ["__future__", "contextlib", "pydantic._internal._schema_generation_shared", "pydantic.aliases", "pydantic.config", "pydantic.errors", "pydantic.fields", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "re", "typing", "typing_extensions", "warnings"]], "pydantic._internal._fields": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport warnings\nfrom copy import copy\nfrom functools import lru_cache\nfrom inspect import Parameter,ismethoddescriptor,signature\nfrom typing import TYPE_CHECKING,Any,Callable,Pattern\n\nfrom pydantic_core import PydanticUndefined\nfrom typing_extensions import TypeIs\n\nfrom pydantic.errors import PydanticUserError\n\nfrom. import _typing_extra\nfrom._config import ConfigWrapper\nfrom._docs_extraction import extract_docstrings_from_cls\nfrom._import_utils import import_cached_base_model,import_cached_field_info\nfrom._namespace_utils import NsResolver\nfrom._repr import Representation\nfrom._utils import can_be_positional\n\nif TYPE_CHECKING:\n from annotated_types import BaseMetadata\n \n from..fields import FieldInfo\n from..main import BaseModel\n from._dataclasses import StandardDataclass\n from._decorators import DecoratorInfos\n \n \nclass PydanticMetadata(Representation):\n ''\n \n __slots__=()\n \n \ndef pydantic_general_metadata(**metadata:Any)->BaseMetadata:\n ''\n\n\n\n\n\n\n \n return _general_metadata_cls()(metadata)\n \n \n@lru_cache(maxsize=None)\ndef _general_metadata_cls()->type[BaseMetadata]:\n ''\n from annotated_types import BaseMetadata\n \n class _PydanticGeneralMetadata(PydanticMetadata,BaseMetadata):\n  ''\n  \n  def __init__(self,metadata:Any):\n   self.__dict__=metadata\n   \n return _PydanticGeneralMetadata\n \n \ndef _update_fields_from_docstrings(cls:type[Any],fields:dict[str,FieldInfo],config_wrapper:ConfigWrapper)->None:\n if config_wrapper.use_attribute_docstrings:\n  fields_docs=extract_docstrings_from_cls(cls)\n  for ann_name,field_info in fields.items():\n   if field_info.description is None and ann_name in fields_docs:\n    field_info.description=fields_docs[ann_name]\n    \n    \ndef collect_model_fields(\ncls:type[BaseModel],\nbases:tuple[type[Any],...],\nconfig_wrapper:ConfigWrapper,\nns_resolver:NsResolver |None,\n*,\ntypevars_map:dict[Any,Any]|None=None,\n)->tuple[dict[str,FieldInfo],set[str]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n BaseModel=import_cached_base_model()\n FieldInfo_=import_cached_field_info()\n \n parent_fields_lookup:dict[str,FieldInfo]={}\n for base in reversed(bases):\n  if model_fields :=getattr(base,'__pydantic_fields__',None):\n   parent_fields_lookup.update(model_fields)\n   \n type_hints=_typing_extra.get_model_type_hints(cls,ns_resolver=ns_resolver)\n \n \n \n annotations=cls.__dict__.get('__annotations__',{})\n fields:dict[str,FieldInfo]={}\n \n class_vars:set[str]=set()\n for ann_name,(ann_type,evaluated)in type_hints.items():\n  if ann_name =='model_config':\n  \n  \n  \n   continue\n   \n  for protected_namespace in config_wrapper.protected_namespaces:\n   ns_violation:bool=False\n   if isinstance(protected_namespace,Pattern):\n    ns_violation=protected_namespace.match(ann_name)is not None\n   elif isinstance(protected_namespace,str):\n    ns_violation=ann_name.startswith(protected_namespace)\n    \n   if ns_violation:\n    for b in bases:\n     if hasattr(b,ann_name):\n      if not(issubclass(b,BaseModel)and ann_name in getattr(b,'__pydantic_fields__',{})):\n       raise NameError(\n       f'Field \"{ann_name}\" conflicts with member {getattr(b,ann_name)}'\n       f' of protected namespace \"{protected_namespace}\".'\n       )\n    else:\n     valid_namespaces=()\n     for pn in config_wrapper.protected_namespaces:\n      if isinstance(pn,Pattern):\n       if not pn.match(ann_name):\n        valid_namespaces +=(f're.compile({pn.pattern})',)\n      else:\n       if not ann_name.startswith(pn):\n        valid_namespaces +=(pn,)\n        \n     warnings.warn(\n     f'Field \"{ann_name}\" in {cls.__name__} has conflict with protected namespace \"{protected_namespace}\".'\n     '\\n\\nYou may be able to resolve this warning by setting'\n     f\" `model_config['protected_namespaces'] = {valid_namespaces}`.\",\n     UserWarning,\n     )\n  if _typing_extra.is_classvar_annotation(ann_type):\n   class_vars.add(ann_name)\n   continue\n  if _is_finalvar_with_default_val(ann_type,getattr(cls,ann_name,PydanticUndefined)):\n   class_vars.add(ann_name)\n   continue\n  if not is_valid_field_name(ann_name):\n   continue\n  if cls.__pydantic_root_model__ and ann_name !='root':\n   raise NameError(\n   f\"Unexpected field with name {ann_name !r}; only 'root' is allowed as a field of a `RootModel`\"\n   )\n   \n   \n   \n  generic_origin=getattr(cls,'__pydantic_generic_metadata__',{}).get('origin')\n  for base in bases:\n   dataclass_fields={\n   field.name for field in(dataclasses.fields(base)if dataclasses.is_dataclass(base)else())\n   }\n   if hasattr(base,ann_name):\n    if base is generic_origin:\n    \n     continue\n     \n    if ann_name in dataclass_fields:\n    \n    \n     continue\n     \n    if ann_name not in annotations:\n    \n     continue\n     \n    warnings.warn(\n    f'Field name \"{ann_name}\" in \"{cls.__qualname__}\" shadows an attribute in parent '\n    f'\"{base.__qualname__}\"',\n    UserWarning,\n    )\n    \n  try:\n   default=getattr(cls,ann_name,PydanticUndefined)\n   if default is PydanticUndefined:\n    raise AttributeError\n  except AttributeError:\n   if ann_name in annotations:\n    field_info=FieldInfo_.from_annotation(ann_type)\n    field_info.evaluated=evaluated\n   else:\n   \n   \n    if ann_name in parent_fields_lookup:\n    \n    \n     field_info=copy(parent_fields_lookup[ann_name])\n    else:\n    \n    \n    \n     field_info=FieldInfo_.from_annotation(ann_type)\n     field_info.evaluated=evaluated\n  else:\n   _warn_on_nested_alias_in_annotation(ann_type,ann_name)\n   if isinstance(default,FieldInfo_)and ismethoddescriptor(default.default):\n   \n   \n   \n   \n    default.default=default.default.__get__(None,cls)\n    \n   field_info=FieldInfo_.from_annotated_attribute(ann_type,default)\n   field_info.evaluated=evaluated\n   \n   \n   \n   try:\n    delattr(cls,ann_name)\n   except AttributeError:\n    pass\n    \n    \n    \n  decorators:DecoratorInfos=cls.__dict__['__pydantic_decorators__']\n  if ann_name in decorators.computed_fields:\n   raise ValueError(\"you can't override a field with a computed field\")\n  fields[ann_name]=field_info\n  \n if typevars_map:\n  for field in fields.values():\n   field.apply_typevars_map(typevars_map)\n   \n _update_fields_from_docstrings(cls,fields,config_wrapper)\n return fields,class_vars\n \n \ndef _warn_on_nested_alias_in_annotation(ann_type:type[Any],ann_name:str)->None:\n FieldInfo=import_cached_field_info()\n \n args=getattr(ann_type,'__args__',None)\n if args:\n  for anno_arg in args:\n   if _typing_extra.is_annotated(anno_arg):\n    for anno_type_arg in _typing_extra.get_args(anno_arg):\n     if isinstance(anno_type_arg,FieldInfo)and anno_type_arg.alias is not None:\n      warnings.warn(\n      f'`alias` specification on field \"{ann_name}\" must be set on outermost annotation to take effect.',\n      UserWarning,\n      )\n      return\n      \n      \ndef _is_finalvar_with_default_val(type_:type[Any],val:Any)->bool:\n FieldInfo=import_cached_field_info()\n \n if not _typing_extra.is_finalvar(type_):\n  return False\n elif val is PydanticUndefined:\n  return False\n elif isinstance(val,FieldInfo)and(val.default is PydanticUndefined and val.default_factory is None):\n  return False\n else:\n  return True\n  \n  \ndef collect_dataclass_fields(\ncls:type[StandardDataclass],\n*,\nns_resolver:NsResolver |None=None,\ntypevars_map:dict[Any,Any]|None=None,\nconfig_wrapper:ConfigWrapper |None=None,\n)->dict[str,FieldInfo]:\n ''\n\n\n\n\n\n\n\n\n\n\n \n FieldInfo_=import_cached_field_info()\n \n fields:dict[str,FieldInfo]={}\n ns_resolver=ns_resolver or NsResolver()\n dataclass_fields=cls.__dataclass_fields__\n \n \n \n \n for base in reversed(cls.__mro__):\n  if not dataclasses.is_dataclass(base):\n   continue\n   \n  with ns_resolver.push(base):\n   for ann_name,dataclass_field in dataclass_fields.items():\n    if ann_name not in base.__dict__.get('__annotations__',{}):\n    \n    \n     continue\n     \n    globalns,localns=ns_resolver.types_namespace\n    ann_type,_=_typing_extra.try_eval_type(dataclass_field.type,globalns,localns)\n    \n    if _typing_extra.is_classvar_annotation(ann_type):\n     continue\n     \n    if(\n    not dataclass_field.init\n    and dataclass_field.default is dataclasses.MISSING\n    and dataclass_field.default_factory is dataclasses.MISSING\n    ):\n    \n    \n     continue\n     \n    if isinstance(dataclass_field.default,FieldInfo_):\n     if dataclass_field.default.init_var:\n      if dataclass_field.default.init is False:\n       raise PydanticUserError(\n       f'Dataclass field {ann_name} has init=False and init_var=True, but these are mutually exclusive.',\n       code='clashing-init-and-init-var',\n       )\n       \n       \n      continue\n     field_info=FieldInfo_.from_annotated_attribute(ann_type,dataclass_field.default)\n    else:\n     field_info=FieldInfo_.from_annotated_attribute(ann_type,dataclass_field)\n     \n    fields[ann_name]=field_info\n    \n    if field_info.default is not PydanticUndefined and isinstance(\n    getattr(cls,ann_name,field_info),FieldInfo_\n    ):\n    \n     setattr(cls,ann_name,field_info.default)\n     \n if typevars_map:\n  for field in fields.values():\n  \n  \n  \n   field.apply_typevars_map(typevars_map)\n   \n if config_wrapper is not None:\n  _update_fields_from_docstrings(cls,fields,config_wrapper)\n  \n return fields\n \n \ndef is_valid_field_name(name:str)->bool:\n return not name.startswith('_')\n \n \ndef is_valid_privateattr_name(name:str)->bool:\n return name.startswith('_')and not name.startswith('__')\n \n \ndef takes_validated_data_argument(\ndefault_factory:Callable[[],Any]|Callable[[dict[str,Any]],Any],\n)->TypeIs[Callable[[dict[str,Any]],Any]]:\n ''\n try:\n  sig=signature(default_factory)\n except(ValueError,TypeError):\n \n \n  return False\n  \n parameters=list(sig.parameters.values())\n \n return len(parameters)==1 and can_be_positional(parameters[0])and parameters[0].default is Parameter.empty\n", ["__future__", "annotated_types", "copy", "dataclasses", "functools", "inspect", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._dataclasses", "pydantic._internal._decorators", "pydantic._internal._docs_extraction", "pydantic._internal._import_utils", "pydantic._internal._namespace_utils", "pydantic._internal._repr", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic.errors", "pydantic.fields", "pydantic.main", "pydantic_core", "typing", "typing_extensions", "warnings"]], "pydantic._internal._known_annotated_metadata": [".py", "from __future__ import annotations\n\nfrom collections import defaultdict\nfrom copy import copy\nfrom functools import lru_cache,partial\nfrom typing import TYPE_CHECKING,Any,Iterable\n\nfrom pydantic_core import CoreSchema,PydanticCustomError,ValidationError,to_jsonable_python\nfrom pydantic_core import core_schema as cs\n\nfrom._fields import PydanticMetadata\nfrom._import_utils import import_cached_field_info\n\nif TYPE_CHECKING:\n pass\n \nSTRICT={'strict'}\nFAIL_FAST={'fail_fast'}\nLENGTH_CONSTRAINTS={'min_length','max_length'}\nINEQUALITY={'le','ge','lt','gt'}\nNUMERIC_CONSTRAINTS={'multiple_of',*INEQUALITY}\nALLOW_INF_NAN={'allow_inf_nan'}\n\nSTR_CONSTRAINTS={\n*LENGTH_CONSTRAINTS,\n*STRICT,\n'strip_whitespace',\n'to_lower',\n'to_upper',\n'pattern',\n'coerce_numbers_to_str',\n}\nBYTES_CONSTRAINTS={*LENGTH_CONSTRAINTS,*STRICT}\n\nLIST_CONSTRAINTS={*LENGTH_CONSTRAINTS,*STRICT,*FAIL_FAST}\nTUPLE_CONSTRAINTS={*LENGTH_CONSTRAINTS,*STRICT,*FAIL_FAST}\nSET_CONSTRAINTS={*LENGTH_CONSTRAINTS,*STRICT,*FAIL_FAST}\nDICT_CONSTRAINTS={*LENGTH_CONSTRAINTS,*STRICT}\nGENERATOR_CONSTRAINTS={*LENGTH_CONSTRAINTS,*STRICT}\nSEQUENCE_CONSTRAINTS={*LENGTH_CONSTRAINTS,*FAIL_FAST}\n\nFLOAT_CONSTRAINTS={*NUMERIC_CONSTRAINTS,*ALLOW_INF_NAN,*STRICT}\nDECIMAL_CONSTRAINTS={'max_digits','decimal_places',*FLOAT_CONSTRAINTS}\nINT_CONSTRAINTS={*NUMERIC_CONSTRAINTS,*ALLOW_INF_NAN,*STRICT}\nBOOL_CONSTRAINTS=STRICT\nUUID_CONSTRAINTS=STRICT\n\nDATE_TIME_CONSTRAINTS={*NUMERIC_CONSTRAINTS,*STRICT}\nTIMEDELTA_CONSTRAINTS={*NUMERIC_CONSTRAINTS,*STRICT}\nTIME_CONSTRAINTS={*NUMERIC_CONSTRAINTS,*STRICT}\nLAX_OR_STRICT_CONSTRAINTS=STRICT\nENUM_CONSTRAINTS=STRICT\nCOMPLEX_CONSTRAINTS=STRICT\n\nUNION_CONSTRAINTS={'union_mode'}\nURL_CONSTRAINTS={\n'max_length',\n'allowed_schemes',\n'host_required',\n'default_host',\n'default_port',\n'default_path',\n}\n\nTEXT_SCHEMA_TYPES=('str','bytes','url','multi-host-url')\nSEQUENCE_SCHEMA_TYPES=('list','tuple','set','frozenset','generator',*TEXT_SCHEMA_TYPES)\nNUMERIC_SCHEMA_TYPES=('float','int','date','time','timedelta','datetime')\n\nCONSTRAINTS_TO_ALLOWED_SCHEMAS:dict[str,set[str]]=defaultdict(set)\n\nconstraint_schema_pairings:list[tuple[set[str],tuple[str,...]]]=[\n(STR_CONSTRAINTS,TEXT_SCHEMA_TYPES),\n(BYTES_CONSTRAINTS,('bytes',)),\n(LIST_CONSTRAINTS,('list',)),\n(TUPLE_CONSTRAINTS,('tuple',)),\n(SET_CONSTRAINTS,('set','frozenset')),\n(DICT_CONSTRAINTS,('dict',)),\n(GENERATOR_CONSTRAINTS,('generator',)),\n(FLOAT_CONSTRAINTS,('float',)),\n(INT_CONSTRAINTS,('int',)),\n(DATE_TIME_CONSTRAINTS,('date','time','datetime','timedelta')),\n\n(STRICT,(*TEXT_SCHEMA_TYPES,*SEQUENCE_SCHEMA_TYPES,*NUMERIC_SCHEMA_TYPES,'typed-dict','model')),\n(UNION_CONSTRAINTS,('union',)),\n(URL_CONSTRAINTS,('url','multi-host-url')),\n(BOOL_CONSTRAINTS,('bool',)),\n(UUID_CONSTRAINTS,('uuid',)),\n(LAX_OR_STRICT_CONSTRAINTS,('lax-or-strict',)),\n(ENUM_CONSTRAINTS,('enum',)),\n(DECIMAL_CONSTRAINTS,('decimal',)),\n(COMPLEX_CONSTRAINTS,('complex',)),\n]\n\nfor constraints,schemas in constraint_schema_pairings:\n for c in constraints:\n  CONSTRAINTS_TO_ALLOWED_SCHEMAS[c].update(schemas)\n  \n  \ndef as_jsonable_value(v:Any)->Any:\n if type(v)not in(int,str,float,bytes,bool,type(None)):\n  return to_jsonable_python(v)\n return v\n \n \ndef expand_grouped_metadata(annotations:Iterable[Any])->Iterable[Any]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n import annotated_types as at\n \n FieldInfo=import_cached_field_info()\n \n for annotation in annotations:\n  if isinstance(annotation,at.GroupedMetadata):\n   yield from annotation\n  elif isinstance(annotation,FieldInfo):\n   yield from annotation.metadata\n   \n   \n   \n   \n   \n   annotation=copy(annotation)\n   annotation.metadata=[]\n   yield annotation\n  else:\n   yield annotation\n   \n   \n@lru_cache\ndef _get_at_to_constraint_map()->dict[type,str]:\n ''\n\n\n\n\n\n \n import annotated_types as at\n \n return{\n at.Gt:'gt',\n at.Ge:'ge',\n at.Lt:'lt',\n at.Le:'le',\n at.MultipleOf:'multiple_of',\n at.MinLen:'min_length',\n at.MaxLen:'max_length',\n }\n \n \ndef apply_known_metadata(annotation:Any,schema:CoreSchema)->CoreSchema |None:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n import annotated_types as at\n \n from._validators import NUMERIC_VALIDATOR_LOOKUP,forbid_inf_nan_check\n \n schema=schema.copy()\n schema_update,other_metadata=collect_known_metadata([annotation])\n schema_type=schema['type']\n \n chain_schema_constraints:set[str]={\n 'pattern',\n 'strip_whitespace',\n 'to_lower',\n 'to_upper',\n 'coerce_numbers_to_str',\n }\n chain_schema_steps:list[CoreSchema]=[]\n \n for constraint,value in schema_update.items():\n  if constraint not in CONSTRAINTS_TO_ALLOWED_SCHEMAS:\n   raise ValueError(f'Unknown constraint {constraint}')\n  allowed_schemas=CONSTRAINTS_TO_ALLOWED_SCHEMAS[constraint]\n  \n  \n  \n  \n  \n  if schema_type in{'function-before','function-wrap','function-after'}and constraint =='strict':\n   schema['schema']=apply_known_metadata(annotation,schema['schema'])\n   return schema\n   \n   \n  if schema_type in allowed_schemas:\n   if constraint =='union_mode'and schema_type =='union':\n    schema['mode']=value\n   else:\n    schema[constraint]=value\n   continue\n   \n   \n  if constraint in chain_schema_constraints:\n  \n   def _apply_constraint_with_incompatibility_info(\n   value:Any,handler:cs.ValidatorFunctionWrapHandler\n   )->Any:\n    try:\n     x=handler(value)\n    except ValidationError as ve:\n    \n    \n    \n    \n    \n     if 'type'in ve.errors()[0]['type']:\n      raise TypeError(\n      f\"Unable to apply constraint '{constraint}' to supplied value {value} for schema of type '{schema_type}'\"\n      )\n     raise ve\n    return x\n    \n   chain_schema_steps.append(\n   cs.no_info_wrap_validator_function(\n   _apply_constraint_with_incompatibility_info,cs.str_schema(**{constraint:value})\n   )\n   )\n  elif constraint in NUMERIC_VALIDATOR_LOOKUP:\n   if constraint in LENGTH_CONSTRAINTS:\n    inner_schema=schema\n    while inner_schema['type']in{'function-before','function-wrap','function-after'}:\n     inner_schema=inner_schema['schema']\n    inner_schema_type=inner_schema['type']\n    if inner_schema_type =='list'or(\n    inner_schema_type =='json-or-python'and inner_schema['json_schema']['type']=='list'\n    ):\n     js_constraint_key='minItems'if constraint =='min_length'else 'maxItems'\n    else:\n     js_constraint_key='minLength'if constraint =='min_length'else 'maxLength'\n   else:\n    js_constraint_key=constraint\n    \n   schema=cs.no_info_after_validator_function(\n   partial(NUMERIC_VALIDATOR_LOOKUP[constraint],**{constraint:value}),schema\n   )\n   metadata=schema.get('metadata',{})\n   if(existing_json_schema_updates :=metadata.get('pydantic_js_updates'))is not None:\n    metadata['pydantic_js_updates']={\n    **existing_json_schema_updates,\n    **{js_constraint_key:as_jsonable_value(value)},\n    }\n   else:\n    metadata['pydantic_js_updates']={js_constraint_key:as_jsonable_value(value)}\n   schema['metadata']=metadata\n  elif constraint =='allow_inf_nan'and value is False:\n   schema=cs.no_info_after_validator_function(\n   forbid_inf_nan_check,\n   schema,\n   )\n  else:\n  \n  \n   raise RuntimeError(f\"Unable to apply constraint '{constraint}' to schema of type '{schema_type}'\")\n   \n for annotation in other_metadata:\n  if(annotation_type :=type(annotation))in(at_to_constraint_map :=_get_at_to_constraint_map()):\n   constraint=at_to_constraint_map[annotation_type]\n   validator=NUMERIC_VALIDATOR_LOOKUP.get(constraint)\n   if validator is None:\n    raise ValueError(f'Unknown constraint {constraint}')\n   schema=cs.no_info_after_validator_function(\n   partial(validator,{constraint:getattr(annotation,constraint)}),schema\n   )\n   continue\n  elif isinstance(annotation,(at.Predicate,at.Not)):\n   predicate_name=f'{annotation.func.__qualname__}'if hasattr(annotation.func,'__qualname__')else ''\n   \n   def val_func(v:Any)->Any:\n    predicate_satisfied=annotation.func(v)\n    \n    \n    if isinstance(annotation,at.Predicate):\n     if not predicate_satisfied:\n      raise PydanticCustomError(\n      'predicate_failed',\n      f'Predicate {predicate_name} failed',\n      )\n    else:\n     if predicate_satisfied:\n      raise PydanticCustomError(\n      'not_operation_failed',\n      f'Not of {predicate_name} failed',\n      )\n      \n    return v\n    \n   schema=cs.no_info_after_validator_function(val_func,schema)\n  else:\n  \n   return None\n   \n if chain_schema_steps:\n  chain_schema_steps=[schema]+chain_schema_steps\n  return cs.chain_schema(chain_schema_steps)\n  \n return schema\n \n \ndef collect_known_metadata(annotations:Iterable[Any])->tuple[dict[str,Any],list[Any]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n annotations=expand_grouped_metadata(annotations)\n \n res:dict[str,Any]={}\n remaining:list[Any]=[]\n \n for annotation in annotations:\n \n  if isinstance(annotation,PydanticMetadata):\n   res.update(annotation.__dict__)\n   \n  elif(annotation_type :=type(annotation))in(at_to_constraint_map :=_get_at_to_constraint_map()):\n   constraint=at_to_constraint_map[annotation_type]\n   res[constraint]=getattr(annotation,constraint)\n  elif isinstance(annotation,type)and issubclass(annotation,PydanticMetadata):\n  \n  \n   res.update({k:v for k,v in vars(annotation).items()if not k.startswith('_')})\n  else:\n   remaining.append(annotation)\n   \n   \n   \n res={k:v for k,v in res.items()if v is not None}\n return res,remaining\n \n \ndef check_metadata(metadata:dict[str,Any],allowed:Iterable[str],source_type:Any)->None:\n ''\n\n\n\n\n\n\n\n\n\n \n unknown=metadata.keys()-set(allowed)\n if unknown:\n  raise TypeError(\n  f'The following constraints cannot be applied to {source_type !r}: {\", \".join([f\"{k !r}\"for k in unknown])}'\n  )\n", ["__future__", "annotated_types", "collections", "copy", "functools", "pydantic._internal._fields", "pydantic._internal._import_utils", "pydantic._internal._validators", "pydantic_core", "pydantic_core.core_schema", "typing"]], "pydantic._internal._dataclasses": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport dataclasses\nimport typing\nimport warnings\nfrom functools import partial,wraps\nfrom typing import Any,ClassVar\n\nfrom pydantic_core import(\nArgsKwargs,\nSchemaSerializer,\nSchemaValidator,\ncore_schema,\n)\nfrom typing_extensions import TypeGuard\n\nfrom..errors import PydanticUndefinedAnnotation\nfrom..plugin._schema_validator import PluggableSchemaValidator,create_schema_validator\nfrom..warnings import PydanticDeprecatedSince20\nfrom. import _config,_decorators\nfrom._fields import collect_dataclass_fields\nfrom._generate_schema import GenerateSchema\nfrom._generics import get_standard_typevars_map\nfrom._mock_val_ser import set_dataclass_mocks\nfrom._namespace_utils import NsResolver\nfrom._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom._signature import generate_pydantic_signature\nfrom._utils import LazyClassAttribute\n\nif typing.TYPE_CHECKING:\n from _typeshed import DataclassInstance as StandardDataclass\n \n from..config import ConfigDict\n from..fields import FieldInfo\n \n class PydanticDataclass(StandardDataclass,typing.Protocol):\n  ''\n\n\n\n\n\n\n\n\n\n  \n  \n  __pydantic_config__:ClassVar[ConfigDict]\n  __pydantic_complete__:ClassVar[bool]\n  __pydantic_core_schema__:ClassVar[core_schema.CoreSchema]\n  __pydantic_decorators__:ClassVar[_decorators.DecoratorInfos]\n  __pydantic_fields__:ClassVar[dict[str,FieldInfo]]\n  __pydantic_serializer__:ClassVar[SchemaSerializer]\n  __pydantic_validator__:ClassVar[SchemaValidator |PluggableSchemaValidator]\n  \nelse:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n \ndef set_dataclass_fields(\ncls:type[StandardDataclass],\nns_resolver:NsResolver |None=None,\nconfig_wrapper:_config.ConfigWrapper |None=None,\n)->None:\n ''\n\n\n\n\n\n \n typevars_map=get_standard_typevars_map(cls)\n fields=collect_dataclass_fields(\n cls,ns_resolver=ns_resolver,typevars_map=typevars_map,config_wrapper=config_wrapper\n )\n \n cls.__pydantic_fields__=fields\n \n \ndef complete_dataclass(\ncls:type[Any],\nconfig_wrapper:_config.ConfigWrapper,\n*,\nraise_errors:bool=True,\nns_resolver:NsResolver |None=None,\n_force_build:bool=False,\n)->bool:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n original_init=cls.__init__\n \n \n \n def __init__(__dataclass_self__:PydanticDataclass,*args:Any,**kwargs:Any)->None:\n  __tracebackhide__=True\n  s=__dataclass_self__\n  s.__pydantic_validator__.validate_python(ArgsKwargs(args,kwargs),self_instance=s)\n  \n __init__.__qualname__=f'{cls.__qualname__}.__init__'\n \n cls.__init__=__init__\n cls.__pydantic_config__=config_wrapper.config_dict\n \n set_dataclass_fields(cls,ns_resolver,config_wrapper=config_wrapper)\n \n if not _force_build and config_wrapper.defer_build:\n  set_dataclass_mocks(cls,cls.__name__)\n  return False\n  \n if hasattr(cls,'__post_init_post_parse__'):\n  warnings.warn(\n  'Support for `__post_init_post_parse__` has been dropped, the method will not be called',DeprecationWarning\n  )\n  \n typevars_map=get_standard_typevars_map(cls)\n gen_schema=GenerateSchema(\n config_wrapper,\n ns_resolver=ns_resolver,\n typevars_map=typevars_map,\n )\n \n \n \n \n cls.__signature__=LazyClassAttribute(\n '__signature__',\n partial(\n generate_pydantic_signature,\n \n \n init=original_init,\n fields=cls.__pydantic_fields__,\n populate_by_name=config_wrapper.populate_by_name,\n extra=config_wrapper.extra,\n is_dataclass=True,\n ),\n )\n get_core_schema=getattr(cls,'__get_pydantic_core_schema__',None)\n try:\n  if get_core_schema:\n   schema=get_core_schema(\n   cls,\n   CallbackGetCoreSchemaHandler(\n   partial(gen_schema.generate_schema,from_dunder_get_core_schema=False),\n   gen_schema,\n   ref_mode='unpack',\n   ),\n   )\n  else:\n   schema=gen_schema.generate_schema(cls,from_dunder_get_core_schema=False)\n except PydanticUndefinedAnnotation as e:\n  if raise_errors:\n   raise\n  set_dataclass_mocks(cls,cls.__name__,f'`{e.name}`')\n  return False\n  \n core_config=config_wrapper.core_config(title=cls.__name__)\n \n try:\n  schema=gen_schema.clean_schema(schema)\n except gen_schema.CollectedInvalid:\n  set_dataclass_mocks(cls,cls.__name__,'all referenced types')\n  return False\n  \n  \n  \n cls=typing.cast('type[PydanticDataclass]',cls)\n \n \n cls.__pydantic_core_schema__=schema\n cls.__pydantic_validator__=validator=create_schema_validator(\n schema,cls,cls.__module__,cls.__qualname__,'dataclass',core_config,config_wrapper.plugin_settings\n )\n cls.__pydantic_serializer__=SchemaSerializer(schema,core_config)\n \n if config_wrapper.validate_assignment:\n \n  @wraps(cls.__setattr__)\n  def validated_setattr(instance:Any,field:str,value:str,/)->None:\n   validator.validate_assignment(instance,field,value)\n   \n  cls.__setattr__=validated_setattr.__get__(None,cls)\n  \n cls.__pydantic_complete__=True\n return True\n \n \ndef is_builtin_dataclass(_cls:type[Any])->TypeGuard[type[StandardDataclass]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return(\n dataclasses.is_dataclass(_cls)\n and not hasattr(_cls,'__pydantic_validator__')\n and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls,'__annotations__',{})))\n )\n", ["__future__", "_typeshed", "dataclasses", "functools", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._decorators", "pydantic._internal._fields", "pydantic._internal._generate_schema", "pydantic._internal._generics", "pydantic._internal._mock_val_ser", "pydantic._internal._namespace_utils", "pydantic._internal._schema_generation_shared", "pydantic._internal._signature", "pydantic._internal._utils", "pydantic.config", "pydantic.errors", "pydantic.fields", "pydantic.plugin._schema_validator", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "typing", "typing_extensions", "warnings"]], "pydantic._internal._std_types_schema": [".py", "''\n\n\n\n\n\n\n\nfrom __future__ import annotations as _annotations\n\nimport collections\nimport collections.abc\nimport dataclasses\nimport os\nimport typing\nfrom functools import partial\nfrom typing import Any,Callable,Iterable,Tuple,TypeVar,cast\n\nimport typing_extensions\nfrom pydantic_core import(\nCoreSchema,\nPydanticCustomError,\ncore_schema,\n)\nfrom typing_extensions import get_args,get_origin\n\nfrom pydantic._internal._serializers import serialize_sequence_via_list\nfrom pydantic.errors import PydanticSchemaGenerationError\nfrom pydantic.types import Strict\n\nfrom..json_schema import JsonSchemaValue\nfrom. import _known_annotated_metadata,_typing_extra\nfrom._import_utils import import_cached_field_info\nfrom._internal_dataclass import slots_true\nfrom._schema_generation_shared import GetCoreSchemaHandler,GetJsonSchemaHandler\n\nFieldInfo=import_cached_field_info()\n\nif typing.TYPE_CHECKING:\n from._generate_schema import GenerateSchema\n \n StdSchemaFunction=Callable[[GenerateSchema,type[Any]],core_schema.CoreSchema]\n \n \n@dataclasses.dataclass(**slots_true)\nclass InnerSchemaValidator:\n ''\n \n core_schema:CoreSchema\n js_schema:JsonSchemaValue |None=None\n js_core_schema:CoreSchema |None=None\n js_schema_update:JsonSchemaValue |None=None\n \n def __get_pydantic_json_schema__(self,_schema:CoreSchema,handler:GetJsonSchemaHandler)->JsonSchemaValue:\n  if self.js_schema is not None:\n   return self.js_schema\n  js_schema=handler(self.js_core_schema or self.core_schema)\n  if self.js_schema_update is not None:\n   js_schema.update(self.js_schema_update)\n  return js_schema\n  \n def __get_pydantic_core_schema__(self,_source_type:Any,_handler:GetCoreSchemaHandler)->CoreSchema:\n  return self.core_schema\n  \n  \ndef path_schema_prepare_pydantic_annotations(\nsource_type:Any,annotations:Iterable[Any]\n)->tuple[Any,list[Any]]|None:\n import pathlib\n \n orig_source_type:Any=get_origin(source_type)or source_type\n if(\n (source_type_args :=get_args(source_type))\n and orig_source_type is os.PathLike\n and source_type_args[0]not in{str,bytes,Any}\n ):\n  return None\n  \n if orig_source_type not in{\n os.PathLike,\n pathlib.Path,\n pathlib.PurePath,\n pathlib.PosixPath,\n pathlib.PurePosixPath,\n pathlib.PureWindowsPath,\n }:\n  return None\n  \n metadata,remaining_annotations=_known_annotated_metadata.collect_known_metadata(annotations)\n _known_annotated_metadata.check_metadata(metadata,_known_annotated_metadata.STR_CONSTRAINTS,orig_source_type)\n \n is_first_arg_byte=source_type_args and source_type_args[0]is bytes\n construct_path=pathlib.PurePath if orig_source_type is os.PathLike else orig_source_type\n constrained_schema=(\n core_schema.bytes_schema(**metadata)if is_first_arg_byte else core_schema.str_schema(**metadata)\n )\n \n def path_validator(input_value:str |bytes)->os.PathLike[Any]:\n  try:\n   if is_first_arg_byte:\n    if isinstance(input_value,bytes):\n     try:\n      input_value=input_value.decode()\n     except UnicodeDecodeError as e:\n      raise PydanticCustomError('bytes_type','Input must be valid bytes')from e\n    else:\n     raise PydanticCustomError('bytes_type','Input must be bytes')\n   elif not isinstance(input_value,str):\n    raise PydanticCustomError('path_type','Input is not a valid path')\n    \n   return construct_path(input_value)\n  except TypeError as e:\n   raise PydanticCustomError('path_type','Input is not a valid path')from e\n   \n instance_schema=core_schema.json_or_python_schema(\n json_schema=core_schema.no_info_after_validator_function(path_validator,constrained_schema),\n python_schema=core_schema.is_instance_schema(orig_source_type),\n )\n \n strict:bool |None=None\n for annotation in annotations:\n  if isinstance(annotation,Strict):\n   strict=annotation.strict\n   \n schema=core_schema.lax_or_strict_schema(\n lax_schema=core_schema.union_schema(\n [\n instance_schema,\n core_schema.no_info_after_validator_function(path_validator,constrained_schema),\n ],\n custom_error_type='path_type',\n custom_error_message=f'Input is not a valid path for {orig_source_type}',\n strict=True,\n ),\n strict_schema=instance_schema,\n serialization=core_schema.to_string_ser_schema(),\n strict=strict,\n )\n \n return(\n orig_source_type,\n [\n InnerSchemaValidator(schema,js_core_schema=constrained_schema,js_schema_update={'format':'path'}),\n *remaining_annotations,\n ],\n )\n \n \ndef deque_validator(\ninput_value:Any,handler:core_schema.ValidatorFunctionWrapHandler,maxlen:None |int\n)->collections.deque[Any]:\n if isinstance(input_value,collections.deque):\n  maxlens=[v for v in(input_value.maxlen,maxlen)if v is not None]\n  if maxlens:\n   maxlen=min(maxlens)\n  return collections.deque(handler(input_value),maxlen=maxlen)\n else:\n  return collections.deque(handler(input_value),maxlen=maxlen)\n  \n  \n@dataclasses.dataclass(**slots_true)\nclass DequeValidator:\n item_source_type:type[Any]\n metadata:dict[str,Any]\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->CoreSchema:\n  if _typing_extra.is_any(self.item_source_type):\n   items_schema=None\n  else:\n   items_schema=handler.generate_schema(self.item_source_type)\n   \n   \n   \n   \n  coerce_instance_wrap=partial(\n  core_schema.no_info_wrap_validator_function,\n  partial(deque_validator,maxlen=self.metadata.get('max_length',None)),\n  )\n  \n  \n  \n  metadata_with_strict_override={**self.metadata,'strict':False}\n  constrained_schema=core_schema.list_schema(items_schema,**metadata_with_strict_override)\n  \n  check_instance=core_schema.json_or_python_schema(\n  json_schema=core_schema.list_schema(),\n  python_schema=core_schema.is_instance_schema(collections.deque),\n  )\n  \n  serialization=core_schema.wrap_serializer_function_ser_schema(\n  serialize_sequence_via_list,schema=items_schema or core_schema.any_schema(),info_arg=True\n  )\n  \n  strict=core_schema.chain_schema([check_instance,coerce_instance_wrap(constrained_schema)])\n  \n  if self.metadata.get('strict',False):\n   schema=strict\n  else:\n   lax=coerce_instance_wrap(constrained_schema)\n   schema=core_schema.lax_or_strict_schema(lax_schema=lax,strict_schema=strict)\n  schema['serialization']=serialization\n  \n  return schema\n  \n  \ndef deque_schema_prepare_pydantic_annotations(\nsource_type:Any,annotations:Iterable[Any]\n)->tuple[Any,list[Any]]|None:\n args=get_args(source_type)\n \n if not args:\n  args=typing.cast(Tuple[Any],(Any,))\n elif len(args)!=1:\n  raise ValueError('Expected deque to have exactly 1 generic parameter')\n  \n item_source_type=args[0]\n \n metadata,remaining_annotations=_known_annotated_metadata.collect_known_metadata(annotations)\n _known_annotated_metadata.check_metadata(metadata,_known_annotated_metadata.SEQUENCE_CONSTRAINTS,source_type)\n \n return(source_type,[DequeValidator(item_source_type,metadata),*remaining_annotations])\n \n \nMAPPING_ORIGIN_MAP:dict[Any,Any]={\ntyping.DefaultDict:collections.defaultdict,\ncollections.defaultdict:collections.defaultdict,\ncollections.OrderedDict:collections.OrderedDict,\ntyping_extensions.OrderedDict:collections.OrderedDict,\ndict:dict,\ntyping.Dict:dict,\ncollections.Counter:collections.Counter,\ntyping.Counter:collections.Counter,\n\ntyping.Mapping:dict,\ntyping.MutableMapping:dict,\n\ncollections.abc.MutableMapping:dict,\ncollections.abc.Mapping:dict,\n}\n\n\ndef defaultdict_validator(\ninput_value:Any,handler:core_schema.ValidatorFunctionWrapHandler,default_default_factory:Callable[[],Any]\n)->collections.defaultdict[Any,Any]:\n if isinstance(input_value,collections.defaultdict):\n  default_factory=input_value.default_factory\n  return collections.defaultdict(default_factory,handler(input_value))\n else:\n  return collections.defaultdict(default_default_factory,handler(input_value))\n  \n  \ndef get_defaultdict_default_default_factory(values_source_type:Any)->Callable[[],Any]:\n def infer_default()->Callable[[],Any]:\n  allowed_default_types:dict[Any,Any]={\n  typing.Tuple:tuple,\n  tuple:tuple,\n  collections.abc.Sequence:tuple,\n  collections.abc.MutableSequence:list,\n  typing.List:list,\n  list:list,\n  typing.Sequence:list,\n  typing.Set:set,\n  set:set,\n  typing.MutableSet:set,\n  collections.abc.MutableSet:set,\n  collections.abc.Set:frozenset,\n  typing.MutableMapping:dict,\n  typing.Mapping:dict,\n  collections.abc.Mapping:dict,\n  collections.abc.MutableMapping:dict,\n  float:float,\n  int:int,\n  str:str,\n  bool:bool,\n  }\n  values_type_origin=get_origin(values_source_type)or values_source_type\n  instructions='set using `DefaultDict[..., Annotated[..., Field(default_factory=...)]]`'\n  if isinstance(values_type_origin,TypeVar):\n  \n   def type_var_default_factory()->None:\n    raise RuntimeError(\n    'Generic defaultdict cannot be used without a concrete value type or an'\n    ' explicit default factory, '+instructions\n    )\n    \n   return type_var_default_factory\n  elif values_type_origin not in allowed_default_types:\n  \n   allowed_msg=', '.join([t.__name__ for t in set(allowed_default_types.values())])\n   raise PydanticSchemaGenerationError(\n   f'Unable to infer a default factory for keys of type {values_source_type}.'\n   f' Only {allowed_msg} are supported, other types require an explicit default factory'\n   ' '+instructions\n   )\n  return allowed_default_types[values_type_origin]\n  \n  \n if _typing_extra.is_annotated(values_source_type):\n  field_info=next((v for v in get_args(values_source_type)if isinstance(v,FieldInfo)),None)\n else:\n  field_info=None\n if field_info and field_info.default_factory:\n \n  default_default_factory=cast(Callable[[],Any],field_info.default_factory)\n else:\n  default_default_factory=infer_default()\n return default_default_factory\n \n \n@dataclasses.dataclass(**slots_true)\nclass MappingValidator:\n mapped_origin:type[Any]\n keys_source_type:type[Any]\n values_source_type:type[Any]\n min_length:int |None=None\n max_length:int |None=None\n strict:bool=False\n \n def serialize_mapping_via_dict(self,v:Any,handler:core_schema.SerializerFunctionWrapHandler)->Any:\n  return handler(v)\n  \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->CoreSchema:\n  if _typing_extra.is_any(self.keys_source_type):\n   keys_schema=None\n  else:\n   keys_schema=handler.generate_schema(self.keys_source_type)\n  if _typing_extra.is_any(self.values_source_type):\n   values_schema=None\n  else:\n   values_schema=handler.generate_schema(self.values_source_type)\n   \n  metadata={'min_length':self.min_length,'max_length':self.max_length,'strict':self.strict}\n  \n  if self.mapped_origin is dict:\n   schema=core_schema.dict_schema(keys_schema,values_schema,**metadata)\n  else:\n   constrained_schema=core_schema.dict_schema(keys_schema,values_schema,**metadata)\n   check_instance=core_schema.json_or_python_schema(\n   json_schema=core_schema.dict_schema(),\n   python_schema=core_schema.is_instance_schema(self.mapped_origin),\n   )\n   \n   if self.mapped_origin is collections.defaultdict:\n    default_default_factory=get_defaultdict_default_default_factory(self.values_source_type)\n    coerce_instance_wrap=partial(\n    core_schema.no_info_wrap_validator_function,\n    partial(defaultdict_validator,default_default_factory=default_default_factory),\n    )\n   else:\n    coerce_instance_wrap=partial(core_schema.no_info_after_validator_function,self.mapped_origin)\n    \n   serialization=core_schema.wrap_serializer_function_ser_schema(\n   self.serialize_mapping_via_dict,\n   schema=core_schema.dict_schema(\n   keys_schema or core_schema.any_schema(),values_schema or core_schema.any_schema()\n   ),\n   info_arg=False,\n   )\n   \n   strict=core_schema.chain_schema([check_instance,coerce_instance_wrap(constrained_schema)])\n   \n   if metadata.get('strict',False):\n    schema=strict\n   else:\n    lax=coerce_instance_wrap(constrained_schema)\n    schema=core_schema.lax_or_strict_schema(lax_schema=lax,strict_schema=strict)\n    schema['serialization']=serialization\n    \n  return schema\n  \n  \ndef mapping_like_prepare_pydantic_annotations(\nsource_type:Any,annotations:Iterable[Any]\n)->tuple[Any,list[Any]]|None:\n origin:Any=get_origin(source_type)\n \n mapped_origin=MAPPING_ORIGIN_MAP.get(origin,None)if origin else MAPPING_ORIGIN_MAP.get(source_type,None)\n if mapped_origin is None:\n  return None\n  \n args=get_args(source_type)\n \n if not args:\n  args=typing.cast(Tuple[Any,Any],(Any,Any))\n elif mapped_origin is collections.Counter:\n \n  if len(args)!=1:\n   raise ValueError('Expected Counter to have exactly 1 generic parameter')\n  args=(args[0],int)\n elif len(args)!=2:\n  raise ValueError('Expected mapping to have exactly 2 generic parameters')\n  \n keys_source_type,values_source_type=args\n \n metadata,remaining_annotations=_known_annotated_metadata.collect_known_metadata(annotations)\n _known_annotated_metadata.check_metadata(metadata,_known_annotated_metadata.SEQUENCE_CONSTRAINTS,source_type)\n \n return(\n source_type,\n [\n MappingValidator(mapped_origin,keys_source_type,values_source_type,**metadata),\n *remaining_annotations,\n ],\n )\n", ["__future__", "collections", "collections.abc", "dataclasses", "functools", "os", "pathlib", "pydantic._internal", "pydantic._internal._generate_schema", "pydantic._internal._import_utils", "pydantic._internal._internal_dataclass", "pydantic._internal._known_annotated_metadata", "pydantic._internal._schema_generation_shared", "pydantic._internal._serializers", "pydantic._internal._typing_extra", "pydantic.errors", "pydantic.json_schema", "pydantic.types", "pydantic_core", "pydantic_core.core_schema", "typing", "typing_extensions"]], "pydantic._internal._model_construction": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport builtins\nimport operator\nimport sys\nimport typing\nimport warnings\nimport weakref\nfrom abc import ABCMeta\nfrom functools import lru_cache,partial\nfrom types import FunctionType\nfrom typing import Any,Callable,Generic,Literal,NoReturn,TypeVar,cast\n\nfrom pydantic_core import PydanticUndefined,SchemaSerializer\nfrom typing_extensions import TypeAliasType,dataclass_transform,deprecated,get_args\n\nfrom..errors import PydanticUndefinedAnnotation,PydanticUserError\nfrom..plugin._schema_validator import create_schema_validator\nfrom..warnings import GenericBeforeBaseModelWarning,PydanticDeprecatedSince20\nfrom._config import ConfigWrapper\nfrom._decorators import DecoratorInfos,PydanticDescriptorProxy,get_attribute_from_bases,unwrap_wrapped_function\nfrom._fields import collect_model_fields,is_valid_field_name,is_valid_privateattr_name\nfrom._generate_schema import GenerateSchema\nfrom._generics import PydanticGenericMetadata,get_model_typevars_map\nfrom._import_utils import import_cached_base_model,import_cached_field_info\nfrom._mock_val_ser import set_model_mocks\nfrom._namespace_utils import NsResolver\nfrom._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom._signature import generate_pydantic_signature\nfrom._typing_extra import(\n_make_forward_ref,\neval_type_backport,\nis_annotated,\nis_classvar_annotation,\nparent_frame_namespace,\n)\nfrom._utils import LazyClassAttribute,SafeGetItemProxy\n\nif typing.TYPE_CHECKING:\n from..fields import ComputedFieldInfo,FieldInfo,ModelPrivateAttr\n from..fields import Field as PydanticModelField\n from..fields import PrivateAttr as PydanticModelPrivateAttr\n from..main import BaseModel\nelse:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n PydanticModelField=object()\n PydanticModelPrivateAttr=object()\n \nobject_setattr=object.__setattr__\n\n\nclass _ModelNamespaceDict(dict):\n ''\n\n \n \n def __setitem__(self,k:str,v:object)->None:\n  existing:Any=self.get(k,None)\n  if existing and v is not existing and isinstance(existing,PydanticDescriptorProxy):\n   warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n   \n  return super().__setitem__(k,v)\n  \n  \ndef NoInitField(\n*,\ninit:Literal[False]=False,\n)->Any:\n ''\n\n\n \n \n \n@dataclass_transform(kw_only_default=True,field_specifiers=(PydanticModelField,PydanticModelPrivateAttr,NoInitField))\nclass ModelMetaclass(ABCMeta):\n def __new__(\n mcs,\n cls_name:str,\n bases:tuple[type[Any],...],\n namespace:dict[str,Any],\n __pydantic_generic_metadata__:PydanticGenericMetadata |None=None,\n __pydantic_reset_parent_namespace__:bool=True,\n _create_model_module:str |None=None,\n **kwargs:Any,\n )->type:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  \n  \n  if bases:\n   base_field_names,class_vars,base_private_attributes=mcs._collect_bases_data(bases)\n   \n   config_wrapper=ConfigWrapper.for_model(bases,namespace,kwargs)\n   namespace['model_config']=config_wrapper.config_dict\n   private_attributes=inspect_namespace(\n   namespace,config_wrapper.ignored_types,class_vars,base_field_names\n   )\n   if private_attributes or base_private_attributes:\n    original_model_post_init=get_model_post_init(namespace,bases)\n    if original_model_post_init is not None:\n    \n    \n     def wrapped_model_post_init(self:BaseModel,context:Any,/)->None:\n      ''\n\n      \n      init_private_attributes(self,context)\n      original_model_post_init(self,context)\n      \n     namespace['model_post_init']=wrapped_model_post_init\n    else:\n     namespace['model_post_init']=init_private_attributes\n     \n   namespace['__class_vars__']=class_vars\n   namespace['__private_attributes__']={**base_private_attributes,**private_attributes}\n   if __pydantic_generic_metadata__:\n    namespace['__pydantic_generic_metadata__']=__pydantic_generic_metadata__\n    \n   cls=cast('type[BaseModel]',super().__new__(mcs,cls_name,bases,namespace,**kwargs))\n   BaseModel_=import_cached_base_model()\n   \n   mro=cls.__mro__\n   if Generic in mro and mro.index(Generic)<mro.index(BaseModel_):\n    warnings.warn(\n    GenericBeforeBaseModelWarning(\n    'Classes should inherit from `BaseModel` before generic classes (e.g. `typing.Generic[T]`) '\n    'for pydantic generics to work properly.'\n    ),\n    stacklevel=2,\n    )\n    \n   cls.__pydantic_custom_init__=not getattr(cls.__init__,'__pydantic_base_init__',False)\n   cls.__pydantic_post_init__=(\n   None if cls.model_post_init is BaseModel_.model_post_init else 'model_post_init'\n   )\n   \n   cls.__pydantic_decorators__=DecoratorInfos.build(cls)\n   \n   \n   if __pydantic_generic_metadata__:\n    cls.__pydantic_generic_metadata__=__pydantic_generic_metadata__\n   else:\n    parent_parameters=getattr(cls,'__pydantic_generic_metadata__',{}).get('parameters',())\n    parameters=getattr(cls,'__parameters__',None)or parent_parameters\n    if parameters and parent_parameters and not all(x in parameters for x in parent_parameters):\n     from..root_model import RootModelRootType\n     \n     missing_parameters=tuple(x for x in parameters if x not in parent_parameters)\n     if RootModelRootType in parent_parameters and RootModelRootType not in parameters:\n     \n     \n     \n     \n     \n     \n     \n      parameters_str=', '.join([x.__name__ for x in missing_parameters])\n      error_message=(\n      f'{cls.__name__} is a subclass of `RootModel`, but does not include the generic type identifier(s) '\n      f'{parameters_str} in its parameters. '\n      f'You should parametrize RootModel directly, e.g., `class {cls.__name__}(RootModel[{parameters_str}]): ...`.'\n      )\n     else:\n      combined_parameters=parent_parameters+missing_parameters\n      parameters_str=', '.join([str(x)for x in combined_parameters])\n      generic_type_label=f'typing.Generic[{parameters_str}]'\n      error_message=(\n      f'All parameters must be present on typing.Generic;'\n      f' you should inherit from {generic_type_label}.'\n      )\n      if Generic not in bases:\n      \n      \n      \n      \n       bases_str=', '.join([x.__name__ for x in bases]+[generic_type_label])\n       error_message +=(\n       f' Note: `typing.Generic` must go last: `class {cls.__name__}({bases_str}): ...`)'\n       )\n     raise TypeError(error_message)\n     \n    cls.__pydantic_generic_metadata__={\n    'origin':None,\n    'args':(),\n    'parameters':parameters,\n    }\n    \n   cls.__pydantic_complete__=False\n   \n   \n   \n   for name,obj in private_attributes.items():\n    obj.__set_name__(cls,name)\n    \n   if __pydantic_reset_parent_namespace__:\n    cls.__pydantic_parent_namespace__=build_lenient_weakvaluedict(parent_frame_namespace())\n   parent_namespace:dict[str,Any]|None=getattr(cls,'__pydantic_parent_namespace__',None)\n   if isinstance(parent_namespace,dict):\n    parent_namespace=unpack_lenient_weakvaluedict(parent_namespace)\n    \n   ns_resolver=NsResolver(parent_namespace=parent_namespace)\n   \n   set_model_fields(cls,bases,config_wrapper,ns_resolver)\n   \n   if config_wrapper.frozen and '__hash__'not in namespace:\n    set_default_hash_func(cls,bases)\n    \n   complete_model_class(\n   cls,\n   cls_name,\n   config_wrapper,\n   raise_errors=False,\n   ns_resolver=ns_resolver,\n   create_model_module=_create_model_module,\n   )\n   \n   \n   \n   cls.__pydantic_computed_fields__={\n   k:v.info for k,v in cls.__pydantic_decorators__.computed_fields.items()\n   }\n   \n   set_deprecated_descriptors(cls)\n   \n   \n   \n   \n   super(cls,cls).__pydantic_init_subclass__(**kwargs)\n   return cls\n  else:\n  \n   for instance_slot in '__pydantic_fields_set__','__pydantic_extra__','__pydantic_private__':\n    namespace.pop(\n    instance_slot,\n    None,\n    )\n   namespace.get('__annotations__',{}).clear()\n   return super().__new__(mcs,cls_name,bases,namespace,**kwargs)\n   \n def mro(cls)->list[type[Any]]:\n  original_mro=super().mro()\n  \n  if cls.__bases__ ==(object,):\n   return original_mro\n   \n  generic_metadata:PydanticGenericMetadata |None=cls.__dict__.get('__pydantic_generic_metadata__')\n  if not generic_metadata:\n   return original_mro\n   \n  assert_err_msg='Unexpected error occurred when generating MRO of generic subclass. Please report this issue on GitHub: https://github.com/pydantic/pydantic/issues.'\n  \n  origin:type[BaseModel]|None\n  origin,args=(\n  generic_metadata['origin'],\n  generic_metadata['args'],\n  )\n  if not origin:\n   return original_mro\n   \n  target_params=origin.__pydantic_generic_metadata__['parameters']\n  param_dict=dict(zip(target_params,args))\n  \n  indexed_origins={origin}\n  \n  new_mro:list[type[Any]]=[cls]\n  for base in original_mro[1:]:\n   base_origin:type[BaseModel]|None=getattr(base,'__pydantic_generic_metadata__',{}).get('origin')\n   base_params:tuple[TypeVar,...]=getattr(base,'__pydantic_generic_metadata__',{}).get('parameters',())\n   \n   if base_origin in indexed_origins:\n    continue\n   elif base not in indexed_origins and base_params:\n    assert set(base_params)<=param_dict.keys(),assert_err_msg\n    new_base_args=tuple(param_dict[param]for param in base_params)\n    new_base=base[new_base_args]\n    new_mro.append(new_base)\n    \n    indexed_origins.add(base_origin or base)\n    \n    if base_origin is not None:\n    \n     continue\n   else:\n    indexed_origins.add(base_origin or base)\n    \n    \n    \n    \n   if base is not new_mro[-1]:\n    new_mro.append(base)\n    \n  return new_mro\n  \n if not typing.TYPE_CHECKING:\n \n \n  def __getattr__(self,item:str)->Any:\n   ''\n   private_attributes=self.__dict__.get('__private_attributes__')\n   if private_attributes and item in private_attributes:\n    return private_attributes[item]\n   raise AttributeError(item)\n   \n @classmethod\n def __prepare__(cls,*args:Any,**kwargs:Any)->dict[str,object]:\n  return _ModelNamespaceDict()\n  \n def __instancecheck__(self,instance:Any)->bool:\n  ''\n\n\n  \n  return hasattr(instance,'__pydantic_validator__')and super().__instancecheck__(instance)\n  \n @staticmethod\n def _collect_bases_data(bases:tuple[type[Any],...])->tuple[set[str],set[str],dict[str,ModelPrivateAttr]]:\n  BaseModel=import_cached_base_model()\n  \n  field_names:set[str]=set()\n  class_vars:set[str]=set()\n  private_attributes:dict[str,ModelPrivateAttr]={}\n  for base in bases:\n   if issubclass(base,BaseModel)and base is not BaseModel:\n   \n    field_names.update(getattr(base,'__pydantic_fields__',{}).keys())\n    class_vars.update(base.__class_vars__)\n    private_attributes.update(base.__private_attributes__)\n  return field_names,class_vars,private_attributes\n  \n @property\n @deprecated('The `__fields__` attribute is deprecated, use `model_fields` instead.',category=None)\n def __fields__(self)->dict[str,FieldInfo]:\n  warnings.warn(\n  'The `__fields__` attribute is deprecated, use `model_fields` instead.',\n  PydanticDeprecatedSince20,\n  stacklevel=2,\n  )\n  return self.model_fields\n  \n @property\n def model_fields(self)->dict[str,FieldInfo]:\n  ''\n\n\n\n  \n  return getattr(self,'__pydantic_fields__',{})\n  \n @property\n def model_computed_fields(self)->dict[str,ComputedFieldInfo]:\n  ''\n\n\n\n  \n  return getattr(self,'__pydantic_computed_fields__',{})\n  \n def __dir__(self)->list[str]:\n  attributes=list(super().__dir__())\n  if '__fields__'in attributes:\n   attributes.remove('__fields__')\n  return attributes\n  \n  \ndef init_private_attributes(self:BaseModel,context:Any,/)->None:\n ''\n\n\n\n\n\n\n \n if getattr(self,'__pydantic_private__',None)is None:\n  pydantic_private={}\n  for name,private_attr in self.__private_attributes__.items():\n   default=private_attr.get_default()\n   if default is not PydanticUndefined:\n    pydantic_private[name]=default\n  object_setattr(self,'__pydantic_private__',pydantic_private)\n  \n  \ndef get_model_post_init(namespace:dict[str,Any],bases:tuple[type[Any],...])->Callable[...,Any]|None:\n ''\n if 'model_post_init'in namespace:\n  return namespace['model_post_init']\n  \n BaseModel=import_cached_base_model()\n \n model_post_init=get_attribute_from_bases(bases,'model_post_init')\n if model_post_init is not BaseModel.model_post_init:\n  return model_post_init\n  \n  \ndef inspect_namespace(\nnamespace:dict[str,Any],\nignored_types:tuple[type[Any],...],\nbase_class_vars:set[str],\nbase_class_fields:set[str],\n)->dict[str,ModelPrivateAttr]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n from..fields import ModelPrivateAttr,PrivateAttr\n \n FieldInfo=import_cached_field_info()\n \n all_ignored_types=ignored_types+default_ignored_types()\n \n private_attributes:dict[str,ModelPrivateAttr]={}\n raw_annotations=namespace.get('__annotations__',{})\n \n if '__root__'in raw_annotations or '__root__'in namespace:\n  raise TypeError(\"To define root models, use `pydantic.RootModel` rather than a field called '__root__'\")\n  \n ignored_names:set[str]=set()\n for var_name,value in list(namespace.items()):\n  if var_name =='model_config'or var_name =='__pydantic_extra__':\n   continue\n  elif(\n  isinstance(value,type)\n  and value.__module__ ==namespace['__module__']\n  and '__qualname__'in namespace\n  and value.__qualname__.startswith(namespace['__qualname__'])\n  ):\n  \n   continue\n  elif isinstance(value,all_ignored_types)or value.__class__.__module__ =='functools':\n   ignored_names.add(var_name)\n   continue\n  elif isinstance(value,ModelPrivateAttr):\n   if var_name.startswith('__'):\n    raise NameError(\n    'Private attributes must not use dunder names;'\n    f' use a single underscore prefix instead of {var_name !r}.'\n    )\n   elif is_valid_field_name(var_name):\n    raise NameError(\n    'Private attributes must not use valid field names;'\n    f' use sunder names, e.g. {\"_\"+var_name !r} instead of {var_name !r}.'\n    )\n   private_attributes[var_name]=value\n   del namespace[var_name]\n  elif isinstance(value,FieldInfo)and not is_valid_field_name(var_name):\n   suggested_name=var_name.lstrip('_')or 'my_field'\n   raise NameError(\n   f'Fields must not use names with leading underscores;'\n   f' e.g., use {suggested_name !r} instead of {var_name !r}.'\n   )\n   \n  elif var_name.startswith('__'):\n   continue\n  elif is_valid_privateattr_name(var_name):\n   if var_name not in raw_annotations or not is_classvar_annotation(raw_annotations[var_name]):\n    private_attributes[var_name]=cast(ModelPrivateAttr,PrivateAttr(default=value))\n    del namespace[var_name]\n  elif var_name in base_class_vars:\n   continue\n  elif var_name not in raw_annotations:\n   if var_name in base_class_fields:\n    raise PydanticUserError(\n    f'Field {var_name !r} defined on a base class was overridden by a non-annotated attribute. '\n    f'All field definitions, including overrides, require a type annotation.',\n    code='model-field-overridden',\n    )\n   elif isinstance(value,FieldInfo):\n    raise PydanticUserError(\n    f'Field {var_name !r} requires a type annotation',code='model-field-missing-annotation'\n    )\n   else:\n    raise PydanticUserError(\n    f'A non-annotated attribute was detected: `{var_name} = {value !r}`. All model fields require a '\n    f'type annotation; if `{var_name}` is not meant to be a field, you may be able to resolve this '\n    f\"error by annotating it as a `ClassVar` or updating `model_config['ignored_types']`.\",\n    code='model-field-missing-annotation',\n    )\n    \n for ann_name,ann_type in raw_annotations.items():\n  if(\n  is_valid_privateattr_name(ann_name)\n  and ann_name not in private_attributes\n  and ann_name not in ignored_names\n  \n  \n  and not is_classvar_annotation(ann_type)\n  and ann_type not in all_ignored_types\n  and getattr(ann_type,'__module__',None)!='functools'\n  ):\n   if isinstance(ann_type,str):\n   \n   \n    frame=sys._getframe(2)\n    if frame is not None:\n     try:\n      ann_type=eval_type_backport(\n      _make_forward_ref(ann_type,is_argument=False,is_class=True),\n      globalns=frame.f_globals,\n      localns=frame.f_locals,\n      )\n     except(NameError,TypeError):\n      pass\n      \n   if is_annotated(ann_type):\n    _,*metadata=get_args(ann_type)\n    private_attr=next((v for v in metadata if isinstance(v,ModelPrivateAttr)),None)\n    if private_attr is not None:\n     private_attributes[ann_name]=private_attr\n     continue\n   private_attributes[ann_name]=PrivateAttr()\n   \n return private_attributes\n \n \ndef set_default_hash_func(cls:type[BaseModel],bases:tuple[type[Any],...])->None:\n base_hash_func=get_attribute_from_bases(bases,'__hash__')\n new_hash_func=make_hash_func(cls)\n if base_hash_func in{None,object.__hash__}or getattr(base_hash_func,'__code__',None)==new_hash_func.__code__:\n \n \n \n \n \n \n  cls.__hash__=new_hash_func\n  \n  \ndef make_hash_func(cls:type[BaseModel])->Any:\n getter=operator.itemgetter(*cls.__pydantic_fields__.keys())if cls.__pydantic_fields__ else lambda _:0\n \n def hash_func(self:Any)->int:\n  try:\n   return hash(getter(self.__dict__))\n  except KeyError:\n  \n  \n  \n  \n   return hash(getter(SafeGetItemProxy(self.__dict__)))\n   \n return hash_func\n \n \ndef set_model_fields(\ncls:type[BaseModel],\nbases:tuple[type[Any],...],\nconfig_wrapper:ConfigWrapper,\nns_resolver:NsResolver |None,\n)->None:\n ''\n\n\n\n\n\n\n \n typevars_map=get_model_typevars_map(cls)\n fields,class_vars=collect_model_fields(cls,bases,config_wrapper,ns_resolver,typevars_map=typevars_map)\n \n cls.__pydantic_fields__=fields\n cls.__class_vars__.update(class_vars)\n \n for k in class_vars:\n \n \n \n \n \n \n \n  value=cls.__private_attributes__.pop(k,None)\n  if value is not None and value.default is not PydanticUndefined:\n   setattr(cls,k,value.default)\n   \n   \ndef complete_model_class(\ncls:type[BaseModel],\ncls_name:str,\nconfig_wrapper:ConfigWrapper,\n*,\nraise_errors:bool=True,\nns_resolver:NsResolver |None=None,\ncreate_model_module:str |None=None,\n)->bool:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if config_wrapper.defer_build:\n  set_model_mocks(cls,cls_name)\n  return False\n  \n typevars_map=get_model_typevars_map(cls)\n gen_schema=GenerateSchema(\n config_wrapper,\n ns_resolver,\n typevars_map,\n )\n \n handler=CallbackGetCoreSchemaHandler(\n partial(gen_schema.generate_schema,from_dunder_get_core_schema=False),\n gen_schema,\n ref_mode='unpack',\n )\n \n try:\n  schema=cls.__get_pydantic_core_schema__(cls,handler)\n except PydanticUndefinedAnnotation as e:\n  if raise_errors:\n   raise\n  set_model_mocks(cls,cls_name,f'`{e.name}`')\n  return False\n  \n core_config=config_wrapper.core_config(title=cls.__name__)\n \n try:\n  schema=gen_schema.clean_schema(schema)\n except gen_schema.CollectedInvalid:\n  set_model_mocks(cls,cls_name)\n  return False\n  \n  \n cls.__pydantic_core_schema__=schema\n \n cls.__pydantic_validator__=create_schema_validator(\n schema,\n cls,\n create_model_module or cls.__module__,\n cls.__qualname__,\n 'create_model'if create_model_module else 'BaseModel',\n core_config,\n config_wrapper.plugin_settings,\n )\n cls.__pydantic_serializer__=SchemaSerializer(schema,core_config)\n cls.__pydantic_complete__=True\n \n \n \n \n cls.__signature__=LazyClassAttribute(\n '__signature__',\n partial(\n generate_pydantic_signature,\n init=cls.__init__,\n fields=cls.__pydantic_fields__,\n populate_by_name=config_wrapper.populate_by_name,\n extra=config_wrapper.extra,\n ),\n )\n return True\n \n \ndef set_deprecated_descriptors(cls:type[BaseModel])->None:\n ''\n for field,field_info in cls.__pydantic_fields__.items():\n  if(msg :=field_info.deprecation_message)is not None:\n   desc=_DeprecatedFieldDescriptor(msg)\n   desc.__set_name__(cls,field)\n   setattr(cls,field,desc)\n   \n for field,computed_field_info in cls.__pydantic_computed_fields__.items():\n  if(\n  (msg :=computed_field_info.deprecation_message)is not None\n  \n  and not hasattr(unwrap_wrapped_function(computed_field_info.wrapped_property),'__deprecated__')\n  ):\n   desc=_DeprecatedFieldDescriptor(msg,computed_field_info.wrapped_property)\n   desc.__set_name__(cls,field)\n   setattr(cls,field,desc)\n   \n   \nclass _DeprecatedFieldDescriptor:\n ''\n\n\n\n\n\n \n \n field_name:str\n \n def __init__(self,msg:str,wrapped_property:property |None=None)->None:\n  self.msg=msg\n  self.wrapped_property=wrapped_property\n  \n def __set_name__(self,cls:type[BaseModel],name:str)->None:\n  self.field_name=name\n  \n def __get__(self,obj:BaseModel |None,obj_type:type[BaseModel]|None=None)->Any:\n  if obj is None:\n   if self.wrapped_property is not None:\n    return self.wrapped_property.__get__(None,obj_type)\n   raise AttributeError(self.field_name)\n   \n  warnings.warn(self.msg,builtins.DeprecationWarning,stacklevel=2)\n  \n  if self.wrapped_property is not None:\n   return self.wrapped_property.__get__(obj,obj_type)\n  return obj.__dict__[self.field_name]\n  \n  \n  \n  \n def __set__(self,obj:Any,value:Any)->NoReturn:\n  raise AttributeError(self.field_name)\n  \n  \nclass _PydanticWeakRef:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,obj:Any):\n  if obj is None:\n  \n  \n   self._wr=None\n  else:\n   self._wr=weakref.ref(obj)\n   \n def __call__(self)->Any:\n  if self._wr is None:\n   return None\n  else:\n   return self._wr()\n   \n def __reduce__(self)->tuple[Callable,tuple[weakref.ReferenceType |None]]:\n  return _PydanticWeakRef,(self(),)\n  \n  \ndef build_lenient_weakvaluedict(d:dict[str,Any]|None)->dict[str,Any]|None:\n ''\n\n\n\n\n\n \n if d is None:\n  return None\n result={}\n for k,v in d.items():\n  try:\n   proxy=_PydanticWeakRef(v)\n  except TypeError:\n   proxy=v\n  result[k]=proxy\n return result\n \n \ndef unpack_lenient_weakvaluedict(d:dict[str,Any]|None)->dict[str,Any]|None:\n ''\n if d is None:\n  return None\n  \n result={}\n for k,v in d.items():\n  if isinstance(v,_PydanticWeakRef):\n   v=v()\n   if v is not None:\n    result[k]=v\n  else:\n   result[k]=v\n return result\n \n \n@lru_cache(maxsize=None)\ndef default_ignored_types()->tuple[type[Any],...]:\n from..fields import ComputedFieldInfo\n \n ignored_types=[\n FunctionType,\n property,\n classmethod,\n staticmethod,\n PydanticDescriptorProxy,\n ComputedFieldInfo,\n TypeAliasType,\n ]\n \n if sys.version_info >=(3,12):\n  ignored_types.append(typing.TypeAliasType)\n  \n return tuple(ignored_types)\n", ["__future__", "abc", "builtins", "functools", "operator", "pydantic._internal._config", "pydantic._internal._decorators", "pydantic._internal._fields", "pydantic._internal._generate_schema", "pydantic._internal._generics", "pydantic._internal._import_utils", "pydantic._internal._mock_val_ser", "pydantic._internal._namespace_utils", "pydantic._internal._schema_generation_shared", "pydantic._internal._signature", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic.errors", "pydantic.fields", "pydantic.main", "pydantic.plugin._schema_validator", "pydantic.root_model", "pydantic.warnings", "pydantic_core", "sys", "types", "typing", "typing_extensions", "warnings", "weakref"]], "pydantic._internal._serializers": [".py", "from __future__ import annotations\n\nimport collections\nimport collections.abc\nimport typing\nfrom typing import Any\n\nfrom pydantic_core import PydanticOmit,core_schema\n\nSEQUENCE_ORIGIN_MAP:dict[Any,Any]={\ntyping.Deque:collections.deque,\ncollections.deque:collections.deque,\nlist:list,\ntyping.List:list,\nset:set,\ntyping.AbstractSet:set,\ntyping.Set:set,\nfrozenset:frozenset,\ntyping.FrozenSet:frozenset,\ntyping.Sequence:list,\ntyping.MutableSequence:list,\ntyping.MutableSet:set,\n\n\ncollections.abc.MutableSet:set,\ncollections.abc.Set:frozenset,\n}\n\n\ndef serialize_sequence_via_list(\nv:Any,handler:core_schema.SerializerFunctionWrapHandler,info:core_schema.SerializationInfo\n)->Any:\n items:list[Any]=[]\n \n mapped_origin=SEQUENCE_ORIGIN_MAP.get(type(v),None)\n if mapped_origin is None:\n \n  return v\n  \n for index,item in enumerate(v):\n  try:\n   v=handler(item,index)\n  except PydanticOmit:\n   pass\n  else:\n   items.append(v)\n   \n if info.mode_is_json():\n  return items\n else:\n  return mapped_origin(items)\n", ["__future__", "collections", "collections.abc", "pydantic_core", "pydantic_core.core_schema", "typing"]], "pydantic._internal._signature": [".py", "from __future__ import annotations\n\nimport dataclasses\nfrom inspect import Parameter,Signature,signature\nfrom typing import TYPE_CHECKING,Any,Callable\n\nfrom pydantic_core import PydanticUndefined\n\nfrom._utils import is_valid_identifier\n\nif TYPE_CHECKING:\n from..config import ExtraValues\n from..fields import FieldInfo\n \n \n \nclass _HAS_DEFAULT_FACTORY_CLASS:\n def __repr__(self):\n  return '<factory>'\n  \n  \n_HAS_DEFAULT_FACTORY=_HAS_DEFAULT_FACTORY_CLASS()\n\n\ndef _field_name_for_signature(field_name:str,field_info:FieldInfo)->str:\n ''\n\n\n\n\n\n\n\n\n\n\n \n if isinstance(field_info.alias,str)and is_valid_identifier(field_info.alias):\n  return field_info.alias\n if isinstance(field_info.validation_alias,str)and is_valid_identifier(field_info.validation_alias):\n  return field_info.validation_alias\n  \n return field_name\n \n \ndef _process_param_defaults(param:Parameter)->Parameter:\n ''\n\n\n\n\n\n\n \n from..fields import FieldInfo\n \n param_default=param.default\n if isinstance(param_default,FieldInfo):\n  annotation=param.annotation\n  \n  \n  \n  if annotation =='Any':\n   annotation=Any\n   \n   \n  default=param_default.default\n  if default is PydanticUndefined:\n   if param_default.default_factory is PydanticUndefined:\n    default=Signature.empty\n   else:\n   \n    default=dataclasses._HAS_DEFAULT_FACTORY\n  return param.replace(\n  annotation=annotation,name=_field_name_for_signature(param.name,param_default),default=default\n  )\n return param\n \n \ndef _generate_signature_parameters(\ninit:Callable[...,None],\nfields:dict[str,FieldInfo],\npopulate_by_name:bool,\nextra:ExtraValues |None,\n)->dict[str,Parameter]:\n ''\n from itertools import islice\n \n present_params=signature(init).parameters.values()\n merged_params:dict[str,Parameter]={}\n var_kw=None\n use_var_kw=False\n \n for param in islice(present_params,1,None):\n \n \n  if fields.get(param.name):\n  \n   if getattr(fields[param.name],'init',True)is False:\n    continue\n   param=param.replace(name=_field_name_for_signature(param.name,fields[param.name]))\n  if param.annotation =='Any':\n   param=param.replace(annotation=Any)\n  if param.kind is param.VAR_KEYWORD:\n   var_kw=param\n   continue\n  merged_params[param.name]=param\n  \n if var_kw:\n  allow_names=populate_by_name\n  for field_name,field in fields.items():\n  \n   param_name=_field_name_for_signature(field_name,field)\n   \n   if field_name in merged_params or param_name in merged_params:\n    continue\n    \n   if not is_valid_identifier(param_name):\n    if allow_names:\n     param_name=field_name\n    else:\n     use_var_kw=True\n     continue\n     \n   if field.is_required():\n    default=Parameter.empty\n   elif field.default_factory is not None:\n   \n    default=_HAS_DEFAULT_FACTORY\n   else:\n    default=field.default\n   merged_params[param_name]=Parameter(\n   param_name,\n   Parameter.KEYWORD_ONLY,\n   annotation=field.rebuild_annotation(),\n   default=default,\n   )\n   \n if extra =='allow':\n  use_var_kw=True\n  \n if var_kw and use_var_kw:\n \n \n  default_model_signature=[\n  ('self',Parameter.POSITIONAL_ONLY),\n  ('data',Parameter.VAR_KEYWORD),\n  ]\n  if[(p.name,p.kind)for p in present_params]==default_model_signature:\n  \n   var_kw_name='extra_data'\n  else:\n  \n   var_kw_name=var_kw.name\n   \n   \n  while var_kw_name in fields:\n   var_kw_name +='_'\n  merged_params[var_kw_name]=var_kw.replace(name=var_kw_name)\n  \n return merged_params\n \n \ndef generate_pydantic_signature(\ninit:Callable[...,None],\nfields:dict[str,FieldInfo],\npopulate_by_name:bool,\nextra:ExtraValues |None,\nis_dataclass:bool=False,\n)->Signature:\n ''\n\n\n\n\n\n\n\n\n\n\n \n merged_params=_generate_signature_parameters(init,fields,populate_by_name,extra)\n \n if is_dataclass:\n  merged_params={k:_process_param_defaults(v)for k,v in merged_params.items()}\n  \n return Signature(parameters=list(merged_params.values()),return_annotation=None)\n", ["__future__", "dataclasses", "inspect", "itertools", "pydantic._internal._utils", "pydantic.config", "pydantic.fields", "pydantic_core", "typing"]], "pydantic._internal._import_utils": [".py", "from functools import lru_cache\nfrom typing import TYPE_CHECKING,Type\n\nif TYPE_CHECKING:\n from pydantic import BaseModel\n from pydantic.fields import FieldInfo\n \n \n@lru_cache(maxsize=None)\ndef import_cached_base_model()->Type['BaseModel']:\n from pydantic import BaseModel\n \n return BaseModel\n \n \n@lru_cache(maxsize=None)\ndef import_cached_field_info()->Type['FieldInfo']:\n from pydantic.fields import FieldInfo\n \n return FieldInfo\n", ["functools", "pydantic", "pydantic.fields", "typing"]], "pydantic._internal._generate_schema": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nimport collections.abc\nimport dataclasses\nimport datetime\nimport inspect\nimport os\nimport pathlib\nimport re\nimport sys\nimport typing\nimport warnings\nfrom contextlib import contextmanager\nfrom copy import copy,deepcopy\nfrom decimal import Decimal\nfrom enum import Enum\nfrom fractions import Fraction\nfrom functools import partial\nfrom inspect import Parameter,_ParameterKind,signature\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom itertools import chain\nfrom operator import attrgetter\nfrom types import FunctionType,LambdaType,MethodType\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nDict,\nFinal,\nForwardRef,\nIterable,\nIterator,\nMapping,\nType,\nTypeVar,\nUnion,\ncast,\noverload,\n)\nfrom uuid import UUID\nfrom warnings import warn\n\nimport typing_extensions\nfrom pydantic_core import(\nCoreSchema,\nMultiHostUrl,\nPydanticCustomError,\nPydanticSerializationUnexpectedValue,\nPydanticUndefined,\nUrl,\ncore_schema,\nto_jsonable_python,\n)\nfrom typing_extensions import Literal,TypeAliasType,TypedDict,get_args,get_origin,is_typeddict\n\nfrom..aliases import AliasChoices,AliasGenerator,AliasPath\nfrom..annotated_handlers import GetCoreSchemaHandler,GetJsonSchemaHandler\nfrom..config import ConfigDict,JsonDict,JsonEncoder,JsonSchemaExtraCallable\nfrom..errors import PydanticSchemaGenerationError,PydanticUndefinedAnnotation,PydanticUserError\nfrom..functional_validators import AfterValidator,BeforeValidator,FieldValidatorModes,PlainValidator,WrapValidator\nfrom..json_schema import JsonSchemaValue\nfrom..version import version_short\nfrom..warnings import PydanticDeprecatedSince20\nfrom. import _core_utils,_decorators,_discriminated_union,_known_annotated_metadata,_typing_extra\nfrom._config import ConfigWrapper,ConfigWrapperStack\nfrom._core_metadata import update_core_metadata\nfrom._core_utils import(\ncollect_invalid_schemas,\ndefine_expected_missing_refs,\nget_ref,\nget_type_ref,\nis_function_with_inner_schema,\nis_list_like_schema_with_items_schema,\nsimplify_schema_references,\nvalidate_core_schema,\n)\nfrom._decorators import(\nDecorator,\nDecoratorInfos,\nFieldSerializerDecoratorInfo,\nFieldValidatorDecoratorInfo,\nModelSerializerDecoratorInfo,\nModelValidatorDecoratorInfo,\nRootValidatorDecoratorInfo,\nValidatorDecoratorInfo,\nget_attribute_from_bases,\ninspect_field_serializer,\ninspect_model_serializer,\ninspect_validator,\n)\nfrom._docs_extraction import extract_docstrings_from_cls\nfrom._fields import collect_dataclass_fields,takes_validated_data_argument\nfrom._forward_ref import PydanticRecursiveRef\nfrom._generics import get_standard_typevars_map,has_instance_in_type,recursively_defined_type_refs,replace_types\nfrom._import_utils import import_cached_base_model,import_cached_field_info\nfrom._mock_val_ser import MockCoreSchema\nfrom._namespace_utils import NamespacesTuple,NsResolver\nfrom._schema_generation_shared import CallbackGetCoreSchemaHandler\nfrom._utils import lenient_issubclass,smart_deepcopy\n\nif TYPE_CHECKING:\n from..fields import ComputedFieldInfo,FieldInfo\n from..main import BaseModel\n from..types import Discriminator\n from._dataclasses import StandardDataclass\n from._schema_generation_shared import GetJsonSchemaFunction\n \n_SUPPORTS_TYPEDDICT=sys.version_info >=(3,12)\n\nFieldDecoratorInfo=Union[ValidatorDecoratorInfo,FieldValidatorDecoratorInfo,FieldSerializerDecoratorInfo]\nFieldDecoratorInfoType=TypeVar('FieldDecoratorInfoType',bound=FieldDecoratorInfo)\nAnyFieldDecorator=Union[\nDecorator[ValidatorDecoratorInfo],\nDecorator[FieldValidatorDecoratorInfo],\nDecorator[FieldSerializerDecoratorInfo],\n]\n\nModifyCoreSchemaWrapHandler=GetCoreSchemaHandler\nGetCoreSchemaFunction=Callable[[Any,ModifyCoreSchemaWrapHandler],core_schema.CoreSchema]\n\nTUPLE_TYPES:list[type]=[tuple,typing.Tuple]\nLIST_TYPES:list[type]=[list,typing.List,collections.abc.MutableSequence]\nSET_TYPES:list[type]=[set,typing.Set,collections.abc.MutableSet]\nFROZEN_SET_TYPES:list[type]=[frozenset,typing.FrozenSet,collections.abc.Set]\nDICT_TYPES:list[type]=[dict,typing.Dict]\nIP_TYPES:list[type]=[IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network]\nSEQUENCE_TYPES:list[type]=[typing.Sequence,collections.abc.Sequence]\nPATH_TYPES:list[type]=[\nos.PathLike,\npathlib.Path,\npathlib.PurePath,\npathlib.PosixPath,\npathlib.PurePosixPath,\npathlib.PureWindowsPath,\n]\nMAPPING_TYPES=[\ntyping.Mapping,\ntyping.MutableMapping,\ncollections.abc.Mapping,\ncollections.abc.MutableMapping,\ncollections.OrderedDict,\ntyping_extensions.OrderedDict,\ntyping.DefaultDict,\ncollections.defaultdict,\ncollections.Counter,\ntyping.Counter,\n]\nDEQUE_TYPES:list[type]=[collections.deque,typing.Deque]\n\n\n\nValidateCallSupportedTypes=Union[\nLambdaType,\nFunctionType,\nMethodType,\npartial,\n]\n\nVALIDATE_CALL_SUPPORTED_TYPES=get_args(ValidateCallSupportedTypes)\n\n_mode_to_validator:dict[\nFieldValidatorModes,type[BeforeValidator |AfterValidator |PlainValidator |WrapValidator]\n]={'before':BeforeValidator,'after':AfterValidator,'plain':PlainValidator,'wrap':WrapValidator}\n\n\ndef check_validator_fields_against_field_name(\ninfo:FieldDecoratorInfo,\nfield:str,\n)->bool:\n ''\n\n\n\n\n\n\n\n \n if '*'in info.fields:\n  return True\n for v_field_name in info.fields:\n  if v_field_name ==field:\n   return True\n return False\n \n \ndef check_decorator_fields_exist(decorators:Iterable[AnyFieldDecorator],fields:Iterable[str])->None:\n ''\n\n\n\n\n\n\n\n\n\n \n fields=set(fields)\n for dec in decorators:\n  if '*'in dec.info.fields:\n   continue\n  if dec.info.check_fields is False:\n   continue\n  for field in dec.info.fields:\n   if field not in fields:\n    raise PydanticUserError(\n    f'Decorators defined with incorrect fields: {dec.cls_ref}.{dec.cls_var_name}'\n    \" (use check_fields=False if you're inheriting from the model and intended this)\",\n    code='decorator-missing-field',\n    )\n    \n    \ndef filter_field_decorator_info_by_field(\nvalidator_functions:Iterable[Decorator[FieldDecoratorInfoType]],field:str\n)->list[Decorator[FieldDecoratorInfoType]]:\n return[dec for dec in validator_functions if check_validator_fields_against_field_name(dec.info,field)]\n \n \ndef apply_each_item_validators(\nschema:core_schema.CoreSchema,\neach_item_validators:list[Decorator[ValidatorDecoratorInfo]],\nfield_name:str |None,\n)->core_schema.CoreSchema:\n\n\n\n if not each_item_validators:\n  return schema\n  \n  \n  \n  \n if schema['type']=='nullable':\n  schema['schema']=apply_each_item_validators(schema['schema'],each_item_validators,field_name)\n  return schema\n elif schema['type']=='tuple':\n  if(variadic_item_index :=schema.get('variadic_item_index'))is not None:\n   schema['items_schema'][variadic_item_index]=apply_validators(\n   schema['items_schema'][variadic_item_index],\n   each_item_validators,\n   field_name,\n   )\n elif is_list_like_schema_with_items_schema(schema):\n  inner_schema=schema.get('items_schema',core_schema.any_schema())\n  schema['items_schema']=apply_validators(inner_schema,each_item_validators,field_name)\n elif schema['type']=='dict':\n  inner_schema=schema.get('values_schema',core_schema.any_schema())\n  schema['values_schema']=apply_validators(inner_schema,each_item_validators,field_name)\n else:\n  raise TypeError(\n  f\"`@validator(..., each_item=True)` cannot be applied to fields with a schema of {schema['type']}\"\n  )\n return schema\n \n \ndef _extract_json_schema_info_from_field_info(\ninfo:FieldInfo |ComputedFieldInfo,\n)->tuple[JsonDict |None,JsonDict |JsonSchemaExtraCallable |None]:\n json_schema_updates={\n 'title':info.title,\n 'description':info.description,\n 'deprecated':bool(info.deprecated)or info.deprecated ==''or None,\n 'examples':to_jsonable_python(info.examples),\n }\n json_schema_updates={k:v for k,v in json_schema_updates.items()if v is not None}\n return(json_schema_updates or None,info.json_schema_extra)\n \n \nJsonEncoders=Dict[Type[Any],JsonEncoder]\n\n\ndef _add_custom_serialization_from_json_encoders(\njson_encoders:JsonEncoders |None,tp:Any,schema:CoreSchema\n)->CoreSchema:\n ''\n\n\n\n\n\n \n if not json_encoders:\n  return schema\n if 'serialization'in schema:\n  return schema\n  \n  \n  \n for base in(tp,*getattr(tp,'__mro__',tp.__class__.__mro__)[:-1]):\n  encoder=json_encoders.get(base)\n  if encoder is None:\n   continue\n   \n  warnings.warn(\n  f'`json_encoders` is deprecated. See https://docs.pydantic.dev/{version_short()}/concepts/serialization/#custom-serializers for alternatives',\n  PydanticDeprecatedSince20,\n  )\n  \n  \n  schema['serialization']=core_schema.plain_serializer_function_ser_schema(encoder,when_used='json')\n  return schema\n  \n return schema\n \n \ndef _get_first_non_null(a:Any,b:Any)->Any:\n ''\n\n\n\n \n return a if a is not None else b\n \n \nclass GenerateSchema:\n ''\n \n __slots__=(\n '_config_wrapper_stack',\n '_ns_resolver',\n '_typevars_map',\n 'field_name_stack',\n 'model_type_stack',\n 'defs',\n )\n \n def __init__(\n self,\n config_wrapper:ConfigWrapper,\n ns_resolver:NsResolver |None=None,\n typevars_map:dict[Any,Any]|None=None,\n )->None:\n \n  self._config_wrapper_stack=ConfigWrapperStack(config_wrapper)\n  self._ns_resolver=ns_resolver or NsResolver()\n  self._typevars_map=typevars_map\n  self.field_name_stack=_FieldNameStack()\n  self.model_type_stack=_ModelTypeStack()\n  self.defs=_Definitions()\n  \n def __init_subclass__(cls)->None:\n  super().__init_subclass__()\n  warnings.warn(\n  'Subclassing `GenerateSchema` is not supported. The API is highly subject to change in minor versions.',\n  UserWarning,\n  stacklevel=2,\n  )\n  \n @property\n def _config_wrapper(self)->ConfigWrapper:\n  return self._config_wrapper_stack.tail\n  \n @property\n def _types_namespace(self)->NamespacesTuple:\n  return self._ns_resolver.types_namespace\n  \n @property\n def _arbitrary_types(self)->bool:\n  return self._config_wrapper.arbitrary_types_allowed\n  \n  \n  \n def _list_schema(self,items_type:Any)->CoreSchema:\n  return core_schema.list_schema(self.generate_schema(items_type))\n  \n def _dict_schema(self,keys_type:Any,values_type:Any)->CoreSchema:\n  return core_schema.dict_schema(self.generate_schema(keys_type),self.generate_schema(values_type))\n  \n def _set_schema(self,items_type:Any)->CoreSchema:\n  return core_schema.set_schema(self.generate_schema(items_type))\n  \n def _frozenset_schema(self,items_type:Any)->CoreSchema:\n  return core_schema.frozenset_schema(self.generate_schema(items_type))\n  \n def _enum_schema(self,enum_type:type[Enum])->CoreSchema:\n  cases:list[Any]=list(enum_type.__members__.values())\n  \n  enum_ref=get_type_ref(enum_type)\n  description=None if not enum_type.__doc__ else inspect.cleandoc(enum_type.__doc__)\n  if(\n  description =='An enumeration.'\n  ):\n   description=None\n  js_updates={'title':enum_type.__name__,'description':description}\n  js_updates={k:v for k,v in js_updates.items()if v is not None}\n  \n  sub_type:Literal['str','int','float']|None=None\n  if issubclass(enum_type,int):\n   sub_type='int'\n   value_ser_type:core_schema.SerSchema=core_schema.simple_ser_schema('int')\n  elif issubclass(enum_type,str):\n  \n   sub_type='str'\n   value_ser_type=core_schema.simple_ser_schema('str')\n  elif issubclass(enum_type,float):\n   sub_type='float'\n   value_ser_type=core_schema.simple_ser_schema('float')\n  else:\n  \n   value_ser_type=core_schema.plain_serializer_function_ser_schema(lambda x:x)\n   \n  if cases:\n  \n   def get_json_schema(schema:CoreSchema,handler:GetJsonSchemaHandler)->JsonSchemaValue:\n    json_schema=handler(schema)\n    original_schema=handler.resolve_ref_schema(json_schema)\n    original_schema.update(js_updates)\n    return json_schema\n    \n    \n   default_missing=getattr(enum_type._missing_,'__func__',None)is Enum._missing_.__func__\n   enum_schema=core_schema.enum_schema(\n   enum_type,\n   cases,\n   sub_type=sub_type,\n   missing=None if default_missing else enum_type._missing_,\n   ref=enum_ref,\n   metadata={'pydantic_js_functions':[get_json_schema]},\n   )\n   \n   if self._config_wrapper.use_enum_values:\n    enum_schema=core_schema.no_info_after_validator_function(\n    attrgetter('value'),enum_schema,serialization=value_ser_type\n    )\n    \n   return enum_schema\n   \n  else:\n  \n   def get_json_schema_no_cases(_,handler:GetJsonSchemaHandler)->JsonSchemaValue:\n    json_schema=handler(core_schema.enum_schema(enum_type,cases,sub_type=sub_type,ref=enum_ref))\n    original_schema=handler.resolve_ref_schema(json_schema)\n    original_schema.update(js_updates)\n    return json_schema\n    \n    \n    \n    \n    \n    \n    \n   return core_schema.is_instance_schema(\n   enum_type,\n   metadata={'pydantic_js_functions':[get_json_schema_no_cases]},\n   )\n   \n def _ip_schema(self,tp:Any)->CoreSchema:\n  from._validators import IP_VALIDATOR_LOOKUP,IpType\n  \n  ip_type_json_schema_format:dict[type[IpType],str]={\n  IPv4Address:'ipv4',\n  IPv4Network:'ipv4network',\n  IPv4Interface:'ipv4interface',\n  IPv6Address:'ipv6',\n  IPv6Network:'ipv6network',\n  IPv6Interface:'ipv6interface',\n  }\n  \n  def ser_ip(ip:Any,info:core_schema.SerializationInfo)->str |IpType:\n   if not isinstance(ip,(tp,str)):\n    raise PydanticSerializationUnexpectedValue(\n    f\"Expected `{tp}` but got `{type(ip)}` with value `'{ip}'` - serialized value may not be as expected.\"\n    )\n   if info.mode =='python':\n    return ip\n   return str(ip)\n   \n  return core_schema.lax_or_strict_schema(\n  lax_schema=core_schema.no_info_plain_validator_function(IP_VALIDATOR_LOOKUP[tp]),\n  strict_schema=core_schema.json_or_python_schema(\n  json_schema=core_schema.no_info_after_validator_function(tp,core_schema.str_schema()),\n  python_schema=core_schema.is_instance_schema(tp),\n  ),\n  serialization=core_schema.plain_serializer_function_ser_schema(ser_ip,info_arg=True,when_used='always'),\n  metadata={\n  'pydantic_js_functions':[lambda _1,_2:{'type':'string','format':ip_type_json_schema_format[tp]}]\n  },\n  )\n  \n def _fraction_schema(self)->CoreSchema:\n  ''\n  from._validators import fraction_validator\n  \n  \n  \n  return core_schema.lax_or_strict_schema(\n  lax_schema=core_schema.no_info_plain_validator_function(fraction_validator),\n  strict_schema=core_schema.json_or_python_schema(\n  json_schema=core_schema.no_info_plain_validator_function(fraction_validator),\n  python_schema=core_schema.is_instance_schema(Fraction),\n  ),\n  \n  serialization=core_schema.to_string_ser_schema(when_used='always'),\n  metadata={'pydantic_js_functions':[lambda _1,_2:{'type':'string','format':'fraction'}]},\n  )\n  \n def _arbitrary_type_schema(self,tp:Any)->CoreSchema:\n  if not isinstance(tp,type):\n   warn(\n   f'{tp !r} is not a Python type (it may be an instance of an object),'\n   ' Pydantic will allow any object with no validation since we cannot even'\n   ' enforce that the input is an instance of the given type.'\n   ' To get rid of this error wrap the type with `pydantic.SkipValidation`.',\n   UserWarning,\n   )\n   return core_schema.any_schema()\n  return core_schema.is_instance_schema(tp)\n  \n def _unknown_type_schema(self,obj:Any)->CoreSchema:\n  raise PydanticSchemaGenerationError(\n  f'Unable to generate pydantic-core schema for {obj !r}. '\n  'Set `arbitrary_types_allowed=True` in the model_config to ignore this error'\n  ' or implement `__get_pydantic_core_schema__` on your type to fully support it.'\n  '\\n\\nIf you got this error by calling handler(<some type>) within'\n  ' `__get_pydantic_core_schema__` then you likely need to call'\n  ' `handler.generate_schema(<some type>)` since we do not call'\n  ' `__get_pydantic_core_schema__` on `<some type>` otherwise to avoid infinite recursion.'\n  )\n  \n def _apply_discriminator_to_union(\n self,schema:CoreSchema,discriminator:str |Discriminator |None\n )->CoreSchema:\n  if discriminator is None:\n   return schema\n  try:\n   return _discriminated_union.apply_discriminator(\n   schema,\n   discriminator,\n   )\n  except _discriminated_union.MissingDefinitionForUnionRef:\n  \n   _discriminated_union.set_discriminator_in_metadata(\n   schema,\n   discriminator,\n   )\n   return schema\n   \n class CollectedInvalid(Exception):\n  pass\n  \n def clean_schema(self,schema:CoreSchema)->CoreSchema:\n  schema=self.collect_definitions(schema)\n  schema=simplify_schema_references(schema)\n  if collect_invalid_schemas(schema):\n   raise self.CollectedInvalid()\n  schema=_discriminated_union.apply_discriminators(schema)\n  schema=validate_core_schema(schema)\n  return schema\n  \n def collect_definitions(self,schema:CoreSchema)->CoreSchema:\n  ref=cast('str | None',schema.get('ref',None))\n  if ref:\n   self.defs.definitions[ref]=schema\n  if 'ref'in schema:\n   schema=core_schema.definition_reference_schema(schema['ref'])\n  return core_schema.definitions_schema(\n  schema,\n  list(self.defs.definitions.values()),\n  )\n  \n def _add_js_function(self,metadata_schema:CoreSchema,js_function:Callable[...,Any])->None:\n  metadata=metadata_schema.get('metadata',{})\n  pydantic_js_functions=metadata.setdefault('pydantic_js_functions',[])\n  \n  \n  \n  \n  if js_function not in pydantic_js_functions:\n   pydantic_js_functions.append(js_function)\n  metadata_schema['metadata']=metadata\n  \n def generate_schema(\n self,\n obj:Any,\n from_dunder_get_core_schema:bool=True,\n )->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  schema:CoreSchema |None=None\n  \n  if from_dunder_get_core_schema:\n   from_property=self._generate_schema_from_property(obj,obj)\n   if from_property is not None:\n    schema=from_property\n    \n  if schema is None:\n   schema=self._generate_schema_inner(obj)\n   \n  metadata_js_function=_extract_get_pydantic_json_schema(obj,schema)\n  if metadata_js_function is not None:\n   metadata_schema=resolve_original_schema(schema,self.defs.definitions)\n   if metadata_schema:\n    self._add_js_function(metadata_schema,metadata_js_function)\n    \n  schema=_add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders,obj,schema)\n  \n  return schema\n  \n def _model_schema(self,cls:type[BaseModel])->core_schema.CoreSchema:\n  ''\n  with self.defs.get_schema_or_ref(cls)as(model_ref,maybe_schema):\n   if maybe_schema is not None:\n    return maybe_schema\n    \n   fields=getattr(cls,'__pydantic_fields__',{})\n   decorators=cls.__pydantic_decorators__\n   computed_fields=decorators.computed_fields\n   check_decorator_fields_exist(\n   chain(\n   decorators.field_validators.values(),\n   decorators.field_serializers.values(),\n   decorators.validators.values(),\n   ),\n   {*fields.keys(),*computed_fields.keys()},\n   )\n   config_wrapper=ConfigWrapper(cls.model_config,check=False)\n   core_config=config_wrapper.core_config(title=cls.__name__)\n   model_validators=decorators.model_validators.values()\n   \n   with self._config_wrapper_stack.push(config_wrapper),self._ns_resolver.push(cls):\n    extras_schema=None\n    if core_config.get('extra_fields_behavior')=='allow':\n     assert cls.__mro__[0]is cls\n     assert cls.__mro__[-1]is object\n     for candidate_cls in cls.__mro__[:-1]:\n      extras_annotation=getattr(candidate_cls,'__annotations__',{}).get(\n      '__pydantic_extra__',None\n      )\n      if extras_annotation is not None:\n       if isinstance(extras_annotation,str):\n        extras_annotation=_typing_extra.eval_type_backport(\n        _typing_extra._make_forward_ref(\n        extras_annotation,is_argument=False,is_class=True\n        ),\n        *self._types_namespace,\n        )\n       tp=get_origin(extras_annotation)\n       if tp not in(Dict,dict):\n        raise PydanticSchemaGenerationError(\n        'The type annotation for `__pydantic_extra__` must be `Dict[str, ...]`'\n        )\n       extra_items_type=self._get_args_resolving_forward_refs(\n       extras_annotation,\n       required=True,\n       )[1]\n       if not _typing_extra.is_any(extra_items_type):\n        extras_schema=self.generate_schema(extra_items_type)\n        break\n        \n    generic_origin:type[BaseModel]|None=getattr(cls,'__pydantic_generic_metadata__',{}).get('origin')\n    \n    if cls.__pydantic_root_model__:\n     root_field=self._common_field_schema('root',fields['root'],decorators)\n     inner_schema=root_field['schema']\n     inner_schema=apply_model_validators(inner_schema,model_validators,'inner')\n     model_schema=core_schema.model_schema(\n     cls,\n     inner_schema,\n     generic_origin=generic_origin,\n     custom_init=getattr(cls,'__pydantic_custom_init__',None),\n     root_model=True,\n     post_init=getattr(cls,'__pydantic_post_init__',None),\n     config=core_config,\n     ref=model_ref,\n     )\n    else:\n     fields_schema:core_schema.CoreSchema=core_schema.model_fields_schema(\n     {k:self._generate_md_field_schema(k,v,decorators)for k,v in fields.items()},\n     computed_fields=[\n     self._computed_field_schema(d,decorators.field_serializers)\n     for d in computed_fields.values()\n     ],\n     extras_schema=extras_schema,\n     model_name=cls.__name__,\n     )\n     inner_schema=apply_validators(fields_schema,decorators.root_validators.values(),None)\n     new_inner_schema=define_expected_missing_refs(inner_schema,recursively_defined_type_refs())\n     if new_inner_schema is not None:\n      inner_schema=new_inner_schema\n     inner_schema=apply_model_validators(inner_schema,model_validators,'inner')\n     \n     model_schema=core_schema.model_schema(\n     cls,\n     inner_schema,\n     generic_origin=generic_origin,\n     custom_init=getattr(cls,'__pydantic_custom_init__',None),\n     root_model=False,\n     post_init=getattr(cls,'__pydantic_post_init__',None),\n     config=core_config,\n     ref=model_ref,\n     )\n     \n    schema=self._apply_model_serializers(model_schema,decorators.model_serializers.values())\n    schema=apply_model_validators(schema,model_validators,'outer')\n    self.defs.definitions[model_ref]=schema\n    return core_schema.definition_reference_schema(model_ref)\n    \n def _unpack_refs_defs(self,schema:CoreSchema)->CoreSchema:\n  ''\n\n  \n  if schema['type']=='definitions':\n   definitions=self.defs.definitions\n   for s in schema['definitions']:\n    definitions[s['ref']]=s\n   return schema['schema']\n  return schema\n  \n def _resolve_self_type(self,obj:Any)->Any:\n  obj=self.model_type_stack.get()\n  if obj is None:\n   raise PydanticUserError('`typing.Self` is invalid in this context',code='invalid-self-type')\n  return obj\n  \n def _generate_schema_from_property(self,obj:Any,source:Any)->core_schema.CoreSchema |None:\n  ''\n\n\n\n\n  \n  \n  if _typing_extra.is_self(obj):\n   obj=self._resolve_self_type(obj)\n  with self.defs.get_schema_or_ref(obj)as(_,maybe_schema):\n   if maybe_schema is not None:\n    return maybe_schema\n  if obj is source:\n   ref_mode='unpack'\n  else:\n   ref_mode='to-def'\n   \n  schema:CoreSchema\n  \n  if(get_schema :=getattr(obj,'__get_pydantic_core_schema__',None))is not None:\n   schema=get_schema(\n   source,CallbackGetCoreSchemaHandler(self._generate_schema_inner,self,ref_mode=ref_mode)\n   )\n  elif(\n  hasattr(obj,'__dict__')\n  \n  \n  \n  and(existing_schema :=obj.__dict__.get('__pydantic_core_schema__'))is not None\n  and not isinstance(existing_schema,MockCoreSchema)\n  ):\n   schema=existing_schema\n  elif(validators :=getattr(obj,'__get_validators__',None))is not None:\n   from pydantic.v1 import BaseModel as BaseModelV1\n   \n   if issubclass(obj,BaseModelV1):\n    warn(\n    f'Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `{obj.__name__}` to V2.',\n    UserWarning,\n    )\n   else:\n    warn(\n    '`__get_validators__` is deprecated and will be removed, use `__get_pydantic_core_schema__` instead.',\n    PydanticDeprecatedSince20,\n    )\n   schema=core_schema.chain_schema([core_schema.with_info_plain_validator_function(v)for v in validators()])\n  else:\n  \n   return None\n   \n  schema=self._unpack_refs_defs(schema)\n  \n  if is_function_with_inner_schema(schema):\n   ref=schema['schema'].pop('ref',None)\n   if ref:\n    schema['ref']=ref\n  else:\n   ref=get_ref(schema)\n   \n  if ref:\n   self.defs.definitions[ref]=schema\n   return core_schema.definition_reference_schema(ref)\n   \n  return schema\n  \n def _resolve_forward_ref(self,obj:Any)->Any:\n \n \n \n \n \n \n \n  try:\n   obj=_typing_extra.eval_type_backport(obj,*self._types_namespace)\n  except NameError as e:\n   raise PydanticUndefinedAnnotation.from_name_error(e)from e\n   \n   \n  if isinstance(obj,ForwardRef):\n   raise PydanticUndefinedAnnotation(obj.__forward_arg__,f'Unable to evaluate forward reference {obj}')\n   \n  if self._typevars_map:\n   obj=replace_types(obj,self._typevars_map)\n   \n  return obj\n  \n @overload\n def _get_args_resolving_forward_refs(self,obj:Any,required:Literal[True])->tuple[Any,...]:...\n \n @overload\n def _get_args_resolving_forward_refs(self,obj:Any)->tuple[Any,...]|None:...\n \n def _get_args_resolving_forward_refs(self,obj:Any,required:bool=False)->tuple[Any,...]|None:\n  args=get_args(obj)\n  if args:\n   if sys.version_info >=(3,9):\n    from types import GenericAlias\n    \n    if isinstance(obj,GenericAlias):\n    \n     args=(_typing_extra._make_forward_ref(a)if isinstance(a,str)else a for a in args)\n   args=tuple(self._resolve_forward_ref(a)if isinstance(a,ForwardRef)else a for a in args)\n  elif required:\n   raise TypeError(f'Expected {obj} to have generic parameters but it had none')\n  return args\n  \n def _get_first_arg_or_any(self,obj:Any)->Any:\n  args=self._get_args_resolving_forward_refs(obj)\n  if not args:\n   return Any\n  return args[0]\n  \n def _get_first_two_args_or_any(self,obj:Any)->tuple[Any,Any]:\n  args=self._get_args_resolving_forward_refs(obj)\n  if not args:\n   return(Any,Any)\n  if len(args)<2:\n   origin=get_origin(obj)\n   raise TypeError(f'Expected two type arguments for {origin}, got 1')\n  return args[0],args[1]\n  \n def _generate_schema_inner(self,obj:Any)->core_schema.CoreSchema:\n  if _typing_extra.is_annotated(obj):\n   return self._annotated_schema(obj)\n   \n  if isinstance(obj,dict):\n  \n   return obj\n   \n  if isinstance(obj,str):\n   obj=ForwardRef(obj)\n   \n  if isinstance(obj,ForwardRef):\n   return self.generate_schema(self._resolve_forward_ref(obj))\n   \n  BaseModel=import_cached_base_model()\n  \n  if lenient_issubclass(obj,BaseModel):\n   with self.model_type_stack.push(obj):\n    return self._model_schema(obj)\n    \n  if isinstance(obj,PydanticRecursiveRef):\n   return core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n   \n  return self.match_type(obj)\n  \n def match_type(self,obj:Any)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if obj is str:\n   return core_schema.str_schema()\n  elif obj is bytes:\n   return core_schema.bytes_schema()\n  elif obj is int:\n   return core_schema.int_schema()\n  elif obj is float:\n   return core_schema.float_schema()\n  elif obj is bool:\n   return core_schema.bool_schema()\n  elif obj is complex:\n   return core_schema.complex_schema()\n  elif _typing_extra.is_any(obj)or obj is object:\n   return core_schema.any_schema()\n  elif obj is datetime.date:\n   return core_schema.date_schema()\n  elif obj is datetime.datetime:\n   return core_schema.datetime_schema()\n  elif obj is datetime.time:\n   return core_schema.time_schema()\n  elif obj is datetime.timedelta:\n   return core_schema.timedelta_schema()\n  elif obj is Decimal:\n   return core_schema.decimal_schema()\n  elif obj is UUID:\n   return core_schema.uuid_schema()\n  elif obj is Url:\n   return core_schema.url_schema()\n  elif obj is Fraction:\n   return self._fraction_schema()\n  elif obj is MultiHostUrl:\n   return core_schema.multi_host_url_schema()\n  elif obj is None or obj is _typing_extra.NoneType:\n   return core_schema.none_schema()\n  elif obj in IP_TYPES:\n   return self._ip_schema(obj)\n  elif obj in TUPLE_TYPES:\n   return self._tuple_schema(obj)\n  elif obj in LIST_TYPES:\n   return self._list_schema(Any)\n  elif obj in SET_TYPES:\n   return self._set_schema(Any)\n  elif obj in FROZEN_SET_TYPES:\n   return self._frozenset_schema(Any)\n  elif obj in SEQUENCE_TYPES:\n   return self._sequence_schema(Any)\n  elif obj in DICT_TYPES:\n   return self._dict_schema(Any,Any)\n  elif _typing_extra.is_type_alias_type(obj):\n   return self._type_alias_type_schema(obj)\n  elif obj is type:\n   return self._type_schema()\n  elif _typing_extra.is_callable(obj):\n   return core_schema.callable_schema()\n  elif _typing_extra.is_literal(obj):\n   return self._literal_schema(obj)\n  elif is_typeddict(obj):\n   return self._typed_dict_schema(obj,None)\n  elif _typing_extra.is_namedtuple(obj):\n   return self._namedtuple_schema(obj,None)\n  elif _typing_extra.is_new_type(obj):\n  \n   return self.generate_schema(obj.__supertype__)\n  elif obj is re.Pattern:\n   return self._pattern_schema(obj)\n  elif _typing_extra.is_hashable(obj):\n   return self._hashable_schema()\n  elif isinstance(obj,typing.TypeVar):\n   return self._unsubstituted_typevar_schema(obj)\n  elif _typing_extra.is_finalvar(obj):\n   if obj is Final:\n    return core_schema.any_schema()\n   return self.generate_schema(\n   self._get_first_arg_or_any(obj),\n   )\n  elif isinstance(obj,VALIDATE_CALL_SUPPORTED_TYPES):\n   return self._call_schema(obj)\n  elif inspect.isclass(obj)and issubclass(obj,Enum):\n   return self._enum_schema(obj)\n  elif _typing_extra.is_zoneinfo_type(obj):\n   return self._zoneinfo_schema()\n   \n  if dataclasses.is_dataclass(obj):\n   return self._dataclass_schema(obj,None)\n   \n  origin=get_origin(obj)\n  if origin is not None:\n   return self._match_generic_type(obj,origin)\n   \n  res=self._get_prepare_pydantic_annotations_for_known_type(obj,())\n  if res is not None:\n   source_type,annotations=res\n   return self._apply_annotations(source_type,annotations)\n   \n  if self._arbitrary_types:\n   return self._arbitrary_type_schema(obj)\n  return self._unknown_type_schema(obj)\n  \n def _match_generic_type(self,obj:Any,origin:Any)->CoreSchema:\n \n \n \n \n  if dataclasses.is_dataclass(origin):\n   return self._dataclass_schema(obj,origin)\n  if _typing_extra.is_namedtuple(origin):\n   return self._namedtuple_schema(obj,origin)\n   \n  from_property=self._generate_schema_from_property(origin,obj)\n  if from_property is not None:\n   return from_property\n   \n  if _typing_extra.is_type_alias_type(origin):\n   return self._type_alias_type_schema(obj)\n  elif _typing_extra.origin_is_union(origin):\n   return self._union_schema(obj)\n  elif origin in TUPLE_TYPES:\n   return self._tuple_schema(obj)\n  elif origin in LIST_TYPES:\n   return self._list_schema(self._get_first_arg_or_any(obj))\n  elif origin in SET_TYPES:\n   return self._set_schema(self._get_first_arg_or_any(obj))\n  elif origin in FROZEN_SET_TYPES:\n   return self._frozenset_schema(self._get_first_arg_or_any(obj))\n  elif origin in DICT_TYPES:\n   return self._dict_schema(*self._get_first_two_args_or_any(obj))\n  elif is_typeddict(origin):\n   return self._typed_dict_schema(obj,origin)\n  elif origin in(typing.Type,type):\n   return self._subclass_schema(obj)\n  elif origin in SEQUENCE_TYPES:\n   return self._sequence_schema(self._get_first_arg_or_any(obj))\n  elif origin in{typing.Iterable,collections.abc.Iterable,typing.Generator,collections.abc.Generator}:\n   return self._iterable_schema(obj)\n  elif origin in(re.Pattern,typing.Pattern):\n   return self._pattern_schema(obj)\n   \n  res=self._get_prepare_pydantic_annotations_for_known_type(obj,())\n  if res is not None:\n   source_type,annotations=res\n   return self._apply_annotations(source_type,annotations)\n   \n  if self._arbitrary_types:\n   return self._arbitrary_type_schema(origin)\n  return self._unknown_type_schema(obj)\n  \n def _generate_td_field_schema(\n self,\n name:str,\n field_info:FieldInfo,\n decorators:DecoratorInfos,\n *,\n required:bool=True,\n )->core_schema.TypedDictField:\n  ''\n  common_field=self._common_field_schema(name,field_info,decorators)\n  return core_schema.typed_dict_field(\n  common_field['schema'],\n  required=False if not field_info.is_required()else required,\n  serialization_exclude=common_field['serialization_exclude'],\n  validation_alias=common_field['validation_alias'],\n  serialization_alias=common_field['serialization_alias'],\n  metadata=common_field['metadata'],\n  )\n  \n def _generate_md_field_schema(\n self,\n name:str,\n field_info:FieldInfo,\n decorators:DecoratorInfos,\n )->core_schema.ModelField:\n  ''\n  common_field=self._common_field_schema(name,field_info,decorators)\n  return core_schema.model_field(\n  common_field['schema'],\n  serialization_exclude=common_field['serialization_exclude'],\n  validation_alias=common_field['validation_alias'],\n  serialization_alias=common_field['serialization_alias'],\n  frozen=common_field['frozen'],\n  metadata=common_field['metadata'],\n  )\n  \n def _generate_dc_field_schema(\n self,\n name:str,\n field_info:FieldInfo,\n decorators:DecoratorInfos,\n )->core_schema.DataclassField:\n  ''\n  common_field=self._common_field_schema(name,field_info,decorators)\n  return core_schema.dataclass_field(\n  name,\n  common_field['schema'],\n  init=field_info.init,\n  init_only=field_info.init_var or None,\n  kw_only=None if field_info.kw_only else False,\n  serialization_exclude=common_field['serialization_exclude'],\n  validation_alias=common_field['validation_alias'],\n  serialization_alias=common_field['serialization_alias'],\n  frozen=common_field['frozen'],\n  metadata=common_field['metadata'],\n  )\n  \n @staticmethod\n def _apply_alias_generator_to_field_info(\n alias_generator:Callable[[str],str]|AliasGenerator,field_info:FieldInfo,field_name:str\n )->None:\n  ''\n\n\n\n\n\n  \n  \n  \n  \n  if(\n  field_info.alias_priority is None\n  or field_info.alias_priority <=1\n  or field_info.alias is None\n  or field_info.validation_alias is None\n  or field_info.serialization_alias is None\n  ):\n   alias,validation_alias,serialization_alias=None,None,None\n   \n   if isinstance(alias_generator,AliasGenerator):\n    alias,validation_alias,serialization_alias=alias_generator.generate_aliases(field_name)\n   elif isinstance(alias_generator,Callable):\n    alias=alias_generator(field_name)\n    if not isinstance(alias,str):\n     raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n     \n     \n     \n     \n   if field_info.alias_priority is None or field_info.alias_priority <=1:\n    field_info.alias_priority=1\n    \n    \n   if field_info.alias_priority ==1:\n    field_info.serialization_alias=_get_first_non_null(serialization_alias,alias)\n    field_info.validation_alias=_get_first_non_null(validation_alias,alias)\n    field_info.alias=alias\n    \n    \n   if field_info.alias is None:\n    field_info.alias=alias\n   if field_info.serialization_alias is None:\n    field_info.serialization_alias=_get_first_non_null(serialization_alias,alias)\n   if field_info.validation_alias is None:\n    field_info.validation_alias=_get_first_non_null(validation_alias,alias)\n    \n @staticmethod\n def _apply_alias_generator_to_computed_field_info(\n alias_generator:Callable[[str],str]|AliasGenerator,\n computed_field_info:ComputedFieldInfo,\n computed_field_name:str,\n ):\n  ''\n\n\n\n\n\n  \n  \n  \n  \n  \n  if(\n  computed_field_info.alias_priority is None\n  or computed_field_info.alias_priority <=1\n  or computed_field_info.alias is None\n  ):\n   alias,validation_alias,serialization_alias=None,None,None\n   \n   if isinstance(alias_generator,AliasGenerator):\n    alias,validation_alias,serialization_alias=alias_generator.generate_aliases(computed_field_name)\n   elif isinstance(alias_generator,Callable):\n    alias=alias_generator(computed_field_name)\n    if not isinstance(alias,str):\n     raise TypeError(f'alias_generator {alias_generator} must return str, not {alias.__class__}')\n     \n     \n     \n     \n   if computed_field_info.alias_priority is None or computed_field_info.alias_priority <=1:\n    computed_field_info.alias_priority=1\n    \n    \n    \n    \n   if computed_field_info.alias_priority ==1:\n    computed_field_info.alias=_get_first_non_null(serialization_alias,alias)\n    \n @staticmethod\n def _apply_field_title_generator_to_field_info(\n config_wrapper:ConfigWrapper,field_info:FieldInfo |ComputedFieldInfo,field_name:str\n )->None:\n  ''\n\n\n\n\n  \n  field_title_generator=field_info.field_title_generator or config_wrapper.field_title_generator\n  \n  if field_title_generator is None:\n   return\n   \n  if field_info.title is None:\n   title=field_title_generator(field_name,field_info)\n   if not isinstance(title,str):\n    raise TypeError(f'field_title_generator {field_title_generator} must return str, not {title.__class__}')\n    \n   field_info.title=title\n   \n def _common_field_schema(\n self,name:str,field_info:FieldInfo,decorators:DecoratorInfos\n )->_CommonField:\n \n  FieldInfo=import_cached_field_info()\n  if not field_info.evaluated:\n  \n   try:\n    evaluated_type=_typing_extra.eval_type(field_info.annotation,*self._types_namespace)\n   except NameError as e:\n    raise PydanticUndefinedAnnotation.from_name_error(e)from e\n   evaluated_type=replace_types(evaluated_type,self._typevars_map)\n   field_info.evaluated=True\n   if not has_instance_in_type(evaluated_type,PydanticRecursiveRef):\n    new_field_info=FieldInfo.from_annotation(evaluated_type)\n    field_info.annotation=new_field_info.annotation\n    \n    \n    for k,v in new_field_info._attributes_set.items():\n    \n    \n    \n    \n     if k not in field_info._attributes_set and k not in field_info.metadata_lookup:\n      setattr(field_info,k,v)\n      \n      \n    field_info.metadata=[*new_field_info.metadata,*field_info.metadata]\n    \n  source_type,annotations=field_info.annotation,field_info.metadata\n  \n  def set_discriminator(schema:CoreSchema)->CoreSchema:\n   schema=self._apply_discriminator_to_union(schema,field_info.discriminator)\n   return schema\n   \n   \n  validators_from_decorators=[]\n  for decorator in filter_field_decorator_info_by_field(decorators.field_validators.values(),name):\n   validators_from_decorators.append(_mode_to_validator[decorator.info.mode]._from_decorator(decorator))\n   \n  with self.field_name_stack.push(name):\n   if field_info.discriminator is not None:\n    schema=self._apply_annotations(\n    source_type,annotations+validators_from_decorators,transform_inner_schema=set_discriminator\n    )\n   else:\n    schema=self._apply_annotations(\n    source_type,\n    annotations+validators_from_decorators,\n    )\n    \n    \n    \n    \n    \n  this_field_validators=filter_field_decorator_info_by_field(decorators.validators.values(),name)\n  if _validators_require_validate_default(this_field_validators):\n   field_info.validate_default=True\n  each_item_validators=[v for v in this_field_validators if v.info.each_item is True]\n  this_field_validators=[v for v in this_field_validators if v not in each_item_validators]\n  schema=apply_each_item_validators(schema,each_item_validators,name)\n  \n  schema=apply_validators(schema,this_field_validators,name)\n  \n  \n  \n  \n  if not field_info.is_required():\n   schema=wrap_default(field_info,schema)\n   \n  schema=self._apply_field_serializers(\n  schema,filter_field_decorator_info_by_field(decorators.field_serializers.values(),name)\n  )\n  self._apply_field_title_generator_to_field_info(self._config_wrapper,field_info,name)\n  \n  pydantic_js_updates,pydantic_js_extra=_extract_json_schema_info_from_field_info(field_info)\n  core_metadata:dict[str,Any]={}\n  update_core_metadata(\n  core_metadata,pydantic_js_updates=pydantic_js_updates,pydantic_js_extra=pydantic_js_extra\n  )\n  \n  alias_generator=self._config_wrapper.alias_generator\n  if alias_generator is not None:\n   self._apply_alias_generator_to_field_info(alias_generator,field_info,name)\n   \n  if isinstance(field_info.validation_alias,(AliasChoices,AliasPath)):\n   validation_alias=field_info.validation_alias.convert_to_aliases()\n  else:\n   validation_alias=field_info.validation_alias\n   \n  return _common_field(\n  schema,\n  serialization_exclude=True if field_info.exclude else None,\n  validation_alias=validation_alias,\n  serialization_alias=field_info.serialization_alias,\n  frozen=field_info.frozen,\n  metadata=core_metadata,\n  )\n  \n def _union_schema(self,union_type:Any)->core_schema.CoreSchema:\n  ''\n  args=self._get_args_resolving_forward_refs(union_type,required=True)\n  choices:list[CoreSchema]=[]\n  nullable=False\n  for arg in args:\n   if arg is None or arg is _typing_extra.NoneType:\n    nullable=True\n   else:\n    choices.append(self.generate_schema(arg))\n    \n  if len(choices)==1:\n   s=choices[0]\n  else:\n   choices_with_tags:list[CoreSchema |tuple[CoreSchema,str]]=[]\n   for choice in choices:\n    tag=choice.get('metadata',{}).get(_core_utils.TAGGED_UNION_TAG_KEY)\n    if tag is not None:\n     choices_with_tags.append((choice,tag))\n    else:\n     choices_with_tags.append(choice)\n   s=core_schema.union_schema(choices_with_tags)\n   \n  if nullable:\n   s=core_schema.nullable_schema(s)\n  return s\n  \n def _type_alias_type_schema(self,obj:TypeAliasType)->CoreSchema:\n  with self.defs.get_schema_or_ref(obj)as(ref,maybe_schema):\n   if maybe_schema is not None:\n    return maybe_schema\n    \n   origin:TypeAliasType=get_origin(obj)or obj\n   typevars_map=get_standard_typevars_map(obj)\n   \n   with self._ns_resolver.push(origin):\n    try:\n     annotation=_typing_extra.eval_type(origin.__value__,*self._types_namespace)\n    except NameError as e:\n     raise PydanticUndefinedAnnotation.from_name_error(e)from e\n    annotation=replace_types(annotation,typevars_map)\n    schema=self.generate_schema(annotation)\n    assert schema['type']!='definitions'\n    schema['ref']=ref\n   self.defs.definitions[ref]=schema\n   return core_schema.definition_reference_schema(ref)\n   \n def _literal_schema(self,literal_type:Any)->CoreSchema:\n  ''\n  expected=_typing_extra.literal_values(literal_type)\n  assert expected,f'literal \"expected\" cannot be empty, obj={literal_type}'\n  schema=core_schema.literal_schema(expected)\n  \n  if self._config_wrapper.use_enum_values and any(isinstance(v,Enum)for v in expected):\n   schema=core_schema.no_info_after_validator_function(\n   lambda v:v.value if isinstance(v,Enum)else v,schema\n   )\n   \n  return schema\n  \n def _typed_dict_schema(self,typed_dict_cls:Any,origin:Any)->core_schema.CoreSchema:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  FieldInfo=import_cached_field_info()\n  \n  with self.model_type_stack.push(typed_dict_cls),self.defs.get_schema_or_ref(typed_dict_cls)as(\n  typed_dict_ref,\n  maybe_schema,\n  ):\n   if maybe_schema is not None:\n    return maybe_schema\n    \n   typevars_map=get_standard_typevars_map(typed_dict_cls)\n   if origin is not None:\n    typed_dict_cls=origin\n    \n   if not _SUPPORTS_TYPEDDICT and type(typed_dict_cls).__module__ =='typing':\n    raise PydanticUserError(\n    'Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.',\n    code='typed-dict-version',\n    )\n    \n   try:\n   \n   \n    config:ConfigDict |None=get_attribute_from_bases(typed_dict_cls,'__pydantic_config__')\n   except AttributeError:\n    config=None\n    \n   with self._config_wrapper_stack.push(config):\n    core_config=self._config_wrapper.core_config(title=typed_dict_cls.__name__)\n    \n    required_keys:frozenset[str]=typed_dict_cls.__required_keys__\n    \n    fields:dict[str,core_schema.TypedDictField]={}\n    \n    decorators=DecoratorInfos.build(typed_dict_cls)\n    \n    if self._config_wrapper.use_attribute_docstrings:\n     field_docstrings=extract_docstrings_from_cls(typed_dict_cls,use_inspect=True)\n    else:\n     field_docstrings=None\n     \n    try:\n     annotations=_typing_extra.get_cls_type_hints(typed_dict_cls,ns_resolver=self._ns_resolver)\n    except NameError as e:\n     raise PydanticUndefinedAnnotation.from_name_error(e)from e\n     \n    for field_name,annotation in annotations.items():\n     annotation=replace_types(annotation,typevars_map)\n     required=field_name in required_keys\n     \n     if _typing_extra.is_required(annotation):\n      required=True\n      annotation=self._get_args_resolving_forward_refs(\n      annotation,\n      required=True,\n      )[0]\n     elif _typing_extra.is_not_required(annotation):\n      required=False\n      annotation=self._get_args_resolving_forward_refs(\n      annotation,\n      required=True,\n      )[0]\n      \n     field_info=FieldInfo.from_annotation(annotation)\n     if(\n     field_docstrings is not None\n     and field_info.description is None\n     and field_name in field_docstrings\n     ):\n      field_info.description=field_docstrings[field_name]\n     self._apply_field_title_generator_to_field_info(self._config_wrapper,field_info,field_name)\n     fields[field_name]=self._generate_td_field_schema(\n     field_name,field_info,decorators,required=required\n     )\n     \n    td_schema=core_schema.typed_dict_schema(\n    fields,\n    cls=typed_dict_cls,\n    computed_fields=[\n    self._computed_field_schema(d,decorators.field_serializers)\n    for d in decorators.computed_fields.values()\n    ],\n    ref=typed_dict_ref,\n    config=core_config,\n    )\n    \n    schema=self._apply_model_serializers(td_schema,decorators.model_serializers.values())\n    schema=apply_model_validators(schema,decorators.model_validators.values(),'all')\n    self.defs.definitions[typed_dict_ref]=schema\n    return core_schema.definition_reference_schema(typed_dict_ref)\n    \n def _namedtuple_schema(self,namedtuple_cls:Any,origin:Any)->core_schema.CoreSchema:\n  ''\n  with self.model_type_stack.push(namedtuple_cls),self.defs.get_schema_or_ref(namedtuple_cls)as(\n  namedtuple_ref,\n  maybe_schema,\n  ):\n   if maybe_schema is not None:\n    return maybe_schema\n   typevars_map=get_standard_typevars_map(namedtuple_cls)\n   if origin is not None:\n    namedtuple_cls=origin\n    \n   try:\n    annotations=_typing_extra.get_cls_type_hints(namedtuple_cls,ns_resolver=self._ns_resolver)\n   except NameError as e:\n    raise PydanticUndefinedAnnotation.from_name_error(e)from e\n   if not annotations:\n   \n    annotations:dict[str,Any]={k:Any for k in namedtuple_cls._fields}\n    \n   if typevars_map:\n    annotations={\n    field_name:replace_types(annotation,typevars_map)\n    for field_name,annotation in annotations.items()\n    }\n    \n   arguments_schema=core_schema.arguments_schema(\n   [\n   self._generate_parameter_schema(\n   field_name,\n   annotation,\n   default=namedtuple_cls._field_defaults.get(field_name,Parameter.empty),\n   )\n   for field_name,annotation in annotations.items()\n   ],\n   metadata={'pydantic_js_prefer_positional_arguments':True},\n   )\n   return core_schema.call_schema(arguments_schema,namedtuple_cls,ref=namedtuple_ref)\n   \n def _generate_parameter_schema(\n self,\n name:str,\n annotation:type[Any],\n default:Any=Parameter.empty,\n mode:Literal['positional_only','positional_or_keyword','keyword_only']|None=None,\n )->core_schema.ArgumentsParameter:\n  ''\n  FieldInfo=import_cached_field_info()\n  \n  if default is Parameter.empty:\n   field=FieldInfo.from_annotation(annotation)\n  else:\n   field=FieldInfo.from_annotated_attribute(annotation,default)\n  assert field.annotation is not None,'field.annotation should not be None when generating a schema'\n  with self.field_name_stack.push(name):\n   schema=self._apply_annotations(field.annotation,[field])\n   \n  if not field.is_required():\n   schema=wrap_default(field,schema)\n   \n  parameter_schema=core_schema.arguments_parameter(name,schema)\n  if mode is not None:\n   parameter_schema['mode']=mode\n  if field.alias is not None:\n   parameter_schema['alias']=field.alias\n  else:\n   alias_generator=self._config_wrapper.alias_generator\n   if isinstance(alias_generator,AliasGenerator)and alias_generator.alias is not None:\n    parameter_schema['alias']=alias_generator.alias(name)\n   elif isinstance(alias_generator,Callable):\n    parameter_schema['alias']=alias_generator(name)\n  return parameter_schema\n  \n def _tuple_schema(self,tuple_type:Any)->core_schema.CoreSchema:\n  ''\n  \n  typevars_map=get_standard_typevars_map(tuple_type)\n  params=self._get_args_resolving_forward_refs(tuple_type)\n  \n  if typevars_map and params:\n   params=tuple(replace_types(param,typevars_map)for param in params)\n   \n   \n   \n  if not params:\n   if tuple_type in TUPLE_TYPES:\n    return core_schema.tuple_schema([core_schema.any_schema()],variadic_item_index=0)\n   else:\n   \n    return core_schema.tuple_schema([])\n  elif params[-1]is Ellipsis:\n   if len(params)==2:\n    return core_schema.tuple_schema([self.generate_schema(params[0])],variadic_item_index=0)\n   else:\n   \n    raise ValueError('Variable tuples can only have one type')\n  elif len(params)==1 and params[0]==():\n  \n  \n   return core_schema.tuple_schema([])\n  else:\n   return core_schema.tuple_schema([self.generate_schema(param)for param in params])\n   \n def _type_schema(self)->core_schema.CoreSchema:\n  return core_schema.custom_error_schema(\n  core_schema.is_instance_schema(type),\n  custom_error_type='is_type',\n  custom_error_message='Input should be a type',\n  )\n  \n def _zoneinfo_schema(self)->core_schema.CoreSchema:\n  ''\n  \n  if sys.version_info <(3,9):\n   assert False,'Unreachable'\n   \n   \n  from zoneinfo import ZoneInfo,ZoneInfoNotFoundError\n  \n  def validate_str_is_valid_iana_tz(value:Any,/)->ZoneInfo:\n   if isinstance(value,ZoneInfo):\n    return value\n   try:\n    return ZoneInfo(value)\n   except(ZoneInfoNotFoundError,ValueError,TypeError):\n    raise PydanticCustomError('zoneinfo_str','invalid timezone: {value}',{'value':value})\n    \n  metadata={'pydantic_js_functions':[lambda _1,_2:{'type':'string','format':'zoneinfo'}]}\n  return core_schema.no_info_plain_validator_function(\n  validate_str_is_valid_iana_tz,\n  serialization=core_schema.to_string_ser_schema(),\n  metadata=metadata,\n  )\n  \n def _union_is_subclass_schema(self,union_type:Any)->core_schema.CoreSchema:\n  ''\n  args=self._get_args_resolving_forward_refs(union_type,required=True)\n  return core_schema.union_schema([self.generate_schema(typing.Type[args])for args in args])\n  \n def _subclass_schema(self,type_:Any)->core_schema.CoreSchema:\n  ''\n  type_param=self._get_first_arg_or_any(type_)\n  \n  \n  type_param=_typing_extra.annotated_type(type_param)or type_param\n  \n  if _typing_extra.is_any(type_param):\n   return self._type_schema()\n  elif _typing_extra.is_type_alias_type(type_param):\n   return self.generate_schema(typing.Type[type_param.__value__])\n  elif isinstance(type_param,typing.TypeVar):\n   if type_param.__bound__:\n    if _typing_extra.origin_is_union(get_origin(type_param.__bound__)):\n     return self._union_is_subclass_schema(type_param.__bound__)\n    return core_schema.is_subclass_schema(type_param.__bound__)\n   elif type_param.__constraints__:\n    return core_schema.union_schema(\n    [self.generate_schema(typing.Type[c])for c in type_param.__constraints__]\n    )\n   else:\n    return self._type_schema()\n  elif _typing_extra.origin_is_union(get_origin(type_param)):\n   return self._union_is_subclass_schema(type_param)\n  else:\n   if _typing_extra.is_self(type_param):\n    type_param=self._resolve_self_type(type_param)\n    \n   if not inspect.isclass(type_param):\n    raise TypeError(f'Expected a class, got {type_param !r}')\n   return core_schema.is_subclass_schema(type_param)\n   \n def _sequence_schema(self,items_type:Any)->core_schema.CoreSchema:\n  ''\n  from._serializers import serialize_sequence_via_list\n  \n  item_type_schema=self.generate_schema(items_type)\n  list_schema=core_schema.list_schema(item_type_schema)\n  \n  json_schema=smart_deepcopy(list_schema)\n  python_schema=core_schema.is_instance_schema(typing.Sequence,cls_repr='Sequence')\n  if not _typing_extra.is_any(items_type):\n   from._validators import sequence_validator\n   \n   python_schema=core_schema.chain_schema(\n   [python_schema,core_schema.no_info_wrap_validator_function(sequence_validator,list_schema)],\n   )\n   \n  serialization=core_schema.wrap_serializer_function_ser_schema(\n  serialize_sequence_via_list,schema=item_type_schema,info_arg=True\n  )\n  return core_schema.json_or_python_schema(\n  json_schema=json_schema,python_schema=python_schema,serialization=serialization\n  )\n  \n def _iterable_schema(self,type_:Any)->core_schema.GeneratorSchema:\n  ''\n  item_type=self._get_first_arg_or_any(type_)\n  \n  return core_schema.generator_schema(self.generate_schema(item_type))\n  \n def _pattern_schema(self,pattern_type:Any)->core_schema.CoreSchema:\n  from. import _validators\n  \n  metadata={'pydantic_js_functions':[lambda _1,_2:{'type':'string','format':'regex'}]}\n  ser=core_schema.plain_serializer_function_ser_schema(\n  attrgetter('pattern'),when_used='json',return_schema=core_schema.str_schema()\n  )\n  if pattern_type is typing.Pattern or pattern_type is re.Pattern:\n  \n   return core_schema.no_info_plain_validator_function(\n   _validators.pattern_either_validator,serialization=ser,metadata=metadata\n   )\n   \n  param=self._get_args_resolving_forward_refs(\n  pattern_type,\n  required=True,\n  )[0]\n  if param is str:\n   return core_schema.no_info_plain_validator_function(\n   _validators.pattern_str_validator,serialization=ser,metadata=metadata\n   )\n  elif param is bytes:\n   return core_schema.no_info_plain_validator_function(\n   _validators.pattern_bytes_validator,serialization=ser,metadata=metadata\n   )\n  else:\n   raise PydanticSchemaGenerationError(f'Unable to generate pydantic-core schema for {pattern_type !r}.')\n   \n def _hashable_schema(self)->core_schema.CoreSchema:\n  return core_schema.custom_error_schema(\n  schema=core_schema.json_or_python_schema(\n  json_schema=core_schema.chain_schema(\n  [core_schema.any_schema(),core_schema.is_instance_schema(collections.abc.Hashable)]\n  ),\n  python_schema=core_schema.is_instance_schema(collections.abc.Hashable),\n  ),\n  custom_error_type='is_hashable',\n  custom_error_message='Input should be hashable',\n  )\n  \n def _dataclass_schema(\n self,dataclass:type[StandardDataclass],origin:type[StandardDataclass]|None\n )->core_schema.CoreSchema:\n  ''\n  with self.model_type_stack.push(dataclass),self.defs.get_schema_or_ref(dataclass)as(\n  dataclass_ref,\n  maybe_schema,\n  ):\n   if maybe_schema is not None:\n    return maybe_schema\n    \n   typevars_map=get_standard_typevars_map(dataclass)\n   if origin is not None:\n    dataclass=origin\n    \n    \n    \n    \n   config=getattr(dataclass,'__pydantic_config__',None)\n   \n   from..dataclasses import is_pydantic_dataclass\n   \n   with self._ns_resolver.push(dataclass),self._config_wrapper_stack.push(config):\n    if is_pydantic_dataclass(dataclass):\n     fields=deepcopy(dataclass.__pydantic_fields__)\n     if typevars_map:\n      for field in fields.values():\n       field.apply_typevars_map(typevars_map,*self._types_namespace)\n    else:\n     fields=collect_dataclass_fields(\n     dataclass,\n     typevars_map=typevars_map,\n     )\n     \n    if self._config_wrapper.extra =='allow':\n    \n     for field_name,field in fields.items():\n      if field.init is False:\n       raise PydanticUserError(\n       f'Field {field_name} has `init=False` and dataclass has config setting `extra=\"allow\"`. '\n       f'This combination is not allowed.',\n       code='dataclass-init-false-extra-allow',\n       )\n       \n    decorators=dataclass.__dict__.get('__pydantic_decorators__')or DecoratorInfos.build(dataclass)\n    \n    \n    args=sorted(\n    (self._generate_dc_field_schema(k,v,decorators)for k,v in fields.items()),\n    key=lambda a:a.get('kw_only')is not False,\n    )\n    has_post_init=hasattr(dataclass,'__post_init__')\n    has_slots=hasattr(dataclass,'__slots__')\n    \n    args_schema=core_schema.dataclass_args_schema(\n    dataclass.__name__,\n    args,\n    computed_fields=[\n    self._computed_field_schema(d,decorators.field_serializers)\n    for d in decorators.computed_fields.values()\n    ],\n    collect_init_only=has_post_init,\n    )\n    \n    inner_schema=apply_validators(args_schema,decorators.root_validators.values(),None)\n    \n    model_validators=decorators.model_validators.values()\n    inner_schema=apply_model_validators(inner_schema,model_validators,'inner')\n    \n    core_config=self._config_wrapper.core_config(title=dataclass.__name__)\n    \n    dc_schema=core_schema.dataclass_schema(\n    dataclass,\n    inner_schema,\n    generic_origin=origin,\n    post_init=has_post_init,\n    ref=dataclass_ref,\n    fields=[field.name for field in dataclasses.fields(dataclass)],\n    slots=has_slots,\n    config=core_config,\n    \n    \n    frozen=self._config_wrapper_stack.tail.frozen,\n    )\n    schema=self._apply_model_serializers(dc_schema,decorators.model_serializers.values())\n    schema=apply_model_validators(schema,model_validators,'outer')\n    self.defs.definitions[dataclass_ref]=schema\n    return core_schema.definition_reference_schema(dataclass_ref)\n    \n def _call_schema(self,function:ValidateCallSupportedTypes)->core_schema.CallSchema:\n  ''\n\n\n  \n  sig=signature(function)\n  globalns,localns=self._types_namespace\n  type_hints=_typing_extra.get_function_type_hints(function,globalns=globalns,localns=localns)\n  \n  mode_lookup:dict[_ParameterKind,Literal['positional_only','positional_or_keyword','keyword_only']]={\n  Parameter.POSITIONAL_ONLY:'positional_only',\n  Parameter.POSITIONAL_OR_KEYWORD:'positional_or_keyword',\n  Parameter.KEYWORD_ONLY:'keyword_only',\n  }\n  \n  arguments_list:list[core_schema.ArgumentsParameter]=[]\n  var_args_schema:core_schema.CoreSchema |None=None\n  var_kwargs_schema:core_schema.CoreSchema |None=None\n  var_kwargs_mode:core_schema.VarKwargsMode |None=None\n  \n  for name,p in sig.parameters.items():\n   if p.annotation is sig.empty:\n    annotation=typing.cast(Any,Any)\n   else:\n    annotation=type_hints[name]\n    \n   parameter_mode=mode_lookup.get(p.kind)\n   if parameter_mode is not None:\n    arg_schema=self._generate_parameter_schema(name,annotation,p.default,parameter_mode)\n    arguments_list.append(arg_schema)\n   elif p.kind ==Parameter.VAR_POSITIONAL:\n    var_args_schema=self.generate_schema(annotation)\n   else:\n    assert p.kind ==Parameter.VAR_KEYWORD,p.kind\n    \n    unpack_type=_typing_extra.unpack_type(annotation)\n    if unpack_type is not None:\n     if not is_typeddict(unpack_type):\n      raise PydanticUserError(\n      f'Expected a `TypedDict` class, got {unpack_type.__name__ !r}',code='unpack-typed-dict'\n      )\n     non_pos_only_param_names={\n     name for name,p in sig.parameters.items()if p.kind !=Parameter.POSITIONAL_ONLY\n     }\n     overlapping_params=non_pos_only_param_names.intersection(unpack_type.__annotations__)\n     if overlapping_params:\n      raise PydanticUserError(\n      f'Typed dictionary {unpack_type.__name__ !r} overlaps with parameter'\n      f\"{'s'if len(overlapping_params)>=2 else ''} \"\n      f\"{', '.join(repr(p)for p in sorted(overlapping_params))}\",\n      code='overlapping-unpack-typed-dict',\n      )\n      \n     var_kwargs_mode='unpacked-typed-dict'\n     var_kwargs_schema=self._typed_dict_schema(unpack_type,None)\n    else:\n     var_kwargs_mode='uniform'\n     var_kwargs_schema=self.generate_schema(annotation)\n     \n  return_schema:core_schema.CoreSchema |None=None\n  config_wrapper=self._config_wrapper\n  if config_wrapper.validate_return:\n   return_hint=sig.return_annotation\n   if return_hint is not sig.empty:\n    return_schema=self.generate_schema(return_hint)\n    \n  return core_schema.call_schema(\n  core_schema.arguments_schema(\n  arguments_list,\n  var_args_schema=var_args_schema,\n  var_kwargs_mode=var_kwargs_mode,\n  var_kwargs_schema=var_kwargs_schema,\n  populate_by_name=config_wrapper.populate_by_name,\n  ),\n  function,\n  return_schema=return_schema,\n  )\n  \n def _unsubstituted_typevar_schema(self,typevar:typing.TypeVar)->core_schema.CoreSchema:\n  assert isinstance(typevar,typing.TypeVar)\n  \n  bound=typevar.__bound__\n  constraints=typevar.__constraints__\n  \n  try:\n   typevar_has_default=typevar.has_default()\n  except AttributeError:\n  \n   typevar_has_default=getattr(typevar,'__default__',None)is not None\n   \n  if(bound is not None)+(len(constraints)!=0)+typevar_has_default >1:\n   raise NotImplementedError(\n   'Pydantic does not support mixing more than one of TypeVar bounds, constraints and defaults'\n   )\n   \n  if typevar_has_default:\n   return self.generate_schema(typevar.__default__)\n  elif constraints:\n   return self._union_schema(typing.Union[constraints])\n  elif bound:\n   schema=self.generate_schema(bound)\n   schema['serialization']=core_schema.wrap_serializer_function_ser_schema(\n   lambda x,h:h(x),schema=core_schema.any_schema()\n   )\n   return schema\n  else:\n   return core_schema.any_schema()\n   \n def _computed_field_schema(\n self,\n d:Decorator[ComputedFieldInfo],\n field_serializers:dict[str,Decorator[FieldSerializerDecoratorInfo]],\n )->core_schema.ComputedField:\n  try:\n  \n  \n  \n   return_type=_decorators.get_function_return_type(\n   d.func,d.info.return_type,localns=self._types_namespace.locals\n   )\n  except NameError as e:\n   raise PydanticUndefinedAnnotation.from_name_error(e)from e\n  if return_type is PydanticUndefined:\n   raise PydanticUserError(\n   'Computed field is missing return type annotation or specifying `return_type`'\n   ' to the `@computed_field` decorator (e.g. `@computed_field(return_type=int|str)`)',\n   code='model-field-missing-annotation',\n   )\n   \n  return_type=replace_types(return_type,self._typevars_map)\n  \n  \n  d.info=dataclasses.replace(d.info,return_type=return_type)\n  return_type_schema=self.generate_schema(return_type)\n  \n  return_type_schema=self._apply_field_serializers(\n  return_type_schema,\n  filter_field_decorator_info_by_field(field_serializers.values(),d.cls_var_name),\n  )\n  \n  alias_generator=self._config_wrapper.alias_generator\n  if alias_generator is not None:\n   self._apply_alias_generator_to_computed_field_info(\n   alias_generator=alias_generator,computed_field_info=d.info,computed_field_name=d.cls_var_name\n   )\n  self._apply_field_title_generator_to_field_info(self._config_wrapper,d.info,d.cls_var_name)\n  \n  pydantic_js_updates,pydantic_js_extra=_extract_json_schema_info_from_field_info(d.info)\n  core_metadata:dict[str,Any]={}\n  update_core_metadata(\n  core_metadata,\n  pydantic_js_updates={'readOnly':True,**(pydantic_js_updates if pydantic_js_updates else{})},\n  pydantic_js_extra=pydantic_js_extra,\n  )\n  return core_schema.computed_field(\n  d.cls_var_name,return_schema=return_type_schema,alias=d.info.alias,metadata=core_metadata\n  )\n  \n def _annotated_schema(self,annotated_type:Any)->core_schema.CoreSchema:\n  ''\n  FieldInfo=import_cached_field_info()\n  \n  source_type,*annotations=self._get_args_resolving_forward_refs(\n  annotated_type,\n  required=True,\n  )\n  schema=self._apply_annotations(source_type,annotations)\n  \n  \n  for annotation in annotations:\n   if isinstance(annotation,FieldInfo):\n    schema=wrap_default(annotation,schema)\n  return schema\n  \n def _get_prepare_pydantic_annotations_for_known_type(\n self,obj:Any,annotations:tuple[Any,...]\n )->tuple[Any,list[Any]]|None:\n  from._std_types_schema import(\n  deque_schema_prepare_pydantic_annotations,\n  mapping_like_prepare_pydantic_annotations,\n  path_schema_prepare_pydantic_annotations,\n  )\n  \n  \n  try:\n   hash(obj)\n  except TypeError:\n  \n   return None\n   \n   \n   \n   \n  obj_origin=get_origin(obj)or obj\n  \n  if obj_origin in PATH_TYPES:\n   return path_schema_prepare_pydantic_annotations(obj,annotations)\n  elif obj_origin in DEQUE_TYPES:\n   return deque_schema_prepare_pydantic_annotations(obj,annotations)\n  elif obj_origin in MAPPING_TYPES:\n   return mapping_like_prepare_pydantic_annotations(obj,annotations)\n  else:\n   return None\n   \n def _apply_annotations(\n self,\n source_type:Any,\n annotations:list[Any],\n transform_inner_schema:Callable[[CoreSchema],CoreSchema]=lambda x:x,\n )->CoreSchema:\n  ''\n\n\n\n\n  \n  annotations=list(_known_annotated_metadata.expand_grouped_metadata(annotations))\n  res=self._get_prepare_pydantic_annotations_for_known_type(source_type,tuple(annotations))\n  if res is not None:\n   source_type,annotations=res\n   \n  pydantic_js_annotation_functions:list[GetJsonSchemaFunction]=[]\n  \n  def inner_handler(obj:Any)->CoreSchema:\n   from_property=self._generate_schema_from_property(obj,source_type)\n   if from_property is None:\n    schema=self._generate_schema_inner(obj)\n   else:\n    schema=from_property\n   metadata_js_function=_extract_get_pydantic_json_schema(obj,schema)\n   if metadata_js_function is not None:\n    metadata_schema=resolve_original_schema(schema,self.defs.definitions)\n    if metadata_schema is not None:\n     self._add_js_function(metadata_schema,metadata_js_function)\n   return transform_inner_schema(schema)\n   \n  get_inner_schema=CallbackGetCoreSchemaHandler(inner_handler,self)\n  \n  for annotation in annotations:\n   if annotation is None:\n    continue\n   get_inner_schema=self._get_wrapped_inner_schema(\n   get_inner_schema,annotation,pydantic_js_annotation_functions\n   )\n   \n  schema=get_inner_schema(source_type)\n  if pydantic_js_annotation_functions:\n   core_metadata=schema.setdefault('metadata',{})\n   update_core_metadata(core_metadata,pydantic_js_annotation_functions=pydantic_js_annotation_functions)\n  return _add_custom_serialization_from_json_encoders(self._config_wrapper.json_encoders,source_type,schema)\n  \n def _apply_single_annotation(self,schema:core_schema.CoreSchema,metadata:Any)->core_schema.CoreSchema:\n  FieldInfo=import_cached_field_info()\n  \n  if isinstance(metadata,FieldInfo):\n   for field_metadata in metadata.metadata:\n    schema=self._apply_single_annotation(schema,field_metadata)\n    \n   if metadata.discriminator is not None:\n    schema=self._apply_discriminator_to_union(schema,metadata.discriminator)\n   return schema\n   \n  if schema['type']=='nullable':\n  \n   inner=schema.get('schema',core_schema.any_schema())\n   inner=self._apply_single_annotation(inner,metadata)\n   if inner:\n    schema['schema']=inner\n   return schema\n   \n  original_schema=schema\n  ref=schema.get('ref',None)\n  if ref is not None:\n   schema=schema.copy()\n   new_ref=ref+f'_{repr(metadata)}'\n   if new_ref in self.defs.definitions:\n    return self.defs.definitions[new_ref]\n   schema['ref']=new_ref\n  elif schema['type']=='definition-ref':\n   ref=schema['schema_ref']\n   if ref in self.defs.definitions:\n    schema=self.defs.definitions[ref].copy()\n    new_ref=ref+f'_{repr(metadata)}'\n    if new_ref in self.defs.definitions:\n     return self.defs.definitions[new_ref]\n    schema['ref']=new_ref\n    \n  maybe_updated_schema=_known_annotated_metadata.apply_known_metadata(metadata,schema.copy())\n  \n  if maybe_updated_schema is not None:\n   return maybe_updated_schema\n  return original_schema\n  \n def _apply_single_annotation_json_schema(\n self,schema:core_schema.CoreSchema,metadata:Any\n )->core_schema.CoreSchema:\n  FieldInfo=import_cached_field_info()\n  \n  if isinstance(metadata,FieldInfo):\n   for field_metadata in metadata.metadata:\n    schema=self._apply_single_annotation_json_schema(schema,field_metadata)\n    \n   pydantic_js_updates,pydantic_js_extra=_extract_json_schema_info_from_field_info(metadata)\n   core_metadata=schema.setdefault('metadata',{})\n   update_core_metadata(\n   core_metadata,pydantic_js_updates=pydantic_js_updates,pydantic_js_extra=pydantic_js_extra\n   )\n  return schema\n  \n def _get_wrapped_inner_schema(\n self,\n get_inner_schema:GetCoreSchemaHandler,\n annotation:Any,\n pydantic_js_annotation_functions:list[GetJsonSchemaFunction],\n )->CallbackGetCoreSchemaHandler:\n  metadata_get_schema:GetCoreSchemaFunction=getattr(annotation,'__get_pydantic_core_schema__',None)or(\n  lambda source,handler:handler(source)\n  )\n  \n  def new_handler(source:Any)->core_schema.CoreSchema:\n   schema=metadata_get_schema(source,get_inner_schema)\n   schema=self._apply_single_annotation(schema,annotation)\n   schema=self._apply_single_annotation_json_schema(schema,annotation)\n   \n   metadata_js_function=_extract_get_pydantic_json_schema(annotation,schema)\n   if metadata_js_function is not None:\n    pydantic_js_annotation_functions.append(metadata_js_function)\n   return schema\n   \n  return CallbackGetCoreSchemaHandler(new_handler,self)\n  \n def _apply_field_serializers(\n self,\n schema:core_schema.CoreSchema,\n serializers:list[Decorator[FieldSerializerDecoratorInfo]],\n )->core_schema.CoreSchema:\n  ''\n  if serializers:\n   schema=copy(schema)\n   if schema['type']=='definitions':\n    inner_schema=schema['schema']\n    schema['schema']=self._apply_field_serializers(inner_schema,serializers)\n    return schema\n   else:\n    ref=typing.cast('str|None',schema.get('ref',None))\n    if ref is not None:\n     self.defs.definitions[ref]=schema\n     schema=core_schema.definition_reference_schema(ref)\n     \n     \n   serializer=serializers[-1]\n   is_field_serializer,info_arg=inspect_field_serializer(serializer.func,serializer.info.mode)\n   \n   try:\n   \n   \n   \n    return_type=_decorators.get_function_return_type(\n    serializer.func,serializer.info.return_type,localns=self._types_namespace.locals\n    )\n   except NameError as e:\n    raise PydanticUndefinedAnnotation.from_name_error(e)from e\n    \n   if return_type is PydanticUndefined:\n    return_schema=None\n   else:\n    return_schema=self.generate_schema(return_type)\n    \n   if serializer.info.mode =='wrap':\n    schema['serialization']=core_schema.wrap_serializer_function_ser_schema(\n    serializer.func,\n    is_field_serializer=is_field_serializer,\n    info_arg=info_arg,\n    return_schema=return_schema,\n    when_used=serializer.info.when_used,\n    )\n   else:\n    assert serializer.info.mode =='plain'\n    schema['serialization']=core_schema.plain_serializer_function_ser_schema(\n    serializer.func,\n    is_field_serializer=is_field_serializer,\n    info_arg=info_arg,\n    return_schema=return_schema,\n    when_used=serializer.info.when_used,\n    )\n  return schema\n  \n def _apply_model_serializers(\n self,schema:core_schema.CoreSchema,serializers:Iterable[Decorator[ModelSerializerDecoratorInfo]]\n )->core_schema.CoreSchema:\n  ''\n  ref:str |None=schema.pop('ref',None)\n  if serializers:\n   serializer=list(serializers)[-1]\n   info_arg=inspect_model_serializer(serializer.func,serializer.info.mode)\n   \n   try:\n   \n   \n   \n    return_type=_decorators.get_function_return_type(\n    serializer.func,serializer.info.return_type,localns=self._types_namespace.locals\n    )\n   except NameError as e:\n    raise PydanticUndefinedAnnotation.from_name_error(e)from e\n   if return_type is PydanticUndefined:\n    return_schema=None\n   else:\n    return_schema=self.generate_schema(return_type)\n    \n   if serializer.info.mode =='wrap':\n    ser_schema:core_schema.SerSchema=core_schema.wrap_serializer_function_ser_schema(\n    serializer.func,\n    info_arg=info_arg,\n    return_schema=return_schema,\n    when_used=serializer.info.when_used,\n    )\n   else:\n   \n    ser_schema=core_schema.plain_serializer_function_ser_schema(\n    serializer.func,\n    info_arg=info_arg,\n    return_schema=return_schema,\n    when_used=serializer.info.when_used,\n    )\n   schema['serialization']=ser_schema\n  if ref:\n   schema['ref']=ref\n  return schema\n  \n  \n_VALIDATOR_F_MATCH:Mapping[\ntuple[FieldValidatorModes,Literal['no-info','with-info']],\nCallable[[Callable[...,Any],core_schema.CoreSchema,str |None],core_schema.CoreSchema],\n]={\n('before','no-info'):lambda f,schema,_:core_schema.no_info_before_validator_function(f,schema),\n('after','no-info'):lambda f,schema,_:core_schema.no_info_after_validator_function(f,schema),\n('plain','no-info'):lambda f,_1,_2:core_schema.no_info_plain_validator_function(f),\n('wrap','no-info'):lambda f,schema,_:core_schema.no_info_wrap_validator_function(f,schema),\n('before','with-info'):lambda f,schema,field_name:core_schema.with_info_before_validator_function(\nf,schema,field_name=field_name\n),\n('after','with-info'):lambda f,schema,field_name:core_schema.with_info_after_validator_function(\nf,schema,field_name=field_name\n),\n('plain','with-info'):lambda f,_,field_name:core_schema.with_info_plain_validator_function(\nf,field_name=field_name\n),\n('wrap','with-info'):lambda f,schema,field_name:core_schema.with_info_wrap_validator_function(\nf,schema,field_name=field_name\n),\n}\n\n\n\n\ndef apply_validators(\nschema:core_schema.CoreSchema,\nvalidators:Iterable[Decorator[RootValidatorDecoratorInfo]]\n|Iterable[Decorator[ValidatorDecoratorInfo]]\n|Iterable[Decorator[FieldValidatorDecoratorInfo]],\nfield_name:str |None,\n)->core_schema.CoreSchema:\n ''\n\n\n\n\n\n\n\n\n \n for validator in validators:\n  info_arg=inspect_validator(validator.func,validator.info.mode)\n  val_type='with-info'if info_arg else 'no-info'\n  \n  schema=_VALIDATOR_F_MATCH[(validator.info.mode,val_type)](validator.func,schema,field_name)\n return schema\n \n \ndef _validators_require_validate_default(validators:Iterable[Decorator[ValidatorDecoratorInfo]])->bool:\n ''\n\n\n\n\n\n\n\n \n for validator in validators:\n  if validator.info.always:\n   return True\n return False\n \n \ndef apply_model_validators(\nschema:core_schema.CoreSchema,\nvalidators:Iterable[Decorator[ModelValidatorDecoratorInfo]],\nmode:Literal['inner','outer','all'],\n)->core_schema.CoreSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n ref:str |None=schema.pop('ref',None)\n for validator in validators:\n  if mode =='inner'and validator.info.mode !='before':\n   continue\n  if mode =='outer'and validator.info.mode =='before':\n   continue\n  info_arg=inspect_validator(validator.func,validator.info.mode)\n  if validator.info.mode =='wrap':\n   if info_arg:\n    schema=core_schema.with_info_wrap_validator_function(function=validator.func,schema=schema)\n   else:\n    schema=core_schema.no_info_wrap_validator_function(function=validator.func,schema=schema)\n  elif validator.info.mode =='before':\n   if info_arg:\n    schema=core_schema.with_info_before_validator_function(function=validator.func,schema=schema)\n   else:\n    schema=core_schema.no_info_before_validator_function(function=validator.func,schema=schema)\n  else:\n   assert validator.info.mode =='after'\n   if info_arg:\n    schema=core_schema.with_info_after_validator_function(function=validator.func,schema=schema)\n   else:\n    schema=core_schema.no_info_after_validator_function(function=validator.func,schema=schema)\n if ref:\n  schema['ref']=ref\n return schema\n \n \ndef wrap_default(field_info:FieldInfo,schema:core_schema.CoreSchema)->core_schema.CoreSchema:\n ''\n\n\n\n\n\n\n\n \n if field_info.default_factory:\n  return core_schema.with_default_schema(\n  schema,\n  default_factory=field_info.default_factory,\n  default_factory_takes_data=takes_validated_data_argument(field_info.default_factory),\n  validate_default=field_info.validate_default,\n  )\n elif field_info.default is not PydanticUndefined:\n  return core_schema.with_default_schema(\n  schema,default=field_info.default,validate_default=field_info.validate_default\n  )\n else:\n  return schema\n  \n  \ndef _extract_get_pydantic_json_schema(tp:Any,schema:CoreSchema)->GetJsonSchemaFunction |None:\n ''\n js_modify_function=getattr(tp,'__get_pydantic_json_schema__',None)\n \n if hasattr(tp,'__modify_schema__'):\n  BaseModel=import_cached_base_model()\n  \n  has_custom_v2_modify_js_func=(\n  js_modify_function is not None\n  and BaseModel.__get_pydantic_json_schema__.__func__\n  not in(js_modify_function,getattr(js_modify_function,'__func__',None))\n  )\n  \n  if not has_custom_v2_modify_js_func:\n   cls_name=getattr(tp,'__name__',None)\n   raise PydanticUserError(\n   f'The `__modify_schema__` method is not supported in Pydantic v2. '\n   f'Use `__get_pydantic_json_schema__` instead{f\" in class `{cls_name}`\"if cls_name else \"\"}.',\n   code='custom-json-schema',\n   )\n   \n   \n if hasattr(tp,'__origin__')and not _typing_extra.is_annotated(tp):\n  return _extract_get_pydantic_json_schema(tp.__origin__,schema)\n  \n if js_modify_function is None:\n  return None\n  \n return js_modify_function\n \n \nclass _CommonField(TypedDict):\n schema:core_schema.CoreSchema\n validation_alias:str |list[str |int]|list[list[str |int]]|None\n serialization_alias:str |None\n serialization_exclude:bool |None\n frozen:bool |None\n metadata:dict[str,Any]\n \n \ndef _common_field(\nschema:core_schema.CoreSchema,\n*,\nvalidation_alias:str |list[str |int]|list[list[str |int]]|None=None,\nserialization_alias:str |None=None,\nserialization_exclude:bool |None=None,\nfrozen:bool |None=None,\nmetadata:Any=None,\n)->_CommonField:\n return{\n 'schema':schema,\n 'validation_alias':validation_alias,\n 'serialization_alias':serialization_alias,\n 'serialization_exclude':serialization_exclude,\n 'frozen':frozen,\n 'metadata':metadata,\n }\n \n \nclass _Definitions:\n ''\n \n def __init__(self)->None:\n  self.seen:set[str]=set()\n  self.definitions:dict[str,core_schema.CoreSchema]={}\n  \n @contextmanager\n def get_schema_or_ref(self,tp:Any)->Iterator[tuple[str,None]|tuple[str,CoreSchema]]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  ref=get_type_ref(tp)\n  \n  if ref in self.seen or ref in self.definitions:\n   yield(ref,core_schema.definition_reference_schema(ref))\n  else:\n   self.seen.add(ref)\n   try:\n    yield(ref,None)\n   finally:\n    self.seen.discard(ref)\n    \n    \ndef resolve_original_schema(schema:CoreSchema,definitions:dict[str,CoreSchema])->CoreSchema |None:\n if schema['type']=='definition-ref':\n  return definitions.get(schema['schema_ref'],None)\n elif schema['type']=='definitions':\n  return schema['schema']\n else:\n  return schema\n  \n  \nclass _FieldNameStack:\n __slots__=('_stack',)\n \n def __init__(self)->None:\n  self._stack:list[str]=[]\n  \n @contextmanager\n def push(self,field_name:str)->Iterator[None]:\n  self._stack.append(field_name)\n  yield\n  self._stack.pop()\n  \n def get(self)->str |None:\n  if self._stack:\n   return self._stack[-1]\n  else:\n   return None\n   \n   \nclass _ModelTypeStack:\n __slots__=('_stack',)\n \n def __init__(self)->None:\n  self._stack:list[type]=[]\n  \n @contextmanager\n def push(self,type_obj:type)->Iterator[None]:\n  self._stack.append(type_obj)\n  yield\n  self._stack.pop()\n  \n def get(self)->type |None:\n  if self._stack:\n   return self._stack[-1]\n  else:\n   return None\n", ["__future__", "collections.abc", "contextlib", "copy", "dataclasses", "datetime", "decimal", "enum", "fractions", "functools", "inspect", "ipaddress", "itertools", "operator", "os", "pathlib", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._core_metadata", "pydantic._internal._core_utils", "pydantic._internal._dataclasses", "pydantic._internal._decorators", "pydantic._internal._discriminated_union", "pydantic._internal._docs_extraction", "pydantic._internal._fields", "pydantic._internal._forward_ref", "pydantic._internal._generics", "pydantic._internal._import_utils", "pydantic._internal._known_annotated_metadata", "pydantic._internal._mock_val_ser", "pydantic._internal._namespace_utils", "pydantic._internal._schema_generation_shared", "pydantic._internal._serializers", "pydantic._internal._std_types_schema", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic._internal._validators", "pydantic.aliases", "pydantic.annotated_handlers", "pydantic.config", "pydantic.dataclasses", "pydantic.errors", "pydantic.fields", "pydantic.functional_validators", "pydantic.json_schema", "pydantic.main", "pydantic.types", "pydantic.v1", "pydantic.version", "pydantic.warnings", "pydantic_core", "pydantic_core.core_schema", "re", "sys", "types", "typing", "typing_extensions", "uuid", "warnings", "zoneinfo"]], "pydantic._internal._core_metadata": [".py", "from __future__ import annotations as _annotations\n\nfrom typing import TYPE_CHECKING,Any,TypedDict,cast\nfrom warnings import warn\n\nif TYPE_CHECKING:\n from..config import JsonDict,JsonSchemaExtraCallable\n from._schema_generation_shared import(\n GetJsonSchemaFunction,\n )\n \n \nclass CoreMetadata(TypedDict,total=False):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n pydantic_js_functions:list[GetJsonSchemaFunction]\n pydantic_js_annotation_functions:list[GetJsonSchemaFunction]\n pydantic_js_prefer_positional_arguments:bool\n pydantic_js_updates:JsonDict\n pydantic_js_extra:JsonDict |JsonSchemaExtraCallable\n \n \ndef update_core_metadata(\ncore_metadata:Any,\n/,\n*,\npydantic_js_functions:list[GetJsonSchemaFunction]|None=None,\npydantic_js_annotation_functions:list[GetJsonSchemaFunction]|None=None,\npydantic_js_updates:JsonDict |None=None,\npydantic_js_extra:JsonDict |JsonSchemaExtraCallable |None=None,\n)->None:\n from..json_schema import PydanticJsonSchemaWarning\n \n \"\"\"Update CoreMetadata instance in place. When we make modifications in this function, they\n    take effect on the `core_metadata` reference passed in as the first (and only) positional argument.\n\n    First, cast to `CoreMetadata`, then finish with a cast to `dict[str, Any]` for core schema compatibility.\n    We do this here, instead of before / after each call to this function so that this typing hack\n    can be easily removed if/when we move `CoreMetadata` to `pydantic-core`.\n\n    For parameter descriptions, see `CoreMetadata` above.\n    \"\"\"\n core_metadata=cast(CoreMetadata,core_metadata)\n \n if pydantic_js_functions:\n  core_metadata.setdefault('pydantic_js_functions',[]).extend(pydantic_js_functions)\n  \n if pydantic_js_annotation_functions:\n  core_metadata.setdefault('pydantic_js_annotation_functions',[]).extend(pydantic_js_annotation_functions)\n  \n if pydantic_js_updates:\n  if(existing_updates :=core_metadata.get('pydantic_js_updates'))is not None:\n   core_metadata['pydantic_js_updates']={**existing_updates,**pydantic_js_updates}\n  else:\n   core_metadata['pydantic_js_updates']=pydantic_js_updates\n   \n if pydantic_js_extra is not None:\n  existing_pydantic_js_extra=core_metadata.get('pydantic_js_extra')\n  if existing_pydantic_js_extra is None:\n   core_metadata['pydantic_js_extra']=pydantic_js_extra\n  if isinstance(existing_pydantic_js_extra,dict):\n   if isinstance(pydantic_js_extra,dict):\n    core_metadata['pydantic_js_extra']={**existing_pydantic_js_extra,**pydantic_js_extra}\n   if callable(pydantic_js_extra):\n    warn(\n    'Composing `dict` and `callable` type `json_schema_extra` is not supported.'\n    'The `callable` type is being ignored.'\n    \"If you'd like support for this behavior, please open an issue on pydantic.\",\n    PydanticJsonSchemaWarning,\n    )\n  if callable(existing_pydantic_js_extra):\n  \n   core_metadata['pydantic_js_extra']=pydantic_js_extra\n", ["__future__", "pydantic._internal._schema_generation_shared", "pydantic.config", "pydantic.json_schema", "typing", "warnings"]], "pydantic._internal._decorators_v1": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nfrom inspect import Parameter,signature\nfrom typing import Any,Dict,Tuple,Union,cast\n\nfrom pydantic_core import core_schema\nfrom typing_extensions import Protocol\n\nfrom..errors import PydanticUserError\nfrom._utils import can_be_positional\n\n\nclass V1OnlyValueValidator(Protocol):\n ''\n \n def __call__(self,__value:Any)->Any:...\n \n \nclass V1ValidatorWithValues(Protocol):\n ''\n \n def __call__(self,__value:Any,values:dict[str,Any])->Any:...\n \n \nclass V1ValidatorWithValuesKwOnly(Protocol):\n ''\n \n def __call__(self,__value:Any,*,values:dict[str,Any])->Any:...\n \n \nclass V1ValidatorWithKwargs(Protocol):\n ''\n \n def __call__(self,__value:Any,**kwargs:Any)->Any:...\n \n \nclass V1ValidatorWithValuesAndKwargs(Protocol):\n ''\n \n def __call__(self,__value:Any,values:dict[str,Any],**kwargs:Any)->Any:...\n \n \nV1Validator=Union[\nV1ValidatorWithValues,V1ValidatorWithValuesKwOnly,V1ValidatorWithKwargs,V1ValidatorWithValuesAndKwargs\n]\n\n\ndef can_be_keyword(param:Parameter)->bool:\n return param.kind in(Parameter.POSITIONAL_OR_KEYWORD,Parameter.KEYWORD_ONLY)\n \n \ndef make_generic_v1_field_validator(validator:V1Validator)->core_schema.WithInfoValidatorFunction:\n ''\n\n\n\n\n\n\n\n\n\n\n \n sig=signature(validator)\n \n needs_values_kw=False\n \n for param_num,(param_name,parameter)in enumerate(sig.parameters.items()):\n  if can_be_keyword(parameter)and param_name in('field','config'):\n   raise PydanticUserError(\n   'The `field` and `config` parameters are not available in Pydantic V2, '\n   'please use the `info` parameter instead.',\n   code='validator-field-config-info',\n   )\n  if parameter.kind is Parameter.VAR_KEYWORD:\n   needs_values_kw=True\n  elif can_be_keyword(parameter)and param_name =='values':\n   needs_values_kw=True\n  elif can_be_positional(parameter)and param_num ==0:\n  \n   continue\n  elif parameter.default is Parameter.empty:\n   raise PydanticUserError(\n   f'Unsupported signature for V1 style validator {validator}: {sig} is not supported.',\n   code='validator-v1-signature',\n   )\n   \n if needs_values_kw:\n \n  val1=cast(V1ValidatorWithValues,validator)\n  \n  def wrapper1(value:Any,info:core_schema.ValidationInfo)->Any:\n   return val1(value,values=info.data)\n   \n  return wrapper1\n else:\n  val2=cast(V1OnlyValueValidator,validator)\n  \n  def wrapper2(value:Any,_:core_schema.ValidationInfo)->Any:\n   return val2(value)\n   \n  return wrapper2\n  \n  \nRootValidatorValues=Dict[str,Any]\n\nRootValidatorFieldsTuple=Tuple[Any,...]\n\n\nclass V1RootValidatorFunction(Protocol):\n ''\n \n def __call__(self,__values:RootValidatorValues)->RootValidatorValues:...\n \n \nclass V2CoreBeforeRootValidator(Protocol):\n ''\n \n def __call__(self,__values:RootValidatorValues,__info:core_schema.ValidationInfo)->RootValidatorValues:...\n \n \nclass V2CoreAfterRootValidator(Protocol):\n ''\n \n def __call__(\n self,__fields_tuple:RootValidatorFieldsTuple,__info:core_schema.ValidationInfo\n )->RootValidatorFieldsTuple:...\n \n \ndef make_v1_generic_root_validator(\nvalidator:V1RootValidatorFunction,pre:bool\n)->V2CoreBeforeRootValidator |V2CoreAfterRootValidator:\n ''\n\n\n\n\n\n\n\n \n if pre is True:\n \n  def _wrapper1(values:RootValidatorValues,_:core_schema.ValidationInfo)->RootValidatorValues:\n   return validator(values)\n   \n  return _wrapper1\n  \n  \n def _wrapper2(fields_tuple:RootValidatorFieldsTuple,_:core_schema.ValidationInfo)->RootValidatorFieldsTuple:\n  if len(fields_tuple)==2:\n  \n   values,init_vars=fields_tuple\n   values=validator(values)\n   return values,init_vars\n  else:\n  \n  \n   model_dict,model_extra,fields_set=fields_tuple\n   if model_extra:\n    fields=set(model_dict.keys())\n    model_dict.update(model_extra)\n    model_dict_new=validator(model_dict)\n    for k in list(model_dict_new.keys()):\n     if k not in fields:\n      model_extra[k]=model_dict_new.pop(k)\n   else:\n    model_dict_new=validator(model_dict)\n   return model_dict_new,model_extra,fields_set\n   \n return _wrapper2\n", ["__future__", "inspect", "pydantic._internal._utils", "pydantic.errors", "pydantic_core", "pydantic_core.core_schema", "typing", "typing_extensions"]], "pydantic._internal._namespace_utils": [".py", "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom functools import cached_property\nfrom typing import Any,Callable,Iterator,Mapping,NamedTuple,TypeVar\n\nfrom typing_extensions import ParamSpec,TypeAlias,TypeAliasType,TypeVarTuple\n\nGlobalsNamespace:TypeAlias='dict[str, Any]'\n''\n\n\n\n\n\nMappingNamespace:TypeAlias=Mapping[str,Any]\n''\n\n\n\n\n\n\n\n_TypeVarLike:TypeAlias='TypeVar | ParamSpec | TypeVarTuple'\n\n\nclass NamespacesTuple(NamedTuple):\n ''\n\n\n\n\n\n\n\n \n \n globals:GlobalsNamespace\n '' \n \n locals:MappingNamespace\n '' \n \n \ndef get_module_ns_of(obj:Any)->dict[str,Any]:\n ''\n\n\n\n \n module_name=getattr(obj,'__module__',None)\n if module_name:\n  try:\n   return sys.modules[module_name].__dict__\n  except KeyError:\n  \n   return{}\n return{}\n \n \n \n \nclass LazyLocalNamespace(Mapping[str,Any]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,*namespaces:MappingNamespace)->None:\n  self._namespaces=namespaces\n  \n @cached_property\n def data(self)->dict[str,Any]:\n  return{k:v for ns in self._namespaces for k,v in ns.items()}\n  \n def __len__(self)->int:\n  return len(self.data)\n  \n def __getitem__(self,key:str)->Any:\n  return self.data[key]\n  \n def __contains__(self,key:object)->bool:\n  return key in self.data\n  \n def __iter__(self)->Iterator[str]:\n  return iter(self.data)\n  \n  \ndef ns_for_function(obj:Callable[...,Any],parent_namespace:MappingNamespace |None=None)->NamespacesTuple:\n ''\n\n\n\n\n\n\n\n\n\n\n \n locals_list:list[MappingNamespace]=[]\n if parent_namespace is not None:\n  locals_list.append(parent_namespace)\n  \n  \n  \n  \n  \n  \n type_params:tuple[_TypeVarLike,...]\n if hasattr(obj,'__type_params__'):\n  type_params=obj.__type_params__\n else:\n  type_params=()\n if parent_namespace is not None:\n \n \n \n  type_params +=parent_namespace.get('__type_params__',())\n  \n locals_list.append({t.__name__:t for t in type_params})\n \n \n globalns=get_module_ns_of(obj)\n \n return NamespacesTuple(globalns,LazyLocalNamespace(*locals_list))\n \n \nclass NsResolver:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n namespaces_tuple:NamespacesTuple |None=None,\n parent_namespace:MappingNamespace |None=None,\n )->None:\n  self._base_ns_tuple=namespaces_tuple or NamespacesTuple({},{})\n  self._parent_ns=parent_namespace\n  self._types_stack:list[type[Any]|TypeAliasType]=[]\n  \n @cached_property\n def types_namespace(self)->NamespacesTuple:\n  ''\n  if not self._types_stack:\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   return self._base_ns_tuple\n   \n  typ=self._types_stack[-1]\n  \n  globalns=get_module_ns_of(typ)\n  \n  locals_list:list[MappingNamespace]=[]\n  \n  \n  if self._parent_ns is not None:\n   locals_list.append(self._parent_ns)\n  if len(self._types_stack)>1:\n   first_type=self._types_stack[0]\n   locals_list.append({first_type.__name__:first_type})\n   \n  if hasattr(typ,'__dict__'):\n  \n   locals_list.append(vars(typ))\n   \n   \n  locals_list.append({typ.__name__:typ})\n  \n  return NamespacesTuple(globalns,LazyLocalNamespace(*locals_list))\n  \n @contextmanager\n def push(self,typ:type[Any]|TypeAliasType,/)->Generator[None]:\n  ''\n  self._types_stack.append(typ)\n  \n  self.__dict__.pop('types_namespace',None)\n  try:\n   yield\n  finally:\n   self._types_stack.pop()\n   self.__dict__.pop('types_namespace',None)\n", ["__future__", "collections.abc", "contextlib", "functools", "sys", "typing", "typing_extensions"]], "pydantic._internal._decorators": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nfrom collections import deque\nfrom dataclasses import dataclass,field\nfrom functools import cached_property,partial,partialmethod\nfrom inspect import Parameter,Signature,isdatadescriptor,ismethoddescriptor,signature\nfrom itertools import islice\nfrom typing import TYPE_CHECKING,Any,Callable,ClassVar,Generic,Iterable,TypeVar,Union\n\nfrom pydantic_core import PydanticUndefined,core_schema\nfrom typing_extensions import Literal,TypeAlias,is_typeddict\n\nfrom..errors import PydanticUserError\nfrom._core_utils import get_type_ref\nfrom._internal_dataclass import slots_true\nfrom._namespace_utils import GlobalsNamespace,MappingNamespace\nfrom._typing_extra import get_function_type_hints\nfrom._utils import can_be_positional\n\nif TYPE_CHECKING:\n from..fields import ComputedFieldInfo\n from..functional_validators import FieldValidatorModes\n \n \n@dataclass(**slots_true)\nclass ValidatorDecoratorInfo:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@validator'\n \n fields:tuple[str,...]\n mode:Literal['before','after']\n each_item:bool\n always:bool\n check_fields:bool |None\n \n \n@dataclass(**slots_true)\nclass FieldValidatorDecoratorInfo:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@field_validator'\n \n fields:tuple[str,...]\n mode:FieldValidatorModes\n check_fields:bool |None\n json_schema_input_type:Any\n \n \n@dataclass(**slots_true)\nclass RootValidatorDecoratorInfo:\n ''\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@root_validator'\n mode:Literal['before','after']\n \n \n@dataclass(**slots_true)\nclass FieldSerializerDecoratorInfo:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@field_serializer'\n fields:tuple[str,...]\n mode:Literal['plain','wrap']\n return_type:Any\n when_used:core_schema.WhenUsed\n check_fields:bool |None\n \n \n@dataclass(**slots_true)\nclass ModelSerializerDecoratorInfo:\n ''\n\n\n\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@model_serializer'\n mode:Literal['plain','wrap']\n return_type:Any\n when_used:core_schema.WhenUsed\n \n \n@dataclass(**slots_true)\nclass ModelValidatorDecoratorInfo:\n ''\n\n\n\n\n\n \n \n decorator_repr:ClassVar[str]='@model_validator'\n mode:Literal['wrap','before','after']\n \n \nDecoratorInfo:TypeAlias=\"\"\"Union[\n    ValidatorDecoratorInfo,\n    FieldValidatorDecoratorInfo,\n    RootValidatorDecoratorInfo,\n    FieldSerializerDecoratorInfo,\n    ModelSerializerDecoratorInfo,\n    ModelValidatorDecoratorInfo,\n    ComputedFieldInfo,\n]\"\"\"\n\nReturnType=TypeVar('ReturnType')\nDecoratedType:TypeAlias=(\n'Union[classmethod[Any, Any, ReturnType], staticmethod[Any, ReturnType], Callable[..., ReturnType], property]'\n)\n\n\n@dataclass\nclass PydanticDescriptorProxy(Generic[ReturnType]):\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n wrapped:DecoratedType[ReturnType]\n decorator_info:DecoratorInfo\n shim:Callable[[Callable[...,Any]],Callable[...,Any]]|None=None\n \n def __post_init__(self):\n  for attr in 'setter','deleter':\n   if hasattr(self.wrapped,attr):\n    f=partial(self._call_wrapped_attr,name=attr)\n    setattr(self,attr,f)\n    \n def _call_wrapped_attr(self,func:Callable[[Any],None],*,name:str)->PydanticDescriptorProxy[ReturnType]:\n  self.wrapped=getattr(self.wrapped,name)(func)\n  if isinstance(self.wrapped,property):\n  \n   from..fields import ComputedFieldInfo\n   \n   if isinstance(self.decorator_info,ComputedFieldInfo):\n    self.decorator_info.wrapped_property=self.wrapped\n  return self\n  \n def __get__(self,obj:object |None,obj_type:type[object]|None=None)->PydanticDescriptorProxy[ReturnType]:\n  try:\n   return self.wrapped.__get__(obj,obj_type)\n  except AttributeError:\n  \n   return self.wrapped\n   \n def __set_name__(self,instance:Any,name:str)->None:\n  if hasattr(self.wrapped,'__set_name__'):\n   self.wrapped.__set_name__(instance,name)\n   \n def __getattr__(self,__name:str)->Any:\n  ''\n  return getattr(self.wrapped,__name)\n  \n  \nDecoratorInfoType=TypeVar('DecoratorInfoType',bound=DecoratorInfo)\n\n\n@dataclass(**slots_true)\nclass Decorator(Generic[DecoratorInfoType]):\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n cls_ref:str\n cls_var_name:str\n func:Callable[...,Any]\n shim:Callable[[Any],Any]|None\n info:DecoratorInfoType\n \n @staticmethod\n def build(\n cls_:Any,\n *,\n cls_var_name:str,\n shim:Callable[[Any],Any]|None,\n info:DecoratorInfoType,\n )->Decorator[DecoratorInfoType]:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  func=get_attribute_from_bases(cls_,cls_var_name)\n  if shim is not None:\n   func=shim(func)\n  func=unwrap_wrapped_function(func,unwrap_partial=False)\n  if not callable(func):\n  \n   attribute=get_attribute_from_base_dicts(cls_,cls_var_name)\n   if isinstance(attribute,PydanticDescriptorProxy):\n    func=unwrap_wrapped_function(attribute.wrapped)\n  return Decorator(\n  cls_ref=get_type_ref(cls_),\n  cls_var_name=cls_var_name,\n  func=func,\n  shim=shim,\n  info=info,\n  )\n  \n def bind_to_cls(self,cls:Any)->Decorator[DecoratorInfoType]:\n  ''\n\n\n\n\n\n\n  \n  return self.build(\n  cls,\n  cls_var_name=self.cls_var_name,\n  shim=self.shim,\n  info=self.info,\n  )\n  \n  \ndef get_bases(tp:type[Any])->tuple[type[Any],...]:\n ''\n\n\n\n\n\n\n \n if is_typeddict(tp):\n  return tp.__orig_bases__\n try:\n  return tp.__bases__\n except AttributeError:\n  return()\n  \n  \ndef mro(tp:type[Any])->tuple[type[Any],...]:\n ''\n\n\n \n \n \n if not is_typeddict(tp):\n  try:\n   return tp.__mro__\n  except AttributeError:\n  \n   pass\n   \n bases=get_bases(tp)\n return(tp,)+mro_for_bases(bases)\n \n \ndef mro_for_bases(bases:tuple[type[Any],...])->tuple[type[Any],...]:\n def merge_seqs(seqs:list[deque[type[Any]]])->Iterable[type[Any]]:\n  while True:\n   non_empty=[seq for seq in seqs if seq]\n   if not non_empty:\n   \n    return\n   candidate:type[Any]|None=None\n   for seq in non_empty:\n    candidate=seq[0]\n    not_head=[s for s in non_empty if candidate in islice(s,1,None)]\n    if not_head:\n    \n     candidate=None\n    else:\n     break\n   if not candidate:\n    raise TypeError('Inconsistent hierarchy, no C3 MRO is possible')\n   yield candidate\n   for seq in non_empty:\n   \n    if seq[0]==candidate:\n     seq.popleft()\n     \n seqs=[deque(mro(base))for base in bases]+[deque(bases)]\n return tuple(merge_seqs(seqs))\n \n \n_sentinel=object()\n\n\ndef get_attribute_from_bases(tp:type[Any]|tuple[type[Any],...],name:str)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if isinstance(tp,tuple):\n  for base in mro_for_bases(tp):\n   attribute=base.__dict__.get(name,_sentinel)\n   if attribute is not _sentinel:\n    attribute_get=getattr(attribute,'__get__',None)\n    if attribute_get is not None:\n     return attribute_get(None,tp)\n    return attribute\n  raise AttributeError(f'{name} not found in {tp}')\n else:\n  try:\n   return getattr(tp,name)\n  except AttributeError:\n   return get_attribute_from_bases(mro(tp),name)\n   \n   \ndef get_attribute_from_base_dicts(tp:type[Any],name:str)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n for base in reversed(mro(tp)):\n  if name in base.__dict__:\n   return base.__dict__[name]\n return tp.__dict__[name]\n \n \n@dataclass(**slots_true)\nclass DecoratorInfos:\n ''\n\n\n\n \n \n validators:dict[str,Decorator[ValidatorDecoratorInfo]]=field(default_factory=dict)\n field_validators:dict[str,Decorator[FieldValidatorDecoratorInfo]]=field(default_factory=dict)\n root_validators:dict[str,Decorator[RootValidatorDecoratorInfo]]=field(default_factory=dict)\n field_serializers:dict[str,Decorator[FieldSerializerDecoratorInfo]]=field(default_factory=dict)\n model_serializers:dict[str,Decorator[ModelSerializerDecoratorInfo]]=field(default_factory=dict)\n model_validators:dict[str,Decorator[ModelValidatorDecoratorInfo]]=field(default_factory=dict)\n computed_fields:dict[str,Decorator[ComputedFieldInfo]]=field(default_factory=dict)\n \n @staticmethod\n def build(model_dc:type[Any])->DecoratorInfos:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  \n  res=DecoratorInfos()\n  for base in reversed(mro(model_dc)[1:]):\n   existing:DecoratorInfos |None=base.__dict__.get('__pydantic_decorators__')\n   if existing is None:\n    existing=DecoratorInfos.build(base)\n   res.validators.update({k:v.bind_to_cls(model_dc)for k,v in existing.validators.items()})\n   res.field_validators.update({k:v.bind_to_cls(model_dc)for k,v in existing.field_validators.items()})\n   res.root_validators.update({k:v.bind_to_cls(model_dc)for k,v in existing.root_validators.items()})\n   res.field_serializers.update({k:v.bind_to_cls(model_dc)for k,v in existing.field_serializers.items()})\n   res.model_serializers.update({k:v.bind_to_cls(model_dc)for k,v in existing.model_serializers.items()})\n   res.model_validators.update({k:v.bind_to_cls(model_dc)for k,v in existing.model_validators.items()})\n   res.computed_fields.update({k:v.bind_to_cls(model_dc)for k,v in existing.computed_fields.items()})\n   \n  to_replace:list[tuple[str,Any]]=[]\n  \n  for var_name,var_value in vars(model_dc).items():\n   if isinstance(var_value,PydanticDescriptorProxy):\n    info=var_value.decorator_info\n    if isinstance(info,ValidatorDecoratorInfo):\n     res.validators[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=var_value.shim,info=info\n     )\n    elif isinstance(info,FieldValidatorDecoratorInfo):\n     res.field_validators[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=var_value.shim,info=info\n     )\n    elif isinstance(info,RootValidatorDecoratorInfo):\n     res.root_validators[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=var_value.shim,info=info\n     )\n    elif isinstance(info,FieldSerializerDecoratorInfo):\n    \n     for field_serializer_decorator in res.field_serializers.values():\n     \n     \n     \n      if field_serializer_decorator.cls_var_name ==var_name:\n       continue\n      for f in info.fields:\n       if f in field_serializer_decorator.info.fields:\n        raise PydanticUserError(\n        'Multiple field serializer functions were defined '\n        f'for field {f !r}, this is not allowed.',\n        code='multiple-field-serializers',\n        )\n     res.field_serializers[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=var_value.shim,info=info\n     )\n    elif isinstance(info,ModelValidatorDecoratorInfo):\n     res.model_validators[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=var_value.shim,info=info\n     )\n    elif isinstance(info,ModelSerializerDecoratorInfo):\n     res.model_serializers[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=var_value.shim,info=info\n     )\n    else:\n     from..fields import ComputedFieldInfo\n     \n     isinstance(var_value,ComputedFieldInfo)\n     res.computed_fields[var_name]=Decorator.build(\n     model_dc,cls_var_name=var_name,shim=None,info=info\n     )\n    to_replace.append((var_name,var_value.wrapped))\n  if to_replace:\n  \n  \n  \n  \n   model_dc.__pydantic_decorators__=res\n   for name,value in to_replace:\n    setattr(model_dc,name,value)\n  return res\n  \n  \ndef inspect_validator(validator:Callable[...,Any],mode:FieldValidatorModes)->bool:\n ''\n\n\n\n\n\n\n\n\n\n \n try:\n  sig=signature(validator)\n except(ValueError,TypeError):\n \n \n  return False\n n_positional=count_positional_required_params(sig)\n if mode =='wrap':\n  if n_positional ==3:\n   return True\n  elif n_positional ==2:\n   return False\n else:\n  assert mode in{'before','after','plain'},f\"invalid mode: {mode !r}, expected 'before', 'after' or 'plain\"\n  if n_positional ==2:\n   return True\n  elif n_positional ==1:\n   return False\n   \n raise PydanticUserError(\n f'Unrecognized field_validator function signature for {validator} with `mode={mode}`:{sig}',\n code='validator-signature',\n )\n \n \ndef inspect_field_serializer(serializer:Callable[...,Any],mode:Literal['plain','wrap'])->tuple[bool,bool]:\n ''\n\n\n\n\n\n\n\n\n\n\n \n try:\n  sig=signature(serializer)\n except(ValueError,TypeError):\n \n \n  return(False,False)\n  \n first=next(iter(sig.parameters.values()),None)\n is_field_serializer=first is not None and first.name =='self'\n \n n_positional=count_positional_required_params(sig)\n if is_field_serializer:\n \n  info_arg=_serializer_info_arg(mode,n_positional -1)\n else:\n  info_arg=_serializer_info_arg(mode,n_positional)\n  \n if info_arg is None:\n  raise PydanticUserError(\n  f'Unrecognized field_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n  code='field-serializer-signature',\n  )\n  \n return is_field_serializer,info_arg\n \n \ndef inspect_annotated_serializer(serializer:Callable[...,Any],mode:Literal['plain','wrap'])->bool:\n ''\n\n\n\n\n\n\n\n\n\n \n try:\n  sig=signature(serializer)\n except(ValueError,TypeError):\n \n \n  return False\n info_arg=_serializer_info_arg(mode,count_positional_required_params(sig))\n if info_arg is None:\n  raise PydanticUserError(\n  f'Unrecognized field_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n  code='field-serializer-signature',\n  )\n else:\n  return info_arg\n  \n  \ndef inspect_model_serializer(serializer:Callable[...,Any],mode:Literal['plain','wrap'])->bool:\n ''\n\n\n\n\n\n\n\n\n\n \n if isinstance(serializer,(staticmethod,classmethod))or not is_instance_method_from_sig(serializer):\n  raise PydanticUserError(\n  '`@model_serializer` must be applied to instance methods',code='model-serializer-instance-method'\n  )\n  \n sig=signature(serializer)\n info_arg=_serializer_info_arg(mode,count_positional_required_params(sig))\n if info_arg is None:\n  raise PydanticUserError(\n  f'Unrecognized model_serializer function signature for {serializer} with `mode={mode}`:{sig}',\n  code='model-serializer-signature',\n  )\n else:\n  return info_arg\n  \n  \ndef _serializer_info_arg(mode:Literal['plain','wrap'],n_positional:int)->bool |None:\n if mode =='plain':\n  if n_positional ==1:\n  \n   return False\n  elif n_positional ==2:\n  \n   return True\n else:\n  assert mode =='wrap',f\"invalid mode: {mode !r}, expected 'plain' or 'wrap'\"\n  if n_positional ==2:\n  \n   return False\n  elif n_positional ==3:\n  \n   return True\n   \n return None\n \n \nAnyDecoratorCallable:TypeAlias=(\n'Union[classmethod[Any, Any, Any], staticmethod[Any, Any], partialmethod[Any], Callable[..., Any]]'\n)\n\n\ndef is_instance_method_from_sig(function:AnyDecoratorCallable)->bool:\n ''\n\n\n\n\n\n\n\n\n\n \n sig=signature(unwrap_wrapped_function(function))\n first=next(iter(sig.parameters.values()),None)\n if first and first.name =='self':\n  return True\n return False\n \n \ndef ensure_classmethod_based_on_signature(function:AnyDecoratorCallable)->Any:\n ''\n\n\n\n\n\n\n \n if not isinstance(\n unwrap_wrapped_function(function,unwrap_class_static_method=False),classmethod\n )and _is_classmethod_from_sig(function):\n  return classmethod(function)\n return function\n \n \ndef _is_classmethod_from_sig(function:AnyDecoratorCallable)->bool:\n sig=signature(unwrap_wrapped_function(function))\n first=next(iter(sig.parameters.values()),None)\n if first and first.name =='cls':\n  return True\n return False\n \n \ndef unwrap_wrapped_function(\nfunc:Any,\n*,\nunwrap_partial:bool=True,\nunwrap_class_static_method:bool=True,\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n unwrap_types=(\n (property,cached_property)\n +((partial,partialmethod)if unwrap_partial else())\n +((staticmethod,classmethod)if unwrap_class_static_method else())\n )\n \n while isinstance(func,unwrap_types):\n  if unwrap_class_static_method and isinstance(func,(classmethod,staticmethod)):\n   func=func.__func__\n  elif isinstance(func,(partial,partialmethod)):\n   func=func.func\n  elif isinstance(func,property):\n   func=func.fget\n  else:\n  \n   assert isinstance(func,cached_property)\n   func=func.func\n   \n return func\n \n \ndef get_function_return_type(\nfunc:Any,\nexplicit_return_type:Any,\nglobalns:GlobalsNamespace |None=None,\nlocalns:MappingNamespace |None=None,\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n if explicit_return_type is PydanticUndefined:\n \n  hints=get_function_type_hints(\n  unwrap_wrapped_function(func),\n  include_keys={'return'},\n  globalns=globalns,\n  localns=localns,\n  )\n  return hints.get('return',PydanticUndefined)\n else:\n  return explicit_return_type\n  \n  \ndef count_positional_required_params(sig:Signature)->int:\n ''\n\n\n\n\n\n\n\n \n parameters=list(sig.parameters.values())\n return sum(\n 1\n for param in parameters\n if can_be_positional(param)\n \n \n \n and(param.default is Parameter.empty or param is parameters[0])\n )\n \n \ndef ensure_property(f:Any)->Any:\n ''\n\n\n\n\n\n\n \n if ismethoddescriptor(f)or isdatadescriptor(f):\n  return f\n else:\n  return property(f)\n", ["__future__", "collections", "dataclasses", "functools", "inspect", "itertools", "pydantic._internal._core_utils", "pydantic._internal._internal_dataclass", "pydantic._internal._namespace_utils", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic.errors", "pydantic.fields", "pydantic.functional_validators", "pydantic_core", "pydantic_core.core_schema", "typing", "typing_extensions"]], "pydantic._internal._validators": [".py", "''\n\n\n\n\nfrom __future__ import annotations as _annotations\n\nimport math\nimport re\nimport typing\nfrom decimal import Decimal\nfrom fractions import Fraction\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom typing import Any,Callable,Union\n\nfrom pydantic_core import PydanticCustomError,core_schema\nfrom pydantic_core._pydantic_core import PydanticKnownError\n\n\ndef sequence_validator(\ninput_value:typing.Sequence[Any],\n/,\nvalidator:core_schema.ValidatorFunctionWrapHandler,\n)->typing.Sequence[Any]:\n ''\n value_type=type(input_value)\n \n \n \n if issubclass(value_type,(str,bytes)):\n  raise PydanticCustomError(\n  'sequence_str',\n  \"'{type_name}' instances are not allowed as a Sequence value\",\n  {'type_name':value_type.__name__},\n  )\n  \n  \n  \n  \n  \n  \n if value_type is tuple:\n  input_value=list(input_value)\n  \n v_list=validator(input_value)\n \n \n if value_type is list:\n  return v_list\n elif issubclass(value_type,range):\n \n  return v_list\n elif value_type is tuple:\n  return tuple(v_list)\n else:\n \n  return value_type(v_list)\n  \n  \ndef import_string(value:Any)->Any:\n if isinstance(value,str):\n  try:\n   return _import_string_logic(value)\n  except ImportError as e:\n   raise PydanticCustomError('import_error','Invalid python path: {error}',{'error':str(e)})from e\n else:\n \n  return value\n  \n  \ndef _import_string_logic(dotted_path:str)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n from importlib import import_module\n \n components=dotted_path.strip().split(':')\n if len(components)>2:\n  raise ImportError(f\"Import strings should have at most one ':'; received {dotted_path !r}\")\n  \n module_path=components[0]\n if not module_path:\n  raise ImportError(f'Import strings should have a nonempty module name; received {dotted_path !r}')\n  \n try:\n  module=import_module(module_path)\n except ModuleNotFoundError as e:\n  if '.'in module_path:\n  \n   maybe_module_path,maybe_attribute=dotted_path.strip().rsplit('.',1)\n   try:\n    return _import_string_logic(f'{maybe_module_path}:{maybe_attribute}')\n   except ImportError:\n    pass\n   raise ImportError(f'No module named {module_path !r}')from e\n  raise e\n  \n if len(components)>1:\n  attribute=components[1]\n  try:\n   return getattr(module,attribute)\n  except AttributeError as e:\n   raise ImportError(f'cannot import name {attribute !r} from {module_path !r}')from e\n else:\n  return module\n  \n  \ndef pattern_either_validator(input_value:Any,/)->typing.Pattern[Any]:\n if isinstance(input_value,typing.Pattern):\n  return input_value\n elif isinstance(input_value,(str,bytes)):\n \n  return compile_pattern(input_value)\n else:\n  raise PydanticCustomError('pattern_type','Input should be a valid pattern')\n  \n  \ndef pattern_str_validator(input_value:Any,/)->typing.Pattern[str]:\n if isinstance(input_value,typing.Pattern):\n  if isinstance(input_value.pattern,str):\n   return input_value\n  else:\n   raise PydanticCustomError('pattern_str_type','Input should be a string pattern')\n elif isinstance(input_value,str):\n  return compile_pattern(input_value)\n elif isinstance(input_value,bytes):\n  raise PydanticCustomError('pattern_str_type','Input should be a string pattern')\n else:\n  raise PydanticCustomError('pattern_type','Input should be a valid pattern')\n  \n  \ndef pattern_bytes_validator(input_value:Any,/)->typing.Pattern[bytes]:\n if isinstance(input_value,typing.Pattern):\n  if isinstance(input_value.pattern,bytes):\n   return input_value\n  else:\n   raise PydanticCustomError('pattern_bytes_type','Input should be a bytes pattern')\n elif isinstance(input_value,bytes):\n  return compile_pattern(input_value)\n elif isinstance(input_value,str):\n  raise PydanticCustomError('pattern_bytes_type','Input should be a bytes pattern')\n else:\n  raise PydanticCustomError('pattern_type','Input should be a valid pattern')\n  \n  \nPatternType=typing.TypeVar('PatternType',str,bytes)\n\n\ndef compile_pattern(pattern:PatternType)->typing.Pattern[PatternType]:\n try:\n  return re.compile(pattern)\n except re.error:\n  raise PydanticCustomError('pattern_regex','Input should be a valid regular expression')\n  \n  \ndef ip_v4_address_validator(input_value:Any,/)->IPv4Address:\n if isinstance(input_value,IPv4Address):\n  return input_value\n  \n try:\n  return IPv4Address(input_value)\n except ValueError:\n  raise PydanticCustomError('ip_v4_address','Input is not a valid IPv4 address')\n  \n  \ndef ip_v6_address_validator(input_value:Any,/)->IPv6Address:\n if isinstance(input_value,IPv6Address):\n  return input_value\n  \n try:\n  return IPv6Address(input_value)\n except ValueError:\n  raise PydanticCustomError('ip_v6_address','Input is not a valid IPv6 address')\n  \n  \ndef ip_v4_network_validator(input_value:Any,/)->IPv4Network:\n ''\n\n\n\n \n if isinstance(input_value,IPv4Network):\n  return input_value\n  \n try:\n  return IPv4Network(input_value)\n except ValueError:\n  raise PydanticCustomError('ip_v4_network','Input is not a valid IPv4 network')\n  \n  \ndef ip_v6_network_validator(input_value:Any,/)->IPv6Network:\n ''\n\n\n\n \n if isinstance(input_value,IPv6Network):\n  return input_value\n  \n try:\n  return IPv6Network(input_value)\n except ValueError:\n  raise PydanticCustomError('ip_v6_network','Input is not a valid IPv6 network')\n  \n  \ndef ip_v4_interface_validator(input_value:Any,/)->IPv4Interface:\n if isinstance(input_value,IPv4Interface):\n  return input_value\n  \n try:\n  return IPv4Interface(input_value)\n except ValueError:\n  raise PydanticCustomError('ip_v4_interface','Input is not a valid IPv4 interface')\n  \n  \ndef ip_v6_interface_validator(input_value:Any,/)->IPv6Interface:\n if isinstance(input_value,IPv6Interface):\n  return input_value\n  \n try:\n  return IPv6Interface(input_value)\n except ValueError:\n  raise PydanticCustomError('ip_v6_interface','Input is not a valid IPv6 interface')\n  \n  \ndef fraction_validator(input_value:Any,/)->Fraction:\n if isinstance(input_value,Fraction):\n  return input_value\n  \n try:\n  return Fraction(input_value)\n except ValueError:\n  raise PydanticCustomError('fraction_parsing','Input is not a valid fraction')\n  \n  \ndef forbid_inf_nan_check(x:Any)->Any:\n if not math.isfinite(x):\n  raise PydanticKnownError('finite_number')\n return x\n \n \ndef _safe_repr(v:Any)->int |float |str:\n ''\n\n\n \n if isinstance(v,(int,float,str)):\n  return v\n return repr(v)\n \n \ndef greater_than_validator(x:Any,gt:Any)->Any:\n try:\n  if not(x >gt):\n   raise PydanticKnownError('greater_than',{'gt':_safe_repr(gt)})\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'gt' to supplied value {x}\")\n  \n  \ndef greater_than_or_equal_validator(x:Any,ge:Any)->Any:\n try:\n  if not(x >=ge):\n   raise PydanticKnownError('greater_than_equal',{'ge':_safe_repr(ge)})\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'ge' to supplied value {x}\")\n  \n  \ndef less_than_validator(x:Any,lt:Any)->Any:\n try:\n  if not(x <lt):\n   raise PydanticKnownError('less_than',{'lt':_safe_repr(lt)})\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'lt' to supplied value {x}\")\n  \n  \ndef less_than_or_equal_validator(x:Any,le:Any)->Any:\n try:\n  if not(x <=le):\n   raise PydanticKnownError('less_than_equal',{'le':_safe_repr(le)})\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'le' to supplied value {x}\")\n  \n  \ndef multiple_of_validator(x:Any,multiple_of:Any)->Any:\n try:\n  if x %multiple_of:\n   raise PydanticKnownError('multiple_of',{'multiple_of':_safe_repr(multiple_of)})\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'multiple_of' to supplied value {x}\")\n  \n  \ndef min_length_validator(x:Any,min_length:Any)->Any:\n try:\n  if not(len(x)>=min_length):\n   raise PydanticKnownError(\n   'too_short',{'field_type':'Value','min_length':min_length,'actual_length':len(x)}\n   )\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'min_length' to supplied value {x}\")\n  \n  \ndef max_length_validator(x:Any,max_length:Any)->Any:\n try:\n  if len(x)>max_length:\n   raise PydanticKnownError(\n   'too_long',\n   {'field_type':'Value','max_length':max_length,'actual_length':len(x)},\n   )\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'max_length' to supplied value {x}\")\n  \n  \ndef _extract_decimal_digits_info(decimal:Decimal)->tuple[int,int]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n decimal_tuple=decimal.as_tuple()\n if not isinstance(decimal_tuple.exponent,int):\n  raise TypeError(f'Unable to extract decimal digits info from supplied value {decimal}')\n exponent=decimal_tuple.exponent\n num_digits=len(decimal_tuple.digits)\n \n if exponent >=0:\n \n \n  num_digits +=exponent\n  decimal_places=0\n else:\n \n \n \n \n \n \n  decimal_places=abs(exponent)\n  num_digits=max(num_digits,decimal_places)\n  \n return decimal_places,num_digits\n \n \ndef max_digits_validator(x:Any,max_digits:Any)->Any:\n _,num_digits=_extract_decimal_digits_info(x)\n _,normalized_num_digits=_extract_decimal_digits_info(x.normalize())\n \n try:\n  if(num_digits >max_digits)and(normalized_num_digits >max_digits):\n   raise PydanticKnownError(\n   'decimal_max_digits',\n   {'max_digits':max_digits},\n   )\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'max_digits' to supplied value {x}\")\n  \n  \ndef decimal_places_validator(x:Any,decimal_places:Any)->Any:\n decimal_places_,_=_extract_decimal_digits_info(x)\n normalized_decimal_places,_=_extract_decimal_digits_info(x.normalize())\n \n try:\n  if(decimal_places_ >decimal_places)and(normalized_decimal_places >decimal_places):\n   raise PydanticKnownError(\n   'decimal_max_places',\n   {'decimal_places':decimal_places},\n   )\n  return x\n except TypeError:\n  raise TypeError(f\"Unable to apply constraint 'decimal_places' to supplied value {x}\")\n  \n  \nNUMERIC_VALIDATOR_LOOKUP:dict[str,Callable]={\n'gt':greater_than_validator,\n'ge':greater_than_or_equal_validator,\n'lt':less_than_validator,\n'le':less_than_or_equal_validator,\n'multiple_of':multiple_of_validator,\n'min_length':min_length_validator,\n'max_length':max_length_validator,\n'max_digits':max_digits_validator,\n'decimal_places':decimal_places_validator,\n}\n\nIpType=Union[IPv4Address,IPv6Address,IPv4Network,IPv6Network,IPv4Interface,IPv6Interface]\n\nIP_VALIDATOR_LOOKUP:dict[type[IpType],Callable]={\nIPv4Address:ip_v4_address_validator,\nIPv6Address:ip_v6_address_validator,\nIPv4Network:ip_v4_network_validator,\nIPv6Network:ip_v6_network_validator,\nIPv4Interface:ip_v4_interface_validator,\nIPv6Interface:ip_v6_interface_validator,\n}\n", ["__future__", "decimal", "fractions", "importlib", "ipaddress", "math", "pydantic_core", "pydantic_core._pydantic_core", "pydantic_core.core_schema", "re", "typing"]], "pydantic._internal._generics": [".py", "from __future__ import annotations\n\nimport sys\nimport types\nimport typing\nfrom collections import ChainMap\nfrom contextlib import contextmanager\nfrom contextvars import ContextVar\nfrom types import prepare_class\nfrom typing import TYPE_CHECKING,Any,Iterator,Mapping,MutableMapping,Tuple,TypeVar\nfrom weakref import WeakValueDictionary\n\nimport typing_extensions\n\nfrom. import _typing_extra\nfrom._core_utils import get_type_ref\nfrom._forward_ref import PydanticRecursiveRef\nfrom._utils import all_identical,is_model_class\n\nif sys.version_info >=(3,10):\n from typing import _UnionGenericAlias\n \nif TYPE_CHECKING:\n from..main import BaseModel\n \nGenericTypesCacheKey=Tuple[Any,Any,Tuple[Any,...]]\n\n\n\n\n\n\n\nKT=TypeVar('KT')\nVT=TypeVar('VT')\n_LIMITED_DICT_SIZE=100\nif TYPE_CHECKING:\n\n class LimitedDict(dict,MutableMapping[KT,VT]):\n  def __init__(self,size_limit:int=_LIMITED_DICT_SIZE):...\n  \nelse:\n\n class LimitedDict(dict):\n  ''\n\n\n  \n  \n  def __init__(self,size_limit:int=_LIMITED_DICT_SIZE):\n   self.size_limit=size_limit\n   super().__init__()\n   \n  def __setitem__(self,key:Any,value:Any,/)->None:\n   super().__setitem__(key,value)\n   if len(self)>self.size_limit:\n    excess=len(self)-self.size_limit+self.size_limit //10\n    to_remove=list(self.keys())[:excess]\n    for k in to_remove:\n     del self[k]\n     \n     \n     \n     \nif sys.version_info >=(3,9):\n GenericTypesCache=WeakValueDictionary[GenericTypesCacheKey,'type[BaseModel]']\nelse:\n GenericTypesCache=WeakValueDictionary\n \nif TYPE_CHECKING:\n\n class DeepChainMap(ChainMap[KT,VT]):\n  ...\n  \nelse:\n\n class DeepChainMap(ChainMap):\n  ''\n\n\n\n  \n  \n  def clear(self)->None:\n   for mapping in self.maps:\n    mapping.clear()\n    \n  def __setitem__(self,key:KT,value:VT)->None:\n   for mapping in self.maps:\n    mapping[key]=value\n    \n  def __delitem__(self,key:KT)->None:\n   hit=False\n   for mapping in self.maps:\n    if key in mapping:\n     del mapping[key]\n     hit=True\n   if not hit:\n    raise KeyError(key)\n    \n    \n    \n    \n    \n    \n_GENERIC_TYPES_CACHE=GenericTypesCache()\n\n\nclass PydanticGenericMetadata(typing_extensions.TypedDict):\n origin:type[BaseModel]|None\n args:tuple[Any,...]\n parameters:tuple[TypeVar,...]\n \n \ndef create_generic_submodel(\nmodel_name:str,origin:type[BaseModel],args:tuple[Any,...],params:tuple[Any,...]\n)->type[BaseModel]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n namespace:dict[str,Any]={'__module__':origin.__module__}\n bases=(origin,)\n meta,ns,kwds=prepare_class(model_name,bases)\n namespace.update(ns)\n created_model=meta(\n model_name,\n bases,\n namespace,\n __pydantic_generic_metadata__={\n 'origin':origin,\n 'args':args,\n 'parameters':params,\n },\n __pydantic_reset_parent_namespace__=False,\n **kwds,\n )\n \n model_module,called_globally=_get_caller_frame_info(depth=3)\n if called_globally:\n  object_by_reference=None\n  reference_name=model_name\n  reference_module_globals=sys.modules[created_model.__module__].__dict__\n  while object_by_reference is not created_model:\n   object_by_reference=reference_module_globals.setdefault(reference_name,created_model)\n   reference_name +='_'\n   \n return created_model\n \n \ndef _get_caller_frame_info(depth:int=2)->tuple[str |None,bool]:\n ''\n\n\n\n\n\n\n\n\n\n \n try:\n  previous_caller_frame=sys._getframe(depth)\n except ValueError as e:\n  raise RuntimeError('This function must be used inside another function')from e\n except AttributeError:\n  return None,False\n frame_globals=previous_caller_frame.f_globals\n return frame_globals.get('__name__'),previous_caller_frame.f_locals is frame_globals\n \n \nDictValues:type[Any]={}.values().__class__\n\n\ndef iter_contained_typevars(v:Any)->Iterator[TypeVar]:\n ''\n\n\n\n \n if isinstance(v,TypeVar):\n  yield v\n elif is_model_class(v):\n  yield from v.__pydantic_generic_metadata__['parameters']\n elif isinstance(v,(DictValues,list)):\n  for var in v:\n   yield from iter_contained_typevars(var)\n else:\n  args=get_args(v)\n  for arg in args:\n   yield from iter_contained_typevars(arg)\n   \n   \ndef get_args(v:Any)->Any:\n pydantic_generic_metadata:PydanticGenericMetadata |None=getattr(v,'__pydantic_generic_metadata__',None)\n if pydantic_generic_metadata:\n  return pydantic_generic_metadata.get('args')\n return typing_extensions.get_args(v)\n \n \ndef get_origin(v:Any)->Any:\n pydantic_generic_metadata:PydanticGenericMetadata |None=getattr(v,'__pydantic_generic_metadata__',None)\n if pydantic_generic_metadata:\n  return pydantic_generic_metadata.get('origin')\n return typing_extensions.get_origin(v)\n \n \ndef get_standard_typevars_map(cls:Any)->dict[TypeVar,Any]|None:\n ''\n\n \n origin=get_origin(cls)\n if origin is None:\n  return None\n if not hasattr(origin,'__parameters__'):\n  return None\n  \n  \n  \n args:tuple[Any,...]=cls.__args__\n parameters:tuple[TypeVar,...]=origin.__parameters__\n return dict(zip(parameters,args))\n \n \ndef get_model_typevars_map(cls:type[BaseModel])->dict[TypeVar,Any]|None:\n ''\n\n\n\n\n \n \n \n generic_metadata=cls.__pydantic_generic_metadata__\n origin=generic_metadata['origin']\n args=generic_metadata['args']\n return dict(zip(iter_contained_typevars(origin),args))\n \n \ndef replace_types(type_:Any,type_map:Mapping[Any,Any]|None)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if not type_map:\n  return type_\n  \n type_args=get_args(type_)\n \n if _typing_extra.is_annotated(type_):\n  annotated_type,*annotations=type_args\n  annotated=replace_types(annotated_type,type_map)\n  for annotation in annotations:\n   annotated=typing_extensions.Annotated[annotated,annotation]\n  return annotated\n  \n origin_type=get_origin(type_)\n \n \n \n if type_args:\n  resolved_type_args=tuple(replace_types(arg,type_map)for arg in type_args)\n  if all_identical(type_args,resolved_type_args):\n  \n  \n   return type_\n   \n  if(\n  origin_type is not None\n  and isinstance(type_,_typing_extra.typing_base)\n  and not isinstance(origin_type,_typing_extra.typing_base)\n  and getattr(type_,'_name',None)is not None\n  ):\n  \n  \n  \n   origin_type=getattr(typing,type_._name)\n  assert origin_type is not None\n  \n  if _typing_extra.origin_is_union(origin_type):\n   if any(_typing_extra.is_any(arg)for arg in resolved_type_args):\n   \n    resolved_type_args=(Any,)\n    \n   resolved_type_args=tuple(\n   arg\n   for arg in resolved_type_args\n   if not(_typing_extra.is_no_return(arg)or _typing_extra.is_never(arg))\n   )\n   \n   \n   \n  if sys.version_info >=(3,10)and origin_type is types.UnionType:\n   return _UnionGenericAlias(origin_type,resolved_type_args)\n   \n  return origin_type[resolved_type_args[0]if len(resolved_type_args)==1 else resolved_type_args]\n  \n  \n  \n  \n if not origin_type and is_model_class(type_):\n  parameters=type_.__pydantic_generic_metadata__['parameters']\n  if not parameters:\n   return type_\n  resolved_type_args=tuple(replace_types(t,type_map)for t in parameters)\n  if all_identical(parameters,resolved_type_args):\n   return type_\n  return type_[resolved_type_args]\n  \n  \n  \n if isinstance(type_,list):\n  resolved_list=[replace_types(element,type_map)for element in type_]\n  if all_identical(type_,resolved_list):\n   return type_\n  return resolved_list\n  \n  \n  \n return type_map.get(type_,type_)\n \n \ndef has_instance_in_type(type_:Any,isinstance_target:Any)->bool:\n ''\n\n \n if isinstance(type_,isinstance_target):\n  return True\n if _typing_extra.is_annotated(type_):\n  return has_instance_in_type(type_.__origin__,isinstance_target)\n if _typing_extra.is_literal(type_):\n  return False\n  \n type_args=get_args(type_)\n \n \n \n for arg in type_args:\n  if has_instance_in_type(arg,isinstance_target):\n   return True\n   \n   \n   \n if(\n isinstance(type_,list)\n \n and not isinstance(type_,typing_extensions.ParamSpec)\n ):\n  for element in type_:\n   if has_instance_in_type(element,isinstance_target):\n    return True\n    \n return False\n \n \ndef check_parameters_count(cls:type[BaseModel],parameters:tuple[Any,...])->None:\n ''\n\n\n\n\n\n\n\n \n actual=len(parameters)\n expected=len(cls.__pydantic_generic_metadata__['parameters'])\n if actual !=expected:\n  description='many'if actual >expected else 'few'\n  raise TypeError(f'Too {description} parameters for {cls}; actual {actual}, expected {expected}')\n  \n  \n_generic_recursion_cache:ContextVar[set[str]|None]=ContextVar('_generic_recursion_cache',default=None)\n\n\n@contextmanager\ndef generic_recursion_self_type(\norigin:type[BaseModel],args:tuple[Any,...]\n)->Iterator[PydanticRecursiveRef |None]:\n ''\n\n\n\n\n\n \n previously_seen_type_refs=_generic_recursion_cache.get()\n if previously_seen_type_refs is None:\n  previously_seen_type_refs=set()\n  token=_generic_recursion_cache.set(previously_seen_type_refs)\n else:\n  token=None\n  \n try:\n  type_ref=get_type_ref(origin,args_override=args)\n  if type_ref in previously_seen_type_refs:\n   self_type=PydanticRecursiveRef(type_ref=type_ref)\n   yield self_type\n  else:\n   previously_seen_type_refs.add(type_ref)\n   yield\n   previously_seen_type_refs.remove(type_ref)\n finally:\n  if token:\n   _generic_recursion_cache.reset(token)\n   \n   \ndef recursively_defined_type_refs()->set[str]:\n visited=_generic_recursion_cache.get()\n if not visited:\n  return set()\n  \n return visited.copy()\n \n \ndef get_cached_generic_type_early(parent:type[BaseModel],typevar_values:Any)->type[BaseModel]|None:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _GENERIC_TYPES_CACHE.get(_early_cache_key(parent,typevar_values))\n \n \ndef get_cached_generic_type_late(\nparent:type[BaseModel],typevar_values:Any,origin:type[BaseModel],args:tuple[Any,...]\n)->type[BaseModel]|None:\n ''\n cached=_GENERIC_TYPES_CACHE.get(_late_cache_key(origin,args,typevar_values))\n if cached is not None:\n  set_cached_generic_type(parent,typevar_values,cached,origin,args)\n return cached\n \n \ndef set_cached_generic_type(\nparent:type[BaseModel],\ntypevar_values:tuple[Any,...],\ntype_:type[BaseModel],\norigin:type[BaseModel]|None=None,\nargs:tuple[Any,...]|None=None,\n)->None:\n ''\n\n \n _GENERIC_TYPES_CACHE[_early_cache_key(parent,typevar_values)]=type_\n if len(typevar_values)==1:\n  _GENERIC_TYPES_CACHE[_early_cache_key(parent,typevar_values[0])]=type_\n if origin and args:\n  _GENERIC_TYPES_CACHE[_late_cache_key(origin,args,typevar_values)]=type_\n  \n  \ndef _union_orderings_key(typevar_values:Any)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n \n if isinstance(typevar_values,tuple):\n  args_data=[]\n  for value in typevar_values:\n   args_data.append(_union_orderings_key(value))\n  return tuple(args_data)\n elif _typing_extra.is_union(typevar_values):\n  return get_args(typevar_values)\n else:\n  return()\n  \n  \ndef _early_cache_key(cls:type[BaseModel],typevar_values:Any)->GenericTypesCacheKey:\n ''\n\n\n\n\n\n\n \n return cls,typevar_values,_union_orderings_key(typevar_values)\n \n \ndef _late_cache_key(origin:type[BaseModel],args:tuple[Any,...],typevar_values:Any)->GenericTypesCacheKey:\n ''\n\n\n\n \n \n \n \n return _union_orderings_key(typevar_values),origin,args\n", ["__future__", "collections", "contextlib", "contextvars", "pydantic._internal", "pydantic._internal._core_utils", "pydantic._internal._forward_ref", "pydantic._internal._typing_extra", "pydantic._internal._utils", "pydantic.main", "sys", "types", "typing", "typing_extensions", "weakref"]], "pydantic._internal._mock_val_ser": [".py", "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING,Any,Callable,Generic,Iterator,Mapping,TypeVar,Union\n\nfrom pydantic_core import CoreSchema,SchemaSerializer,SchemaValidator\nfrom typing_extensions import Literal\n\nfrom..errors import PydanticErrorCodes,PydanticUserError\nfrom..plugin._schema_validator import PluggableSchemaValidator\n\nif TYPE_CHECKING:\n from..dataclasses import PydanticDataclass\n from..main import BaseModel\n from..type_adapter import TypeAdapter\n \n \nValSer=TypeVar('ValSer',bound=Union[SchemaValidator,PluggableSchemaValidator,SchemaSerializer])\nT=TypeVar('T')\n\n\nclass MockCoreSchema(Mapping[str,Any]):\n ''\n\n \n \n __slots__='_error_message','_code','_attempt_rebuild','_built_memo'\n \n def __init__(\n self,\n error_message:str,\n *,\n code:PydanticErrorCodes,\n attempt_rebuild:Callable[[],CoreSchema |None]|None=None,\n )->None:\n  self._error_message=error_message\n  self._code:PydanticErrorCodes=code\n  self._attempt_rebuild=attempt_rebuild\n  self._built_memo:CoreSchema |None=None\n  \n def __getitem__(self,key:str)->Any:\n  return self._get_built().__getitem__(key)\n  \n def __len__(self)->int:\n  return self._get_built().__len__()\n  \n def __iter__(self)->Iterator[str]:\n  return self._get_built().__iter__()\n  \n def _get_built(self)->CoreSchema:\n  if self._built_memo is not None:\n   return self._built_memo\n   \n  if self._attempt_rebuild:\n   schema=self._attempt_rebuild()\n   if schema is not None:\n    self._built_memo=schema\n    return schema\n  raise PydanticUserError(self._error_message,code=self._code)\n  \n def rebuild(self)->CoreSchema |None:\n  self._built_memo=None\n  if self._attempt_rebuild:\n   schema=self._attempt_rebuild()\n   if schema is not None:\n    return schema\n   else:\n    raise PydanticUserError(self._error_message,code=self._code)\n  return None\n  \n  \nclass MockValSer(Generic[ValSer]):\n ''\n\n \n \n __slots__='_error_message','_code','_val_or_ser','_attempt_rebuild'\n \n def __init__(\n self,\n error_message:str,\n *,\n code:PydanticErrorCodes,\n val_or_ser:Literal['validator','serializer'],\n attempt_rebuild:Callable[[],ValSer |None]|None=None,\n )->None:\n  self._error_message=error_message\n  self._val_or_ser=SchemaValidator if val_or_ser =='validator'else SchemaSerializer\n  self._code:PydanticErrorCodes=code\n  self._attempt_rebuild=attempt_rebuild\n  \n def __getattr__(self,item:str)->None:\n  __tracebackhide__=True\n  if self._attempt_rebuild:\n   val_ser=self._attempt_rebuild()\n   if val_ser is not None:\n    return getattr(val_ser,item)\n    \n    \n  getattr(self._val_or_ser,item)\n  raise PydanticUserError(self._error_message,code=self._code)\n  \n def rebuild(self)->ValSer |None:\n  if self._attempt_rebuild:\n   val_ser=self._attempt_rebuild()\n   if val_ser is not None:\n    return val_ser\n   else:\n    raise PydanticUserError(self._error_message,code=self._code)\n  return None\n  \n  \ndef set_type_adapter_mocks(adapter:TypeAdapter,type_repr:str)->None:\n ''\n\n\n\n\n \n undefined_type_error_message=(\n f'`TypeAdapter[{type_repr}]` is not fully defined; you should define `{type_repr}` and all referenced types,'\n f' then call `.rebuild()` on the instance.'\n )\n \n def attempt_rebuild_fn(attr_fn:Callable[[TypeAdapter],T])->Callable[[],T |None]:\n  def handler()->T |None:\n   if adapter.rebuild(raise_errors=False,_parent_namespace_depth=5)is not False:\n    return attr_fn(adapter)\n   else:\n    return None\n    \n  return handler\n  \n adapter.core_schema=MockCoreSchema(\n undefined_type_error_message,\n code='class-not-fully-defined',\n attempt_rebuild=attempt_rebuild_fn(lambda ta:ta.core_schema),\n )\n adapter.validator=MockValSer(\n undefined_type_error_message,\n code='class-not-fully-defined',\n val_or_ser='validator',\n attempt_rebuild=attempt_rebuild_fn(lambda ta:ta.validator),\n )\n adapter.serializer=MockValSer(\n undefined_type_error_message,\n code='class-not-fully-defined',\n val_or_ser='serializer',\n attempt_rebuild=attempt_rebuild_fn(lambda ta:ta.serializer),\n )\n \n \ndef set_model_mocks(cls:type[BaseModel],cls_name:str,undefined_name:str='all referenced types')->None:\n ''\n\n\n\n\n\n \n undefined_type_error_message=(\n f'`{cls_name}` is not fully defined; you should define {undefined_name},'\n f' then call `{cls_name}.model_rebuild()`.'\n )\n \n def attempt_rebuild_fn(attr_fn:Callable[[type[BaseModel]],T])->Callable[[],T |None]:\n  def handler()->T |None:\n   if cls.model_rebuild(raise_errors=False,_parent_namespace_depth=5)is not False:\n    return attr_fn(cls)\n   else:\n    return None\n    \n  return handler\n  \n cls.__pydantic_core_schema__=MockCoreSchema(\n undefined_type_error_message,\n code='class-not-fully-defined',\n attempt_rebuild=attempt_rebuild_fn(lambda c:c.__pydantic_core_schema__),\n )\n cls.__pydantic_validator__=MockValSer(\n undefined_type_error_message,\n code='class-not-fully-defined',\n val_or_ser='validator',\n attempt_rebuild=attempt_rebuild_fn(lambda c:c.__pydantic_validator__),\n )\n cls.__pydantic_serializer__=MockValSer(\n undefined_type_error_message,\n code='class-not-fully-defined',\n val_or_ser='serializer',\n attempt_rebuild=attempt_rebuild_fn(lambda c:c.__pydantic_serializer__),\n )\n \n \ndef set_dataclass_mocks(\ncls:type[PydanticDataclass],cls_name:str,undefined_name:str='all referenced types'\n)->None:\n ''\n\n\n\n\n\n \n from..dataclasses import rebuild_dataclass\n \n undefined_type_error_message=(\n f'`{cls_name}` is not fully defined; you should define {undefined_name},'\n f' then call `pydantic.dataclasses.rebuild_dataclass({cls_name})`.'\n )\n \n def attempt_rebuild_fn(attr_fn:Callable[[type[PydanticDataclass]],T])->Callable[[],T |None]:\n  def handler()->T |None:\n   if rebuild_dataclass(cls,raise_errors=False,_parent_namespace_depth=5)is not False:\n    return attr_fn(cls)\n   else:\n    return None\n    \n  return handler\n  \n cls.__pydantic_core_schema__=MockCoreSchema(\n undefined_type_error_message,\n code='class-not-fully-defined',\n attempt_rebuild=attempt_rebuild_fn(lambda c:c.__pydantic_core_schema__),\n )\n cls.__pydantic_validator__=MockValSer(\n undefined_type_error_message,\n code='class-not-fully-defined',\n val_or_ser='validator',\n attempt_rebuild=attempt_rebuild_fn(lambda c:c.__pydantic_validator__),\n )\n cls.__pydantic_serializer__=MockValSer(\n undefined_type_error_message,\n code='class-not-fully-defined',\n val_or_ser='serializer',\n attempt_rebuild=attempt_rebuild_fn(lambda c:c.__pydantic_serializer__),\n )\n", ["__future__", "pydantic.dataclasses", "pydantic.errors", "pydantic.main", "pydantic.plugin._schema_validator", "pydantic.type_adapter", "pydantic_core", "typing", "typing_extensions"]], "pydantic._internal._forward_ref": [".py", "from __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom typing import Union\n\n\n@dataclass\nclass PydanticRecursiveRef:\n type_ref:str\n \n __name__='PydanticRecursiveRef'\n __hash__=object.__hash__\n \n def __call__(self)->None:\n  ''\n\n  \n  \n def __or__(self,other):\n  return Union[self,other]\n  \n def __ror__(self,other):\n  return Union[other,self]\n", ["__future__", "dataclasses", "typing"]], "pydantic._internal._git": [".py", "''\n\nfrom __future__ import annotations\n\nimport os\nimport subprocess\n\n\ndef is_git_repo(dir:str)->bool:\n ''\n return os.path.exists(os.path.join(dir,'.git'))\n \n \ndef have_git()->bool:\n ''\n try:\n  subprocess.check_output(['git','--help'])\n  return True\n except subprocess.CalledProcessError:\n  return False\n except OSError:\n  return False\n  \n  \ndef git_revision(dir:str)->str:\n ''\n return subprocess.check_output(['git','rev-parse','--short','HEAD'],cwd=dir).decode('utf-8').strip()\n", ["__future__", "os", "subprocess"]], "pydantic.deprecated": [".py", "", [], 1], "pydantic.deprecated.copy_internals": [".py", "from __future__ import annotations as _annotations\n\nimport typing\nfrom copy import deepcopy\nfrom enum import Enum\nfrom typing import Any,Tuple\n\nimport typing_extensions\n\nfrom.._internal import(\n_model_construction,\n_typing_extra,\n_utils,\n)\n\nif typing.TYPE_CHECKING:\n from.. import BaseModel\n from.._internal._utils import AbstractSetIntStr,MappingIntStrAny\n \n AnyClassMethod=classmethod[Any,Any,Any]\n TupleGenerator=typing.Generator[Tuple[str,Any],None,None]\n Model=typing.TypeVar('Model',bound='BaseModel')\n \n IncEx:typing_extensions.TypeAlias='set[int] | set[str] | dict[int, Any] | dict[str, Any] | None'\n \n_object_setattr=_model_construction.object_setattr\n\n\ndef _iter(\nself:BaseModel,\nto_dict:bool=False,\nby_alias:bool=False,\ninclude:AbstractSetIntStr |MappingIntStrAny |None=None,\nexclude:AbstractSetIntStr |MappingIntStrAny |None=None,\nexclude_unset:bool=False,\nexclude_defaults:bool=False,\nexclude_none:bool=False,\n)->TupleGenerator:\n\n\n if exclude is not None:\n  exclude=_utils.ValueItems.merge(\n  {k:v.exclude for k,v in self.__pydantic_fields__.items()if v.exclude is not None},exclude\n  )\n  \n if include is not None:\n  include=_utils.ValueItems.merge({k:True for k in self.__pydantic_fields__},include,intersect=True)\n  \n allowed_keys=_calculate_keys(self,include=include,exclude=exclude,exclude_unset=exclude_unset)\n if allowed_keys is None and not(to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n \n  yield from self.__dict__.items()\n  if self.__pydantic_extra__:\n   yield from self.__pydantic_extra__.items()\n  return\n  \n value_exclude=_utils.ValueItems(self,exclude)if exclude is not None else None\n value_include=_utils.ValueItems(self,include)if include is not None else None\n \n if self.__pydantic_extra__ is None:\n  items=self.__dict__.items()\n else:\n  items=list(self.__dict__.items())+list(self.__pydantic_extra__.items())\n  \n for field_key,v in items:\n  if(allowed_keys is not None and field_key not in allowed_keys)or(exclude_none and v is None):\n   continue\n   \n  if exclude_defaults:\n   try:\n    field=self.__pydantic_fields__[field_key]\n   except KeyError:\n    pass\n   else:\n    if not field.is_required()and field.default ==v:\n     continue\n     \n  if by_alias and field_key in self.__pydantic_fields__:\n   dict_key=self.__pydantic_fields__[field_key].alias or field_key\n  else:\n   dict_key=field_key\n   \n  if to_dict or value_include or value_exclude:\n   v=_get_value(\n   type(self),\n   v,\n   to_dict=to_dict,\n   by_alias=by_alias,\n   include=value_include and value_include.for_element(field_key),\n   exclude=value_exclude and value_exclude.for_element(field_key),\n   exclude_unset=exclude_unset,\n   exclude_defaults=exclude_defaults,\n   exclude_none=exclude_none,\n   )\n  yield dict_key,v\n  \n  \ndef _copy_and_set_values(\nself:Model,\nvalues:dict[str,Any],\nfields_set:set[str],\nextra:dict[str,Any]|None=None,\nprivate:dict[str,Any]|None=None,\n*,\ndeep:bool,\n)->Model:\n if deep:\n \n  values=deepcopy(values)\n  extra=deepcopy(extra)\n  private=deepcopy(private)\n  \n cls=self.__class__\n m=cls.__new__(cls)\n _object_setattr(m,'__dict__',values)\n _object_setattr(m,'__pydantic_extra__',extra)\n _object_setattr(m,'__pydantic_fields_set__',fields_set)\n _object_setattr(m,'__pydantic_private__',private)\n \n return m\n \n \n@typing.no_type_check\ndef _get_value(\ncls:type[BaseModel],\nv:Any,\nto_dict:bool,\nby_alias:bool,\ninclude:AbstractSetIntStr |MappingIntStrAny |None,\nexclude:AbstractSetIntStr |MappingIntStrAny |None,\nexclude_unset:bool,\nexclude_defaults:bool,\nexclude_none:bool,\n)->Any:\n from.. import BaseModel\n \n if isinstance(v,BaseModel):\n  if to_dict:\n   return v.model_dump(\n   by_alias=by_alias,\n   exclude_unset=exclude_unset,\n   exclude_defaults=exclude_defaults,\n   include=include,\n   exclude=exclude,\n   exclude_none=exclude_none,\n   )\n  else:\n   return v.copy(include=include,exclude=exclude)\n   \n value_exclude=_utils.ValueItems(v,exclude)if exclude else None\n value_include=_utils.ValueItems(v,include)if include else None\n \n if isinstance(v,dict):\n  return{\n  k_:_get_value(\n  cls,\n  v_,\n  to_dict=to_dict,\n  by_alias=by_alias,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  include=value_include and value_include.for_element(k_),\n  exclude=value_exclude and value_exclude.for_element(k_),\n  exclude_none=exclude_none,\n  )\n  for k_,v_ in v.items()\n  if(not value_exclude or not value_exclude.is_excluded(k_))\n  and(not value_include or value_include.is_included(k_))\n  }\n  \n elif _utils.sequence_like(v):\n  seq_args=(\n  _get_value(\n  cls,\n  v_,\n  to_dict=to_dict,\n  by_alias=by_alias,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  include=value_include and value_include.for_element(i),\n  exclude=value_exclude and value_exclude.for_element(i),\n  exclude_none=exclude_none,\n  )\n  for i,v_ in enumerate(v)\n  if(not value_exclude or not value_exclude.is_excluded(i))\n  and(not value_include or value_include.is_included(i))\n  )\n  \n  return v.__class__(*seq_args)if _typing_extra.is_namedtuple(v.__class__)else v.__class__(seq_args)\n  \n elif isinstance(v,Enum)and getattr(cls.model_config,'use_enum_values',False):\n  return v.value\n  \n else:\n  return v\n  \n  \ndef _calculate_keys(\nself:BaseModel,\ninclude:MappingIntStrAny |None,\nexclude:MappingIntStrAny |None,\nexclude_unset:bool,\nupdate:typing.Dict[str,Any]|None=None,\n)->typing.AbstractSet[str]|None:\n if include is None and exclude is None and exclude_unset is False:\n  return None\n  \n keys:typing.AbstractSet[str]\n if exclude_unset:\n  keys=self.__pydantic_fields_set__.copy()\n else:\n  keys=set(self.__dict__.keys())\n  keys=keys |(self.__pydantic_extra__ or{}).keys()\n  \n if include is not None:\n  keys &=include.keys()\n  \n if update:\n  keys -=update.keys()\n  \n if exclude:\n  keys -={k for k,v in exclude.items()if _utils.ValueItems.is_true(v)}\n  \n return keys\n", ["__future__", "copy", "enum", "pydantic", "pydantic._internal", "pydantic._internal._model_construction", "pydantic._internal._typing_extra", "pydantic._internal._utils", "typing", "typing_extensions"]], "pydantic.deprecated.parse": [".py", "from __future__ import annotations\n\nimport json\nimport pickle\nimport warnings\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING,Any,Callable\n\nfrom typing_extensions import deprecated\n\nfrom..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n \nclass Protocol(str,Enum):\n json='json'\n pickle='pickle'\n \n \n@deprecated('`load_str_bytes` is deprecated.',category=None)\ndef load_str_bytes(\nb:str |bytes,\n*,\ncontent_type:str |None=None,\nencoding:str='utf8',\nproto:Protocol |None=None,\nallow_pickle:bool=False,\njson_loads:Callable[[str],Any]=json.loads,\n)->Any:\n warnings.warn('`load_str_bytes` is deprecated.',category=PydanticDeprecatedSince20,stacklevel=2)\n if proto is None and content_type:\n  if content_type.endswith(('json','javascript')):\n   pass\n  elif allow_pickle and content_type.endswith('pickle'):\n   proto=Protocol.pickle\n  else:\n   raise TypeError(f'Unknown content-type: {content_type}')\n   \n proto=proto or Protocol.json\n \n if proto ==Protocol.json:\n  if isinstance(b,bytes):\n   b=b.decode(encoding)\n  return json_loads(b)\n elif proto ==Protocol.pickle:\n  if not allow_pickle:\n   raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n  bb=b if isinstance(b,bytes)else b.encode()\n  return pickle.loads(bb)\n else:\n  raise TypeError(f'Unknown protocol: {proto}')\n  \n  \n@deprecated('`load_file` is deprecated.',category=None)\ndef load_file(\npath:str |Path,\n*,\ncontent_type:str |None=None,\nencoding:str='utf8',\nproto:Protocol |None=None,\nallow_pickle:bool=False,\njson_loads:Callable[[str],Any]=json.loads,\n)->Any:\n warnings.warn('`load_file` is deprecated.',category=PydanticDeprecatedSince20,stacklevel=2)\n path=Path(path)\n b=path.read_bytes()\n if content_type is None:\n  if path.suffix in('.js','.json'):\n   proto=Protocol.json\n  elif path.suffix =='.pkl':\n   proto=Protocol.pickle\n   \n return load_str_bytes(\n b,proto=proto,content_type=content_type,encoding=encoding,allow_pickle=allow_pickle,json_loads=json_loads\n )\n", ["__future__", "enum", "json", "pathlib", "pickle", "pydantic.warnings", "typing", "typing_extensions", "warnings"]], "pydantic.deprecated.decorator": [".py", "import warnings\nfrom functools import wraps\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,List,Mapping,Optional,Tuple,Type,TypeVar,Union,overload\n\nfrom typing_extensions import deprecated\n\nfrom.._internal import _config,_typing_extra\nfrom..alias_generators import to_pascal\nfrom..errors import PydanticUserError\nfrom..functional_validators import field_validator\nfrom..main import BaseModel,create_model\nfrom..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n__all__=('validate_arguments',)\n\nif TYPE_CHECKING:\n AnyCallable=Callable[...,Any]\n \n AnyCallableT=TypeVar('AnyCallableT',bound=AnyCallable)\n ConfigType=Union[None,Type[Any],Dict[str,Any]]\n \n \n@overload\ndef validate_arguments(\nfunc:None=None,*,config:'ConfigType'=None\n)->Callable[['AnyCallableT'],'AnyCallableT']:...\n\n\n@overload\ndef validate_arguments(func:'AnyCallableT')->'AnyCallableT':...\n\n\n@deprecated(\n'The `validate_arguments` method is deprecated; use `validate_call` instead.',\ncategory=None,\n)\ndef validate_arguments(func:Optional['AnyCallableT']=None,*,config:'ConfigType'=None)->Any:\n ''\n warnings.warn(\n 'The `validate_arguments` method is deprecated; use `validate_call` instead.',\n PydanticDeprecatedSince20,\n stacklevel=2,\n )\n \n def validate(_func:'AnyCallable')->'AnyCallable':\n  vd=ValidatedFunction(_func,config)\n  \n  @wraps(_func)\n  def wrapper_function(*args:Any,**kwargs:Any)->Any:\n   return vd.call(*args,**kwargs)\n   \n  wrapper_function.vd=vd\n  wrapper_function.validate=vd.init_model_instance\n  wrapper_function.raw_function=vd.raw_function\n  wrapper_function.model=vd.model\n  return wrapper_function\n  \n if func:\n  return validate(func)\n else:\n  return validate\n  \n  \nALT_V_ARGS='v__args'\nALT_V_KWARGS='v__kwargs'\nV_POSITIONAL_ONLY_NAME='v__positional_only'\nV_DUPLICATE_KWARGS='v__duplicate_kwargs'\n\n\nclass ValidatedFunction:\n def __init__(self,function:'AnyCallable',config:'ConfigType'):\n  from inspect import Parameter,signature\n  \n  parameters:Mapping[str,Parameter]=signature(function).parameters\n  \n  if parameters.keys()&{ALT_V_ARGS,ALT_V_KWARGS,V_POSITIONAL_ONLY_NAME,V_DUPLICATE_KWARGS}:\n   raise PydanticUserError(\n   f'\"{ALT_V_ARGS}\", \"{ALT_V_KWARGS}\", \"{V_POSITIONAL_ONLY_NAME}\" and \"{V_DUPLICATE_KWARGS}\" '\n   f'are not permitted as argument names when using the \"{validate_arguments.__name__}\" decorator',\n   code=None,\n   )\n   \n  self.raw_function=function\n  self.arg_mapping:Dict[int,str]={}\n  self.positional_only_args:set[str]=set()\n  self.v_args_name='args'\n  self.v_kwargs_name='kwargs'\n  \n  type_hints=_typing_extra.get_type_hints(function,include_extras=True)\n  takes_args=False\n  takes_kwargs=False\n  fields:Dict[str,Tuple[Any,Any]]={}\n  for i,(name,p)in enumerate(parameters.items()):\n   if p.annotation is p.empty:\n    annotation=Any\n   else:\n    annotation=type_hints[name]\n    \n   default=...if p.default is p.empty else p.default\n   if p.kind ==Parameter.POSITIONAL_ONLY:\n    self.arg_mapping[i]=name\n    fields[name]=annotation,default\n    fields[V_POSITIONAL_ONLY_NAME]=List[str],None\n    self.positional_only_args.add(name)\n   elif p.kind ==Parameter.POSITIONAL_OR_KEYWORD:\n    self.arg_mapping[i]=name\n    fields[name]=annotation,default\n    fields[V_DUPLICATE_KWARGS]=List[str],None\n   elif p.kind ==Parameter.KEYWORD_ONLY:\n    fields[name]=annotation,default\n   elif p.kind ==Parameter.VAR_POSITIONAL:\n    self.v_args_name=name\n    fields[name]=Tuple[annotation,...],None\n    takes_args=True\n   else:\n    assert p.kind ==Parameter.VAR_KEYWORD,p.kind\n    self.v_kwargs_name=name\n    fields[name]=Dict[str,annotation],None\n    takes_kwargs=True\n    \n    \n  if not takes_args and self.v_args_name in fields:\n   self.v_args_name=ALT_V_ARGS\n   \n   \n  if not takes_kwargs and self.v_kwargs_name in fields:\n   self.v_kwargs_name=ALT_V_KWARGS\n   \n  if not takes_args:\n  \n   fields[self.v_args_name]=List[Any],None\n   \n  if not takes_kwargs:\n  \n   fields[self.v_kwargs_name]=Dict[Any,Any],None\n   \n  self.create_model(fields,takes_args,takes_kwargs,config)\n  \n def init_model_instance(self,*args:Any,**kwargs:Any)->BaseModel:\n  values=self.build_values(args,kwargs)\n  return self.model(**values)\n  \n def call(self,*args:Any,**kwargs:Any)->Any:\n  m=self.init_model_instance(*args,**kwargs)\n  return self.execute(m)\n  \n def build_values(self,args:Tuple[Any,...],kwargs:Dict[str,Any])->Dict[str,Any]:\n  values:Dict[str,Any]={}\n  if args:\n   arg_iter=enumerate(args)\n   while True:\n    try:\n     i,a=next(arg_iter)\n    except StopIteration:\n     break\n    arg_name=self.arg_mapping.get(i)\n    if arg_name is not None:\n     values[arg_name]=a\n    else:\n     values[self.v_args_name]=[a]+[a for _,a in arg_iter]\n     break\n     \n  var_kwargs:Dict[str,Any]={}\n  wrong_positional_args=[]\n  duplicate_kwargs=[]\n  fields_alias=[\n  field.alias\n  for name,field in self.model.__pydantic_fields__.items()\n  if name not in(self.v_args_name,self.v_kwargs_name)\n  ]\n  non_var_fields=set(self.model.__pydantic_fields__)-{self.v_args_name,self.v_kwargs_name}\n  for k,v in kwargs.items():\n   if k in non_var_fields or k in fields_alias:\n    if k in self.positional_only_args:\n     wrong_positional_args.append(k)\n    if k in values:\n     duplicate_kwargs.append(k)\n    values[k]=v\n   else:\n    var_kwargs[k]=v\n    \n  if var_kwargs:\n   values[self.v_kwargs_name]=var_kwargs\n  if wrong_positional_args:\n   values[V_POSITIONAL_ONLY_NAME]=wrong_positional_args\n  if duplicate_kwargs:\n   values[V_DUPLICATE_KWARGS]=duplicate_kwargs\n  return values\n  \n def execute(self,m:BaseModel)->Any:\n  d={\n  k:v\n  for k,v in m.__dict__.items()\n  if k in m.__pydantic_fields_set__ or m.__pydantic_fields__[k].default_factory\n  }\n  var_kwargs=d.pop(self.v_kwargs_name,{})\n  \n  if self.v_args_name in d:\n   args_:List[Any]=[]\n   in_kwargs=False\n   kwargs={}\n   for name,value in d.items():\n    if in_kwargs:\n     kwargs[name]=value\n    elif name ==self.v_args_name:\n     args_ +=value\n     in_kwargs=True\n    else:\n     args_.append(value)\n   return self.raw_function(*args_,**kwargs,**var_kwargs)\n  elif self.positional_only_args:\n   args_=[]\n   kwargs={}\n   for name,value in d.items():\n    if name in self.positional_only_args:\n     args_.append(value)\n    else:\n     kwargs[name]=value\n   return self.raw_function(*args_,**kwargs,**var_kwargs)\n  else:\n   return self.raw_function(**d,**var_kwargs)\n   \n def create_model(self,fields:Dict[str,Any],takes_args:bool,takes_kwargs:bool,config:'ConfigType')->None:\n  pos_args=len(self.arg_mapping)\n  \n  config_wrapper=_config.ConfigWrapper(config)\n  \n  if config_wrapper.alias_generator:\n   raise PydanticUserError(\n   'Setting the \"alias_generator\" property on custom Config for '\n   '@validate_arguments is not yet supported, please remove.',\n   code=None,\n   )\n  if config_wrapper.extra is None:\n   config_wrapper.config_dict['extra']='forbid'\n   \n  class DecoratorBaseModel(BaseModel):\n   @field_validator(self.v_args_name,check_fields=False)\n   @classmethod\n   def check_args(cls,v:Optional[List[Any]])->Optional[List[Any]]:\n    if takes_args or v is None:\n     return v\n     \n    raise TypeError(f'{pos_args} positional arguments expected but {pos_args+len(v)} given')\n    \n   @field_validator(self.v_kwargs_name,check_fields=False)\n   @classmethod\n   def check_kwargs(cls,v:Optional[Dict[str,Any]])->Optional[Dict[str,Any]]:\n    if takes_kwargs or v is None:\n     return v\n     \n    plural=''if len(v)==1 else 's'\n    keys=', '.join(map(repr,v.keys()))\n    raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n    \n   @field_validator(V_POSITIONAL_ONLY_NAME,check_fields=False)\n   @classmethod\n   def check_positional_only(cls,v:Optional[List[str]])->None:\n    if v is None:\n     return\n     \n    plural=''if len(v)==1 else 's'\n    keys=', '.join(map(repr,v))\n    raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n    \n   @field_validator(V_DUPLICATE_KWARGS,check_fields=False)\n   @classmethod\n   def check_duplicate_kwargs(cls,v:Optional[List[str]])->None:\n    if v is None:\n     return\n     \n    plural=''if len(v)==1 else 's'\n    keys=', '.join(map(repr,v))\n    raise TypeError(f'multiple values for argument{plural}: {keys}')\n    \n   model_config=config_wrapper.config_dict\n   \n  self.model=create_model(to_pascal(self.raw_function.__name__),__base__=DecoratorBaseModel,**fields)\n", ["functools", "inspect", "pydantic._internal", "pydantic._internal._config", "pydantic._internal._typing_extra", "pydantic.alias_generators", "pydantic.errors", "pydantic.functional_validators", "pydantic.main", "pydantic.warnings", "typing", "typing_extensions", "warnings"]], "pydantic.deprecated.class_validators": [".py", "''\n\nfrom __future__ import annotations as _annotations\n\nfrom functools import partial,partialmethod\nfrom types import FunctionType\nfrom typing import TYPE_CHECKING,Any,Callable,TypeVar,Union,overload\nfrom warnings import warn\n\nfrom typing_extensions import Literal,Protocol,TypeAlias,deprecated\n\nfrom.._internal import _decorators,_decorators_v1\nfrom..errors import PydanticUserError\nfrom..warnings import PydanticDeprecatedSince20\n\n_ALLOW_REUSE_WARNING_MESSAGE='`allow_reuse` is deprecated and will be ignored; it should no longer be necessary'\n\n\nif TYPE_CHECKING:\n\n class _OnlyValueValidatorClsMethod(Protocol):\n  def __call__(self,__cls:Any,__value:Any)->Any:...\n  \n class _V1ValidatorWithValuesClsMethod(Protocol):\n  def __call__(self,__cls:Any,__value:Any,values:dict[str,Any])->Any:...\n  \n class _V1ValidatorWithValuesKwOnlyClsMethod(Protocol):\n  def __call__(self,__cls:Any,__value:Any,*,values:dict[str,Any])->Any:...\n  \n class _V1ValidatorWithKwargsClsMethod(Protocol):\n  def __call__(self,__cls:Any,**kwargs:Any)->Any:...\n  \n class _V1ValidatorWithValuesAndKwargsClsMethod(Protocol):\n  def __call__(self,__cls:Any,values:dict[str,Any],**kwargs:Any)->Any:...\n  \n class _V1RootValidatorClsMethod(Protocol):\n  def __call__(\n  self,__cls:Any,__values:_decorators_v1.RootValidatorValues\n  )->_decorators_v1.RootValidatorValues:...\n  \n V1Validator=Union[\n _OnlyValueValidatorClsMethod,\n _V1ValidatorWithValuesClsMethod,\n _V1ValidatorWithValuesKwOnlyClsMethod,\n _V1ValidatorWithKwargsClsMethod,\n _V1ValidatorWithValuesAndKwargsClsMethod,\n _decorators_v1.V1ValidatorWithValues,\n _decorators_v1.V1ValidatorWithValuesKwOnly,\n _decorators_v1.V1ValidatorWithKwargs,\n _decorators_v1.V1ValidatorWithValuesAndKwargs,\n ]\n \n V1RootValidator=Union[\n _V1RootValidatorClsMethod,\n _decorators_v1.V1RootValidatorFunction,\n ]\n \n _PartialClsOrStaticMethod:TypeAlias=Union[classmethod[Any,Any,Any],staticmethod[Any,Any],partialmethod[Any]]\n \n \n \n \n _V1ValidatorType=TypeVar('_V1ValidatorType',V1Validator,_PartialClsOrStaticMethod)\n _V1RootValidatorFunctionType=TypeVar(\n '_V1RootValidatorFunctionType',\n _decorators_v1.V1RootValidatorFunction,\n _V1RootValidatorClsMethod,\n _PartialClsOrStaticMethod,\n )\nelse:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n \n@deprecated(\n'Pydantic V1 style `@validator` validators are deprecated.'\n' You should migrate to Pydantic V2 style `@field_validator` validators,'\n' see the migration guide for more details',\ncategory=None,\n)\ndef validator(\n__field:str,\n*fields:str,\npre:bool=False,\neach_item:bool=False,\nalways:bool=False,\ncheck_fields:bool |None=None,\nallow_reuse:bool=False,\n)->Callable[[_V1ValidatorType],_V1ValidatorType]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n warn(\n 'Pydantic V1 style `@validator` validators are deprecated.'\n ' You should migrate to Pydantic V2 style `@field_validator` validators,'\n ' see the migration guide for more details',\n DeprecationWarning,\n stacklevel=2,\n )\n \n if allow_reuse is True:\n  warn(_ALLOW_REUSE_WARNING_MESSAGE,DeprecationWarning)\n fields=__field,*fields\n if isinstance(fields[0],FunctionType):\n  raise PydanticUserError(\n  '`@validator` should be used with fields and keyword arguments, not bare. '\n  \"E.g. usage should be `@validator('<field_name>', ...)`\",\n  code='validator-no-fields',\n  )\n elif not all(isinstance(field,str)for field in fields):\n  raise PydanticUserError(\n  '`@validator` fields should be passed as separate string args. '\n  \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\",\n  code='validator-invalid-fields',\n  )\n  \n mode:Literal['before','after']='before'if pre is True else 'after'\n \n def dec(f:Any)->_decorators.PydanticDescriptorProxy[Any]:\n  if _decorators.is_instance_method_from_sig(f):\n   raise PydanticUserError(\n   '`@validator` cannot be applied to instance methods',code='validator-instance-method'\n   )\n   \n  f=_decorators.ensure_classmethod_based_on_signature(f)\n  wrap=_decorators_v1.make_generic_v1_field_validator\n  validator_wrapper_info=_decorators.ValidatorDecoratorInfo(\n  fields=fields,\n  mode=mode,\n  each_item=each_item,\n  always=always,\n  check_fields=check_fields,\n  )\n  return _decorators.PydanticDescriptorProxy(f,validator_wrapper_info,shim=wrap)\n  \n return dec\n \n \n@overload\ndef root_validator(\n*,\n\n\nskip_on_failure:Literal[True],\nallow_reuse:bool=...,\n)->Callable[\n[_V1RootValidatorFunctionType],\n_V1RootValidatorFunctionType,\n]:...\n\n\n@overload\ndef root_validator(\n*,\n\n\npre:Literal[True],\nallow_reuse:bool=...,\n)->Callable[\n[_V1RootValidatorFunctionType],\n_V1RootValidatorFunctionType,\n]:...\n\n\n@overload\ndef root_validator(\n*,\n\n\npre:Literal[False],\nskip_on_failure:Literal[True],\nallow_reuse:bool=...,\n)->Callable[\n[_V1RootValidatorFunctionType],\n_V1RootValidatorFunctionType,\n]:...\n\n\n@deprecated(\n'Pydantic V1 style `@root_validator` validators are deprecated.'\n' You should migrate to Pydantic V2 style `@model_validator` validators,'\n' see the migration guide for more details',\ncategory=None,\n)\ndef root_validator(\n*__args,\npre:bool=False,\nskip_on_failure:bool=False,\nallow_reuse:bool=False,\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n warn(\n 'Pydantic V1 style `@root_validator` validators are deprecated.'\n ' You should migrate to Pydantic V2 style `@model_validator` validators,'\n ' see the migration guide for more details',\n DeprecationWarning,\n stacklevel=2,\n )\n \n if __args:\n \n  return root_validator()(*__args)\n  \n if allow_reuse is True:\n  warn(_ALLOW_REUSE_WARNING_MESSAGE,DeprecationWarning)\n mode:Literal['before','after']='before'if pre is True else 'after'\n if pre is False and skip_on_failure is not True:\n  raise PydanticUserError(\n  'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'\n  ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.',\n  code='root-validator-pre-skip',\n  )\n  \n wrap=partial(_decorators_v1.make_v1_generic_root_validator,pre=pre)\n \n def dec(f:Callable[...,Any]|classmethod[Any,Any,Any]|staticmethod[Any,Any])->Any:\n  if _decorators.is_instance_method_from_sig(f):\n   raise TypeError('`@root_validator` cannot be applied to instance methods')\n   \n  res=_decorators.ensure_classmethod_based_on_signature(f)\n  dec_info=_decorators.RootValidatorDecoratorInfo(mode=mode)\n  return _decorators.PydanticDescriptorProxy(res,dec_info,shim=wrap)\n  \n return dec\n", ["__future__", "functools", "pydantic._internal", "pydantic._internal._decorators", "pydantic._internal._decorators_v1", "pydantic.errors", "pydantic.warnings", "types", "typing", "typing_extensions", "warnings"]], "pydantic.deprecated.json": [".py", "import datetime\nimport warnings\nfrom collections import deque\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom pathlib import Path\nfrom re import Pattern\nfrom types import GeneratorType\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,Type,Union\nfrom uuid import UUID\n\nfrom typing_extensions import deprecated\n\nfrom.._internal._import_utils import import_cached_base_model\nfrom..color import Color\nfrom..networks import NameEmail\nfrom..types import SecretBytes,SecretStr\nfrom..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n__all__='pydantic_encoder','custom_pydantic_encoder','timedelta_isoformat'\n\n\ndef isoformat(o:Union[datetime.date,datetime.time])->str:\n return o.isoformat()\n \n \ndef decimal_encoder(dec_value:Decimal)->Union[int,float]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n \n exponent=dec_value.as_tuple().exponent\n if isinstance(exponent,int)and exponent >=0:\n  return int(dec_value)\n else:\n  return float(dec_value)\n  \n  \nENCODERS_BY_TYPE:Dict[Type[Any],Callable[[Any],Any]]={\nbytes:lambda o:o.decode(),\nColor:str,\ndatetime.date:isoformat,\ndatetime.datetime:isoformat,\ndatetime.time:isoformat,\ndatetime.timedelta:lambda td:td.total_seconds(),\nDecimal:decimal_encoder,\nEnum:lambda o:o.value,\nfrozenset:list,\ndeque:list,\nGeneratorType:list,\nIPv4Address:str,\nIPv4Interface:str,\nIPv4Network:str,\nIPv6Address:str,\nIPv6Interface:str,\nIPv6Network:str,\nNameEmail:str,\nPath:str,\nPattern:lambda o:o.pattern,\nSecretBytes:str,\nSecretStr:str,\nset:list,\nUUID:str,\n}\n\n\n@deprecated(\n'`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\ncategory=None,\n)\ndef pydantic_encoder(obj:Any)->Any:\n warnings.warn(\n '`pydantic_encoder` is deprecated, use `pydantic_core.to_jsonable_python` instead.',\n category=PydanticDeprecatedSince20,\n stacklevel=2,\n )\n from dataclasses import asdict,is_dataclass\n \n BaseModel=import_cached_base_model()\n \n if isinstance(obj,BaseModel):\n  return obj.model_dump()\n elif is_dataclass(obj):\n  return asdict(obj)\n  \n  \n for base in obj.__class__.__mro__[:-1]:\n  try:\n   encoder=ENCODERS_BY_TYPE[base]\n  except KeyError:\n   continue\n  return encoder(obj)\n else:\n  raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")\n  \n  \n  \n@deprecated(\n'`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\ncategory=None,\n)\ndef custom_pydantic_encoder(type_encoders:Dict[Any,Callable[[Type[Any]],Any]],obj:Any)->Any:\n warnings.warn(\n '`custom_pydantic_encoder` is deprecated, use `BaseModel.model_dump` instead.',\n category=PydanticDeprecatedSince20,\n stacklevel=2,\n )\n \n for base in obj.__class__.__mro__[:-1]:\n  try:\n   encoder=type_encoders[base]\n  except KeyError:\n   continue\n   \n  return encoder(obj)\n else:\n  return pydantic_encoder(obj)\n  \n  \n@deprecated('`timedelta_isoformat` is deprecated.',category=None)\ndef timedelta_isoformat(td:datetime.timedelta)->str:\n ''\n warnings.warn('`timedelta_isoformat` is deprecated.',category=PydanticDeprecatedSince20,stacklevel=2)\n minutes,seconds=divmod(td.seconds,60)\n hours,minutes=divmod(minutes,60)\n return f'{\"-\"if td.days <0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'\n", ["collections", "dataclasses", "datetime", "decimal", "enum", "ipaddress", "pathlib", "pydantic._internal._import_utils", "pydantic.color", "pydantic.networks", "pydantic.types", "pydantic.warnings", "re", "types", "typing", "typing_extensions", "uuid", "warnings"]], "pydantic.deprecated.tools": [".py", "from __future__ import annotations\n\nimport json\nimport warnings\nfrom typing import TYPE_CHECKING,Any,Callable,Type,TypeVar,Union\n\nfrom typing_extensions import deprecated\n\nfrom..json_schema import DEFAULT_REF_TEMPLATE,GenerateJsonSchema\nfrom..type_adapter import TypeAdapter\nfrom..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n__all__='parse_obj_as','schema_of','schema_json_of'\n\nNameFactory=Union[str,Callable[[Type[Any]],str]]\n\n\nT=TypeVar('T')\n\n\n@deprecated(\n'`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\ncategory=None,\n)\ndef parse_obj_as(type_:type[T],obj:Any,type_name:NameFactory |None=None)->T:\n warnings.warn(\n '`parse_obj_as` is deprecated. Use `pydantic.TypeAdapter.validate_python` instead.',\n category=PydanticDeprecatedSince20,\n stacklevel=2,\n )\n if type_name is not None:\n  warnings.warn(\n  'The type_name parameter is deprecated. parse_obj_as no longer creates temporary models',\n  DeprecationWarning,\n  stacklevel=2,\n  )\n return TypeAdapter(type_).validate_python(obj)\n \n \n@deprecated(\n'`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\ncategory=None,\n)\ndef schema_of(\ntype_:Any,\n*,\ntitle:NameFactory |None=None,\nby_alias:bool=True,\nref_template:str=DEFAULT_REF_TEMPLATE,\nschema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\n)->dict[str,Any]:\n ''\n warnings.warn(\n '`schema_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n category=PydanticDeprecatedSince20,\n stacklevel=2,\n )\n res=TypeAdapter(type_).json_schema(\n by_alias=by_alias,\n schema_generator=schema_generator,\n ref_template=ref_template,\n )\n if title is not None:\n  if isinstance(title,str):\n   res['title']=title\n  else:\n   warnings.warn(\n   'Passing a callable for the `title` parameter is deprecated and no longer supported',\n   DeprecationWarning,\n   stacklevel=2,\n   )\n   res['title']=title(type_)\n return res\n \n \n@deprecated(\n'`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\ncategory=None,\n)\ndef schema_json_of(\ntype_:Any,\n*,\ntitle:NameFactory |None=None,\nby_alias:bool=True,\nref_template:str=DEFAULT_REF_TEMPLATE,\nschema_generator:type[GenerateJsonSchema]=GenerateJsonSchema,\n**dumps_kwargs:Any,\n)->str:\n ''\n warnings.warn(\n '`schema_json_of` is deprecated. Use `pydantic.TypeAdapter.json_schema` instead.',\n category=PydanticDeprecatedSince20,\n stacklevel=2,\n )\n return json.dumps(\n schema_of(type_,title=title,by_alias=by_alias,ref_template=ref_template,schema_generator=schema_generator),\n **dumps_kwargs,\n )\n", ["__future__", "json", "pydantic.json_schema", "pydantic.type_adapter", "pydantic.warnings", "typing", "typing_extensions", "warnings"]], "pydantic.deprecated.config": [".py", "from __future__ import annotations as _annotations\n\nimport warnings\nfrom typing import TYPE_CHECKING,Any\n\nfrom typing_extensions import Literal,deprecated\n\nfrom.._internal import _config\nfrom..warnings import PydanticDeprecatedSince20\n\nif not TYPE_CHECKING:\n\n\n DeprecationWarning=PydanticDeprecatedSince20\n \n__all__='BaseConfig','Extra'\n\n\nclass _ConfigMetaclass(type):\n def __getattr__(self,item:str)->Any:\n  try:\n   obj=_config.config_defaults[item]\n   warnings.warn(_config.DEPRECATION_MESSAGE,DeprecationWarning)\n   return obj\n  except KeyError as exc:\n   raise AttributeError(f\"type object '{self.__name__}' has no attribute {exc}\")from exc\n   \n   \n@deprecated('BaseConfig is deprecated. Use the `pydantic.ConfigDict` instead.',category=PydanticDeprecatedSince20)\nclass BaseConfig(metaclass=_ConfigMetaclass):\n ''\n\n\n\n \n \n def __getattr__(self,item:str)->Any:\n  try:\n   obj=super().__getattribute__(item)\n   warnings.warn(_config.DEPRECATION_MESSAGE,DeprecationWarning)\n   return obj\n  except AttributeError as exc:\n   try:\n    return getattr(type(self),item)\n   except AttributeError:\n   \n    raise AttributeError(str(exc))from exc\n    \n def __init_subclass__(cls,**kwargs:Any)->None:\n  warnings.warn(_config.DEPRECATION_MESSAGE,DeprecationWarning)\n  return super().__init_subclass__(**kwargs)\n  \n  \nclass _ExtraMeta(type):\n def __getattribute__(self,__name:str)->Any:\n \n  if __name in{'allow','ignore','forbid'}:\n   warnings.warn(\n   \"`pydantic.config.Extra` is deprecated, use literal values instead (e.g. `extra='allow'`)\",\n   DeprecationWarning,\n   stacklevel=2,\n   )\n  return super().__getattribute__(__name)\n  \n  \n@deprecated(\n\"Extra is deprecated. Use literal values instead (e.g. `extra='allow'`)\",category=PydanticDeprecatedSince20\n)\nclass Extra(metaclass=_ExtraMeta):\n allow:Literal['allow']='allow'\n ignore:Literal['ignore']='ignore'\n forbid:Literal['forbid']='forbid'\n", ["__future__", "pydantic._internal", "pydantic._internal._config", "pydantic.warnings", "typing", "typing_extensions", "warnings"]], "pydantic.experimental": [".py", "''\n\nimport warnings\n\nfrom pydantic.warnings import PydanticExperimentalWarning\n\nwarnings.warn(\n'This module is experimental, its contents are subject to change and deprecation.',\ncategory=PydanticExperimentalWarning,\n)\n", ["pydantic.warnings", "warnings"], 1], "pydantic.experimental.pipeline": [".py", "''\n\nfrom __future__ import annotations\n\nimport datetime\nimport operator\nimport re\nimport sys\nfrom collections import deque\nfrom collections.abc import Container\nfrom dataclasses import dataclass\nfrom decimal import Decimal\nfrom functools import cached_property,partial\nfrom typing import TYPE_CHECKING,Any,Callable,Generic,Pattern,Protocol,TypeVar,Union,overload\n\nimport annotated_types\nfrom typing_extensions import Annotated\n\nif TYPE_CHECKING:\n from pydantic_core import core_schema as cs\n \n from pydantic import GetCoreSchemaHandler\n \nfrom pydantic._internal._internal_dataclass import slots_true as _slots_true\n\nif sys.version_info <(3,10):\n EllipsisType=type(Ellipsis)\nelse:\n from types import EllipsisType\n \n__all__=['validate_as','validate_as_deferred','transform']\n\n_slots_frozen={**_slots_true,'frozen':True}\n\n\n@dataclass(**_slots_frozen)\nclass _ValidateAs:\n tp:type[Any]\n strict:bool=False\n \n \n@dataclass\nclass _ValidateAsDefer:\n func:Callable[[],type[Any]]\n \n @cached_property\n def tp(self)->type[Any]:\n  return self.func()\n  \n  \n@dataclass(**_slots_frozen)\nclass _Transform:\n func:Callable[[Any],Any]\n \n \n@dataclass(**_slots_frozen)\nclass _PipelineOr:\n left:_Pipeline[Any,Any]\n right:_Pipeline[Any,Any]\n \n \n@dataclass(**_slots_frozen)\nclass _PipelineAnd:\n left:_Pipeline[Any,Any]\n right:_Pipeline[Any,Any]\n \n \n@dataclass(**_slots_frozen)\nclass _Eq:\n value:Any\n \n \n@dataclass(**_slots_frozen)\nclass _NotEq:\n value:Any\n \n \n@dataclass(**_slots_frozen)\nclass _In:\n values:Container[Any]\n \n \n@dataclass(**_slots_frozen)\nclass _NotIn:\n values:Container[Any]\n \n \n_ConstraintAnnotation=Union[\nannotated_types.Le,\nannotated_types.Ge,\nannotated_types.Lt,\nannotated_types.Gt,\nannotated_types.Len,\nannotated_types.MultipleOf,\nannotated_types.Timezone,\nannotated_types.Interval,\nannotated_types.Predicate,\n\n_Eq,\n_NotEq,\n_In,\n_NotIn,\n\nPattern[str],\n]\n\n\n@dataclass(**_slots_frozen)\nclass _Constraint:\n constraint:_ConstraintAnnotation\n \n \n_Step=Union[_ValidateAs,_ValidateAsDefer,_Transform,_PipelineOr,_PipelineAnd,_Constraint]\n\n_InT=TypeVar('_InT')\n_OutT=TypeVar('_OutT')\n_NewOutT=TypeVar('_NewOutT')\n\n\nclass _FieldTypeMarker:\n pass\n \n \n \n \n \n \n@dataclass(**_slots_true)\nclass _Pipeline(Generic[_InT,_OutT]):\n ''\n \n _steps:tuple[_Step,...]\n \n def transform(\n self,\n func:Callable[[_OutT],_NewOutT],\n )->_Pipeline[_InT,_NewOutT]:\n  ''\n\n\n\n  \n  return _Pipeline[_InT,_NewOutT](self._steps+(_Transform(func),))\n  \n @overload\n def validate_as(self,tp:type[_NewOutT],*,strict:bool=...)->_Pipeline[_InT,_NewOutT]:...\n \n @overload\n def validate_as(self,tp:EllipsisType,*,strict:bool=...)->_Pipeline[_InT,Any]:\n  ...\n  \n def validate_as(self,tp:type[_NewOutT]|EllipsisType,*,strict:bool=False)->_Pipeline[_InT,Any]:\n  ''\n\n\n\n\n\n  \n  if isinstance(tp,EllipsisType):\n   return _Pipeline[_InT,Any](self._steps+(_ValidateAs(_FieldTypeMarker,strict=strict),))\n  return _Pipeline[_InT,_NewOutT](self._steps+(_ValidateAs(tp,strict=strict),))\n  \n def validate_as_deferred(self,func:Callable[[],type[_NewOutT]])->_Pipeline[_InT,_NewOutT]:\n  ''\n\n\n\n  \n  return _Pipeline[_InT,_NewOutT](self._steps+(_ValidateAsDefer(func),))\n  \n  \n @overload\n def constrain(self:_Pipeline[_InT,_NewOutGe],constraint:annotated_types.Ge)->_Pipeline[_InT,_NewOutGe]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_NewOutGt],constraint:annotated_types.Gt)->_Pipeline[_InT,_NewOutGt]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_NewOutLe],constraint:annotated_types.Le)->_Pipeline[_InT,_NewOutLe]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_NewOutLt],constraint:annotated_types.Lt)->_Pipeline[_InT,_NewOutLt]:...\n \n @overload\n def constrain(\n self:_Pipeline[_InT,_NewOutLen],constraint:annotated_types.Len\n )->_Pipeline[_InT,_NewOutLen]:...\n \n @overload\n def constrain(\n self:_Pipeline[_InT,_NewOutT],constraint:annotated_types.MultipleOf\n )->_Pipeline[_InT,_NewOutT]:...\n \n @overload\n def constrain(\n self:_Pipeline[_InT,_NewOutDatetime],constraint:annotated_types.Timezone\n )->_Pipeline[_InT,_NewOutDatetime]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_OutT],constraint:annotated_types.Predicate)->_Pipeline[_InT,_OutT]:...\n \n @overload\n def constrain(\n self:_Pipeline[_InT,_NewOutInterval],constraint:annotated_types.Interval\n )->_Pipeline[_InT,_NewOutInterval]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_OutT],constraint:_Eq)->_Pipeline[_InT,_OutT]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_OutT],constraint:_NotEq)->_Pipeline[_InT,_OutT]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_OutT],constraint:_In)->_Pipeline[_InT,_OutT]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_OutT],constraint:_NotIn)->_Pipeline[_InT,_OutT]:...\n \n @overload\n def constrain(self:_Pipeline[_InT,_NewOutT],constraint:Pattern[str])->_Pipeline[_InT,_NewOutT]:...\n \n def constrain(self,constraint:_ConstraintAnnotation)->Any:\n  ''\n\n\n\n\n\n  \n  return _Pipeline[_InT,_OutT](self._steps+(_Constraint(constraint),))\n  \n def predicate(self:_Pipeline[_InT,_NewOutT],func:Callable[[_NewOutT],bool])->_Pipeline[_InT,_NewOutT]:\n  ''\n  return self.constrain(annotated_types.Predicate(func))\n  \n def gt(self:_Pipeline[_InT,_NewOutGt],gt:_NewOutGt)->_Pipeline[_InT,_NewOutGt]:\n  ''\n  return self.constrain(annotated_types.Gt(gt))\n  \n def lt(self:_Pipeline[_InT,_NewOutLt],lt:_NewOutLt)->_Pipeline[_InT,_NewOutLt]:\n  ''\n  return self.constrain(annotated_types.Lt(lt))\n  \n def ge(self:_Pipeline[_InT,_NewOutGe],ge:_NewOutGe)->_Pipeline[_InT,_NewOutGe]:\n  ''\n  return self.constrain(annotated_types.Ge(ge))\n  \n def le(self:_Pipeline[_InT,_NewOutLe],le:_NewOutLe)->_Pipeline[_InT,_NewOutLe]:\n  ''\n  return self.constrain(annotated_types.Le(le))\n  \n def len(self:_Pipeline[_InT,_NewOutLen],min_len:int,max_len:int |None=None)->_Pipeline[_InT,_NewOutLen]:\n  ''\n  return self.constrain(annotated_types.Len(min_len,max_len))\n  \n @overload\n def multiple_of(self:_Pipeline[_InT,_NewOutDiv],multiple_of:_NewOutDiv)->_Pipeline[_InT,_NewOutDiv]:...\n \n @overload\n def multiple_of(self:_Pipeline[_InT,_NewOutMod],multiple_of:_NewOutMod)->_Pipeline[_InT,_NewOutMod]:...\n \n def multiple_of(self:_Pipeline[_InT,Any],multiple_of:Any)->_Pipeline[_InT,Any]:\n  ''\n  return self.constrain(annotated_types.MultipleOf(multiple_of))\n  \n def eq(self:_Pipeline[_InT,_OutT],value:_OutT)->_Pipeline[_InT,_OutT]:\n  ''\n  return self.constrain(_Eq(value))\n  \n def not_eq(self:_Pipeline[_InT,_OutT],value:_OutT)->_Pipeline[_InT,_OutT]:\n  ''\n  return self.constrain(_NotEq(value))\n  \n def in_(self:_Pipeline[_InT,_OutT],values:Container[_OutT])->_Pipeline[_InT,_OutT]:\n  ''\n  return self.constrain(_In(values))\n  \n def not_in(self:_Pipeline[_InT,_OutT],values:Container[_OutT])->_Pipeline[_InT,_OutT]:\n  ''\n  return self.constrain(_NotIn(values))\n  \n  \n def datetime_tz_naive(self:_Pipeline[_InT,datetime.datetime])->_Pipeline[_InT,datetime.datetime]:\n  return self.constrain(annotated_types.Timezone(None))\n  \n def datetime_tz_aware(self:_Pipeline[_InT,datetime.datetime])->_Pipeline[_InT,datetime.datetime]:\n  return self.constrain(annotated_types.Timezone(...))\n  \n def datetime_tz(\n self:_Pipeline[_InT,datetime.datetime],tz:datetime.tzinfo\n )->_Pipeline[_InT,datetime.datetime]:\n  return self.constrain(annotated_types.Timezone(tz))\n  \n def datetime_with_tz(\n self:_Pipeline[_InT,datetime.datetime],tz:datetime.tzinfo |None\n )->_Pipeline[_InT,datetime.datetime]:\n  return self.transform(partial(datetime.datetime.replace,tzinfo=tz))\n  \n  \n def str_lower(self:_Pipeline[_InT,str])->_Pipeline[_InT,str]:\n  return self.transform(str.lower)\n  \n def str_upper(self:_Pipeline[_InT,str])->_Pipeline[_InT,str]:\n  return self.transform(str.upper)\n  \n def str_title(self:_Pipeline[_InT,str])->_Pipeline[_InT,str]:\n  return self.transform(str.title)\n  \n def str_strip(self:_Pipeline[_InT,str])->_Pipeline[_InT,str]:\n  return self.transform(str.strip)\n  \n def str_pattern(self:_Pipeline[_InT,str],pattern:str)->_Pipeline[_InT,str]:\n  return self.constrain(re.compile(pattern))\n  \n def str_contains(self:_Pipeline[_InT,str],substring:str)->_Pipeline[_InT,str]:\n  return self.predicate(lambda v:substring in v)\n  \n def str_starts_with(self:_Pipeline[_InT,str],prefix:str)->_Pipeline[_InT,str]:\n  return self.predicate(lambda v:v.startswith(prefix))\n  \n def str_ends_with(self:_Pipeline[_InT,str],suffix:str)->_Pipeline[_InT,str]:\n  return self.predicate(lambda v:v.endswith(suffix))\n  \n  \n def otherwise(self,other:_Pipeline[_OtherIn,_OtherOut])->_Pipeline[_InT |_OtherIn,_OutT |_OtherOut]:\n  ''\n  return _Pipeline((_PipelineOr(self,other),))\n  \n __or__=otherwise\n \n def then(self,other:_Pipeline[_OutT,_OtherOut])->_Pipeline[_InT,_OtherOut]:\n  ''\n  return _Pipeline((_PipelineAnd(self,other),))\n  \n __and__=then\n \n def __get_pydantic_core_schema__(self,source_type:Any,handler:GetCoreSchemaHandler)->cs.CoreSchema:\n  from pydantic_core import core_schema as cs\n  \n  queue=deque(self._steps)\n  \n  s=None\n  \n  while queue:\n   step=queue.popleft()\n   s=_apply_step(step,s,handler,source_type)\n   \n  s=s or cs.any_schema()\n  return s\n  \n def __supports_type__(self,_:_OutT)->bool:\n  raise NotImplementedError\n  \n  \nvalidate_as=_Pipeline[Any,Any](()).validate_as\nvalidate_as_deferred=_Pipeline[Any,Any](()).validate_as_deferred\ntransform=_Pipeline[Any,Any]((_ValidateAs(_FieldTypeMarker),)).transform\n\n\ndef _check_func(\nfunc:Callable[[Any],bool],predicate_err:str |Callable[[],str],s:cs.CoreSchema |None\n)->cs.CoreSchema:\n from pydantic_core import core_schema as cs\n \n def handler(v:Any)->Any:\n  if func(v):\n   return v\n  raise ValueError(f'Expected {predicate_err if isinstance(predicate_err,str)else predicate_err()}')\n  \n if s is None:\n  return cs.no_info_plain_validator_function(handler)\n else:\n  return cs.no_info_after_validator_function(handler,s)\n  \n  \ndef _apply_step(step:_Step,s:cs.CoreSchema |None,handler:GetCoreSchemaHandler,source_type:Any)->cs.CoreSchema:\n from pydantic_core import core_schema as cs\n \n if isinstance(step,_ValidateAs):\n  s=_apply_parse(s,step.tp,step.strict,handler,source_type)\n elif isinstance(step,_ValidateAsDefer):\n  s=_apply_parse(s,step.tp,False,handler,source_type)\n elif isinstance(step,_Transform):\n  s=_apply_transform(s,step.func,handler)\n elif isinstance(step,_Constraint):\n  s=_apply_constraint(s,step.constraint)\n elif isinstance(step,_PipelineOr):\n  s=cs.union_schema([handler(step.left),handler(step.right)])\n else:\n  assert isinstance(step,_PipelineAnd)\n  s=cs.chain_schema([handler(step.left),handler(step.right)])\n return s\n \n \ndef _apply_parse(\ns:cs.CoreSchema |None,\ntp:type[Any],\nstrict:bool,\nhandler:GetCoreSchemaHandler,\nsource_type:Any,\n)->cs.CoreSchema:\n from pydantic_core import core_schema as cs\n \n from pydantic import Strict\n \n if tp is _FieldTypeMarker:\n  return handler(source_type)\n  \n if strict:\n  tp=Annotated[tp,Strict()]\n  \n if s and s['type']=='any':\n  return handler(tp)\n else:\n  return cs.chain_schema([s,handler(tp)])if s else handler(tp)\n  \n  \ndef _apply_transform(\ns:cs.CoreSchema |None,func:Callable[[Any],Any],handler:GetCoreSchemaHandler\n)->cs.CoreSchema:\n from pydantic_core import core_schema as cs\n \n if s is None:\n  return cs.no_info_plain_validator_function(func)\n  \n if s['type']=='str':\n  if func is str.strip:\n   s=s.copy()\n   s['strip_whitespace']=True\n   return s\n  elif func is str.lower:\n   s=s.copy()\n   s['to_lower']=True\n   return s\n  elif func is str.upper:\n   s=s.copy()\n   s['to_upper']=True\n   return s\n   \n return cs.no_info_after_validator_function(func,s)\n \n \ndef _apply_constraint(\ns:cs.CoreSchema |None,constraint:_ConstraintAnnotation\n)->cs.CoreSchema:\n ''\n if isinstance(constraint,annotated_types.Gt):\n  gt=constraint.gt\n  if s and s['type']in{'int','float','decimal'}:\n   s=s.copy()\n   if s['type']=='int'and isinstance(gt,int):\n    s['gt']=gt\n   elif s['type']=='float'and isinstance(gt,float):\n    s['gt']=gt\n   elif s['type']=='decimal'and isinstance(gt,Decimal):\n    s['gt']=gt\n  else:\n  \n   def check_gt(v:Any)->bool:\n    return v >gt\n    \n   s=_check_func(check_gt,f'> {gt}',s)\n elif isinstance(constraint,annotated_types.Ge):\n  ge=constraint.ge\n  if s and s['type']in{'int','float','decimal'}:\n   s=s.copy()\n   if s['type']=='int'and isinstance(ge,int):\n    s['ge']=ge\n   elif s['type']=='float'and isinstance(ge,float):\n    s['ge']=ge\n   elif s['type']=='decimal'and isinstance(ge,Decimal):\n    s['ge']=ge\n    \n  def check_ge(v:Any)->bool:\n   return v >=ge\n   \n  s=_check_func(check_ge,f'>= {ge}',s)\n elif isinstance(constraint,annotated_types.Lt):\n  lt=constraint.lt\n  if s and s['type']in{'int','float','decimal'}:\n   s=s.copy()\n   if s['type']=='int'and isinstance(lt,int):\n    s['lt']=lt\n   elif s['type']=='float'and isinstance(lt,float):\n    s['lt']=lt\n   elif s['type']=='decimal'and isinstance(lt,Decimal):\n    s['lt']=lt\n    \n  def check_lt(v:Any)->bool:\n   return v <lt\n   \n  s=_check_func(check_lt,f'< {lt}',s)\n elif isinstance(constraint,annotated_types.Le):\n  le=constraint.le\n  if s and s['type']in{'int','float','decimal'}:\n   s=s.copy()\n   if s['type']=='int'and isinstance(le,int):\n    s['le']=le\n   elif s['type']=='float'and isinstance(le,float):\n    s['le']=le\n   elif s['type']=='decimal'and isinstance(le,Decimal):\n    s['le']=le\n    \n  def check_le(v:Any)->bool:\n   return v <=le\n   \n  s=_check_func(check_le,f'<= {le}',s)\n elif isinstance(constraint,annotated_types.Len):\n  min_len=constraint.min_length\n  max_len=constraint.max_length\n  \n  if s and s['type']in{'str','list','tuple','set','frozenset','dict'}:\n   assert(\n   s['type']=='str'\n   or s['type']=='list'\n   or s['type']=='tuple'\n   or s['type']=='set'\n   or s['type']=='dict'\n   or s['type']=='frozenset'\n   )\n   s=s.copy()\n   if min_len !=0:\n    s['min_length']=min_len\n   if max_len is not None:\n    s['max_length']=max_len\n    \n  def check_len(v:Any)->bool:\n   if max_len is not None:\n    return(min_len <=len(v))and(len(v)<=max_len)\n   return min_len <=len(v)\n   \n  s=_check_func(check_len,f'length >= {min_len} and length <= {max_len}',s)\n elif isinstance(constraint,annotated_types.MultipleOf):\n  multiple_of=constraint.multiple_of\n  if s and s['type']in{'int','float','decimal'}:\n   s=s.copy()\n   if s['type']=='int'and isinstance(multiple_of,int):\n    s['multiple_of']=multiple_of\n   elif s['type']=='float'and isinstance(multiple_of,float):\n    s['multiple_of']=multiple_of\n   elif s['type']=='decimal'and isinstance(multiple_of,Decimal):\n    s['multiple_of']=multiple_of\n    \n  def check_multiple_of(v:Any)->bool:\n   return v %multiple_of ==0\n   \n  s=_check_func(check_multiple_of,f'% {multiple_of} == 0',s)\n elif isinstance(constraint,annotated_types.Timezone):\n  tz=constraint.tz\n  \n  if tz is ...:\n   if s and s['type']=='datetime':\n    s=s.copy()\n    s['tz_constraint']='aware'\n   else:\n   \n    def check_tz_aware(v:object)->bool:\n     assert isinstance(v,datetime.datetime)\n     return v.tzinfo is not None\n     \n    s=_check_func(check_tz_aware,'timezone aware',s)\n  elif tz is None:\n   if s and s['type']=='datetime':\n    s=s.copy()\n    s['tz_constraint']='naive'\n   else:\n   \n    def check_tz_naive(v:object)->bool:\n     assert isinstance(v,datetime.datetime)\n     return v.tzinfo is None\n     \n    s=_check_func(check_tz_naive,'timezone naive',s)\n  else:\n   raise NotImplementedError('Constraining to a specific timezone is not yet supported')\n elif isinstance(constraint,annotated_types.Interval):\n  if constraint.ge:\n   s=_apply_constraint(s,annotated_types.Ge(constraint.ge))\n  if constraint.gt:\n   s=_apply_constraint(s,annotated_types.Gt(constraint.gt))\n  if constraint.le:\n   s=_apply_constraint(s,annotated_types.Le(constraint.le))\n  if constraint.lt:\n   s=_apply_constraint(s,annotated_types.Lt(constraint.lt))\n  assert s is not None\n elif isinstance(constraint,annotated_types.Predicate):\n  func=constraint.func\n  \n  if func.__name__ =='<lambda>':\n  \n  \n  \n   import inspect\n   \n   try:\n   \n    source=inspect.getsource(func).strip()\n    if source.endswith(')'):\n     source=source[:-1]\n    lambda_source_code='`'+''.join(''.join(source.split('lambda ')[1:]).split(':')[1:]).strip()+'`'\n   except OSError:\n   \n    lambda_source_code='lambda'\n    \n   s=_check_func(func,lambda_source_code,s)\n  else:\n   s=_check_func(func,func.__name__,s)\n elif isinstance(constraint,_NotEq):\n  value=constraint.value\n  \n  def check_not_eq(v:Any)->bool:\n   return operator.__ne__(v,value)\n   \n  s=_check_func(check_not_eq,f'!= {value}',s)\n elif isinstance(constraint,_Eq):\n  value=constraint.value\n  \n  def check_eq(v:Any)->bool:\n   return operator.__eq__(v,value)\n   \n  s=_check_func(check_eq,f'== {value}',s)\n elif isinstance(constraint,_In):\n  values=constraint.values\n  \n  def check_in(v:Any)->bool:\n   return operator.__contains__(values,v)\n   \n  s=_check_func(check_in,f'in {values}',s)\n elif isinstance(constraint,_NotIn):\n  values=constraint.values\n  \n  def check_not_in(v:Any)->bool:\n   return operator.__not__(operator.__contains__(values,v))\n   \n  s=_check_func(check_not_in,f'not in {values}',s)\n else:\n  assert isinstance(constraint,Pattern)\n  if s and s['type']=='str':\n   s=s.copy()\n   s['pattern']=constraint.pattern\n  else:\n  \n   def check_pattern(v:object)->bool:\n    assert isinstance(v,str)\n    return constraint.match(v)is not None\n    \n   s=_check_func(check_pattern,f'~ {constraint.pattern}',s)\n return s\n \n \nclass _SupportsRange(annotated_types.SupportsLe,annotated_types.SupportsGe,Protocol):\n pass\n \n \nclass _SupportsLen(Protocol):\n def __len__(self)->int:...\n \n \n_NewOutGt=TypeVar('_NewOutGt',bound=annotated_types.SupportsGt)\n_NewOutGe=TypeVar('_NewOutGe',bound=annotated_types.SupportsGe)\n_NewOutLt=TypeVar('_NewOutLt',bound=annotated_types.SupportsLt)\n_NewOutLe=TypeVar('_NewOutLe',bound=annotated_types.SupportsLe)\n_NewOutLen=TypeVar('_NewOutLen',bound=_SupportsLen)\n_NewOutDiv=TypeVar('_NewOutDiv',bound=annotated_types.SupportsDiv)\n_NewOutMod=TypeVar('_NewOutMod',bound=annotated_types.SupportsMod)\n_NewOutDatetime=TypeVar('_NewOutDatetime',bound=datetime.datetime)\n_NewOutInterval=TypeVar('_NewOutInterval',bound=_SupportsRange)\n_OtherIn=TypeVar('_OtherIn')\n_OtherOut=TypeVar('_OtherOut')\n", ["__future__", "annotated_types", "collections", "collections.abc", "dataclasses", "datetime", "decimal", "functools", "inspect", "operator", "pydantic", "pydantic._internal._internal_dataclass", "pydantic_core", "pydantic_core.core_schema", "re", "sys", "types", "typing", "typing_extensions"]], "pydantic.v1.main": [".py", "import warnings\nfrom abc import ABCMeta\nfrom copy import deepcopy\nfrom enum import Enum\nfrom functools import partial\nfrom pathlib import Path\nfrom types import FunctionType,prepare_class,resolve_bases\nfrom typing import(\nTYPE_CHECKING,\nAbstractSet,\nAny,\nCallable,\nClassVar,\nDict,\nList,\nMapping,\nOptional,\nTuple,\nType,\nTypeVar,\nUnion,\ncast,\nno_type_check,\noverload,\n)\n\nfrom typing_extensions import dataclass_transform\n\nfrom pydantic.v1.class_validators import ValidatorGroup,extract_root_validators,extract_validators,inherit_validators\nfrom pydantic.v1.config import BaseConfig,Extra,inherit_config,prepare_config\nfrom pydantic.v1.error_wrappers import ErrorWrapper,ValidationError\nfrom pydantic.v1.errors import ConfigError,DictError,ExtraError,MissingError\nfrom pydantic.v1.fields import(\nMAPPING_LIKE_SHAPES,\nField,\nModelField,\nModelPrivateAttr,\nPrivateAttr,\nUndefined,\nis_finalvar_with_default_val,\n)\nfrom pydantic.v1.json import custom_pydantic_encoder,pydantic_encoder\nfrom pydantic.v1.parse import Protocol,load_file,load_str_bytes\nfrom pydantic.v1.schema import default_ref_template,model_schema\nfrom pydantic.v1.types import PyObject,StrBytes\nfrom pydantic.v1.typing import(\nAnyCallable,\nget_args,\nget_origin,\nis_classvar,\nis_namedtuple,\nis_union,\nresolve_annotations,\nupdate_model_forward_refs,\n)\nfrom pydantic.v1.utils import(\nDUNDER_ATTRIBUTES,\nROOT_KEY,\nClassAttribute,\nGetterDict,\nRepresentation,\nValueItems,\ngenerate_model_signature,\nis_valid_field,\nis_valid_private_name,\nlenient_issubclass,\nsequence_like,\nsmart_deepcopy,\nunique_list,\nvalidate_field_name,\n)\n\nif TYPE_CHECKING:\n from inspect import Signature\n \n from pydantic.v1.class_validators import ValidatorListDict\n from pydantic.v1.types import ModelOrDc\n from pydantic.v1.typing import(\n AbstractSetIntStr,\n AnyClassMethod,\n CallableGenerator,\n DictAny,\n DictStrAny,\n MappingIntStrAny,\n ReprArgs,\n SetStr,\n TupleGenerator,\n )\n \n Model=TypeVar('Model',bound='BaseModel')\n \n__all__='BaseModel','create_model','validate_model'\n\n_T=TypeVar('_T')\n\n\ndef validate_custom_root_type(fields:Dict[str,ModelField])->None:\n if len(fields)>1:\n  raise ValueError(f'{ROOT_KEY} cannot be mixed with other fields')\n  \n  \ndef generate_hash_function(frozen:bool)->Optional[Callable[[Any],int]]:\n def hash_function(self_:Any)->int:\n  return hash(self_.__class__)+hash(tuple(self_.__dict__.values()))\n  \n return hash_function if frozen else None\n \n \n \nANNOTATED_FIELD_UNTOUCHED_TYPES:Tuple[Any,...]=(property,type,classmethod,staticmethod)\n\nUNTOUCHED_TYPES:Tuple[Any,...]=(FunctionType,)+ANNOTATED_FIELD_UNTOUCHED_TYPES\n\n\n\n\n_is_base_model_class_defined=False\n\n\n@dataclass_transform(kw_only_default=True,field_specifiers=(Field,))\nclass ModelMetaclass(ABCMeta):\n @no_type_check\n def __new__(mcs,name,bases,namespace,**kwargs):\n  fields:Dict[str,ModelField]={}\n  config=BaseConfig\n  validators:'ValidatorListDict'={}\n  \n  pre_root_validators,post_root_validators=[],[]\n  private_attributes:Dict[str,ModelPrivateAttr]={}\n  base_private_attributes:Dict[str,ModelPrivateAttr]={}\n  slots:SetStr=namespace.get('__slots__',())\n  slots={slots}if isinstance(slots,str)else set(slots)\n  class_vars:SetStr=set()\n  hash_func:Optional[Callable[[Any],int]]=None\n  \n  for base in reversed(bases):\n   if _is_base_model_class_defined and issubclass(base,BaseModel)and base !=BaseModel:\n    fields.update(smart_deepcopy(base.__fields__))\n    config=inherit_config(base.__config__,config)\n    validators=inherit_validators(base.__validators__,validators)\n    pre_root_validators +=base.__pre_root_validators__\n    post_root_validators +=base.__post_root_validators__\n    base_private_attributes.update(base.__private_attributes__)\n    class_vars.update(base.__class_vars__)\n    hash_func=base.__hash__\n    \n  resolve_forward_refs=kwargs.pop('__resolve_forward_refs__',True)\n  allowed_config_kwargs:SetStr={\n  key\n  for key in dir(config)\n  if not(key.startswith('__')and key.endswith('__'))\n  }\n  config_kwargs={key:kwargs.pop(key)for key in kwargs.keys()&allowed_config_kwargs}\n  config_from_namespace=namespace.get('Config')\n  if config_kwargs and config_from_namespace:\n   raise TypeError('Specifying config in two places is ambiguous, use either Config attribute or class kwargs')\n  config=inherit_config(config_from_namespace,config,**config_kwargs)\n  \n  validators=inherit_validators(extract_validators(namespace),validators)\n  vg=ValidatorGroup(validators)\n  \n  for f in fields.values():\n   f.set_config(config)\n   extra_validators=vg.get_validators(f.name)\n   if extra_validators:\n    f.class_validators.update(extra_validators)\n    \n    f.populate_validators()\n    \n  prepare_config(config,name)\n  \n  untouched_types=ANNOTATED_FIELD_UNTOUCHED_TYPES\n  \n  def is_untouched(v:Any)->bool:\n   return isinstance(v,untouched_types)or v.__class__.__name__ =='cython_function_or_method'\n   \n  if(namespace.get('__module__'),namespace.get('__qualname__'))!=('pydantic.main','BaseModel'):\n   annotations=resolve_annotations(namespace.get('__annotations__',{}),namespace.get('__module__',None))\n   \n   for ann_name,ann_type in annotations.items():\n    if is_classvar(ann_type):\n     class_vars.add(ann_name)\n    elif is_finalvar_with_default_val(ann_type,namespace.get(ann_name,Undefined)):\n     class_vars.add(ann_name)\n    elif is_valid_field(ann_name):\n     validate_field_name(bases,ann_name)\n     value=namespace.get(ann_name,Undefined)\n     allowed_types=get_args(ann_type)if is_union(get_origin(ann_type))else(ann_type,)\n     if(\n     is_untouched(value)\n     and ann_type !=PyObject\n     and not any(\n     lenient_issubclass(get_origin(allowed_type),Type)for allowed_type in allowed_types\n     )\n     ):\n      continue\n     fields[ann_name]=ModelField.infer(\n     name=ann_name,\n     value=value,\n     annotation=ann_type,\n     class_validators=vg.get_validators(ann_name),\n     config=config,\n     )\n    elif ann_name not in namespace and config.underscore_attrs_are_private:\n     private_attributes[ann_name]=PrivateAttr()\n     \n   untouched_types=UNTOUCHED_TYPES+config.keep_untouched\n   for var_name,value in namespace.items():\n    can_be_changed=var_name not in class_vars and not is_untouched(value)\n    if isinstance(value,ModelPrivateAttr):\n     if not is_valid_private_name(var_name):\n      raise NameError(\n      f'Private attributes \"{var_name}\" must not be a valid field name; '\n      f'Use sunder or dunder names, e. g. \"_{var_name}\" or \"__{var_name}__\"'\n      )\n     private_attributes[var_name]=value\n    elif config.underscore_attrs_are_private and is_valid_private_name(var_name)and can_be_changed:\n     private_attributes[var_name]=PrivateAttr(default=value)\n    elif is_valid_field(var_name)and var_name not in annotations and can_be_changed:\n     validate_field_name(bases,var_name)\n     inferred=ModelField.infer(\n     name=var_name,\n     value=value,\n     annotation=annotations.get(var_name,Undefined),\n     class_validators=vg.get_validators(var_name),\n     config=config,\n     )\n     if var_name in fields:\n      if lenient_issubclass(inferred.type_,fields[var_name].type_):\n       inferred.type_=fields[var_name].type_\n      else:\n       raise TypeError(\n       f'The type of {name}.{var_name} differs from the new default value; '\n       f'if you wish to change the type of this field, please use a type annotation'\n       )\n     fields[var_name]=inferred\n     \n  _custom_root_type=ROOT_KEY in fields\n  if _custom_root_type:\n   validate_custom_root_type(fields)\n  vg.check_for_unused()\n  if config.json_encoders:\n   json_encoder=partial(custom_pydantic_encoder,config.json_encoders)\n  else:\n   json_encoder=pydantic_encoder\n  pre_rv_new,post_rv_new=extract_root_validators(namespace)\n  \n  if hash_func is None:\n   hash_func=generate_hash_function(config.frozen)\n   \n  exclude_from_namespace=fields |private_attributes.keys()|{'__slots__'}\n  new_namespace={\n  '__config__':config,\n  '__fields__':fields,\n  '__exclude_fields__':{\n  name:field.field_info.exclude for name,field in fields.items()if field.field_info.exclude is not None\n  }\n  or None,\n  '__include_fields__':{\n  name:field.field_info.include for name,field in fields.items()if field.field_info.include is not None\n  }\n  or None,\n  '__validators__':vg.validators,\n  '__pre_root_validators__':unique_list(\n  pre_root_validators+pre_rv_new,\n  name_factory=lambda v:v.__name__,\n  ),\n  '__post_root_validators__':unique_list(\n  post_root_validators+post_rv_new,\n  name_factory=lambda skip_on_failure_and_v:skip_on_failure_and_v[1].__name__,\n  ),\n  '__schema_cache__':{},\n  '__json_encoder__':staticmethod(json_encoder),\n  '__custom_root_type__':_custom_root_type,\n  '__private_attributes__':{**base_private_attributes,**private_attributes},\n  '__slots__':slots |private_attributes.keys(),\n  '__hash__':hash_func,\n  '__class_vars__':class_vars,\n  **{n:v for n,v in namespace.items()if n not in exclude_from_namespace},\n  }\n  \n  cls=super().__new__(mcs,name,bases,new_namespace,**kwargs)\n  \n  cls.__signature__=ClassAttribute('__signature__',generate_model_signature(cls.__init__,fields,config))\n  if resolve_forward_refs:\n   cls.__try_update_forward_refs__()\n   \n   \n   \n  for name,obj in namespace.items():\n   if name not in new_namespace:\n    set_name=getattr(obj,'__set_name__',None)\n    if callable(set_name):\n     set_name(cls,name)\n     \n  return cls\n  \n def __instancecheck__(self,instance:Any)->bool:\n  ''\n\n\n\n  \n  return hasattr(instance,'__post_root_validators__')and super().__instancecheck__(instance)\n  \n  \nobject_setattr=object.__setattr__\n\n\nclass BaseModel(Representation,metaclass=ModelMetaclass):\n if TYPE_CHECKING:\n \n  __fields__:ClassVar[Dict[str,ModelField]]={}\n  __include_fields__:ClassVar[Optional[Mapping[str,Any]]]=None\n  __exclude_fields__:ClassVar[Optional[Mapping[str,Any]]]=None\n  __validators__:ClassVar[Dict[str,AnyCallable]]={}\n  __pre_root_validators__:ClassVar[List[AnyCallable]]\n  __post_root_validators__:ClassVar[List[Tuple[bool,AnyCallable]]]\n  __config__:ClassVar[Type[BaseConfig]]=BaseConfig\n  __json_encoder__:ClassVar[Callable[[Any],Any]]=lambda x:x\n  __schema_cache__:ClassVar['DictAny']={}\n  __custom_root_type__:ClassVar[bool]=False\n  __signature__:ClassVar['Signature']\n  __private_attributes__:ClassVar[Dict[str,ModelPrivateAttr]]\n  __class_vars__:ClassVar[SetStr]\n  __fields_set__:ClassVar[SetStr]=set()\n  \n Config=BaseConfig\n __slots__=('__dict__','__fields_set__')\n __doc__=''\n \n def __init__(__pydantic_self__,**data:Any)->None:\n  ''\n\n\n\n  \n  \n  values,fields_set,validation_error=validate_model(__pydantic_self__.__class__,data)\n  if validation_error:\n   raise validation_error\n  try:\n   object_setattr(__pydantic_self__,'__dict__',values)\n  except TypeError as e:\n   raise TypeError(\n   'Model values must be a dict; you may not have returned a dictionary from a root validator'\n   )from e\n  object_setattr(__pydantic_self__,'__fields_set__',fields_set)\n  __pydantic_self__._init_private_attributes()\n  \n @no_type_check\n def __setattr__(self,name,value):\n  if name in self.__private_attributes__ or name in DUNDER_ATTRIBUTES:\n   return object_setattr(self,name,value)\n   \n  if self.__config__.extra is not Extra.allow and name not in self.__fields__:\n   raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n  elif not self.__config__.allow_mutation or self.__config__.frozen:\n   raise TypeError(f'\"{self.__class__.__name__}\" is immutable and does not support item assignment')\n  elif name in self.__fields__ and self.__fields__[name].final:\n   raise TypeError(\n   f'\"{self.__class__.__name__}\" object \"{name}\" field is final and does not support reassignment'\n   )\n  elif self.__config__.validate_assignment:\n   new_values={**self.__dict__,name:value}\n   \n   for validator in self.__pre_root_validators__:\n    try:\n     new_values=validator(self.__class__,new_values)\n    except(ValueError,TypeError,AssertionError)as exc:\n     raise ValidationError([ErrorWrapper(exc,loc=ROOT_KEY)],self.__class__)\n     \n   known_field=self.__fields__.get(name,None)\n   if known_field:\n   \n   \n   \n   \n    if not known_field.field_info.allow_mutation:\n     raise TypeError(f'\"{known_field.name}\" has allow_mutation set to False and cannot be assigned')\n    dict_without_original_value={k:v for k,v in self.__dict__.items()if k !=name}\n    value,error_=known_field.validate(value,dict_without_original_value,loc=name,cls=self.__class__)\n    if error_:\n     raise ValidationError([error_],self.__class__)\n    else:\n     new_values[name]=value\n     \n   errors=[]\n   for skip_on_failure,validator in self.__post_root_validators__:\n    if skip_on_failure and errors:\n     continue\n    try:\n     new_values=validator(self.__class__,new_values)\n    except(ValueError,TypeError,AssertionError)as exc:\n     errors.append(ErrorWrapper(exc,loc=ROOT_KEY))\n   if errors:\n    raise ValidationError(errors,self.__class__)\n    \n    \n    \n   object_setattr(self,'__dict__',new_values)\n  else:\n   self.__dict__[name]=value\n   \n  self.__fields_set__.add(name)\n  \n def __getstate__(self)->'DictAny':\n  private_attrs=((k,getattr(self,k,Undefined))for k in self.__private_attributes__)\n  return{\n  '__dict__':self.__dict__,\n  '__fields_set__':self.__fields_set__,\n  '__private_attribute_values__':{k:v for k,v in private_attrs if v is not Undefined},\n  }\n  \n def __setstate__(self,state:'DictAny')->None:\n  object_setattr(self,'__dict__',state['__dict__'])\n  object_setattr(self,'__fields_set__',state['__fields_set__'])\n  for name,value in state.get('__private_attribute_values__',{}).items():\n   object_setattr(self,name,value)\n   \n def _init_private_attributes(self)->None:\n  for name,private_attr in self.__private_attributes__.items():\n   default=private_attr.get_default()\n   if default is not Undefined:\n    object_setattr(self,name,default)\n    \n def dict(\n self,\n *,\n include:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n exclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n by_alias:bool=False,\n skip_defaults:Optional[bool]=None,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n )->'DictStrAny':\n  ''\n\n\n  \n  if skip_defaults is not None:\n   warnings.warn(\n   f'{self.__class__.__name__}.dict(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\n   DeprecationWarning,\n   )\n   exclude_unset=skip_defaults\n   \n  return dict(\n  self._iter(\n  to_dict=True,\n  by_alias=by_alias,\n  include=include,\n  exclude=exclude,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  )\n  )\n  \n def json(\n self,\n *,\n include:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n exclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n by_alias:bool=False,\n skip_defaults:Optional[bool]=None,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n encoder:Optional[Callable[[Any],Any]]=None,\n models_as_dict:bool=True,\n **dumps_kwargs:Any,\n )->str:\n  ''\n\n\n\n  \n  if skip_defaults is not None:\n   warnings.warn(\n   f'{self.__class__.__name__}.json(): \"skip_defaults\" is deprecated and replaced by \"exclude_unset\"',\n   DeprecationWarning,\n   )\n   exclude_unset=skip_defaults\n  encoder=cast(Callable[[Any],Any],encoder or self.__json_encoder__)\n  \n  \n  \n  \n  data=dict(\n  self._iter(\n  to_dict=models_as_dict,\n  by_alias=by_alias,\n  include=include,\n  exclude=exclude,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  )\n  )\n  if self.__custom_root_type__:\n   data=data[ROOT_KEY]\n  return self.__config__.json_dumps(data,default=encoder,**dumps_kwargs)\n  \n @classmethod\n def _enforce_dict_if_root(cls,obj:Any)->Any:\n  if cls.__custom_root_type__ and(\n  not(isinstance(obj,dict)and obj.keys()=={ROOT_KEY})\n  and not(isinstance(obj,BaseModel)and obj.__fields__.keys()=={ROOT_KEY})\n  or cls.__fields__[ROOT_KEY].shape in MAPPING_LIKE_SHAPES\n  ):\n   return{ROOT_KEY:obj}\n  else:\n   return obj\n   \n @classmethod\n def parse_obj(cls:Type['Model'],obj:Any)->'Model':\n  obj=cls._enforce_dict_if_root(obj)\n  if not isinstance(obj,dict):\n   try:\n    obj=dict(obj)\n   except(TypeError,ValueError)as e:\n    exc=TypeError(f'{cls.__name__} expected dict not {obj.__class__.__name__}')\n    raise ValidationError([ErrorWrapper(exc,loc=ROOT_KEY)],cls)from e\n  return cls(**obj)\n  \n @classmethod\n def parse_raw(\n cls:Type['Model'],\n b:StrBytes,\n *,\n content_type:str=None,\n encoding:str='utf8',\n proto:Protocol=None,\n allow_pickle:bool=False,\n )->'Model':\n  try:\n   obj=load_str_bytes(\n   b,\n   proto=proto,\n   content_type=content_type,\n   encoding=encoding,\n   allow_pickle=allow_pickle,\n   json_loads=cls.__config__.json_loads,\n   )\n  except(ValueError,TypeError,UnicodeDecodeError)as e:\n   raise ValidationError([ErrorWrapper(e,loc=ROOT_KEY)],cls)\n  return cls.parse_obj(obj)\n  \n @classmethod\n def parse_file(\n cls:Type['Model'],\n path:Union[str,Path],\n *,\n content_type:str=None,\n encoding:str='utf8',\n proto:Protocol=None,\n allow_pickle:bool=False,\n )->'Model':\n  obj=load_file(\n  path,\n  proto=proto,\n  content_type=content_type,\n  encoding=encoding,\n  allow_pickle=allow_pickle,\n  json_loads=cls.__config__.json_loads,\n  )\n  return cls.parse_obj(obj)\n  \n @classmethod\n def from_orm(cls:Type['Model'],obj:Any)->'Model':\n  if not cls.__config__.orm_mode:\n   raise ConfigError('You must have the config attribute orm_mode=True to use from_orm')\n  obj={ROOT_KEY:obj}if cls.__custom_root_type__ else cls._decompose_class(obj)\n  m=cls.__new__(cls)\n  values,fields_set,validation_error=validate_model(cls,obj)\n  if validation_error:\n   raise validation_error\n  object_setattr(m,'__dict__',values)\n  object_setattr(m,'__fields_set__',fields_set)\n  m._init_private_attributes()\n  return m\n  \n @classmethod\n def construct(cls:Type['Model'],_fields_set:Optional['SetStr']=None,**values:Any)->'Model':\n  ''\n\n\n\n  \n  m=cls.__new__(cls)\n  fields_values:Dict[str,Any]={}\n  for name,field in cls.__fields__.items():\n   if field.alt_alias and field.alias in values:\n    fields_values[name]=values[field.alias]\n   elif name in values:\n    fields_values[name]=values[name]\n   elif not field.required:\n    fields_values[name]=field.get_default()\n  fields_values.update(values)\n  object_setattr(m,'__dict__',fields_values)\n  if _fields_set is None:\n   _fields_set=set(values.keys())\n  object_setattr(m,'__fields_set__',_fields_set)\n  m._init_private_attributes()\n  return m\n  \n def _copy_and_set_values(self:'Model',values:'DictStrAny',fields_set:'SetStr',*,deep:bool)->'Model':\n  if deep:\n  \n   values=deepcopy(values)\n   \n  cls=self.__class__\n  m=cls.__new__(cls)\n  object_setattr(m,'__dict__',values)\n  object_setattr(m,'__fields_set__',fields_set)\n  for name in self.__private_attributes__:\n   value=getattr(self,name,Undefined)\n   if value is not Undefined:\n    if deep:\n     value=deepcopy(value)\n    object_setattr(m,name,value)\n    \n  return m\n  \n def copy(\n self:'Model',\n *,\n include:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n exclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n update:Optional['DictStrAny']=None,\n deep:bool=False,\n )->'Model':\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  values=dict(\n  self._iter(to_dict=False,by_alias=False,include=include,exclude=exclude,exclude_unset=False),\n  **(update or{}),\n  )\n  \n  \n  if update:\n   fields_set=self.__fields_set__ |update.keys()\n  else:\n   fields_set=set(self.__fields_set__)\n   \n  return self._copy_and_set_values(values,fields_set,deep=deep)\n  \n @classmethod\n def schema(cls,by_alias:bool=True,ref_template:str=default_ref_template)->'DictStrAny':\n  cached=cls.__schema_cache__.get((by_alias,ref_template))\n  if cached is not None:\n   return cached\n  s=model_schema(cls,by_alias=by_alias,ref_template=ref_template)\n  cls.__schema_cache__[(by_alias,ref_template)]=s\n  return s\n  \n @classmethod\n def schema_json(\n cls,*,by_alias:bool=True,ref_template:str=default_ref_template,**dumps_kwargs:Any\n )->str:\n  from pydantic.v1.json import pydantic_encoder\n  \n  return cls.__config__.json_dumps(\n  cls.schema(by_alias=by_alias,ref_template=ref_template),default=pydantic_encoder,**dumps_kwargs\n  )\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  \n @classmethod\n def validate(cls:Type['Model'],value:Any)->'Model':\n  if isinstance(value,cls):\n   copy_on_model_validation=cls.__config__.copy_on_model_validation\n   \n   deep_copy:Optional[bool]=None\n   if copy_on_model_validation not in{'deep','shallow','none'}:\n   \n    warnings.warn(\n    \"`copy_on_model_validation` should be a string: 'deep', 'shallow' or 'none'\",DeprecationWarning\n    )\n    if copy_on_model_validation:\n     deep_copy=False\n     \n   if copy_on_model_validation =='shallow':\n   \n    deep_copy=False\n   elif copy_on_model_validation =='deep':\n   \n    deep_copy=True\n    \n   if deep_copy is None:\n    return value\n   else:\n    return value._copy_and_set_values(value.__dict__,value.__fields_set__,deep=deep_copy)\n    \n  value=cls._enforce_dict_if_root(value)\n  \n  if isinstance(value,dict):\n   return cls(**value)\n  elif cls.__config__.orm_mode:\n   return cls.from_orm(value)\n  else:\n   try:\n    value_as_dict=dict(value)\n   except(TypeError,ValueError)as e:\n    raise DictError()from e\n   return cls(**value_as_dict)\n   \n @classmethod\n def _decompose_class(cls:Type['Model'],obj:Any)->GetterDict:\n  if isinstance(obj,GetterDict):\n   return obj\n  return cls.__config__.getter_dict(obj)\n  \n @classmethod\n @no_type_check\n def _get_value(\n cls,\n v:Any,\n to_dict:bool,\n by_alias:bool,\n include:Optional[Union['AbstractSetIntStr','MappingIntStrAny']],\n exclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny']],\n exclude_unset:bool,\n exclude_defaults:bool,\n exclude_none:bool,\n )->Any:\n  if isinstance(v,BaseModel):\n   if to_dict:\n    v_dict=v.dict(\n    by_alias=by_alias,\n    exclude_unset=exclude_unset,\n    exclude_defaults=exclude_defaults,\n    include=include,\n    exclude=exclude,\n    exclude_none=exclude_none,\n    )\n    if ROOT_KEY in v_dict:\n     return v_dict[ROOT_KEY]\n    return v_dict\n   else:\n    return v.copy(include=include,exclude=exclude)\n    \n  value_exclude=ValueItems(v,exclude)if exclude else None\n  value_include=ValueItems(v,include)if include else None\n  \n  if isinstance(v,dict):\n   return{\n   k_:cls._get_value(\n   v_,\n   to_dict=to_dict,\n   by_alias=by_alias,\n   exclude_unset=exclude_unset,\n   exclude_defaults=exclude_defaults,\n   include=value_include and value_include.for_element(k_),\n   exclude=value_exclude and value_exclude.for_element(k_),\n   exclude_none=exclude_none,\n   )\n   for k_,v_ in v.items()\n   if(not value_exclude or not value_exclude.is_excluded(k_))\n   and(not value_include or value_include.is_included(k_))\n   }\n   \n  elif sequence_like(v):\n   seq_args=(\n   cls._get_value(\n   v_,\n   to_dict=to_dict,\n   by_alias=by_alias,\n   exclude_unset=exclude_unset,\n   exclude_defaults=exclude_defaults,\n   include=value_include and value_include.for_element(i),\n   exclude=value_exclude and value_exclude.for_element(i),\n   exclude_none=exclude_none,\n   )\n   for i,v_ in enumerate(v)\n   if(not value_exclude or not value_exclude.is_excluded(i))\n   and(not value_include or value_include.is_included(i))\n   )\n   \n   return v.__class__(*seq_args)if is_namedtuple(v.__class__)else v.__class__(seq_args)\n   \n  elif isinstance(v,Enum)and getattr(cls.Config,'use_enum_values',False):\n   return v.value\n   \n  else:\n   return v\n   \n @classmethod\n def __try_update_forward_refs__(cls,**localns:Any)->None:\n  ''\n\n\n  \n  update_model_forward_refs(cls,cls.__fields__.values(),cls.__config__.json_encoders,localns,(NameError,))\n  \n @classmethod\n def update_forward_refs(cls,**localns:Any)->None:\n  ''\n\n  \n  update_model_forward_refs(cls,cls.__fields__.values(),cls.__config__.json_encoders,localns)\n  \n def __iter__(self)->'TupleGenerator':\n  ''\n\n  \n  yield from self.__dict__.items()\n  \n def _iter(\n self,\n to_dict:bool=False,\n by_alias:bool=False,\n include:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n exclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny']]=None,\n exclude_unset:bool=False,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n )->'TupleGenerator':\n \n \n  if exclude is not None or self.__exclude_fields__ is not None:\n   exclude=ValueItems.merge(self.__exclude_fields__,exclude)\n   \n  if include is not None or self.__include_fields__ is not None:\n   include=ValueItems.merge(self.__include_fields__,include,intersect=True)\n   \n  allowed_keys=self._calculate_keys(\n  include=include,exclude=exclude,exclude_unset=exclude_unset\n  )\n  if allowed_keys is None and not(to_dict or by_alias or exclude_unset or exclude_defaults or exclude_none):\n  \n   yield from self.__dict__.items()\n   return\n   \n  value_exclude=ValueItems(self,exclude)if exclude is not None else None\n  value_include=ValueItems(self,include)if include is not None else None\n  \n  for field_key,v in self.__dict__.items():\n   if(allowed_keys is not None and field_key not in allowed_keys)or(exclude_none and v is None):\n    continue\n    \n   if exclude_defaults:\n    model_field=self.__fields__.get(field_key)\n    if not getattr(model_field,'required',True)and getattr(model_field,'default',_missing)==v:\n     continue\n     \n   if by_alias and field_key in self.__fields__:\n    dict_key=self.__fields__[field_key].alias\n   else:\n    dict_key=field_key\n    \n   if to_dict or value_include or value_exclude:\n    v=self._get_value(\n    v,\n    to_dict=to_dict,\n    by_alias=by_alias,\n    include=value_include and value_include.for_element(field_key),\n    exclude=value_exclude and value_exclude.for_element(field_key),\n    exclude_unset=exclude_unset,\n    exclude_defaults=exclude_defaults,\n    exclude_none=exclude_none,\n    )\n   yield dict_key,v\n   \n def _calculate_keys(\n self,\n include:Optional['MappingIntStrAny'],\n exclude:Optional['MappingIntStrAny'],\n exclude_unset:bool,\n update:Optional['DictStrAny']=None,\n )->Optional[AbstractSet[str]]:\n  if include is None and exclude is None and exclude_unset is False:\n   return None\n   \n  keys:AbstractSet[str]\n  if exclude_unset:\n   keys=self.__fields_set__.copy()\n  else:\n   keys=self.__dict__.keys()\n   \n  if include is not None:\n   keys &=include.keys()\n   \n  if update:\n   keys -=update.keys()\n   \n  if exclude:\n   keys -={k for k,v in exclude.items()if ValueItems.is_true(v)}\n   \n  return keys\n  \n def __eq__(self,other:Any)->bool:\n  if isinstance(other,BaseModel):\n   return self.dict()==other.dict()\n  else:\n   return self.dict()==other\n   \n def __repr_args__(self)->'ReprArgs':\n  return[\n  (k,v)\n  for k,v in self.__dict__.items()\n  if k not in DUNDER_ATTRIBUTES and(k not in self.__fields__ or self.__fields__[k].field_info.repr)\n  ]\n  \n  \n_is_base_model_class_defined=True\n\n\n@overload\ndef create_model(\n__model_name:str,\n*,\n__config__:Optional[Type[BaseConfig]]=None,\n__base__:None=None,\n__module__:str=__name__,\n__validators__:Dict[str,'AnyClassMethod']=None,\n__cls_kwargs__:Dict[str,Any]=None,\n**field_definitions:Any,\n)->Type['BaseModel']:\n ...\n \n \n@overload\ndef create_model(\n__model_name:str,\n*,\n__config__:Optional[Type[BaseConfig]]=None,\n__base__:Union[Type['Model'],Tuple[Type['Model'],...]],\n__module__:str=__name__,\n__validators__:Dict[str,'AnyClassMethod']=None,\n__cls_kwargs__:Dict[str,Any]=None,\n**field_definitions:Any,\n)->Type['Model']:\n ...\n \n \ndef create_model(\n__model_name:str,\n*,\n__config__:Optional[Type[BaseConfig]]=None,\n__base__:Union[None,Type['Model'],Tuple[Type['Model'],...]]=None,\n__module__:str=__name__,\n__validators__:Dict[str,'AnyClassMethod']=None,\n__cls_kwargs__:Dict[str,Any]=None,\n__slots__:Optional[Tuple[str,...]]=None,\n**field_definitions:Any,\n)->Type['Model']:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if __slots__ is not None:\n \n  warnings.warn('__slots__ should not be passed to create_model',RuntimeWarning)\n  \n if __base__ is not None:\n  if __config__ is not None:\n   raise ConfigError('to avoid confusion __config__ and __base__ cannot be used together')\n  if not isinstance(__base__,tuple):\n   __base__=(__base__,)\n else:\n  __base__=(cast(Type['Model'],BaseModel),)\n  \n __cls_kwargs__=__cls_kwargs__ or{}\n \n fields={}\n annotations={}\n \n for f_name,f_def in field_definitions.items():\n  if not is_valid_field(f_name):\n   warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"',RuntimeWarning)\n  if isinstance(f_def,tuple):\n   try:\n    f_annotation,f_value=f_def\n   except ValueError as e:\n    raise ConfigError(\n    'field definitions should either be a tuple of (<type>, <default>) or just a '\n    'default value, unfortunately this means tuples as '\n    'default values are not allowed'\n    )from e\n  else:\n   f_annotation,f_value=None,f_def\n   \n  if f_annotation:\n   annotations[f_name]=f_annotation\n  fields[f_name]=f_value\n  \n namespace:'DictStrAny'={'__annotations__':annotations,'__module__':__module__}\n if __validators__:\n  namespace.update(__validators__)\n namespace.update(fields)\n if __config__:\n  namespace['Config']=inherit_config(__config__,BaseConfig)\n resolved_bases=resolve_bases(__base__)\n meta,ns,kwds=prepare_class(__model_name,resolved_bases,kwds=__cls_kwargs__)\n if resolved_bases is not __base__:\n  ns['__orig_bases__']=__base__\n namespace.update(ns)\n return meta(__model_name,resolved_bases,namespace,**kwds)\n \n \n_missing=object()\n\n\ndef validate_model(\nmodel:Type[BaseModel],input_data:'DictStrAny',cls:'ModelOrDc'=None\n)->Tuple['DictStrAny','SetStr',Optional[ValidationError]]:\n ''\n\n \n values={}\n errors=[]\n \n names_used=set()\n \n fields_set=set()\n config=model.__config__\n check_extra=config.extra is not Extra.ignore\n cls_=cls or model\n \n for validator in model.__pre_root_validators__:\n  try:\n   input_data=validator(cls_,input_data)\n  except(ValueError,TypeError,AssertionError)as exc:\n   return{},set(),ValidationError([ErrorWrapper(exc,loc=ROOT_KEY)],cls_)\n   \n for name,field in model.__fields__.items():\n  value=input_data.get(field.alias,_missing)\n  using_name=False\n  if value is _missing and config.allow_population_by_field_name and field.alt_alias:\n   value=input_data.get(field.name,_missing)\n   using_name=True\n   \n  if value is _missing:\n   if field.required:\n    errors.append(ErrorWrapper(MissingError(),loc=field.alias))\n    continue\n    \n   value=field.get_default()\n   \n   if not config.validate_all and not field.validate_always:\n    values[name]=value\n    continue\n  else:\n   fields_set.add(name)\n   if check_extra:\n    names_used.add(field.name if using_name else field.alias)\n    \n  v_,errors_=field.validate(value,values,loc=field.alias,cls=cls_)\n  if isinstance(errors_,ErrorWrapper):\n   errors.append(errors_)\n  elif isinstance(errors_,list):\n   errors.extend(errors_)\n  else:\n   values[name]=v_\n   \n if check_extra:\n  if isinstance(input_data,GetterDict):\n   extra=input_data.extra_keys()-names_used\n  else:\n   extra=input_data.keys()-names_used\n  if extra:\n   fields_set |=extra\n   if config.extra is Extra.allow:\n    for f in extra:\n     values[f]=input_data[f]\n   else:\n    for f in sorted(extra):\n     errors.append(ErrorWrapper(ExtraError(),loc=f))\n     \n for skip_on_failure,validator in model.__post_root_validators__:\n  if skip_on_failure and errors:\n   continue\n  try:\n   values=validator(cls_,values)\n  except(ValueError,TypeError,AssertionError)as exc:\n   errors.append(ErrorWrapper(exc,loc=ROOT_KEY))\n   \n if errors:\n  return values,fields_set,ValidationError(errors,cls_)\n else:\n  return values,fields_set,None\n", ["abc", "copy", "enum", "functools", "inspect", "pathlib", "pydantic.v1.class_validators", "pydantic.v1.config", "pydantic.v1.error_wrappers", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.json", "pydantic.v1.parse", "pydantic.v1.schema", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "types", "typing", "typing_extensions", "warnings"]], "pydantic.v1": [".py", "\nfrom pydantic.v1 import dataclasses\nfrom pydantic.v1.annotated_types import create_model_from_namedtuple,create_model_from_typeddict\nfrom pydantic.v1.class_validators import root_validator,validator\nfrom pydantic.v1.config import BaseConfig,ConfigDict,Extra\nfrom pydantic.v1.decorator import validate_arguments\nfrom pydantic.v1.env_settings import BaseSettings\nfrom pydantic.v1.error_wrappers import ValidationError\nfrom pydantic.v1.errors import *\nfrom pydantic.v1.fields import Field,PrivateAttr,Required\nfrom pydantic.v1.main import *\nfrom pydantic.v1.networks import *\nfrom pydantic.v1.parse import Protocol\nfrom pydantic.v1.tools import *\nfrom pydantic.v1.types import *\nfrom pydantic.v1.version import VERSION,compiled\n\n__version__=VERSION\n\n\n\n__all__=[\n\n'create_model_from_namedtuple',\n'create_model_from_typeddict',\n\n'dataclasses',\n\n'root_validator',\n'validator',\n\n'BaseConfig',\n'ConfigDict',\n'Extra',\n\n'validate_arguments',\n\n'BaseSettings',\n\n'ValidationError',\n\n'Field',\n'Required',\n\n'BaseModel',\n'create_model',\n'validate_model',\n\n'AnyUrl',\n'AnyHttpUrl',\n'FileUrl',\n'HttpUrl',\n'stricturl',\n'EmailStr',\n'NameEmail',\n'IPvAnyAddress',\n'IPvAnyInterface',\n'IPvAnyNetwork',\n'PostgresDsn',\n'CockroachDsn',\n'AmqpDsn',\n'RedisDsn',\n'MongoDsn',\n'KafkaDsn',\n'validate_email',\n\n'Protocol',\n\n'parse_file_as',\n'parse_obj_as',\n'parse_raw_as',\n'schema_of',\n'schema_json_of',\n\n'NoneStr',\n'NoneBytes',\n'StrBytes',\n'NoneStrBytes',\n'StrictStr',\n'ConstrainedBytes',\n'conbytes',\n'ConstrainedList',\n'conlist',\n'ConstrainedSet',\n'conset',\n'ConstrainedFrozenSet',\n'confrozenset',\n'ConstrainedStr',\n'constr',\n'PyObject',\n'ConstrainedInt',\n'conint',\n'PositiveInt',\n'NegativeInt',\n'NonNegativeInt',\n'NonPositiveInt',\n'ConstrainedFloat',\n'confloat',\n'PositiveFloat',\n'NegativeFloat',\n'NonNegativeFloat',\n'NonPositiveFloat',\n'FiniteFloat',\n'ConstrainedDecimal',\n'condecimal',\n'ConstrainedDate',\n'condate',\n'UUID1',\n'UUID3',\n'UUID4',\n'UUID5',\n'FilePath',\n'DirectoryPath',\n'Json',\n'JsonWrapper',\n'SecretField',\n'SecretStr',\n'SecretBytes',\n'StrictBool',\n'StrictBytes',\n'StrictInt',\n'StrictFloat',\n'PaymentCardNumber',\n'PrivateAttr',\n'ByteSize',\n'PastDate',\n'FutureDate',\n\n'compiled',\n'VERSION',\n]\n", ["pydantic.v1", "pydantic.v1.annotated_types", "pydantic.v1.class_validators", "pydantic.v1.config", "pydantic.v1.dataclasses", "pydantic.v1.decorator", "pydantic.v1.env_settings", "pydantic.v1.error_wrappers", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.networks", "pydantic.v1.parse", "pydantic.v1.tools", "pydantic.v1.types", "pydantic.v1.version"], 1], "pydantic.v1.version": [".py", "__all__='compiled','VERSION','version_info'\n\nVERSION='1.10.19'\n\ntry:\n import cython\nexcept ImportError:\n compiled:bool=False\nelse:\n try:\n  compiled=cython.compiled\n except AttributeError:\n  compiled=False\n  \n  \ndef version_info()->str:\n import platform\n import sys\n from importlib import import_module\n from pathlib import Path\n \n optional_deps=[]\n for p in('devtools','dotenv','email-validator','typing-extensions'):\n  try:\n   import_module(p.replace('-','_'))\n  except ImportError:\n   continue\n  optional_deps.append(p)\n  \n info={\n 'pydantic version':VERSION,\n 'pydantic compiled':compiled,\n 'install path':Path(__file__).resolve().parent,\n 'python version':sys.version,\n 'platform':platform.platform(),\n 'optional deps. installed':optional_deps,\n }\n return '\\n'.join('{:>30} {}'.format(k+':',str(v).replace('\\n',' '))for k,v in info.items())\n", ["cython", "importlib", "pathlib", "platform", "sys"]], "pydantic.v1.parse": [".py", "import json\nimport pickle\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any,Callable,Union\n\nfrom pydantic.v1.types import StrBytes\n\n\nclass Protocol(str,Enum):\n json='json'\n pickle='pickle'\n \n \ndef load_str_bytes(\nb:StrBytes,\n*,\ncontent_type:str=None,\nencoding:str='utf8',\nproto:Protocol=None,\nallow_pickle:bool=False,\njson_loads:Callable[[str],Any]=json.loads,\n)->Any:\n if proto is None and content_type:\n  if content_type.endswith(('json','javascript')):\n   pass\n  elif allow_pickle and content_type.endswith('pickle'):\n   proto=Protocol.pickle\n  else:\n   raise TypeError(f'Unknown content-type: {content_type}')\n   \n proto=proto or Protocol.json\n \n if proto ==Protocol.json:\n  if isinstance(b,bytes):\n   b=b.decode(encoding)\n  return json_loads(b)\n elif proto ==Protocol.pickle:\n  if not allow_pickle:\n   raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n  bb=b if isinstance(b,bytes)else b.encode()\n  return pickle.loads(bb)\n else:\n  raise TypeError(f'Unknown protocol: {proto}')\n  \n  \ndef load_file(\npath:Union[str,Path],\n*,\ncontent_type:str=None,\nencoding:str='utf8',\nproto:Protocol=None,\nallow_pickle:bool=False,\njson_loads:Callable[[str],Any]=json.loads,\n)->Any:\n path=Path(path)\n b=path.read_bytes()\n if content_type is None:\n  if path.suffix in('.js','.json'):\n   proto=Protocol.json\n  elif path.suffix =='.pkl':\n   proto=Protocol.pickle\n   \n return load_str_bytes(\n b,proto=proto,content_type=content_type,encoding=encoding,allow_pickle=allow_pickle,json_loads=json_loads\n )\n", ["enum", "json", "pathlib", "pickle", "pydantic.v1.types", "typing"]], "pydantic.v1.generics": [".py", "import sys\nimport types\nimport typing\nfrom typing import(\nTYPE_CHECKING,\nAny,\nClassVar,\nDict,\nForwardRef,\nGeneric,\nIterator,\nList,\nMapping,\nOptional,\nTuple,\nType,\nTypeVar,\nUnion,\ncast,\n)\nfrom weakref import WeakKeyDictionary,WeakValueDictionary\n\nfrom typing_extensions import Annotated,Literal as ExtLiteral\n\nfrom pydantic.v1.class_validators import gather_all_validators\nfrom pydantic.v1.fields import DeferredType\nfrom pydantic.v1.main import BaseModel,create_model\nfrom pydantic.v1.types import JsonWrapper\nfrom pydantic.v1.typing import display_as_type,get_all_type_hints,get_args,get_origin,typing_base\nfrom pydantic.v1.utils import all_identical,lenient_issubclass\n\nif sys.version_info >=(3,10):\n from typing import _UnionGenericAlias\nif sys.version_info >=(3,8):\n from typing import Literal\n \nGenericModelT=TypeVar('GenericModelT',bound='GenericModel')\nTypeVarType=Any\n\nCacheKey=Tuple[Type[Any],Any,Tuple[Any,...]]\nParametrization=Mapping[TypeVarType,Type[Any]]\n\n\n\nif sys.version_info >=(3,9):\n GenericTypesCache=WeakValueDictionary[CacheKey,Type[BaseModel]]\n AssignedParameters=WeakKeyDictionary[Type[BaseModel],Parametrization]\nelse:\n GenericTypesCache=WeakValueDictionary\n AssignedParameters=WeakKeyDictionary\n \n \n \n_generic_types_cache=GenericTypesCache()\n\n\n\n\n\n\n_assigned_parameters=AssignedParameters()\n\n\nclass GenericModel(BaseModel):\n __slots__=()\n __concrete__:ClassVar[bool]=False\n \n if TYPE_CHECKING:\n \n \n \n  __parameters__:ClassVar[Tuple[TypeVarType,...]]\n  \n  \n def __class_getitem__(cls:Type[GenericModelT],params:Union[Type[Any],Tuple[Type[Any],...]])->Type[Any]:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  def _cache_key(_params:Any)->CacheKey:\n   args=get_args(_params)\n   \n   if len(args)==2 and isinstance(args[0],list):\n    args=(tuple(args[0]),args[1])\n   return cls,_params,args\n   \n  cached=_generic_types_cache.get(_cache_key(params))\n  if cached is not None:\n   return cached\n  if cls.__concrete__ and Generic not in cls.__bases__:\n   raise TypeError('Cannot parameterize a concrete instantiation of a generic model')\n  if not isinstance(params,tuple):\n   params=(params,)\n  if cls is GenericModel and any(isinstance(param,TypeVar)for param in params):\n   raise TypeError('Type parameters should be placed on typing.Generic, not GenericModel')\n  if not hasattr(cls,'__parameters__'):\n   raise TypeError(f'Type {cls.__name__} must inherit from typing.Generic before being parameterized')\n   \n  check_parameters_count(cls,params)\n  \n  typevars_map:Dict[TypeVarType,Type[Any]]=dict(zip(cls.__parameters__,params))\n  if all_identical(typevars_map.keys(),typevars_map.values())and typevars_map:\n   return cls\n   \n   \n  model_name=cls.__concrete_name__(params)\n  validators=gather_all_validators(cls)\n  \n  type_hints=get_all_type_hints(cls).items()\n  instance_type_hints={k:v for k,v in type_hints if get_origin(v)is not ClassVar}\n  \n  fields={k:(DeferredType(),cls.__fields__[k].field_info)for k in instance_type_hints if k in cls.__fields__}\n  \n  model_module,called_globally=get_caller_frame_info()\n  created_model=cast(\n  Type[GenericModel],\n  create_model(\n  model_name,\n  __module__=model_module or cls.__module__,\n  __base__=(cls,)+tuple(cls.__parameterized_bases__(typevars_map)),\n  __config__=None,\n  __validators__=validators,\n  __cls_kwargs__=None,\n  **fields,\n  ),\n  )\n  \n  _assigned_parameters[created_model]=typevars_map\n  \n  if called_globally:\n   object_by_reference=None\n   reference_name=model_name\n   reference_module_globals=sys.modules[created_model.__module__].__dict__\n   while object_by_reference is not created_model:\n    object_by_reference=reference_module_globals.setdefault(reference_name,created_model)\n    reference_name +='_'\n    \n  created_model.Config=cls.Config\n  \n  \n  \n  \n  \n  new_params=tuple(\n  {param:None for param in iter_contained_typevars(typevars_map.values())}\n  )\n  created_model.__concrete__=not new_params\n  if new_params:\n   created_model.__parameters__=new_params\n   \n   \n   \n  _generic_types_cache[_cache_key(params)]=created_model\n  if len(params)==1:\n   _generic_types_cache[_cache_key(params[0])]=created_model\n   \n   \n   \n  _prepare_model_fields(created_model,fields,instance_type_hints,typevars_map)\n  \n  return created_model\n  \n @classmethod\n def __concrete_name__(cls:Type[Any],params:Tuple[Type[Any],...])->str:\n  ''\n\n\n\n\n\n\n\n\n  \n  param_names=[display_as_type(param)for param in params]\n  params_component=', '.join(param_names)\n  return f'{cls.__name__}[{params_component}]'\n  \n @classmethod\n def __parameterized_bases__(cls,typevars_map:Parametrization)->Iterator[Type[Any]]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  def build_base_model(\n  base_model:Type[GenericModel],mapped_types:Parametrization\n  )->Iterator[Type[GenericModel]]:\n   base_parameters=tuple(mapped_types[param]for param in base_model.__parameters__)\n   parameterized_base=base_model.__class_getitem__(base_parameters)\n   if parameterized_base is base_model or parameterized_base is cls:\n   \n    return\n   yield parameterized_base\n   \n  for base_model in cls.__bases__:\n   if not issubclass(base_model,GenericModel):\n   \n    continue\n   elif not getattr(base_model,'__parameters__',None):\n   \n   \n   \n    continue\n   elif cls in _assigned_parameters:\n    if base_model in _assigned_parameters:\n    \n    \n    \n    \n    \n     continue\n    else:\n    \n    \n    \n     mapped_types:Parametrization={\n     key:typevars_map.get(value,value)for key,value in _assigned_parameters[cls].items()\n     }\n     yield from build_base_model(base_model,mapped_types)\n   else:\n   \n   \n    yield from build_base_model(base_model,typevars_map)\n    \n    \ndef replace_types(type_:Any,type_map:Mapping[Any,Any])->Any:\n ''\n\n\n\n\n\n\n\n\n\n \n if not type_map:\n  return type_\n  \n type_args=get_args(type_)\n origin_type=get_origin(type_)\n \n if origin_type is Annotated:\n  annotated_type,*annotations=type_args\n  return Annotated[replace_types(annotated_type,type_map),tuple(annotations)]\n  \n if(origin_type is ExtLiteral)or(sys.version_info >=(3,8)and origin_type is Literal):\n  return type_map.get(type_,type_)\n  \n  \n if type_args:\n  resolved_type_args=tuple(replace_types(arg,type_map)for arg in type_args)\n  if all_identical(type_args,resolved_type_args):\n  \n  \n   return type_\n  if(\n  origin_type is not None\n  and isinstance(type_,typing_base)\n  and not isinstance(origin_type,typing_base)\n  and getattr(type_,'_name',None)is not None\n  ):\n  \n  \n  \n   origin_type=getattr(typing,type_._name)\n  assert origin_type is not None\n  \n  \n  if sys.version_info >=(3,10)and origin_type is types.UnionType:\n   return _UnionGenericAlias(origin_type,resolved_type_args)\n  return origin_type[resolved_type_args]\n  \n  \n  \n if not origin_type and lenient_issubclass(type_,GenericModel)and not type_.__concrete__:\n  type_args=type_.__parameters__\n  resolved_type_args=tuple(replace_types(t,type_map)for t in type_args)\n  if all_identical(type_args,resolved_type_args):\n   return type_\n  return type_[resolved_type_args]\n  \n  \n  \n if isinstance(type_,(List,list)):\n  resolved_list=list(replace_types(element,type_map)for element in type_)\n  if all_identical(type_,resolved_list):\n   return type_\n  return resolved_list\n  \n  \n  \n if not origin_type and lenient_issubclass(type_,JsonWrapper):\n  type_.inner_type=replace_types(type_.inner_type,type_map)\n  return type_\n  \n  \n  \n new_type=type_map.get(type_,type_)\n \n if isinstance(new_type,str):\n  return ForwardRef(new_type)\n else:\n  return new_type\n  \n  \ndef check_parameters_count(cls:Type[GenericModel],parameters:Tuple[Any,...])->None:\n actual=len(parameters)\n expected=len(cls.__parameters__)\n if actual !=expected:\n  description='many'if actual >expected else 'few'\n  raise TypeError(f'Too {description} parameters for {cls.__name__}; actual {actual}, expected {expected}')\n  \n  \nDictValues:Type[Any]={}.values().__class__\n\n\ndef iter_contained_typevars(v:Any)->Iterator[TypeVarType]:\n ''\n if isinstance(v,TypeVar):\n  yield v\n elif hasattr(v,'__parameters__')and not get_origin(v)and lenient_issubclass(v,GenericModel):\n  yield from v.__parameters__\n elif isinstance(v,(DictValues,list)):\n  for var in v:\n   yield from iter_contained_typevars(var)\n else:\n  args=get_args(v)\n  for arg in args:\n   yield from iter_contained_typevars(arg)\n   \n   \ndef get_caller_frame_info()->Tuple[Optional[str],bool]:\n ''\n\n\n\n\n\n \n try:\n  previous_caller_frame=sys._getframe(2)\n except ValueError as e:\n  raise RuntimeError('This function must be used inside another function')from e\n except AttributeError:\n  return None,False\n frame_globals=previous_caller_frame.f_globals\n return frame_globals.get('__name__'),previous_caller_frame.f_locals is frame_globals\n \n \ndef _prepare_model_fields(\ncreated_model:Type[GenericModel],\nfields:Mapping[str,Any],\ninstance_type_hints:Mapping[str,type],\ntypevars_map:Mapping[Any,type],\n)->None:\n ''\n\n \n \n for key,field in created_model.__fields__.items():\n  if key not in fields:\n   assert field.type_.__class__ is not DeferredType\n   \n   continue\n   \n  assert field.type_.__class__ is DeferredType,field.type_.__class__\n  \n  field_type_hint=instance_type_hints[key]\n  concrete_type=replace_types(field_type_hint,typevars_map)\n  field.type_=concrete_type\n  field.outer_type_=concrete_type\n  field.prepare()\n  created_model.__annotations__[key]=concrete_type\n", ["pydantic.v1.class_validators", "pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "sys", "types", "typing", "typing_extensions", "weakref"]], "pydantic.v1.env_settings": [".py", "import os\nimport warnings\nfrom pathlib import Path\nfrom typing import AbstractSet,Any,Callable,ClassVar,Dict,List,Mapping,Optional,Tuple,Type,Union\n\nfrom pydantic.v1.config import BaseConfig,Extra\nfrom pydantic.v1.fields import ModelField\nfrom pydantic.v1.main import BaseModel\nfrom pydantic.v1.types import JsonWrapper\nfrom pydantic.v1.typing import StrPath,display_as_type,get_origin,is_union\nfrom pydantic.v1.utils import deep_update,lenient_issubclass,path_type,sequence_like\n\nenv_file_sentinel=str(object())\n\nSettingsSourceCallable=Callable[['BaseSettings'],Dict[str,Any]]\nDotenvType=Union[StrPath,List[StrPath],Tuple[StrPath,...]]\n\n\nclass SettingsError(ValueError):\n pass\n \n \nclass BaseSettings(BaseModel):\n ''\n\n\n\n\n \n \n def __init__(\n __pydantic_self__,\n _env_file:Optional[DotenvType]=env_file_sentinel,\n _env_file_encoding:Optional[str]=None,\n _env_nested_delimiter:Optional[str]=None,\n _secrets_dir:Optional[StrPath]=None,\n **values:Any,\n )->None:\n \n  super().__init__(\n  **__pydantic_self__._build_values(\n  values,\n  _env_file=_env_file,\n  _env_file_encoding=_env_file_encoding,\n  _env_nested_delimiter=_env_nested_delimiter,\n  _secrets_dir=_secrets_dir,\n  )\n  )\n  \n def _build_values(\n self,\n init_kwargs:Dict[str,Any],\n _env_file:Optional[DotenvType]=None,\n _env_file_encoding:Optional[str]=None,\n _env_nested_delimiter:Optional[str]=None,\n _secrets_dir:Optional[StrPath]=None,\n )->Dict[str,Any]:\n \n  init_settings=InitSettingsSource(init_kwargs=init_kwargs)\n  env_settings=EnvSettingsSource(\n  env_file=(_env_file if _env_file !=env_file_sentinel else self.__config__.env_file),\n  env_file_encoding=(\n  _env_file_encoding if _env_file_encoding is not None else self.__config__.env_file_encoding\n  ),\n  env_nested_delimiter=(\n  _env_nested_delimiter if _env_nested_delimiter is not None else self.__config__.env_nested_delimiter\n  ),\n  env_prefix_len=len(self.__config__.env_prefix),\n  )\n  file_secret_settings=SecretsSettingsSource(secrets_dir=_secrets_dir or self.__config__.secrets_dir)\n  \n  sources=self.__config__.customise_sources(\n  init_settings=init_settings,env_settings=env_settings,file_secret_settings=file_secret_settings\n  )\n  if sources:\n   return deep_update(*reversed([source(self)for source in sources]))\n  else:\n  \n  \n   return{}\n   \n class Config(BaseConfig):\n  env_prefix:str=''\n  env_file:Optional[DotenvType]=None\n  env_file_encoding:Optional[str]=None\n  env_nested_delimiter:Optional[str]=None\n  secrets_dir:Optional[StrPath]=None\n  validate_all:bool=True\n  extra:Extra=Extra.forbid\n  arbitrary_types_allowed:bool=True\n  case_sensitive:bool=False\n  \n  @classmethod\n  def prepare_field(cls,field:ModelField)->None:\n   env_names:Union[List[str],AbstractSet[str]]\n   field_info_from_config=cls.get_field_info(field.name)\n   \n   env=field_info_from_config.get('env')or field.field_info.extra.get('env')\n   if env is None:\n    if field.has_alias:\n     warnings.warn(\n     'aliases are no longer used by BaseSettings to define which environment variables to read. '\n     'Instead use the \"env\" field setting. '\n     'See https://pydantic-docs.helpmanual.io/usage/settings/#environment-variable-names',\n     FutureWarning,\n     )\n    env_names={cls.env_prefix+field.name}\n   elif isinstance(env,str):\n    env_names={env}\n   elif isinstance(env,(set,frozenset)):\n    env_names=env\n   elif sequence_like(env):\n    env_names=list(env)\n   else:\n    raise TypeError(f'invalid field env: {env !r} ({display_as_type(env)}); should be string, list or set')\n    \n   if not cls.case_sensitive:\n    env_names=env_names.__class__(n.lower()for n in env_names)\n   field.field_info.extra['env_names']=env_names\n   \n  @classmethod\n  def customise_sources(\n  cls,\n  init_settings:SettingsSourceCallable,\n  env_settings:SettingsSourceCallable,\n  file_secret_settings:SettingsSourceCallable,\n  )->Tuple[SettingsSourceCallable,...]:\n   return init_settings,env_settings,file_secret_settings\n   \n  @classmethod\n  def parse_env_var(cls,field_name:str,raw_val:str)->Any:\n   return cls.json_loads(raw_val)\n   \n   \n __config__:ClassVar[Type[Config]]\n \n \nclass InitSettingsSource:\n __slots__=('init_kwargs',)\n \n def __init__(self,init_kwargs:Dict[str,Any]):\n  self.init_kwargs=init_kwargs\n  \n def __call__(self,settings:BaseSettings)->Dict[str,Any]:\n  return self.init_kwargs\n  \n def __repr__(self)->str:\n  return f'InitSettingsSource(init_kwargs={self.init_kwargs !r})'\n  \n  \nclass EnvSettingsSource:\n __slots__=('env_file','env_file_encoding','env_nested_delimiter','env_prefix_len')\n \n def __init__(\n self,\n env_file:Optional[DotenvType],\n env_file_encoding:Optional[str],\n env_nested_delimiter:Optional[str]=None,\n env_prefix_len:int=0,\n ):\n  self.env_file:Optional[DotenvType]=env_file\n  self.env_file_encoding:Optional[str]=env_file_encoding\n  self.env_nested_delimiter:Optional[str]=env_nested_delimiter\n  self.env_prefix_len:int=env_prefix_len\n  \n def __call__(self,settings:BaseSettings)->Dict[str,Any]:\n  ''\n\n  \n  d:Dict[str,Any]={}\n  \n  if settings.__config__.case_sensitive:\n   env_vars:Mapping[str,Optional[str]]=os.environ\n  else:\n   env_vars={k.lower():v for k,v in os.environ.items()}\n   \n  dotenv_vars=self._read_env_files(settings.__config__.case_sensitive)\n  if dotenv_vars:\n   env_vars={**dotenv_vars,**env_vars}\n   \n  for field in settings.__fields__.values():\n   env_val:Optional[str]=None\n   for env_name in field.field_info.extra['env_names']:\n    env_val=env_vars.get(env_name)\n    if env_val is not None:\n     break\n     \n   is_complex,allow_parse_failure=self.field_is_complex(field)\n   if is_complex:\n    if env_val is None:\n    \n     env_val_built=self.explode_env_vars(field,env_vars)\n     if env_val_built:\n      d[field.alias]=env_val_built\n    else:\n    \n     try:\n      env_val=settings.__config__.parse_env_var(field.name,env_val)\n     except ValueError as e:\n      if not allow_parse_failure:\n       raise SettingsError(f'error parsing env var \"{env_name}\"')from e\n       \n     if isinstance(env_val,dict):\n      d[field.alias]=deep_update(env_val,self.explode_env_vars(field,env_vars))\n     else:\n      d[field.alias]=env_val\n   elif env_val is not None:\n   \n    d[field.alias]=env_val\n    \n  return d\n  \n def _read_env_files(self,case_sensitive:bool)->Dict[str,Optional[str]]:\n  env_files=self.env_file\n  if env_files is None:\n   return{}\n   \n  if isinstance(env_files,(str,os.PathLike)):\n   env_files=[env_files]\n   \n  dotenv_vars={}\n  for env_file in env_files:\n   env_path=Path(env_file).expanduser()\n   if env_path.is_file():\n    dotenv_vars.update(\n    read_env_file(env_path,encoding=self.env_file_encoding,case_sensitive=case_sensitive)\n    )\n    \n  return dotenv_vars\n  \n def field_is_complex(self,field:ModelField)->Tuple[bool,bool]:\n  ''\n\n  \n  if lenient_issubclass(field.annotation,JsonWrapper):\n   return False,False\n   \n  if field.is_complex():\n   allow_parse_failure=False\n  elif is_union(get_origin(field.type_))and field.sub_fields and any(f.is_complex()for f in field.sub_fields):\n   allow_parse_failure=True\n  else:\n   return False,False\n   \n  return True,allow_parse_failure\n  \n def explode_env_vars(self,field:ModelField,env_vars:Mapping[str,Optional[str]])->Dict[str,Any]:\n  ''\n\n\n\n  \n  prefixes=[f'{env_name}{self.env_nested_delimiter}'for env_name in field.field_info.extra['env_names']]\n  result:Dict[str,Any]={}\n  for env_name,env_val in env_vars.items():\n   if not any(env_name.startswith(prefix)for prefix in prefixes):\n    continue\n    \n   env_name_without_prefix=env_name[self.env_prefix_len:]\n   _,*keys,last_key=env_name_without_prefix.split(self.env_nested_delimiter)\n   env_var=result\n   for key in keys:\n    env_var=env_var.setdefault(key,{})\n   env_var[last_key]=env_val\n   \n  return result\n  \n def __repr__(self)->str:\n  return(\n  f'EnvSettingsSource(env_file={self.env_file !r}, env_file_encoding={self.env_file_encoding !r}, '\n  f'env_nested_delimiter={self.env_nested_delimiter !r})'\n  )\n  \n  \nclass SecretsSettingsSource:\n __slots__=('secrets_dir',)\n \n def __init__(self,secrets_dir:Optional[StrPath]):\n  self.secrets_dir:Optional[StrPath]=secrets_dir\n  \n def __call__(self,settings:BaseSettings)->Dict[str,Any]:\n  ''\n\n  \n  secrets:Dict[str,Optional[str]]={}\n  \n  if self.secrets_dir is None:\n   return secrets\n   \n  secrets_path=Path(self.secrets_dir).expanduser()\n  \n  if not secrets_path.exists():\n   warnings.warn(f'directory \"{secrets_path}\" does not exist')\n   return secrets\n   \n  if not secrets_path.is_dir():\n   raise SettingsError(f'secrets_dir must reference a directory, not a {path_type(secrets_path)}')\n   \n  for field in settings.__fields__.values():\n   for env_name in field.field_info.extra['env_names']:\n    path=find_case_path(secrets_path,env_name,settings.__config__.case_sensitive)\n    if not path:\n    \n     continue\n     \n    if path.is_file():\n     secret_value=path.read_text().strip()\n     if field.is_complex():\n      try:\n       secret_value=settings.__config__.parse_env_var(field.name,secret_value)\n      except ValueError as e:\n       raise SettingsError(f'error parsing env var \"{env_name}\"')from e\n       \n     secrets[field.alias]=secret_value\n    else:\n     warnings.warn(\n     f'attempted to load secret file \"{path}\" but found a {path_type(path)} instead.',\n     stacklevel=4,\n     )\n  return secrets\n  \n def __repr__(self)->str:\n  return f'SecretsSettingsSource(secrets_dir={self.secrets_dir !r})'\n  \n  \ndef read_env_file(\nfile_path:StrPath,*,encoding:str=None,case_sensitive:bool=False\n)->Dict[str,Optional[str]]:\n try:\n  from dotenv import dotenv_values\n except ImportError as e:\n  raise ImportError('python-dotenv is not installed, run `pip install pydantic[dotenv]`')from e\n  \n file_vars:Dict[str,Optional[str]]=dotenv_values(file_path,encoding=encoding or 'utf8')\n if not case_sensitive:\n  return{k.lower():v for k,v in file_vars.items()}\n else:\n  return file_vars\n  \n  \ndef find_case_path(dir_path:Path,file_name:str,case_sensitive:bool)->Optional[Path]:\n ''\n\n \n for f in dir_path.iterdir():\n  if f.name ==file_name:\n   return f\n  elif not case_sensitive and f.name.lower()==file_name.lower():\n   return f\n return None\n", ["dotenv", "os", "pathlib", "pydantic.v1.config", "pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "typing", "warnings"]], "pydantic.v1.decorator": [".py", "from functools import wraps\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,List,Mapping,Optional,Tuple,Type,TypeVar,Union,overload\n\nfrom pydantic.v1 import validator\nfrom pydantic.v1.config import Extra\nfrom pydantic.v1.errors import ConfigError\nfrom pydantic.v1.main import BaseModel,create_model\nfrom pydantic.v1.typing import get_all_type_hints\nfrom pydantic.v1.utils import to_camel\n\n__all__=('validate_arguments',)\n\nif TYPE_CHECKING:\n from pydantic.v1.typing import AnyCallable\n \n AnyCallableT=TypeVar('AnyCallableT',bound=AnyCallable)\n ConfigType=Union[None,Type[Any],Dict[str,Any]]\n \n \n@overload\ndef validate_arguments(func:None=None,*,config:'ConfigType'=None)->Callable[['AnyCallableT'],'AnyCallableT']:\n ...\n \n \n@overload\ndef validate_arguments(func:'AnyCallableT')->'AnyCallableT':\n ...\n \n \ndef validate_arguments(func:Optional['AnyCallableT']=None,*,config:'ConfigType'=None)->Any:\n ''\n\n \n \n def validate(_func:'AnyCallable')->'AnyCallable':\n  vd=ValidatedFunction(_func,config)\n  \n  @wraps(_func)\n  def wrapper_function(*args:Any,**kwargs:Any)->Any:\n   return vd.call(*args,**kwargs)\n   \n  wrapper_function.vd=vd\n  wrapper_function.validate=vd.init_model_instance\n  wrapper_function.raw_function=vd.raw_function\n  wrapper_function.model=vd.model\n  return wrapper_function\n  \n if func:\n  return validate(func)\n else:\n  return validate\n  \n  \nALT_V_ARGS='v__args'\nALT_V_KWARGS='v__kwargs'\nV_POSITIONAL_ONLY_NAME='v__positional_only'\nV_DUPLICATE_KWARGS='v__duplicate_kwargs'\n\n\nclass ValidatedFunction:\n def __init__(self,function:'AnyCallableT',config:'ConfigType'):\n  from inspect import Parameter,signature\n  \n  parameters:Mapping[str,Parameter]=signature(function).parameters\n  \n  if parameters.keys()&{ALT_V_ARGS,ALT_V_KWARGS,V_POSITIONAL_ONLY_NAME,V_DUPLICATE_KWARGS}:\n   raise ConfigError(\n   f'\"{ALT_V_ARGS}\", \"{ALT_V_KWARGS}\", \"{V_POSITIONAL_ONLY_NAME}\" and \"{V_DUPLICATE_KWARGS}\" '\n   f'are not permitted as argument names when using the \"{validate_arguments.__name__}\" decorator'\n   )\n   \n  self.raw_function=function\n  self.arg_mapping:Dict[int,str]={}\n  self.positional_only_args=set()\n  self.v_args_name='args'\n  self.v_kwargs_name='kwargs'\n  \n  type_hints=get_all_type_hints(function)\n  takes_args=False\n  takes_kwargs=False\n  fields:Dict[str,Tuple[Any,Any]]={}\n  for i,(name,p)in enumerate(parameters.items()):\n   if p.annotation is p.empty:\n    annotation=Any\n   else:\n    annotation=type_hints[name]\n    \n   default=...if p.default is p.empty else p.default\n   if p.kind ==Parameter.POSITIONAL_ONLY:\n    self.arg_mapping[i]=name\n    fields[name]=annotation,default\n    fields[V_POSITIONAL_ONLY_NAME]=List[str],None\n    self.positional_only_args.add(name)\n   elif p.kind ==Parameter.POSITIONAL_OR_KEYWORD:\n    self.arg_mapping[i]=name\n    fields[name]=annotation,default\n    fields[V_DUPLICATE_KWARGS]=List[str],None\n   elif p.kind ==Parameter.KEYWORD_ONLY:\n    fields[name]=annotation,default\n   elif p.kind ==Parameter.VAR_POSITIONAL:\n    self.v_args_name=name\n    fields[name]=Tuple[annotation,...],None\n    takes_args=True\n   else:\n    assert p.kind ==Parameter.VAR_KEYWORD,p.kind\n    self.v_kwargs_name=name\n    fields[name]=Dict[str,annotation],None\n    takes_kwargs=True\n    \n    \n  if not takes_args and self.v_args_name in fields:\n   self.v_args_name=ALT_V_ARGS\n   \n   \n  if not takes_kwargs and self.v_kwargs_name in fields:\n   self.v_kwargs_name=ALT_V_KWARGS\n   \n  if not takes_args:\n  \n   fields[self.v_args_name]=List[Any],None\n   \n  if not takes_kwargs:\n  \n   fields[self.v_kwargs_name]=Dict[Any,Any],None\n   \n  self.create_model(fields,takes_args,takes_kwargs,config)\n  \n def init_model_instance(self,*args:Any,**kwargs:Any)->BaseModel:\n  values=self.build_values(args,kwargs)\n  return self.model(**values)\n  \n def call(self,*args:Any,**kwargs:Any)->Any:\n  m=self.init_model_instance(*args,**kwargs)\n  return self.execute(m)\n  \n def build_values(self,args:Tuple[Any,...],kwargs:Dict[str,Any])->Dict[str,Any]:\n  values:Dict[str,Any]={}\n  if args:\n   arg_iter=enumerate(args)\n   while True:\n    try:\n     i,a=next(arg_iter)\n    except StopIteration:\n     break\n    arg_name=self.arg_mapping.get(i)\n    if arg_name is not None:\n     values[arg_name]=a\n    else:\n     values[self.v_args_name]=[a]+[a for _,a in arg_iter]\n     break\n     \n  var_kwargs:Dict[str,Any]={}\n  wrong_positional_args=[]\n  duplicate_kwargs=[]\n  fields_alias=[\n  field.alias\n  for name,field in self.model.__fields__.items()\n  if name not in(self.v_args_name,self.v_kwargs_name)\n  ]\n  non_var_fields=set(self.model.__fields__)-{self.v_args_name,self.v_kwargs_name}\n  for k,v in kwargs.items():\n   if k in non_var_fields or k in fields_alias:\n    if k in self.positional_only_args:\n     wrong_positional_args.append(k)\n    if k in values:\n     duplicate_kwargs.append(k)\n    values[k]=v\n   else:\n    var_kwargs[k]=v\n    \n  if var_kwargs:\n   values[self.v_kwargs_name]=var_kwargs\n  if wrong_positional_args:\n   values[V_POSITIONAL_ONLY_NAME]=wrong_positional_args\n  if duplicate_kwargs:\n   values[V_DUPLICATE_KWARGS]=duplicate_kwargs\n  return values\n  \n def execute(self,m:BaseModel)->Any:\n  d={k:v for k,v in m._iter()if k in m.__fields_set__ or m.__fields__[k].default_factory}\n  var_kwargs=d.pop(self.v_kwargs_name,{})\n  \n  if self.v_args_name in d:\n   args_:List[Any]=[]\n   in_kwargs=False\n   kwargs={}\n   for name,value in d.items():\n    if in_kwargs:\n     kwargs[name]=value\n    elif name ==self.v_args_name:\n     args_ +=value\n     in_kwargs=True\n    else:\n     args_.append(value)\n   return self.raw_function(*args_,**kwargs,**var_kwargs)\n  elif self.positional_only_args:\n   args_=[]\n   kwargs={}\n   for name,value in d.items():\n    if name in self.positional_only_args:\n     args_.append(value)\n    else:\n     kwargs[name]=value\n   return self.raw_function(*args_,**kwargs,**var_kwargs)\n  else:\n   return self.raw_function(**d,**var_kwargs)\n   \n def create_model(self,fields:Dict[str,Any],takes_args:bool,takes_kwargs:bool,config:'ConfigType')->None:\n  pos_args=len(self.arg_mapping)\n  \n  class CustomConfig:\n   pass\n   \n  if not TYPE_CHECKING:\n   if isinstance(config,dict):\n    CustomConfig=type('Config',(),config)\n   elif config is not None:\n    CustomConfig=config\n    \n  if hasattr(CustomConfig,'fields')or hasattr(CustomConfig,'alias_generator'):\n   raise ConfigError(\n   'Setting the \"fields\" and \"alias_generator\" property on custom Config for '\n   '@validate_arguments is not yet supported, please remove.'\n   )\n   \n  class DecoratorBaseModel(BaseModel):\n   @validator(self.v_args_name,check_fields=False,allow_reuse=True)\n   def check_args(cls,v:Optional[List[Any]])->Optional[List[Any]]:\n    if takes_args or v is None:\n     return v\n     \n    raise TypeError(f'{pos_args} positional arguments expected but {pos_args+len(v)} given')\n    \n   @validator(self.v_kwargs_name,check_fields=False,allow_reuse=True)\n   def check_kwargs(cls,v:Optional[Dict[str,Any]])->Optional[Dict[str,Any]]:\n    if takes_kwargs or v is None:\n     return v\n     \n    plural=''if len(v)==1 else 's'\n    keys=', '.join(map(repr,v.keys()))\n    raise TypeError(f'unexpected keyword argument{plural}: {keys}')\n    \n   @validator(V_POSITIONAL_ONLY_NAME,check_fields=False,allow_reuse=True)\n   def check_positional_only(cls,v:Optional[List[str]])->None:\n    if v is None:\n     return\n     \n    plural=''if len(v)==1 else 's'\n    keys=', '.join(map(repr,v))\n    raise TypeError(f'positional-only argument{plural} passed as keyword argument{plural}: {keys}')\n    \n   @validator(V_DUPLICATE_KWARGS,check_fields=False,allow_reuse=True)\n   def check_duplicate_kwargs(cls,v:Optional[List[str]])->None:\n    if v is None:\n     return\n     \n    plural=''if len(v)==1 else 's'\n    keys=', '.join(map(repr,v))\n    raise TypeError(f'multiple values for argument{plural}: {keys}')\n    \n   class Config(CustomConfig):\n    extra=getattr(CustomConfig,'extra',Extra.forbid)\n    \n  self.model=create_model(to_camel(self.raw_function.__name__),__base__=DecoratorBaseModel,**fields)\n", ["functools", "inspect", "pydantic.v1", "pydantic.v1.config", "pydantic.v1.errors", "pydantic.v1.main", "pydantic.v1.typing", "pydantic.v1.utils", "typing"]], "pydantic.v1.class_validators": [".py", "import warnings\nfrom collections import ChainMap\nfrom functools import partial,partialmethod,wraps\nfrom itertools import chain\nfrom types import FunctionType\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,Iterable,List,Optional,Set,Tuple,Type,Union,overload\n\nfrom pydantic.v1.errors import ConfigError\nfrom pydantic.v1.typing import AnyCallable\nfrom pydantic.v1.utils import ROOT_KEY,in_ipython\n\nif TYPE_CHECKING:\n from pydantic.v1.typing import AnyClassMethod\n \n \nclass Validator:\n __slots__='func','pre','each_item','always','check_fields','skip_on_failure'\n \n def __init__(\n self,\n func:AnyCallable,\n pre:bool=False,\n each_item:bool=False,\n always:bool=False,\n check_fields:bool=False,\n skip_on_failure:bool=False,\n ):\n  self.func=func\n  self.pre=pre\n  self.each_item=each_item\n  self.always=always\n  self.check_fields=check_fields\n  self.skip_on_failure=skip_on_failure\n  \n  \nif TYPE_CHECKING:\n from inspect import Signature\n \n from pydantic.v1.config import BaseConfig\n from pydantic.v1.fields import ModelField\n from pydantic.v1.types import ModelOrDc\n \n ValidatorCallable=Callable[[Optional[ModelOrDc],Any,Dict[str,Any],ModelField,Type[BaseConfig]],Any]\n ValidatorsList=List[ValidatorCallable]\n ValidatorListDict=Dict[str,List[Validator]]\n \n_FUNCS:Set[str]=set()\nVALIDATOR_CONFIG_KEY='__validator_config__'\nROOT_VALIDATOR_CONFIG_KEY='__root_validator_config__'\n\n\ndef validator(\n*fields:str,\npre:bool=False,\neach_item:bool=False,\nalways:bool=False,\ncheck_fields:bool=True,\nwhole:Optional[bool]=None,\nallow_reuse:bool=False,\n)->Callable[[AnyCallable],'AnyClassMethod']:\n ''\n\n\n\n\n\n\n\n\n \n if not fields:\n  raise ConfigError('validator with no fields specified')\n elif isinstance(fields[0],FunctionType):\n  raise ConfigError(\n  \"validators should be used with fields and keyword arguments, not bare. \"\n  \"E.g. usage should be `@validator('<field_name>', ...)`\"\n  )\n elif not all(isinstance(field,str)for field in fields):\n  raise ConfigError(\n  \"validator fields should be passed as separate string args. \"\n  \"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`\"\n  )\n  \n if whole is not None:\n  warnings.warn(\n  'The \"whole\" keyword argument is deprecated, use \"each_item\" (inverse meaning, default False) instead',\n  DeprecationWarning,\n  )\n  assert each_item is False,'\"each_item\" and \"whole\" conflict, remove \"whole\"'\n  each_item=not whole\n  \n def dec(f:AnyCallable)->'AnyClassMethod':\n  f_cls=_prepare_validator(f,allow_reuse)\n  setattr(\n  f_cls,\n  VALIDATOR_CONFIG_KEY,\n  (\n  fields,\n  Validator(func=f_cls.__func__,pre=pre,each_item=each_item,always=always,check_fields=check_fields),\n  ),\n  )\n  return f_cls\n  \n return dec\n \n \n@overload\ndef root_validator(_func:AnyCallable)->'AnyClassMethod':\n ...\n \n \n@overload\ndef root_validator(\n*,pre:bool=False,allow_reuse:bool=False,skip_on_failure:bool=False\n)->Callable[[AnyCallable],'AnyClassMethod']:\n ...\n \n \ndef root_validator(\n_func:Optional[AnyCallable]=None,*,pre:bool=False,allow_reuse:bool=False,skip_on_failure:bool=False\n)->Union['AnyClassMethod',Callable[[AnyCallable],'AnyClassMethod']]:\n ''\n\n\n \n if _func:\n  f_cls=_prepare_validator(_func,allow_reuse)\n  setattr(\n  f_cls,ROOT_VALIDATOR_CONFIG_KEY,Validator(func=f_cls.__func__,pre=pre,skip_on_failure=skip_on_failure)\n  )\n  return f_cls\n  \n def dec(f:AnyCallable)->'AnyClassMethod':\n  f_cls=_prepare_validator(f,allow_reuse)\n  setattr(\n  f_cls,ROOT_VALIDATOR_CONFIG_KEY,Validator(func=f_cls.__func__,pre=pre,skip_on_failure=skip_on_failure)\n  )\n  return f_cls\n  \n return dec\n \n \ndef _prepare_validator(function:AnyCallable,allow_reuse:bool)->'AnyClassMethod':\n ''\n\n\n \n f_cls=function if isinstance(function,classmethod)else classmethod(function)\n if not in_ipython()and not allow_reuse:\n  ref=(\n  getattr(f_cls.__func__,'__module__','<No __module__>')\n  +'.'\n  +getattr(f_cls.__func__,'__qualname__',f'<No __qualname__: id:{id(f_cls.__func__)}>')\n  )\n  if ref in _FUNCS:\n   raise ConfigError(f'duplicate validator function \"{ref}\"; if this is intended, set `allow_reuse=True`')\n  _FUNCS.add(ref)\n return f_cls\n \n \nclass ValidatorGroup:\n def __init__(self,validators:'ValidatorListDict')->None:\n  self.validators=validators\n  self.used_validators={'*'}\n  \n def get_validators(self,name:str)->Optional[Dict[str,Validator]]:\n  self.used_validators.add(name)\n  validators=self.validators.get(name,[])\n  if name !=ROOT_KEY:\n   validators +=self.validators.get('*',[])\n  if validators:\n   return{getattr(v.func,'__name__',f'<No __name__: id:{id(v.func)}>'):v for v in validators}\n  else:\n   return None\n   \n def check_for_unused(self)->None:\n  unused_validators=set(\n  chain.from_iterable(\n  (\n  getattr(v.func,'__name__',f'<No __name__: id:{id(v.func)}>')\n  for v in self.validators[f]\n  if v.check_fields\n  )\n  for f in(self.validators.keys()-self.used_validators)\n  )\n  )\n  if unused_validators:\n   fn=', '.join(unused_validators)\n   raise ConfigError(\n   f\"Validators defined with incorrect fields: {fn} \"\n   f\"(use check_fields=False if you're inheriting from the model and intended this)\"\n   )\n   \n   \ndef extract_validators(namespace:Dict[str,Any])->Dict[str,List[Validator]]:\n validators:Dict[str,List[Validator]]={}\n for var_name,value in namespace.items():\n  validator_config=getattr(value,VALIDATOR_CONFIG_KEY,None)\n  if validator_config:\n   fields,v=validator_config\n   for field in fields:\n    if field in validators:\n     validators[field].append(v)\n    else:\n     validators[field]=[v]\n return validators\n \n \ndef extract_root_validators(namespace:Dict[str,Any])->Tuple[List[AnyCallable],List[Tuple[bool,AnyCallable]]]:\n from inspect import signature\n \n pre_validators:List[AnyCallable]=[]\n post_validators:List[Tuple[bool,AnyCallable]]=[]\n for name,value in namespace.items():\n  validator_config:Optional[Validator]=getattr(value,ROOT_VALIDATOR_CONFIG_KEY,None)\n  if validator_config:\n   sig=signature(validator_config.func)\n   args=list(sig.parameters.keys())\n   if args[0]=='self':\n    raise ConfigError(\n    f'Invalid signature for root validator {name}: {sig}, \"self\" not permitted as first argument, '\n    f'should be: (cls, values).'\n    )\n   if len(args)!=2:\n    raise ConfigError(f'Invalid signature for root validator {name}: {sig}, should be: (cls, values).')\n    \n   if validator_config.pre:\n    pre_validators.append(validator_config.func)\n   else:\n    post_validators.append((validator_config.skip_on_failure,validator_config.func))\n return pre_validators,post_validators\n \n \ndef inherit_validators(base_validators:'ValidatorListDict',validators:'ValidatorListDict')->'ValidatorListDict':\n for field,field_validators in base_validators.items():\n  if field not in validators:\n   validators[field]=[]\n  validators[field]+=field_validators\n return validators\n \n \ndef make_generic_validator(validator:AnyCallable)->'ValidatorCallable':\n ''\n\n\n\n\n\n\n\n \n from inspect import signature\n \n if not isinstance(validator,(partial,partialmethod)):\n \n  sig=signature(validator)\n  args=list(sig.parameters.keys())\n else:\n \n  sig=signature(validator.func)\n  args=[\n  k\n  for k in signature(validator.func).parameters.keys()\n  if k not in validator.args |validator.keywords.keys()\n  ]\n  \n first_arg=args.pop(0)\n if first_arg =='self':\n  raise ConfigError(\n  f'Invalid signature for validator {validator}: {sig}, \"self\" not permitted as first argument, '\n  f'should be: (cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n  )\n elif first_arg =='cls':\n \n  return wraps(validator)(_generic_validator_cls(validator,sig,set(args[1:])))\n else:\n \n  return wraps(validator)(_generic_validator_basic(validator,sig,set(args)))\n  \n  \ndef prep_validators(v_funcs:Iterable[AnyCallable])->'ValidatorsList':\n return[make_generic_validator(f)for f in v_funcs if f]\n \n \nall_kwargs={'values','field','config'}\n\n\ndef _generic_validator_cls(validator:AnyCallable,sig:'Signature',args:Set[str])->'ValidatorCallable':\n\n has_kwargs=False\n if 'kwargs'in args:\n  has_kwargs=True\n  args -={'kwargs'}\n  \n if not args.issubset(all_kwargs):\n  raise ConfigError(\n  f'Invalid signature for validator {validator}: {sig}, should be: '\n  f'(cls, value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n  )\n  \n if has_kwargs:\n  return lambda cls,v,values,field,config:validator(cls,v,values=values,field=field,config=config)\n elif args ==set():\n  return lambda cls,v,values,field,config:validator(cls,v)\n elif args =={'values'}:\n  return lambda cls,v,values,field,config:validator(cls,v,values=values)\n elif args =={'field'}:\n  return lambda cls,v,values,field,config:validator(cls,v,field=field)\n elif args =={'config'}:\n  return lambda cls,v,values,field,config:validator(cls,v,config=config)\n elif args =={'values','field'}:\n  return lambda cls,v,values,field,config:validator(cls,v,values=values,field=field)\n elif args =={'values','config'}:\n  return lambda cls,v,values,field,config:validator(cls,v,values=values,config=config)\n elif args =={'field','config'}:\n  return lambda cls,v,values,field,config:validator(cls,v,field=field,config=config)\n else:\n \n  return lambda cls,v,values,field,config:validator(cls,v,values=values,field=field,config=config)\n  \n  \ndef _generic_validator_basic(validator:AnyCallable,sig:'Signature',args:Set[str])->'ValidatorCallable':\n has_kwargs=False\n if 'kwargs'in args:\n  has_kwargs=True\n  args -={'kwargs'}\n  \n if not args.issubset(all_kwargs):\n  raise ConfigError(\n  f'Invalid signature for validator {validator}: {sig}, should be: '\n  f'(value, values, config, field), \"values\", \"config\" and \"field\" are all optional.'\n  )\n  \n if has_kwargs:\n  return lambda cls,v,values,field,config:validator(v,values=values,field=field,config=config)\n elif args ==set():\n  return lambda cls,v,values,field,config:validator(v)\n elif args =={'values'}:\n  return lambda cls,v,values,field,config:validator(v,values=values)\n elif args =={'field'}:\n  return lambda cls,v,values,field,config:validator(v,field=field)\n elif args =={'config'}:\n  return lambda cls,v,values,field,config:validator(v,config=config)\n elif args =={'values','field'}:\n  return lambda cls,v,values,field,config:validator(v,values=values,field=field)\n elif args =={'values','config'}:\n  return lambda cls,v,values,field,config:validator(v,values=values,config=config)\n elif args =={'field','config'}:\n  return lambda cls,v,values,field,config:validator(v,field=field,config=config)\n else:\n \n  return lambda cls,v,values,field,config:validator(v,values=values,field=field,config=config)\n  \n  \ndef gather_all_validators(type_:'ModelOrDc')->Dict[str,'AnyClassMethod']:\n all_attributes=ChainMap(*[cls.__dict__ for cls in type_.__mro__])\n return{\n k:v\n for k,v in all_attributes.items()\n if hasattr(v,VALIDATOR_CONFIG_KEY)or hasattr(v,ROOT_VALIDATOR_CONFIG_KEY)\n }\n", ["collections", "functools", "inspect", "itertools", "pydantic.v1.config", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "types", "typing", "warnings"]], "pydantic.v1.json": [".py", "import datetime\nfrom collections import deque\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom pathlib import Path\nfrom re import Pattern\nfrom types import GeneratorType\nfrom typing import Any,Callable,Dict,Type,Union\nfrom uuid import UUID\n\nfrom pydantic.v1.color import Color\nfrom pydantic.v1.networks import NameEmail\nfrom pydantic.v1.types import SecretBytes,SecretStr\n\n__all__='pydantic_encoder','custom_pydantic_encoder','timedelta_isoformat'\n\n\ndef isoformat(o:Union[datetime.date,datetime.time])->str:\n return o.isoformat()\n \n \ndef decimal_encoder(dec_value:Decimal)->Union[int,float]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n if dec_value.as_tuple().exponent >=0:\n  return int(dec_value)\n else:\n  return float(dec_value)\n  \n  \nENCODERS_BY_TYPE:Dict[Type[Any],Callable[[Any],Any]]={\nbytes:lambda o:o.decode(),\nColor:str,\ndatetime.date:isoformat,\ndatetime.datetime:isoformat,\ndatetime.time:isoformat,\ndatetime.timedelta:lambda td:td.total_seconds(),\nDecimal:decimal_encoder,\nEnum:lambda o:o.value,\nfrozenset:list,\ndeque:list,\nGeneratorType:list,\nIPv4Address:str,\nIPv4Interface:str,\nIPv4Network:str,\nIPv6Address:str,\nIPv6Interface:str,\nIPv6Network:str,\nNameEmail:str,\nPath:str,\nPattern:lambda o:o.pattern,\nSecretBytes:str,\nSecretStr:str,\nset:list,\nUUID:str,\n}\n\n\ndef pydantic_encoder(obj:Any)->Any:\n from dataclasses import asdict,is_dataclass\n \n from pydantic.v1.main import BaseModel\n \n if isinstance(obj,BaseModel):\n  return obj.dict()\n elif is_dataclass(obj):\n  return asdict(obj)\n  \n  \n for base in obj.__class__.__mro__[:-1]:\n  try:\n   encoder=ENCODERS_BY_TYPE[base]\n  except KeyError:\n   continue\n  return encoder(obj)\n else:\n  raise TypeError(f\"Object of type '{obj.__class__.__name__}' is not JSON serializable\")\n  \n  \ndef custom_pydantic_encoder(type_encoders:Dict[Any,Callable[[Type[Any]],Any]],obj:Any)->Any:\n\n for base in obj.__class__.__mro__[:-1]:\n  try:\n   encoder=type_encoders[base]\n  except KeyError:\n   continue\n   \n  return encoder(obj)\n else:\n  return pydantic_encoder(obj)\n  \n  \ndef timedelta_isoformat(td:datetime.timedelta)->str:\n ''\n\n \n minutes,seconds=divmod(td.seconds,60)\n hours,minutes=divmod(minutes,60)\n return f'{\"-\"if td.days <0 else \"\"}P{abs(td.days)}DT{hours:d}H{minutes:d}M{seconds:d}.{td.microseconds:06d}S'\n", ["collections", "dataclasses", "datetime", "decimal", "enum", "ipaddress", "pathlib", "pydantic.v1.color", "pydantic.v1.main", "pydantic.v1.networks", "pydantic.v1.types", "re", "types", "typing", "uuid"]], "pydantic.v1.types": [".py", "import abc\nimport math\nimport re\nimport warnings\nfrom datetime import date\nfrom decimal import Decimal,InvalidOperation\nfrom enum import Enum\nfrom pathlib import Path\nfrom types import new_class\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nClassVar,\nDict,\nFrozenSet,\nList,\nOptional,\nPattern,\nSet,\nTuple,\nType,\nTypeVar,\nUnion,\ncast,\noverload,\n)\nfrom uuid import UUID\nfrom weakref import WeakSet\n\nfrom pydantic.v1 import errors\nfrom pydantic.v1.datetime_parse import parse_date\nfrom pydantic.v1.utils import import_string,update_not_none\nfrom pydantic.v1.validators import(\nbytes_validator,\nconstr_length_validator,\nconstr_lower,\nconstr_strip_whitespace,\nconstr_upper,\ndecimal_validator,\nfloat_finite_validator,\nfloat_validator,\nfrozenset_validator,\nint_validator,\nlist_validator,\nnumber_multiple_validator,\nnumber_size_validator,\npath_exists_validator,\npath_validator,\nset_validator,\nstr_validator,\nstrict_bytes_validator,\nstrict_float_validator,\nstrict_int_validator,\nstrict_str_validator,\n)\n\n__all__=[\n'NoneStr',\n'NoneBytes',\n'StrBytes',\n'NoneStrBytes',\n'StrictStr',\n'ConstrainedBytes',\n'conbytes',\n'ConstrainedList',\n'conlist',\n'ConstrainedSet',\n'conset',\n'ConstrainedFrozenSet',\n'confrozenset',\n'ConstrainedStr',\n'constr',\n'PyObject',\n'ConstrainedInt',\n'conint',\n'PositiveInt',\n'NegativeInt',\n'NonNegativeInt',\n'NonPositiveInt',\n'ConstrainedFloat',\n'confloat',\n'PositiveFloat',\n'NegativeFloat',\n'NonNegativeFloat',\n'NonPositiveFloat',\n'FiniteFloat',\n'ConstrainedDecimal',\n'condecimal',\n'UUID1',\n'UUID3',\n'UUID4',\n'UUID5',\n'FilePath',\n'DirectoryPath',\n'Json',\n'JsonWrapper',\n'SecretField',\n'SecretStr',\n'SecretBytes',\n'StrictBool',\n'StrictBytes',\n'StrictInt',\n'StrictFloat',\n'PaymentCardNumber',\n'ByteSize',\n'PastDate',\n'FutureDate',\n'ConstrainedDate',\n'condate',\n]\n\nNoneStr=Optional[str]\nNoneBytes=Optional[bytes]\nStrBytes=Union[str,bytes]\nNoneStrBytes=Optional[StrBytes]\nOptionalInt=Optional[int]\nOptionalIntFloat=Union[OptionalInt,float]\nOptionalIntFloatDecimal=Union[OptionalIntFloat,Decimal]\nOptionalDate=Optional[date]\nStrIntFloat=Union[str,int,float]\n\nif TYPE_CHECKING:\n from typing_extensions import Annotated\n \n from pydantic.v1.dataclasses import Dataclass\n from pydantic.v1.main import BaseModel\n from pydantic.v1.typing import CallableGenerator\n \n ModelOrDc=Type[Union[BaseModel,Dataclass]]\n \nT=TypeVar('T')\n_DEFINED_TYPES:'WeakSet[type]'=WeakSet()\n\n\n@overload\ndef _registered(typ:Type[T])->Type[T]:\n pass\n \n \n@overload\ndef _registered(typ:'ConstrainedNumberMeta')->'ConstrainedNumberMeta':\n pass\n \n \ndef _registered(typ:Union[Type[T],'ConstrainedNumberMeta'])->Union[Type[T],'ConstrainedNumberMeta']:\n\n\n\n\n\n\n _DEFINED_TYPES.add(typ)\n return typ\n \n \nclass ConstrainedNumberMeta(type):\n def __new__(cls,name:str,bases:Any,dct:Dict[str,Any])->'ConstrainedInt':\n  new_cls=cast('ConstrainedInt',type.__new__(cls,name,bases,dct))\n  \n  if new_cls.gt is not None and new_cls.ge is not None:\n   raise errors.ConfigError('bounds gt and ge cannot be specified at the same time')\n  if new_cls.lt is not None and new_cls.le is not None:\n   raise errors.ConfigError('bounds lt and le cannot be specified at the same time')\n   \n  return _registered(new_cls)\n  \n  \n  \n  \nif TYPE_CHECKING:\n StrictBool=bool\nelse:\n\n class StrictBool(int):\n  ''\n\n  \n  \n  @classmethod\n  def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n   field_schema.update(type='boolean')\n   \n  @classmethod\n  def __get_validators__(cls)->'CallableGenerator':\n   yield cls.validate\n   \n  @classmethod\n  def validate(cls,value:Any)->bool:\n   ''\n\n   \n   if isinstance(value,bool):\n    return value\n    \n   raise errors.StrictBoolError()\n   \n   \n   \n   \n   \nclass ConstrainedInt(int,metaclass=ConstrainedNumberMeta):\n strict:bool=False\n gt:OptionalInt=None\n ge:OptionalInt=None\n lt:OptionalInt=None\n le:OptionalInt=None\n multiple_of:OptionalInt=None\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(\n  field_schema,\n  exclusiveMinimum=cls.gt,\n  exclusiveMaximum=cls.lt,\n  minimum=cls.ge,\n  maximum=cls.le,\n  multipleOf=cls.multiple_of,\n  )\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield strict_int_validator if cls.strict else int_validator\n  yield number_size_validator\n  yield number_multiple_validator\n  \n  \ndef conint(\n*,\nstrict:bool=False,\ngt:Optional[int]=None,\nge:Optional[int]=None,\nlt:Optional[int]=None,\nle:Optional[int]=None,\nmultiple_of:Optional[int]=None,\n)->Type[int]:\n\n namespace=dict(strict=strict,gt=gt,ge=ge,lt=lt,le=le,multiple_of=multiple_of)\n return type('ConstrainedIntValue',(ConstrainedInt,),namespace)\n \n \nif TYPE_CHECKING:\n PositiveInt=int\n NegativeInt=int\n NonPositiveInt=int\n NonNegativeInt=int\n StrictInt=int\nelse:\n\n class PositiveInt(ConstrainedInt):\n  gt=0\n  \n class NegativeInt(ConstrainedInt):\n  lt=0\n  \n class NonPositiveInt(ConstrainedInt):\n  le=0\n  \n class NonNegativeInt(ConstrainedInt):\n  ge=0\n  \n class StrictInt(ConstrainedInt):\n  strict=True\n  \n  \n  \n  \n  \nclass ConstrainedFloat(float,metaclass=ConstrainedNumberMeta):\n strict:bool=False\n gt:OptionalIntFloat=None\n ge:OptionalIntFloat=None\n lt:OptionalIntFloat=None\n le:OptionalIntFloat=None\n multiple_of:OptionalIntFloat=None\n allow_inf_nan:Optional[bool]=None\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(\n  field_schema,\n  exclusiveMinimum=cls.gt,\n  exclusiveMaximum=cls.lt,\n  minimum=cls.ge,\n  maximum=cls.le,\n  multipleOf=cls.multiple_of,\n  )\n  \n  if field_schema.get('exclusiveMinimum')==-math.inf:\n   del field_schema['exclusiveMinimum']\n  if field_schema.get('minimum')==-math.inf:\n   del field_schema['minimum']\n  if field_schema.get('exclusiveMaximum')==math.inf:\n   del field_schema['exclusiveMaximum']\n  if field_schema.get('maximum')==math.inf:\n   del field_schema['maximum']\n   \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield strict_float_validator if cls.strict else float_validator\n  yield number_size_validator\n  yield number_multiple_validator\n  yield float_finite_validator\n  \n  \ndef confloat(\n*,\nstrict:bool=False,\ngt:float=None,\nge:float=None,\nlt:float=None,\nle:float=None,\nmultiple_of:float=None,\nallow_inf_nan:Optional[bool]=None,\n)->Type[float]:\n\n namespace=dict(strict=strict,gt=gt,ge=ge,lt=lt,le=le,multiple_of=multiple_of,allow_inf_nan=allow_inf_nan)\n return type('ConstrainedFloatValue',(ConstrainedFloat,),namespace)\n \n \nif TYPE_CHECKING:\n PositiveFloat=float\n NegativeFloat=float\n NonPositiveFloat=float\n NonNegativeFloat=float\n StrictFloat=float\n FiniteFloat=float\nelse:\n\n class PositiveFloat(ConstrainedFloat):\n  gt=0\n  \n class NegativeFloat(ConstrainedFloat):\n  lt=0\n  \n class NonPositiveFloat(ConstrainedFloat):\n  le=0\n  \n class NonNegativeFloat(ConstrainedFloat):\n  ge=0\n  \n class StrictFloat(ConstrainedFloat):\n  strict=True\n  \n class FiniteFloat(ConstrainedFloat):\n  allow_inf_nan=False\n  \n  \n  \n  \n  \nclass ConstrainedBytes(bytes):\n strip_whitespace=False\n to_upper=False\n to_lower=False\n min_length:OptionalInt=None\n max_length:OptionalInt=None\n strict:bool=False\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(field_schema,minLength=cls.min_length,maxLength=cls.max_length)\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield strict_bytes_validator if cls.strict else bytes_validator\n  yield constr_strip_whitespace\n  yield constr_upper\n  yield constr_lower\n  yield constr_length_validator\n  \n  \ndef conbytes(\n*,\nstrip_whitespace:bool=False,\nto_upper:bool=False,\nto_lower:bool=False,\nmin_length:Optional[int]=None,\nmax_length:Optional[int]=None,\nstrict:bool=False,\n)->Type[bytes]:\n\n namespace=dict(\n strip_whitespace=strip_whitespace,\n to_upper=to_upper,\n to_lower=to_lower,\n min_length=min_length,\n max_length=max_length,\n strict=strict,\n )\n return _registered(type('ConstrainedBytesValue',(ConstrainedBytes,),namespace))\n \n \nif TYPE_CHECKING:\n StrictBytes=bytes\nelse:\n\n class StrictBytes(ConstrainedBytes):\n  strict=True\n  \n  \n  \n  \n  \nclass ConstrainedStr(str):\n strip_whitespace=False\n to_upper=False\n to_lower=False\n min_length:OptionalInt=None\n max_length:OptionalInt=None\n curtail_length:OptionalInt=None\n regex:Optional[Union[str,Pattern[str]]]=None\n strict=False\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(\n  field_schema,\n  minLength=cls.min_length,\n  maxLength=cls.max_length,\n  pattern=cls.regex and cls._get_pattern(cls.regex),\n  )\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield strict_str_validator if cls.strict else str_validator\n  yield constr_strip_whitespace\n  yield constr_upper\n  yield constr_lower\n  yield constr_length_validator\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:Union[str])->Union[str]:\n  if cls.curtail_length and len(value)>cls.curtail_length:\n   value=value[:cls.curtail_length]\n   \n  if cls.regex:\n   if not re.match(cls.regex,value):\n    raise errors.StrRegexError(pattern=cls._get_pattern(cls.regex))\n    \n  return value\n  \n @staticmethod\n def _get_pattern(regex:Union[str,Pattern[str]])->str:\n  return regex if isinstance(regex,str)else regex.pattern\n  \n  \ndef constr(\n*,\nstrip_whitespace:bool=False,\nto_upper:bool=False,\nto_lower:bool=False,\nstrict:bool=False,\nmin_length:Optional[int]=None,\nmax_length:Optional[int]=None,\ncurtail_length:Optional[int]=None,\nregex:Optional[str]=None,\n)->Type[str]:\n\n namespace=dict(\n strip_whitespace=strip_whitespace,\n to_upper=to_upper,\n to_lower=to_lower,\n strict=strict,\n min_length=min_length,\n max_length=max_length,\n curtail_length=curtail_length,\n regex=regex and re.compile(regex),\n )\n return _registered(type('ConstrainedStrValue',(ConstrainedStr,),namespace))\n \n \nif TYPE_CHECKING:\n StrictStr=str\nelse:\n\n class StrictStr(ConstrainedStr):\n  strict=True\n  \n  \n  \n  \n  \n  \nclass ConstrainedSet(set):\n\n __origin__=set\n __args__:Set[Type[T]]\n \n min_items:Optional[int]=None\n max_items:Optional[int]=None\n item_type:Type[T]\n \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.set_length_validator\n  \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(field_schema,minItems=cls.min_items,maxItems=cls.max_items)\n  \n @classmethod\n def set_length_validator(cls,v:'Optional[Set[T]]')->'Optional[Set[T]]':\n  if v is None:\n   return None\n   \n  v=set_validator(v)\n  v_len=len(v)\n  \n  if cls.min_items is not None and v_len <cls.min_items:\n   raise errors.SetMinLengthError(limit_value=cls.min_items)\n   \n  if cls.max_items is not None and v_len >cls.max_items:\n   raise errors.SetMaxLengthError(limit_value=cls.max_items)\n   \n  return v\n  \n  \ndef conset(item_type:Type[T],*,min_items:Optional[int]=None,max_items:Optional[int]=None)->Type[Set[T]]:\n\n namespace={'min_items':min_items,'max_items':max_items,'item_type':item_type,'__args__':[item_type]}\n \n return new_class('ConstrainedSetValue',(ConstrainedSet,),{},lambda ns:ns.update(namespace))\n \n \n \nclass ConstrainedFrozenSet(frozenset):\n\n __origin__=frozenset\n __args__:FrozenSet[Type[T]]\n \n min_items:Optional[int]=None\n max_items:Optional[int]=None\n item_type:Type[T]\n \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.frozenset_length_validator\n  \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(field_schema,minItems=cls.min_items,maxItems=cls.max_items)\n  \n @classmethod\n def frozenset_length_validator(cls,v:'Optional[FrozenSet[T]]')->'Optional[FrozenSet[T]]':\n  if v is None:\n   return None\n   \n  v=frozenset_validator(v)\n  v_len=len(v)\n  \n  if cls.min_items is not None and v_len <cls.min_items:\n   raise errors.FrozenSetMinLengthError(limit_value=cls.min_items)\n   \n  if cls.max_items is not None and v_len >cls.max_items:\n   raise errors.FrozenSetMaxLengthError(limit_value=cls.max_items)\n   \n  return v\n  \n  \ndef confrozenset(\nitem_type:Type[T],*,min_items:Optional[int]=None,max_items:Optional[int]=None\n)->Type[FrozenSet[T]]:\n\n namespace={'min_items':min_items,'max_items':max_items,'item_type':item_type,'__args__':[item_type]}\n \n return new_class('ConstrainedFrozenSetValue',(ConstrainedFrozenSet,),{},lambda ns:ns.update(namespace))\n \n \n \n \n \n \nclass ConstrainedList(list):\n\n __origin__=list\n __args__:Tuple[Type[T],...]\n \n min_items:Optional[int]=None\n max_items:Optional[int]=None\n unique_items:Optional[bool]=None\n item_type:Type[T]\n \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.list_length_validator\n  if cls.unique_items:\n   yield cls.unique_items_validator\n   \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(field_schema,minItems=cls.min_items,maxItems=cls.max_items,uniqueItems=cls.unique_items)\n  \n @classmethod\n def list_length_validator(cls,v:'Optional[List[T]]')->'Optional[List[T]]':\n  if v is None:\n   return None\n   \n  v=list_validator(v)\n  v_len=len(v)\n  \n  if cls.min_items is not None and v_len <cls.min_items:\n   raise errors.ListMinLengthError(limit_value=cls.min_items)\n   \n  if cls.max_items is not None and v_len >cls.max_items:\n   raise errors.ListMaxLengthError(limit_value=cls.max_items)\n   \n  return v\n  \n @classmethod\n def unique_items_validator(cls,v:'Optional[List[T]]')->'Optional[List[T]]':\n  if v is None:\n   return None\n   \n  for i,value in enumerate(v,start=1):\n   if value in v[i:]:\n    raise errors.ListUniqueItemsError()\n    \n  return v\n  \n  \ndef conlist(\nitem_type:Type[T],*,min_items:Optional[int]=None,max_items:Optional[int]=None,unique_items:bool=None\n)->Type[List[T]]:\n\n namespace=dict(\n min_items=min_items,max_items=max_items,unique_items=unique_items,item_type=item_type,__args__=(item_type,)\n )\n \n return new_class('ConstrainedListValue',(ConstrainedList,),{},lambda ns:ns.update(namespace))\n \n \n \n \n \nif TYPE_CHECKING:\n PyObject=Callable[...,Any]\nelse:\n\n class PyObject:\n  validate_always=True\n  \n  @classmethod\n  def __get_validators__(cls)->'CallableGenerator':\n   yield cls.validate\n   \n  @classmethod\n  def validate(cls,value:Any)->Any:\n   if isinstance(value,Callable):\n    return value\n    \n   try:\n    value=str_validator(value)\n   except errors.StrError:\n    raise errors.PyObjectError(error_message='value is neither a valid import path not a valid callable')\n    \n   try:\n    return import_string(value)\n   except ImportError as e:\n    raise errors.PyObjectError(error_message=str(e))\n    \n    \n    \n    \n    \nclass ConstrainedDecimal(Decimal,metaclass=ConstrainedNumberMeta):\n gt:OptionalIntFloatDecimal=None\n ge:OptionalIntFloatDecimal=None\n lt:OptionalIntFloatDecimal=None\n le:OptionalIntFloatDecimal=None\n max_digits:OptionalInt=None\n decimal_places:OptionalInt=None\n multiple_of:OptionalIntFloatDecimal=None\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(\n  field_schema,\n  exclusiveMinimum=cls.gt,\n  exclusiveMaximum=cls.lt,\n  minimum=cls.ge,\n  maximum=cls.le,\n  multipleOf=cls.multiple_of,\n  )\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield decimal_validator\n  yield number_size_validator\n  yield number_multiple_validator\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:Decimal)->Decimal:\n  try:\n   normalized_value=value.normalize()\n  except InvalidOperation:\n   normalized_value=value\n  digit_tuple,exponent=normalized_value.as_tuple()[1:]\n  if exponent in{'F','n','N'}:\n   raise errors.DecimalIsNotFiniteError()\n   \n  if exponent >=0:\n  \n   digits=len(digit_tuple)+exponent\n   decimals=0\n  else:\n  \n  \n  \n  \n  \n   if abs(exponent)>len(digit_tuple):\n    digits=decimals=abs(exponent)\n   else:\n    digits=len(digit_tuple)\n    decimals=abs(exponent)\n  whole_digits=digits -decimals\n  \n  if cls.max_digits is not None and digits >cls.max_digits:\n   raise errors.DecimalMaxDigitsError(max_digits=cls.max_digits)\n   \n  if cls.decimal_places is not None and decimals >cls.decimal_places:\n   raise errors.DecimalMaxPlacesError(decimal_places=cls.decimal_places)\n   \n  if cls.max_digits is not None and cls.decimal_places is not None:\n   expected=cls.max_digits -cls.decimal_places\n   if whole_digits >expected:\n    raise errors.DecimalWholeDigitsError(whole_digits=expected)\n    \n  return value\n  \n  \ndef condecimal(\n*,\ngt:Decimal=None,\nge:Decimal=None,\nlt:Decimal=None,\nle:Decimal=None,\nmax_digits:Optional[int]=None,\ndecimal_places:Optional[int]=None,\nmultiple_of:Decimal=None,\n)->Type[Decimal]:\n\n namespace=dict(\n gt=gt,ge=ge,lt=lt,le=le,max_digits=max_digits,decimal_places=decimal_places,multiple_of=multiple_of\n )\n return type('ConstrainedDecimalValue',(ConstrainedDecimal,),namespace)\n \n \n \n \nif TYPE_CHECKING:\n UUID1=UUID\n UUID3=UUID\n UUID4=UUID\n UUID5=UUID\nelse:\n\n class UUID1(UUID):\n  _required_version=1\n  \n  @classmethod\n  def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n   field_schema.update(type='string',format=f'uuid{cls._required_version}')\n   \n class UUID3(UUID1):\n  _required_version=3\n  \n class UUID4(UUID1):\n  _required_version=4\n  \n class UUID5(UUID1):\n  _required_version=5\n  \n  \n  \n  \nif TYPE_CHECKING:\n FilePath=Path\n DirectoryPath=Path\nelse:\n\n class FilePath(Path):\n  @classmethod\n  def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n   field_schema.update(format='file-path')\n   \n  @classmethod\n  def __get_validators__(cls)->'CallableGenerator':\n   yield path_validator\n   yield path_exists_validator\n   yield cls.validate\n   \n  @classmethod\n  def validate(cls,value:Path)->Path:\n   if not value.is_file():\n    raise errors.PathNotAFileError(path=value)\n    \n   return value\n   \n class DirectoryPath(Path):\n  @classmethod\n  def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n   field_schema.update(format='directory-path')\n   \n  @classmethod\n  def __get_validators__(cls)->'CallableGenerator':\n   yield path_validator\n   yield path_exists_validator\n   yield cls.validate\n   \n  @classmethod\n  def validate(cls,value:Path)->Path:\n   if not value.is_dir():\n    raise errors.PathNotADirectoryError(path=value)\n    \n   return value\n   \n   \n   \n   \n   \nclass JsonWrapper:\n pass\n \n \nclass JsonMeta(type):\n def __getitem__(self,t:Type[Any])->Type[JsonWrapper]:\n  if t is Any:\n   return Json\n  return _registered(type('JsonWrapperValue',(JsonWrapper,),{'inner_type':t}))\n  \n  \nif TYPE_CHECKING:\n Json=Annotated[T,...]\n \nelse:\n\n class Json(metaclass=JsonMeta):\n  @classmethod\n  def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n   field_schema.update(type='string',format='json-string')\n   \n   \n   \n   \n   \nclass SecretField(abc.ABC):\n ''\n\n\n\n\n\n\n\n \n \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,self.__class__)and self.get_secret_value()==other.get_secret_value()\n  \n def __str__(self)->str:\n  return '**********'if self.get_secret_value()else ''\n  \n def __hash__(self)->int:\n  return hash(self.get_secret_value())\n  \n @abc.abstractmethod\n def get_secret_value(self)->Any:\n  ...\n  \n  \nclass SecretStr(SecretField):\n min_length:OptionalInt=None\n max_length:OptionalInt=None\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(\n  field_schema,\n  type='string',\n  writeOnly=True,\n  format='password',\n  minLength=cls.min_length,\n  maxLength=cls.max_length,\n  )\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  yield constr_length_validator\n  \n @classmethod\n def validate(cls,value:Any)->'SecretStr':\n  if isinstance(value,cls):\n   return value\n  value=str_validator(value)\n  return cls(value)\n  \n def __init__(self,value:str):\n  self._secret_value=value\n  \n def __repr__(self)->str:\n  return f\"SecretStr('{self}')\"\n  \n def __len__(self)->int:\n  return len(self._secret_value)\n  \n def display(self)->str:\n  warnings.warn('`secret_str.display()` is deprecated, use `str(secret_str)` instead',DeprecationWarning)\n  return str(self)\n  \n def get_secret_value(self)->str:\n  return self._secret_value\n  \n  \nclass SecretBytes(SecretField):\n min_length:OptionalInt=None\n max_length:OptionalInt=None\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(\n  field_schema,\n  type='string',\n  writeOnly=True,\n  format='password',\n  minLength=cls.min_length,\n  maxLength=cls.max_length,\n  )\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  yield constr_length_validator\n  \n @classmethod\n def validate(cls,value:Any)->'SecretBytes':\n  if isinstance(value,cls):\n   return value\n  value=bytes_validator(value)\n  return cls(value)\n  \n def __init__(self,value:bytes):\n  self._secret_value=value\n  \n def __repr__(self)->str:\n  return f\"SecretBytes(b'{self}')\"\n  \n def __len__(self)->int:\n  return len(self._secret_value)\n  \n def display(self)->str:\n  warnings.warn('`secret_bytes.display()` is deprecated, use `str(secret_bytes)` instead',DeprecationWarning)\n  return str(self)\n  \n def get_secret_value(self)->bytes:\n  return self._secret_value\n  \n  \n  \n  \n  \nclass PaymentCardBrand(str,Enum):\n\n\n amex='American Express'\n mastercard='Mastercard'\n visa='Visa'\n other='other'\n \n def __str__(self)->str:\n  return self.value\n  \n  \nclass PaymentCardNumber(str):\n ''\n\n \n \n strip_whitespace:ClassVar[bool]=True\n min_length:ClassVar[int]=12\n max_length:ClassVar[int]=19\n bin:str\n last4:str\n brand:PaymentCardBrand\n \n def __init__(self,card_number:str):\n  self.bin=card_number[:6]\n  self.last4=card_number[-4:]\n  self.brand=self._get_brand(card_number)\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield str_validator\n  yield constr_strip_whitespace\n  yield constr_length_validator\n  yield cls.validate_digits\n  yield cls.validate_luhn_check_digit\n  yield cls\n  yield cls.validate_length_for_brand\n  \n @property\n def masked(self)->str:\n  num_masked=len(self)-10\n  return f'{self.bin}{\"*\"*num_masked}{self.last4}'\n  \n @classmethod\n def validate_digits(cls,card_number:str)->str:\n  if not card_number.isdigit():\n   raise errors.NotDigitError\n  return card_number\n  \n @classmethod\n def validate_luhn_check_digit(cls,card_number:str)->str:\n  ''\n\n  \n  sum_=int(card_number[-1])\n  length=len(card_number)\n  parity=length %2\n  for i in range(length -1):\n   digit=int(card_number[i])\n   if i %2 ==parity:\n    digit *=2\n   if digit >9:\n    digit -=9\n   sum_ +=digit\n  valid=sum_ %10 ==0\n  if not valid:\n   raise errors.LuhnValidationError\n  return card_number\n  \n @classmethod\n def validate_length_for_brand(cls,card_number:'PaymentCardNumber')->'PaymentCardNumber':\n  ''\n\n\n  \n  required_length:Union[None,int,str]=None\n  if card_number.brand in PaymentCardBrand.mastercard:\n   required_length=16\n   valid=len(card_number)==required_length\n  elif card_number.brand ==PaymentCardBrand.visa:\n   required_length='13, 16 or 19'\n   valid=len(card_number)in{13,16,19}\n  elif card_number.brand ==PaymentCardBrand.amex:\n   required_length=15\n   valid=len(card_number)==required_length\n  else:\n   valid=True\n  if not valid:\n   raise errors.InvalidLengthForBrand(brand=card_number.brand,required_length=required_length)\n  return card_number\n  \n @staticmethod\n def _get_brand(card_number:str)->PaymentCardBrand:\n  if card_number[0]=='4':\n   brand=PaymentCardBrand.visa\n  elif 51 <=int(card_number[:2])<=55:\n   brand=PaymentCardBrand.mastercard\n  elif card_number[:2]in{'34','37'}:\n   brand=PaymentCardBrand.amex\n  else:\n   brand=PaymentCardBrand.other\n  return brand\n  \n  \n  \n  \nBYTE_SIZES={\n'b':1,\n'kb':10 **3,\n'mb':10 **6,\n'gb':10 **9,\n'tb':10 **12,\n'pb':10 **15,\n'eb':10 **18,\n'kib':2 **10,\n'mib':2 **20,\n'gib':2 **30,\n'tib':2 **40,\n'pib':2 **50,\n'eib':2 **60,\n}\nBYTE_SIZES.update({k.lower()[0]:v for k,v in BYTE_SIZES.items()if 'i'not in k})\nbyte_string_re=re.compile(r'^\\s*(\\d*\\.?\\d+)\\s*(\\w+)?',re.IGNORECASE)\n\n\nclass ByteSize(int):\n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  \n @classmethod\n def validate(cls,v:StrIntFloat)->'ByteSize':\n  try:\n   return cls(int(v))\n  except ValueError:\n   pass\n   \n  str_match=byte_string_re.match(str(v))\n  if str_match is None:\n   raise errors.InvalidByteSize()\n   \n  scalar,unit=str_match.groups()\n  if unit is None:\n   unit='b'\n   \n  try:\n   unit_mult=BYTE_SIZES[unit.lower()]\n  except KeyError:\n   raise errors.InvalidByteSizeUnit(unit=unit)\n   \n  return cls(int(float(scalar)*unit_mult))\n  \n def human_readable(self,decimal:bool=False)->str:\n  if decimal:\n   divisor=1000\n   units=['B','KB','MB','GB','TB','PB']\n   final_unit='EB'\n  else:\n   divisor=1024\n   units=['B','KiB','MiB','GiB','TiB','PiB']\n   final_unit='EiB'\n   \n  num=float(self)\n  for unit in units:\n   if abs(num)<divisor:\n    return f'{num:0.1f}{unit}'\n   num /=divisor\n   \n  return f'{num:0.1f}{final_unit}'\n  \n def to(self,unit:str)->float:\n  try:\n   unit_div=BYTE_SIZES[unit.lower()]\n  except KeyError:\n   raise errors.InvalidByteSizeUnit(unit=unit)\n   \n  return self /unit_div\n  \n  \n  \n  \nif TYPE_CHECKING:\n PastDate=date\n FutureDate=date\nelse:\n\n class PastDate(date):\n  @classmethod\n  def __get_validators__(cls)->'CallableGenerator':\n   yield parse_date\n   yield cls.validate\n   \n  @classmethod\n  def validate(cls,value:date)->date:\n   if value >=date.today():\n    raise errors.DateNotInThePastError()\n    \n   return value\n   \n class FutureDate(date):\n  @classmethod\n  def __get_validators__(cls)->'CallableGenerator':\n   yield parse_date\n   yield cls.validate\n   \n  @classmethod\n  def validate(cls,value:date)->date:\n   if value <=date.today():\n    raise errors.DateNotInTheFutureError()\n    \n   return value\n   \n   \nclass ConstrainedDate(date,metaclass=ConstrainedNumberMeta):\n gt:OptionalDate=None\n ge:OptionalDate=None\n lt:OptionalDate=None\n le:OptionalDate=None\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(field_schema,exclusiveMinimum=cls.gt,exclusiveMaximum=cls.lt,minimum=cls.ge,maximum=cls.le)\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield parse_date\n  yield number_size_validator\n  \n  \ndef condate(\n*,\ngt:date=None,\nge:date=None,\nlt:date=None,\nle:date=None,\n)->Type[date]:\n\n namespace=dict(gt=gt,ge=ge,lt=lt,le=le)\n return type('ConstrainedDateValue',(ConstrainedDate,),namespace)\n", ["abc", "datetime", "decimal", "enum", "math", "pathlib", "pydantic.v1", "pydantic.v1.dataclasses", "pydantic.v1.datetime_parse", "pydantic.v1.errors", "pydantic.v1.main", "pydantic.v1.typing", "pydantic.v1.utils", "pydantic.v1.validators", "re", "types", "typing", "typing_extensions", "uuid", "warnings", "weakref"]], "pydantic.v1.schema": [".py", "import re\nimport warnings\nfrom collections import defaultdict\nfrom dataclasses import is_dataclass\nfrom datetime import date,datetime,time,timedelta\nfrom decimal import Decimal\nfrom enum import Enum\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom pathlib import Path\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nDict,\nForwardRef,\nFrozenSet,\nGeneric,\nIterable,\nList,\nOptional,\nPattern,\nSequence,\nSet,\nTuple,\nType,\nTypeVar,\nUnion,\ncast,\n)\nfrom uuid import UUID\n\nfrom typing_extensions import Annotated,Literal\n\nfrom pydantic.v1.fields import(\nMAPPING_LIKE_SHAPES,\nSHAPE_DEQUE,\nSHAPE_FROZENSET,\nSHAPE_GENERIC,\nSHAPE_ITERABLE,\nSHAPE_LIST,\nSHAPE_SEQUENCE,\nSHAPE_SET,\nSHAPE_SINGLETON,\nSHAPE_TUPLE,\nSHAPE_TUPLE_ELLIPSIS,\nFieldInfo,\nModelField,\n)\nfrom pydantic.v1.json import pydantic_encoder\nfrom pydantic.v1.networks import AnyUrl,EmailStr\nfrom pydantic.v1.types import(\nConstrainedDecimal,\nConstrainedFloat,\nConstrainedFrozenSet,\nConstrainedInt,\nConstrainedList,\nConstrainedSet,\nConstrainedStr,\nSecretBytes,\nSecretStr,\nStrictBytes,\nStrictStr,\nconbytes,\ncondecimal,\nconfloat,\nconfrozenset,\nconint,\nconlist,\nconset,\nconstr,\n)\nfrom pydantic.v1.typing import(\nall_literal_values,\nget_args,\nget_origin,\nget_sub_types,\nis_callable_type,\nis_literal_type,\nis_namedtuple,\nis_none_type,\nis_union,\n)\nfrom pydantic.v1.utils import ROOT_KEY,get_model,lenient_issubclass\n\nif TYPE_CHECKING:\n from pydantic.v1.dataclasses import Dataclass\n from pydantic.v1.main import BaseModel\n \ndefault_prefix='#/definitions/'\ndefault_ref_template='#/definitions/{model}'\n\nTypeModelOrEnum=Union[Type['BaseModel'],Type[Enum]]\nTypeModelSet=Set[TypeModelOrEnum]\n\n\ndef _apply_modify_schema(\nmodify_schema:Callable[...,None],field:Optional[ModelField],field_schema:Dict[str,Any]\n)->None:\n from inspect import signature\n \n sig=signature(modify_schema)\n args=set(sig.parameters.keys())\n if 'field'in args or 'kwargs'in args:\n  modify_schema(field_schema,field=field)\n else:\n  modify_schema(field_schema)\n  \n  \ndef schema(\nmodels:Sequence[Union[Type['BaseModel'],Type['Dataclass']]],\n*,\nby_alias:bool=True,\ntitle:Optional[str]=None,\ndescription:Optional[str]=None,\nref_prefix:Optional[str]=None,\nref_template:str=default_ref_template,\n)->Dict[str,Any]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n clean_models=[get_model(model)for model in models]\n flat_models=get_flat_models_from_models(clean_models)\n model_name_map=get_model_name_map(flat_models)\n definitions={}\n output_schema:Dict[str,Any]={}\n if title:\n  output_schema['title']=title\n if description:\n  output_schema['description']=description\n for model in clean_models:\n  m_schema,m_definitions,m_nested_models=model_process_schema(\n  model,\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  )\n  definitions.update(m_definitions)\n  model_name=model_name_map[model]\n  definitions[model_name]=m_schema\n if definitions:\n  output_schema['definitions']=definitions\n return output_schema\n \n \ndef model_schema(\nmodel:Union[Type['BaseModel'],Type['Dataclass']],\nby_alias:bool=True,\nref_prefix:Optional[str]=None,\nref_template:str=default_ref_template,\n)->Dict[str,Any]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n model=get_model(model)\n flat_models=get_flat_models_from_model(model)\n model_name_map=get_model_name_map(flat_models)\n model_name=model_name_map[model]\n m_schema,m_definitions,nested_models=model_process_schema(\n model,by_alias=by_alias,model_name_map=model_name_map,ref_prefix=ref_prefix,ref_template=ref_template\n )\n if model_name in nested_models:\n \n  m_definitions[model_name]=m_schema\n  m_schema=get_schema_ref(model_name,ref_prefix,ref_template,False)\n if m_definitions:\n  m_schema.update({'definitions':m_definitions})\n return m_schema\n \n \ndef get_field_info_schema(field:ModelField,schema_overrides:bool=False)->Tuple[Dict[str,Any],bool]:\n\n\n\n schema_:Dict[str,Any]={}\n if field.field_info.title or not lenient_issubclass(field.type_,Enum):\n  schema_['title']=field.field_info.title or field.alias.title().replace('_',' ')\n  \n if field.field_info.title:\n  schema_overrides=True\n  \n if field.field_info.description:\n  schema_['description']=field.field_info.description\n  schema_overrides=True\n  \n if not field.required and field.default is not None and not is_callable_type(field.outer_type_):\n  schema_['default']=encode_default(field.default)\n  schema_overrides=True\n  \n return schema_,schema_overrides\n \n \ndef field_schema(\nfield:ModelField,\n*,\nby_alias:bool=True,\nmodel_name_map:Dict[TypeModelOrEnum,str],\nref_prefix:Optional[str]=None,\nref_template:str=default_ref_template,\nknown_models:Optional[TypeModelSet]=None,\n)->Tuple[Dict[str,Any],Dict[str,Any],Set[str]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n s,schema_overrides=get_field_info_schema(field)\n \n validation_schema=get_field_schema_validations(field)\n if validation_schema:\n  s.update(validation_schema)\n  schema_overrides=True\n  \n f_schema,f_definitions,f_nested_models=field_type_schema(\n field,\n by_alias=by_alias,\n model_name_map=model_name_map,\n schema_overrides=schema_overrides,\n ref_prefix=ref_prefix,\n ref_template=ref_template,\n known_models=known_models or set(),\n )\n \n \n if '$ref'in f_schema:\n  return f_schema,f_definitions,f_nested_models\n else:\n  s.update(f_schema)\n  return s,f_definitions,f_nested_models\n  \n  \nnumeric_types=(int,float,Decimal)\n_str_types_attrs:Tuple[Tuple[str,Union[type,Tuple[type,...]],str],...]=(\n('max_length',numeric_types,'maxLength'),\n('min_length',numeric_types,'minLength'),\n('regex',str,'pattern'),\n)\n\n_numeric_types_attrs:Tuple[Tuple[str,Union[type,Tuple[type,...]],str],...]=(\n('gt',numeric_types,'exclusiveMinimum'),\n('lt',numeric_types,'exclusiveMaximum'),\n('ge',numeric_types,'minimum'),\n('le',numeric_types,'maximum'),\n('multiple_of',numeric_types,'multipleOf'),\n)\n\n\ndef get_field_schema_validations(field:ModelField)->Dict[str,Any]:\n ''\n\n\n \n f_schema:Dict[str,Any]={}\n \n if lenient_issubclass(field.type_,Enum):\n \n  if field.field_info.extra:\n   f_schema.update(field.field_info.extra)\n  return f_schema\n  \n if lenient_issubclass(field.type_,(str,bytes)):\n  for attr_name,t,keyword in _str_types_attrs:\n   attr=getattr(field.field_info,attr_name,None)\n   if isinstance(attr,t):\n    f_schema[keyword]=attr\n if lenient_issubclass(field.type_,numeric_types)and not issubclass(field.type_,bool):\n  for attr_name,t,keyword in _numeric_types_attrs:\n   attr=getattr(field.field_info,attr_name,None)\n   if isinstance(attr,t):\n    f_schema[keyword]=attr\n if field.field_info is not None and field.field_info.const:\n  f_schema['const']=field.default\n if field.field_info.extra:\n  f_schema.update(field.field_info.extra)\n modify_schema=getattr(field.outer_type_,'__modify_schema__',None)\n if modify_schema:\n  _apply_modify_schema(modify_schema,field,f_schema)\n return f_schema\n \n \ndef get_model_name_map(unique_models:TypeModelSet)->Dict[TypeModelOrEnum,str]:\n ''\n\n\n\n\n\n\n\n \n name_model_map={}\n conflicting_names:Set[str]=set()\n for model in unique_models:\n  model_name=normalize_name(model.__name__)\n  if model_name in conflicting_names:\n   model_name=get_long_model_name(model)\n   name_model_map[model_name]=model\n  elif model_name in name_model_map:\n   conflicting_names.add(model_name)\n   conflicting_model=name_model_map.pop(model_name)\n   name_model_map[get_long_model_name(conflicting_model)]=conflicting_model\n   name_model_map[get_long_model_name(model)]=model\n  else:\n   name_model_map[model_name]=model\n return{v:k for k,v in name_model_map.items()}\n \n \ndef get_flat_models_from_model(model:Type['BaseModel'],known_models:Optional[TypeModelSet]=None)->TypeModelSet:\n ''\n\n\n\n\n\n\n\n\n \n known_models=known_models or set()\n flat_models:TypeModelSet=set()\n flat_models.add(model)\n known_models |=flat_models\n fields=cast(Sequence[ModelField],model.__fields__.values())\n flat_models |=get_flat_models_from_fields(fields,known_models=known_models)\n return flat_models\n \n \ndef get_flat_models_from_field(field:ModelField,known_models:TypeModelSet)->TypeModelSet:\n ''\n\n\n\n\n\n\n\n\n\n \n from pydantic.v1.main import BaseModel\n \n flat_models:TypeModelSet=set()\n \n field_type=field.type_\n if lenient_issubclass(getattr(field_type,'__pydantic_model__',None),BaseModel):\n  field_type=field_type.__pydantic_model__\n  \n if field.sub_fields and not lenient_issubclass(field_type,BaseModel):\n  flat_models |=get_flat_models_from_fields(field.sub_fields,known_models=known_models)\n elif lenient_issubclass(field_type,BaseModel)and field_type not in known_models:\n  flat_models |=get_flat_models_from_model(field_type,known_models=known_models)\n elif lenient_issubclass(field_type,Enum):\n  flat_models.add(field_type)\n return flat_models\n \n \ndef get_flat_models_from_fields(fields:Sequence[ModelField],known_models:TypeModelSet)->TypeModelSet:\n ''\n\n\n\n\n\n\n\n\n\n \n flat_models:TypeModelSet=set()\n for field in fields:\n  flat_models |=get_flat_models_from_field(field,known_models=known_models)\n return flat_models\n \n \ndef get_flat_models_from_models(models:Sequence[Type['BaseModel']])->TypeModelSet:\n ''\n\n\n\n \n flat_models:TypeModelSet=set()\n for model in models:\n  flat_models |=get_flat_models_from_model(model)\n return flat_models\n \n \ndef get_long_model_name(model:TypeModelOrEnum)->str:\n return f'{model.__module__}__{model.__qualname__}'.replace('.','__')\n \n \ndef field_type_schema(\nfield:ModelField,\n*,\nby_alias:bool,\nmodel_name_map:Dict[TypeModelOrEnum,str],\nref_template:str,\nschema_overrides:bool=False,\nref_prefix:Optional[str]=None,\nknown_models:TypeModelSet,\n)->Tuple[Dict[str,Any],Dict[str,Any],Set[str]]:\n ''\n\n\n\n\n \n from pydantic.v1.main import BaseModel\n \n definitions={}\n nested_models:Set[str]=set()\n f_schema:Dict[str,Any]\n if field.shape in{\n SHAPE_LIST,\n SHAPE_TUPLE_ELLIPSIS,\n SHAPE_SEQUENCE,\n SHAPE_SET,\n SHAPE_FROZENSET,\n SHAPE_ITERABLE,\n SHAPE_DEQUE,\n }:\n  items_schema,f_definitions,f_nested_models=field_singleton_schema(\n  field,\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  known_models=known_models,\n  )\n  definitions.update(f_definitions)\n  nested_models.update(f_nested_models)\n  f_schema={'type':'array','items':items_schema}\n  if field.shape in{SHAPE_SET,SHAPE_FROZENSET}:\n   f_schema['uniqueItems']=True\n   \n elif field.shape in MAPPING_LIKE_SHAPES:\n  f_schema={'type':'object'}\n  key_field=cast(ModelField,field.key_field)\n  regex=getattr(key_field.type_,'regex',None)\n  items_schema,f_definitions,f_nested_models=field_singleton_schema(\n  field,\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  known_models=known_models,\n  )\n  definitions.update(f_definitions)\n  nested_models.update(f_nested_models)\n  if regex:\n  \n  \n   f_schema['patternProperties']={ConstrainedStr._get_pattern(regex):items_schema}\n  if items_schema:\n  \n   f_schema['additionalProperties']=items_schema\n elif field.shape ==SHAPE_TUPLE or(field.shape ==SHAPE_GENERIC and not issubclass(field.type_,BaseModel)):\n  sub_schema=[]\n  sub_fields=cast(List[ModelField],field.sub_fields)\n  for sf in sub_fields:\n   sf_schema,sf_definitions,sf_nested_models=field_type_schema(\n   sf,\n   by_alias=by_alias,\n   model_name_map=model_name_map,\n   ref_prefix=ref_prefix,\n   ref_template=ref_template,\n   known_models=known_models,\n   )\n   definitions.update(sf_definitions)\n   nested_models.update(sf_nested_models)\n   sub_schema.append(sf_schema)\n   \n  sub_fields_len=len(sub_fields)\n  if field.shape ==SHAPE_GENERIC:\n   all_of_schemas=sub_schema[0]if sub_fields_len ==1 else{'type':'array','items':sub_schema}\n   f_schema={'allOf':[all_of_schemas]}\n  else:\n   f_schema={\n   'type':'array',\n   'minItems':sub_fields_len,\n   'maxItems':sub_fields_len,\n   }\n   if sub_fields_len >=1:\n    f_schema['items']=sub_schema\n else:\n  assert field.shape in{SHAPE_SINGLETON,SHAPE_GENERIC},field.shape\n  f_schema,f_definitions,f_nested_models=field_singleton_schema(\n  field,\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  schema_overrides=schema_overrides,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  known_models=known_models,\n  )\n  definitions.update(f_definitions)\n  nested_models.update(f_nested_models)\n  \n  \n if field.type_ !=field.outer_type_:\n  if field.shape ==SHAPE_GENERIC:\n   field_type=field.type_\n  else:\n   field_type=field.outer_type_\n  modify_schema=getattr(field_type,'__modify_schema__',None)\n  if modify_schema:\n   _apply_modify_schema(modify_schema,field,f_schema)\n return f_schema,definitions,nested_models\n \n \ndef model_process_schema(\nmodel:TypeModelOrEnum,\n*,\nby_alias:bool=True,\nmodel_name_map:Dict[TypeModelOrEnum,str],\nref_prefix:Optional[str]=None,\nref_template:str=default_ref_template,\nknown_models:Optional[TypeModelSet]=None,\nfield:Optional[ModelField]=None,\n)->Tuple[Dict[str,Any],Dict[str,Any],Set[str]]:\n ''\n\n\n\n\n\n \n from inspect import getdoc,signature\n \n known_models=known_models or set()\n if lenient_issubclass(model,Enum):\n  model=cast(Type[Enum],model)\n  s=enum_process_schema(model,field=field)\n  return s,{},set()\n model=cast(Type['BaseModel'],model)\n s={'title':model.__config__.title or model.__name__}\n doc=getdoc(model)\n if doc:\n  s['description']=doc\n known_models.add(model)\n m_schema,m_definitions,nested_models=model_type_schema(\n model,\n by_alias=by_alias,\n model_name_map=model_name_map,\n ref_prefix=ref_prefix,\n ref_template=ref_template,\n known_models=known_models,\n )\n s.update(m_schema)\n schema_extra=model.__config__.schema_extra\n if callable(schema_extra):\n  if len(signature(schema_extra).parameters)==1:\n   schema_extra(s)\n  else:\n   schema_extra(s,model)\n else:\n  s.update(schema_extra)\n return s,m_definitions,nested_models\n \n \ndef model_type_schema(\nmodel:Type['BaseModel'],\n*,\nby_alias:bool,\nmodel_name_map:Dict[TypeModelOrEnum,str],\nref_template:str,\nref_prefix:Optional[str]=None,\nknown_models:TypeModelSet,\n)->Tuple[Dict[str,Any],Dict[str,Any],Set[str]]:\n ''\n\n\n\n\n \n properties={}\n required=[]\n definitions:Dict[str,Any]={}\n nested_models:Set[str]=set()\n for k,f in model.__fields__.items():\n  try:\n   f_schema,f_definitions,f_nested_models=field_schema(\n   f,\n   by_alias=by_alias,\n   model_name_map=model_name_map,\n   ref_prefix=ref_prefix,\n   ref_template=ref_template,\n   known_models=known_models,\n   )\n  except SkipField as skip:\n   warnings.warn(skip.message,UserWarning)\n   continue\n  definitions.update(f_definitions)\n  nested_models.update(f_nested_models)\n  if by_alias:\n   properties[f.alias]=f_schema\n   if f.required:\n    required.append(f.alias)\n  else:\n   properties[k]=f_schema\n   if f.required:\n    required.append(k)\n if ROOT_KEY in properties:\n  out_schema=properties[ROOT_KEY]\n  out_schema['title']=model.__config__.title or model.__name__\n else:\n  out_schema={'type':'object','properties':properties}\n  if required:\n   out_schema['required']=required\n if model.__config__.extra =='forbid':\n  out_schema['additionalProperties']=False\n return out_schema,definitions,nested_models\n \n \ndef enum_process_schema(enum:Type[Enum],*,field:Optional[ModelField]=None)->Dict[str,Any]:\n ''\n\n\n\n \n import inspect\n \n schema_:Dict[str,Any]={\n 'title':enum.__name__,\n \n \n 'description':inspect.cleandoc(enum.__doc__ or 'An enumeration.'),\n \n 'enum':[item.value for item in cast(Iterable[Enum],enum)],\n }\n \n add_field_type_to_schema(enum,schema_)\n \n modify_schema=getattr(enum,'__modify_schema__',None)\n if modify_schema:\n  _apply_modify_schema(modify_schema,field,schema_)\n  \n return schema_\n \n \ndef field_singleton_sub_fields_schema(\nfield:ModelField,\n*,\nby_alias:bool,\nmodel_name_map:Dict[TypeModelOrEnum,str],\nref_template:str,\nschema_overrides:bool=False,\nref_prefix:Optional[str]=None,\nknown_models:TypeModelSet,\n)->Tuple[Dict[str,Any],Dict[str,Any],Set[str]]:\n ''\n\n\n\n\n \n sub_fields=cast(List[ModelField],field.sub_fields)\n definitions={}\n nested_models:Set[str]=set()\n if len(sub_fields)==1:\n  return field_type_schema(\n  sub_fields[0],\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  schema_overrides=schema_overrides,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  known_models=known_models,\n  )\n else:\n  s:Dict[str,Any]={}\n  \n  field_has_discriminator:bool=field.discriminator_key is not None\n  if field_has_discriminator:\n   assert field.sub_fields_mapping is not None\n   \n   discriminator_models_refs:Dict[str,Union[str,Dict[str,Any]]]={}\n   \n   for discriminator_value,sub_field in field.sub_fields_mapping.items():\n    if isinstance(discriminator_value,Enum):\n     discriminator_value=str(discriminator_value.value)\n     \n    if is_union(get_origin(sub_field.type_)):\n     sub_models=get_sub_types(sub_field.type_)\n     discriminator_models_refs[discriminator_value]={\n     model_name_map[sub_model]:get_schema_ref(\n     model_name_map[sub_model],ref_prefix,ref_template,False\n     )\n     for sub_model in sub_models\n     }\n    else:\n     sub_field_type=sub_field.type_\n     if hasattr(sub_field_type,'__pydantic_model__'):\n      sub_field_type=sub_field_type.__pydantic_model__\n      \n     discriminator_model_name=model_name_map[sub_field_type]\n     discriminator_model_ref=get_schema_ref(discriminator_model_name,ref_prefix,ref_template,False)\n     discriminator_models_refs[discriminator_value]=discriminator_model_ref['$ref']\n     \n   s['discriminator']={\n   'propertyName':field.discriminator_alias if by_alias else field.discriminator_key,\n   'mapping':discriminator_models_refs,\n   }\n   \n  sub_field_schemas=[]\n  for sf in sub_fields:\n   sub_schema,sub_definitions,sub_nested_models=field_type_schema(\n   sf,\n   by_alias=by_alias,\n   model_name_map=model_name_map,\n   schema_overrides=schema_overrides,\n   ref_prefix=ref_prefix,\n   ref_template=ref_template,\n   known_models=known_models,\n   )\n   definitions.update(sub_definitions)\n   if schema_overrides and 'allOf'in sub_schema:\n   \n   \n   \n    sub_schema=sub_schema['allOf'][0]\n    \n   if sub_schema.keys()=={'discriminator','oneOf'}:\n   \n    sub_schema.pop('discriminator')\n   sub_field_schemas.append(sub_schema)\n   nested_models.update(sub_nested_models)\n  s['oneOf'if field_has_discriminator else 'anyOf']=sub_field_schemas\n  return s,definitions,nested_models\n  \n  \n  \n  \nfield_class_to_schema:Tuple[Tuple[Any,Dict[str,Any]],...]=(\n(Path,{'type':'string','format':'path'}),\n(datetime,{'type':'string','format':'date-time'}),\n(date,{'type':'string','format':'date'}),\n(time,{'type':'string','format':'time'}),\n(timedelta,{'type':'number','format':'time-delta'}),\n(IPv4Network,{'type':'string','format':'ipv4network'}),\n(IPv6Network,{'type':'string','format':'ipv6network'}),\n(IPv4Interface,{'type':'string','format':'ipv4interface'}),\n(IPv6Interface,{'type':'string','format':'ipv6interface'}),\n(IPv4Address,{'type':'string','format':'ipv4'}),\n(IPv6Address,{'type':'string','format':'ipv6'}),\n(Pattern,{'type':'string','format':'regex'}),\n(str,{'type':'string'}),\n(bytes,{'type':'string','format':'binary'}),\n(bool,{'type':'boolean'}),\n(int,{'type':'integer'}),\n(float,{'type':'number'}),\n(Decimal,{'type':'number'}),\n(UUID,{'type':'string','format':'uuid'}),\n(dict,{'type':'object'}),\n(list,{'type':'array','items':{}}),\n(tuple,{'type':'array','items':{}}),\n(set,{'type':'array','items':{},'uniqueItems':True}),\n(frozenset,{'type':'array','items':{},'uniqueItems':True}),\n)\n\njson_scheme={'type':'string','format':'json-string'}\n\n\ndef add_field_type_to_schema(field_type:Any,schema_:Dict[str,Any])->None:\n ''\n\n\n\n\n \n for type_,t_schema in field_class_to_schema:\n \n  if lenient_issubclass(field_type,type_)or field_type is type_ is Pattern:\n   schema_.update(t_schema)\n   break\n   \n   \ndef get_schema_ref(name:str,ref_prefix:Optional[str],ref_template:str,schema_overrides:bool)->Dict[str,Any]:\n if ref_prefix:\n  schema_ref={'$ref':ref_prefix+name}\n else:\n  schema_ref={'$ref':ref_template.format(model=name)}\n return{'allOf':[schema_ref]}if schema_overrides else schema_ref\n \n \ndef field_singleton_schema(\nfield:ModelField,\n*,\nby_alias:bool,\nmodel_name_map:Dict[TypeModelOrEnum,str],\nref_template:str,\nschema_overrides:bool=False,\nref_prefix:Optional[str]=None,\nknown_models:TypeModelSet,\n)->Tuple[Dict[str,Any],Dict[str,Any],Set[str]]:\n ''\n\n\n\n \n from pydantic.v1.main import BaseModel\n \n definitions:Dict[str,Any]={}\n nested_models:Set[str]=set()\n field_type=field.type_\n \n \n \n if field.sub_fields and(\n (field.field_info and field.field_info.const)or not lenient_issubclass(field_type,BaseModel)\n ):\n  return field_singleton_sub_fields_schema(\n  field,\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  schema_overrides=schema_overrides,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  known_models=known_models,\n  )\n if field_type is Any or field_type is object or field_type.__class__ ==TypeVar or get_origin(field_type)is type:\n  return{},definitions,nested_models\n if is_none_type(field_type):\n  return{'type':'null'},definitions,nested_models\n if is_callable_type(field_type):\n  raise SkipField(f'Callable {field.name} was excluded from schema since JSON schema has no equivalent type.')\n f_schema:Dict[str,Any]={}\n if field.field_info is not None and field.field_info.const:\n  f_schema['const']=field.default\n  \n if is_literal_type(field_type):\n  values=tuple(x.value if isinstance(x,Enum)else x for x in all_literal_values(field_type))\n  \n  if len({v.__class__ for v in values})>1:\n   return field_schema(\n   multitypes_literal_field_for_schema(values,field),\n   by_alias=by_alias,\n   model_name_map=model_name_map,\n   ref_prefix=ref_prefix,\n   ref_template=ref_template,\n   known_models=known_models,\n   )\n   \n   \n  field_type=values[0].__class__\n  f_schema['enum']=list(values)\n  add_field_type_to_schema(field_type,f_schema)\n elif lenient_issubclass(field_type,Enum):\n  enum_name=model_name_map[field_type]\n  f_schema,schema_overrides=get_field_info_schema(field,schema_overrides)\n  f_schema.update(get_schema_ref(enum_name,ref_prefix,ref_template,schema_overrides))\n  definitions[enum_name]=enum_process_schema(field_type,field=field)\n elif is_namedtuple(field_type):\n  sub_schema,*_=model_process_schema(\n  field_type.__pydantic_model__,\n  by_alias=by_alias,\n  model_name_map=model_name_map,\n  ref_prefix=ref_prefix,\n  ref_template=ref_template,\n  known_models=known_models,\n  field=field,\n  )\n  items_schemas=list(sub_schema['properties'].values())\n  f_schema.update(\n  {\n  'type':'array',\n  'items':items_schemas,\n  'minItems':len(items_schemas),\n  'maxItems':len(items_schemas),\n  }\n  )\n elif not hasattr(field_type,'__pydantic_model__'):\n  add_field_type_to_schema(field_type,f_schema)\n  \n  modify_schema=getattr(field_type,'__modify_schema__',None)\n  if modify_schema:\n   _apply_modify_schema(modify_schema,field,f_schema)\n   \n if f_schema:\n  return f_schema,definitions,nested_models\n  \n  \n if lenient_issubclass(getattr(field_type,'__pydantic_model__',None),BaseModel):\n  field_type=field_type.__pydantic_model__\n  \n if issubclass(field_type,BaseModel):\n  model_name=model_name_map[field_type]\n  if field_type not in known_models:\n   sub_schema,sub_definitions,sub_nested_models=model_process_schema(\n   field_type,\n   by_alias=by_alias,\n   model_name_map=model_name_map,\n   ref_prefix=ref_prefix,\n   ref_template=ref_template,\n   known_models=known_models,\n   field=field,\n   )\n   definitions.update(sub_definitions)\n   definitions[model_name]=sub_schema\n   nested_models.update(sub_nested_models)\n  else:\n   nested_models.add(model_name)\n  schema_ref=get_schema_ref(model_name,ref_prefix,ref_template,schema_overrides)\n  return schema_ref,definitions,nested_models\n  \n  \n args=get_args(field_type)\n if args is not None and not args and Generic in field_type.__bases__:\n  return f_schema,definitions,nested_models\n  \n raise ValueError(f'Value not declarable with JSON Schema, field: {field}')\n \n \ndef multitypes_literal_field_for_schema(values:Tuple[Any,...],field:ModelField)->ModelField:\n ''\n\n\n \n literal_distinct_types=defaultdict(list)\n for v in values:\n  literal_distinct_types[v.__class__].append(v)\n distinct_literals=(Literal[tuple(same_type_values)]for same_type_values in literal_distinct_types.values())\n \n return ModelField(\n name=field.name,\n type_=Union[tuple(distinct_literals)],\n class_validators=field.class_validators,\n model_config=field.model_config,\n default=field.default,\n required=field.required,\n alias=field.alias,\n field_info=field.field_info,\n )\n \n \ndef encode_default(dft:Any)->Any:\n from pydantic.v1.main import BaseModel\n \n if isinstance(dft,BaseModel)or is_dataclass(dft):\n  dft=cast('dict[str, Any]',pydantic_encoder(dft))\n  \n if isinstance(dft,dict):\n  return{encode_default(k):encode_default(v)for k,v in dft.items()}\n elif isinstance(dft,Enum):\n  return dft.value\n elif isinstance(dft,(int,float,str)):\n  return dft\n elif isinstance(dft,(list,tuple)):\n  t=dft.__class__\n  seq_args=(encode_default(v)for v in dft)\n  return t(*seq_args)if is_namedtuple(t)else t(seq_args)\n elif dft is None:\n  return None\n else:\n  return pydantic_encoder(dft)\n  \n  \n_map_types_constraint:Dict[Any,Callable[...,type]]={int:conint,float:confloat,Decimal:condecimal}\n\n\ndef get_annotation_from_field_info(\nannotation:Any,field_info:FieldInfo,field_name:str,validate_assignment:bool=False\n)->Type[Any]:\n ''\n\n\n\n\n\n\n \n constraints=field_info.get_constraints()\n used_constraints:Set[str]=set()\n if constraints:\n  annotation,used_constraints=get_annotation_with_constraints(annotation,field_info)\n if validate_assignment:\n  used_constraints.add('allow_mutation')\n  \n unused_constraints=constraints -used_constraints\n if unused_constraints:\n  raise ValueError(\n  f'On field \"{field_name}\" the following field constraints are set but not enforced: '\n  f'{\", \".join(unused_constraints)}. '\n  f'\\nFor more details see https://docs.pydantic.dev/usage/schema/#unenforced-field-constraints'\n  )\n  \n return annotation\n \n \ndef get_annotation_with_constraints(annotation:Any,field_info:FieldInfo)->Tuple[Type[Any],Set[str]]:\n ''\n\n\n\n\n\n \n used_constraints:Set[str]=set()\n \n def go(type_:Any)->Type[Any]:\n  if(\n  is_literal_type(type_)\n  or isinstance(type_,ForwardRef)\n  or lenient_issubclass(type_,(ConstrainedList,ConstrainedSet,ConstrainedFrozenSet))\n  ):\n   return type_\n  origin=get_origin(type_)\n  if origin is not None:\n   args:Tuple[Any,...]=get_args(type_)\n   if any(isinstance(a,ForwardRef)for a in args):\n   \n    return type_\n    \n   if origin is Annotated:\n    return go(args[0])\n   if is_union(origin):\n    return Union[tuple(go(a)for a in args)]\n    \n   if issubclass(origin,List)and(\n   field_info.min_items is not None\n   or field_info.max_items is not None\n   or field_info.unique_items is not None\n   ):\n    used_constraints.update({'min_items','max_items','unique_items'})\n    return conlist(\n    go(args[0]),\n    min_items=field_info.min_items,\n    max_items=field_info.max_items,\n    unique_items=field_info.unique_items,\n    )\n    \n   if issubclass(origin,Set)and(field_info.min_items is not None or field_info.max_items is not None):\n    used_constraints.update({'min_items','max_items'})\n    return conset(go(args[0]),min_items=field_info.min_items,max_items=field_info.max_items)\n    \n   if issubclass(origin,FrozenSet)and(field_info.min_items is not None or field_info.max_items is not None):\n    used_constraints.update({'min_items','max_items'})\n    return confrozenset(go(args[0]),min_items=field_info.min_items,max_items=field_info.max_items)\n    \n   for t in(Tuple,List,Set,FrozenSet,Sequence):\n    if issubclass(origin,t):\n     return t[tuple(go(a)for a in args)]\n     \n   if issubclass(origin,Dict):\n    return Dict[args[0],go(args[1])]\n    \n  attrs:Optional[Tuple[str,...]]=None\n  constraint_func:Optional[Callable[...,type]]=None\n  if isinstance(type_,type):\n   if issubclass(type_,(SecretStr,SecretBytes)):\n    attrs=('max_length','min_length')\n    \n    def constraint_func(**kw:Any)->Type[Any]:\n     return type(type_.__name__,(type_,),kw)\n     \n   elif issubclass(type_,str)and not issubclass(type_,(EmailStr,AnyUrl)):\n    attrs=('max_length','min_length','regex')\n    if issubclass(type_,StrictStr):\n    \n     def constraint_func(**kw:Any)->Type[Any]:\n      return type(type_.__name__,(type_,),kw)\n      \n    else:\n     constraint_func=constr\n   elif issubclass(type_,bytes):\n    attrs=('max_length','min_length','regex')\n    if issubclass(type_,StrictBytes):\n    \n     def constraint_func(**kw:Any)->Type[Any]:\n      return type(type_.__name__,(type_,),kw)\n      \n    else:\n     constraint_func=conbytes\n   elif issubclass(type_,numeric_types)and not issubclass(\n   type_,\n   (\n   ConstrainedInt,\n   ConstrainedFloat,\n   ConstrainedDecimal,\n   ConstrainedList,\n   ConstrainedSet,\n   ConstrainedFrozenSet,\n   bool,\n   ),\n   ):\n   \n    attrs=('gt','lt','ge','le','multiple_of')\n    if issubclass(type_,float):\n     attrs +=('allow_inf_nan',)\n    if issubclass(type_,Decimal):\n     attrs +=('max_digits','decimal_places')\n    numeric_type=next(t for t in numeric_types if issubclass(type_,t))\n    constraint_func=_map_types_constraint[numeric_type]\n    \n  if attrs:\n   used_constraints.update(set(attrs))\n   kwargs={\n   attr_name:attr\n   for attr_name,attr in((attr_name,getattr(field_info,attr_name))for attr_name in attrs)\n   if attr is not None\n   }\n   if kwargs:\n    constraint_func=cast(Callable[...,type],constraint_func)\n    return constraint_func(**kwargs)\n  return type_\n  \n return go(annotation),used_constraints\n \n \ndef normalize_name(name:str)->str:\n ''\n\n \n return re.sub(r'[^a-zA-Z0-9.\\-_]','_',name)\n \n \nclass SkipField(Exception):\n ''\n\n \n \n def __init__(self,message:str)->None:\n  self.message=message\n", ["collections", "dataclasses", "datetime", "decimal", "enum", "inspect", "ipaddress", "pathlib", "pydantic.v1.dataclasses", "pydantic.v1.fields", "pydantic.v1.json", "pydantic.v1.main", "pydantic.v1.networks", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "re", "typing", "typing_extensions", "uuid", "warnings"]], "pydantic.v1.fields": [".py", "import copy\nimport re\nfrom collections import Counter as CollectionCounter,defaultdict,deque\nfrom collections.abc import Callable,Hashable as CollectionsHashable,Iterable as CollectionsIterable\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCounter,\nDefaultDict,\nDeque,\nDict,\nForwardRef,\nFrozenSet,\nGenerator,\nIterable,\nIterator,\nList,\nMapping,\nOptional,\nPattern,\nSequence,\nSet,\nTuple,\nType,\nTypeVar,\nUnion,\n)\n\nfrom typing_extensions import Annotated,Final\n\nfrom pydantic.v1 import errors as errors_\nfrom pydantic.v1.class_validators import Validator,make_generic_validator,prep_validators\nfrom pydantic.v1.error_wrappers import ErrorWrapper\nfrom pydantic.v1.errors import ConfigError,InvalidDiscriminator,MissingDiscriminator,NoneIsNotAllowedError\nfrom pydantic.v1.types import Json,JsonWrapper\nfrom pydantic.v1.typing import(\nNoArgAnyCallable,\nconvert_generics,\ndisplay_as_type,\nget_args,\nget_origin,\nis_finalvar,\nis_literal_type,\nis_new_type,\nis_none_type,\nis_typeddict,\nis_typeddict_special,\nis_union,\nnew_type_supertype,\n)\nfrom pydantic.v1.utils import(\nPyObjectStr,\nRepresentation,\nValueItems,\nget_discriminator_alias_and_values,\nget_unique_discriminator_alias,\nlenient_isinstance,\nlenient_issubclass,\nsequence_like,\nsmart_deepcopy,\n)\nfrom pydantic.v1.validators import constant_validator,dict_validator,find_validators,validate_json\n\nRequired:Any=Ellipsis\n\nT=TypeVar('T')\n\n\nclass UndefinedType:\n def __repr__(self)->str:\n  return 'PydanticUndefined'\n  \n def __copy__(self:T)->T:\n  return self\n  \n def __reduce__(self)->str:\n  return 'Undefined'\n  \n def __deepcopy__(self:T,_:Any)->T:\n  return self\n  \n  \nUndefined=UndefinedType()\n\nif TYPE_CHECKING:\n from pydantic.v1.class_validators import ValidatorsList\n from pydantic.v1.config import BaseConfig\n from pydantic.v1.error_wrappers import ErrorList\n from pydantic.v1.types import ModelOrDc\n from pydantic.v1.typing import AbstractSetIntStr,MappingIntStrAny,ReprArgs\n \n ValidateReturn=Tuple[Optional[Any],Optional[ErrorList]]\n LocStr=Union[Tuple[Union[int,str],...],str]\n BoolUndefined=Union[bool,UndefinedType]\n \n \nclass FieldInfo(Representation):\n ''\n\n \n \n __slots__=(\n 'default',\n 'default_factory',\n 'alias',\n 'alias_priority',\n 'title',\n 'description',\n 'exclude',\n 'include',\n 'const',\n 'gt',\n 'ge',\n 'lt',\n 'le',\n 'multiple_of',\n 'allow_inf_nan',\n 'max_digits',\n 'decimal_places',\n 'min_items',\n 'max_items',\n 'unique_items',\n 'min_length',\n 'max_length',\n 'allow_mutation',\n 'repr',\n 'regex',\n 'discriminator',\n 'extra',\n )\n \n \n __field_constraints__={\n 'min_length':None,\n 'max_length':None,\n 'regex':None,\n 'gt':None,\n 'lt':None,\n 'ge':None,\n 'le':None,\n 'multiple_of':None,\n 'allow_inf_nan':None,\n 'max_digits':None,\n 'decimal_places':None,\n 'min_items':None,\n 'max_items':None,\n 'unique_items':None,\n 'allow_mutation':True,\n }\n \n def __init__(self,default:Any=Undefined,**kwargs:Any)->None:\n  self.default=default\n  self.default_factory=kwargs.pop('default_factory',None)\n  self.alias=kwargs.pop('alias',None)\n  self.alias_priority=kwargs.pop('alias_priority',2 if self.alias is not None else None)\n  self.title=kwargs.pop('title',None)\n  self.description=kwargs.pop('description',None)\n  self.exclude=kwargs.pop('exclude',None)\n  self.include=kwargs.pop('include',None)\n  self.const=kwargs.pop('const',None)\n  self.gt=kwargs.pop('gt',None)\n  self.ge=kwargs.pop('ge',None)\n  self.lt=kwargs.pop('lt',None)\n  self.le=kwargs.pop('le',None)\n  self.multiple_of=kwargs.pop('multiple_of',None)\n  self.allow_inf_nan=kwargs.pop('allow_inf_nan',None)\n  self.max_digits=kwargs.pop('max_digits',None)\n  self.decimal_places=kwargs.pop('decimal_places',None)\n  self.min_items=kwargs.pop('min_items',None)\n  self.max_items=kwargs.pop('max_items',None)\n  self.unique_items=kwargs.pop('unique_items',None)\n  self.min_length=kwargs.pop('min_length',None)\n  self.max_length=kwargs.pop('max_length',None)\n  self.allow_mutation=kwargs.pop('allow_mutation',True)\n  self.regex=kwargs.pop('regex',None)\n  self.discriminator=kwargs.pop('discriminator',None)\n  self.repr=kwargs.pop('repr',True)\n  self.extra=kwargs\n  \n def __repr_args__(self)->'ReprArgs':\n  field_defaults_to_hide:Dict[str,Any]={\n  'repr':True,\n  **self.__field_constraints__,\n  }\n  \n  attrs=((s,getattr(self,s))for s in self.__slots__)\n  return[(a,v)for a,v in attrs if v !=field_defaults_to_hide.get(a,None)]\n  \n def get_constraints(self)->Set[str]:\n  ''\n\n\n\n  \n  return{attr for attr,default in self.__field_constraints__.items()if getattr(self,attr)!=default}\n  \n def update_from_config(self,from_config:Dict[str,Any])->None:\n  ''\n\n  \n  for attr_name,value in from_config.items():\n   try:\n    current_value=getattr(self,attr_name)\n   except AttributeError:\n   \n   \n    self.extra.setdefault(attr_name,value)\n   else:\n    if current_value is self.__field_constraints__.get(attr_name,None):\n     setattr(self,attr_name,value)\n    elif attr_name =='exclude':\n     self.exclude=ValueItems.merge(value,current_value)\n    elif attr_name =='include':\n     self.include=ValueItems.merge(value,current_value,intersect=True)\n     \n def _validate(self)->None:\n  if self.default is not Undefined and self.default_factory is not None:\n   raise ValueError('cannot specify both default and default_factory')\n   \n   \ndef Field(\ndefault:Any=Undefined,\n*,\ndefault_factory:Optional[NoArgAnyCallable]=None,\nalias:Optional[str]=None,\ntitle:Optional[str]=None,\ndescription:Optional[str]=None,\nexclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny',Any]]=None,\ninclude:Optional[Union['AbstractSetIntStr','MappingIntStrAny',Any]]=None,\nconst:Optional[bool]=None,\ngt:Optional[float]=None,\nge:Optional[float]=None,\nlt:Optional[float]=None,\nle:Optional[float]=None,\nmultiple_of:Optional[float]=None,\nallow_inf_nan:Optional[bool]=None,\nmax_digits:Optional[int]=None,\ndecimal_places:Optional[int]=None,\nmin_items:Optional[int]=None,\nmax_items:Optional[int]=None,\nunique_items:Optional[bool]=None,\nmin_length:Optional[int]=None,\nmax_length:Optional[int]=None,\nallow_mutation:bool=True,\nregex:Optional[str]=None,\ndiscriminator:Optional[str]=None,\nrepr:bool=True,\n**extra:Any,\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n field_info=FieldInfo(\n default,\n default_factory=default_factory,\n alias=alias,\n title=title,\n description=description,\n exclude=exclude,\n include=include,\n const=const,\n gt=gt,\n ge=ge,\n lt=lt,\n le=le,\n multiple_of=multiple_of,\n allow_inf_nan=allow_inf_nan,\n max_digits=max_digits,\n decimal_places=decimal_places,\n min_items=min_items,\n max_items=max_items,\n unique_items=unique_items,\n min_length=min_length,\n max_length=max_length,\n allow_mutation=allow_mutation,\n regex=regex,\n discriminator=discriminator,\n repr=repr,\n **extra,\n )\n field_info._validate()\n return field_info\n \n \n \nSHAPE_SINGLETON=1\nSHAPE_LIST=2\nSHAPE_SET=3\nSHAPE_MAPPING=4\nSHAPE_TUPLE=5\nSHAPE_TUPLE_ELLIPSIS=6\nSHAPE_SEQUENCE=7\nSHAPE_FROZENSET=8\nSHAPE_ITERABLE=9\nSHAPE_GENERIC=10\nSHAPE_DEQUE=11\nSHAPE_DICT=12\nSHAPE_DEFAULTDICT=13\nSHAPE_COUNTER=14\nSHAPE_NAME_LOOKUP={\nSHAPE_LIST:'List[{}]',\nSHAPE_SET:'Set[{}]',\nSHAPE_TUPLE_ELLIPSIS:'Tuple[{}, ...]',\nSHAPE_SEQUENCE:'Sequence[{}]',\nSHAPE_FROZENSET:'FrozenSet[{}]',\nSHAPE_ITERABLE:'Iterable[{}]',\nSHAPE_DEQUE:'Deque[{}]',\nSHAPE_DICT:'Dict[{}]',\nSHAPE_DEFAULTDICT:'DefaultDict[{}]',\nSHAPE_COUNTER:'Counter[{}]',\n}\n\nMAPPING_LIKE_SHAPES:Set[int]={SHAPE_DEFAULTDICT,SHAPE_DICT,SHAPE_MAPPING,SHAPE_COUNTER}\n\n\nclass ModelField(Representation):\n __slots__=(\n 'type_',\n 'outer_type_',\n 'annotation',\n 'sub_fields',\n 'sub_fields_mapping',\n 'key_field',\n 'validators',\n 'pre_validators',\n 'post_validators',\n 'default',\n 'default_factory',\n 'required',\n 'final',\n 'model_config',\n 'name',\n 'alias',\n 'has_alias',\n 'field_info',\n 'discriminator_key',\n 'discriminator_alias',\n 'validate_always',\n 'allow_none',\n 'shape',\n 'class_validators',\n 'parse_json',\n )\n \n def __init__(\n self,\n *,\n name:str,\n type_:Type[Any],\n class_validators:Optional[Dict[str,Validator]],\n model_config:Type['BaseConfig'],\n default:Any=None,\n default_factory:Optional[NoArgAnyCallable]=None,\n required:'BoolUndefined'=Undefined,\n final:bool=False,\n alias:Optional[str]=None,\n field_info:Optional[FieldInfo]=None,\n )->None:\n  self.name:str=name\n  self.has_alias:bool=alias is not None\n  self.alias:str=alias if alias is not None else name\n  self.annotation=type_\n  self.type_:Any=convert_generics(type_)\n  self.outer_type_:Any=type_\n  self.class_validators=class_validators or{}\n  self.default:Any=default\n  self.default_factory:Optional[NoArgAnyCallable]=default_factory\n  self.required:'BoolUndefined'=required\n  self.final:bool=final\n  self.model_config=model_config\n  self.field_info:FieldInfo=field_info or FieldInfo(default)\n  self.discriminator_key:Optional[str]=self.field_info.discriminator\n  self.discriminator_alias:Optional[str]=self.discriminator_key\n  \n  self.allow_none:bool=False\n  self.validate_always:bool=False\n  self.sub_fields:Optional[List[ModelField]]=None\n  self.sub_fields_mapping:Optional[Dict[str,'ModelField']]=None\n  self.key_field:Optional[ModelField]=None\n  self.validators:'ValidatorsList'=[]\n  self.pre_validators:Optional['ValidatorsList']=None\n  self.post_validators:Optional['ValidatorsList']=None\n  self.parse_json:bool=False\n  self.shape:int=SHAPE_SINGLETON\n  self.model_config.prepare_field(self)\n  self.prepare()\n  \n def get_default(self)->Any:\n  return smart_deepcopy(self.default)if self.default_factory is None else self.default_factory()\n  \n @staticmethod\n def _get_field_info(\n field_name:str,annotation:Any,value:Any,config:Type['BaseConfig']\n )->Tuple[FieldInfo,Any]:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  field_info_from_config=config.get_field_info(field_name)\n  \n  field_info=None\n  if get_origin(annotation)is Annotated:\n   field_infos=[arg for arg in get_args(annotation)[1:]if isinstance(arg,FieldInfo)]\n   if len(field_infos)>1:\n    raise ValueError(f'cannot specify multiple `Annotated` `Field`s for {field_name !r}')\n   field_info=next(iter(field_infos),None)\n   if field_info is not None:\n    field_info=copy.copy(field_info)\n    field_info.update_from_config(field_info_from_config)\n    if field_info.default not in(Undefined,Required):\n     raise ValueError(f'`Field` default cannot be set in `Annotated` for {field_name !r}')\n    if value is not Undefined and value is not Required:\n    \n     field_info.default=value\n     \n  if isinstance(value,FieldInfo):\n   if field_info is not None:\n    raise ValueError(f'cannot specify `Annotated` and value `Field`s together for {field_name !r}')\n   field_info=value\n   field_info.update_from_config(field_info_from_config)\n  elif field_info is None:\n   field_info=FieldInfo(value,**field_info_from_config)\n  value=None if field_info.default_factory is not None else field_info.default\n  field_info._validate()\n  return field_info,value\n  \n @classmethod\n def infer(\n cls,\n *,\n name:str,\n value:Any,\n annotation:Any,\n class_validators:Optional[Dict[str,Validator]],\n config:Type['BaseConfig'],\n )->'ModelField':\n  from pydantic.v1.schema import get_annotation_from_field_info\n  \n  field_info,value=cls._get_field_info(name,annotation,value,config)\n  required:'BoolUndefined'=Undefined\n  if value is Required:\n   required=True\n   value=None\n  elif value is not Undefined:\n   required=False\n  annotation=get_annotation_from_field_info(annotation,field_info,name,config.validate_assignment)\n  \n  return cls(\n  name=name,\n  type_=annotation,\n  alias=field_info.alias,\n  class_validators=class_validators,\n  default=value,\n  default_factory=field_info.default_factory,\n  required=required,\n  model_config=config,\n  field_info=field_info,\n  )\n  \n def set_config(self,config:Type['BaseConfig'])->None:\n  self.model_config=config\n  info_from_config=config.get_field_info(self.name)\n  config.prepare_field(self)\n  new_alias=info_from_config.get('alias')\n  new_alias_priority=info_from_config.get('alias_priority')or 0\n  if new_alias and new_alias_priority >=(self.field_info.alias_priority or 0):\n   self.field_info.alias=new_alias\n   self.field_info.alias_priority=new_alias_priority\n   self.alias=new_alias\n  new_exclude=info_from_config.get('exclude')\n  if new_exclude is not None:\n   self.field_info.exclude=ValueItems.merge(self.field_info.exclude,new_exclude)\n  new_include=info_from_config.get('include')\n  if new_include is not None:\n   self.field_info.include=ValueItems.merge(self.field_info.include,new_include,intersect=True)\n   \n @property\n def alt_alias(self)->bool:\n  return self.name !=self.alias\n  \n def prepare(self)->None:\n  ''\n\n\n\n\n  \n  self._set_default_and_type()\n  if self.type_.__class__ is ForwardRef or self.type_.__class__ is DeferredType:\n  \n  \n   return\n   \n  self._type_analysis()\n  if self.required is Undefined:\n   self.required=True\n  if self.default is Undefined and self.default_factory is None:\n   self.default=None\n  self.populate_validators()\n  \n def _set_default_and_type(self)->None:\n  ''\n\n  \n  if self.default_factory is not None:\n   if self.type_ is Undefined:\n    raise errors_.ConfigError(\n    f'you need to set the type of field {self.name !r} when using `default_factory`'\n    )\n   return\n   \n  default_value=self.get_default()\n  \n  if default_value is not None and self.type_ is Undefined:\n   self.type_=default_value.__class__\n   self.outer_type_=self.type_\n   self.annotation=self.type_\n   \n  if self.type_ is Undefined:\n   raise errors_.ConfigError(f'unable to infer type for attribute \"{self.name}\"')\n   \n  if self.required is False and default_value is None:\n   self.allow_none=True\n   \n def _type_analysis(self)->None:\n \n  if lenient_issubclass(self.type_,JsonWrapper):\n   self.type_=self.type_.inner_type\n   self.parse_json=True\n  elif lenient_issubclass(self.type_,Json):\n   self.type_=Any\n   self.parse_json=True\n  elif isinstance(self.type_,TypeVar):\n   if self.type_.__bound__:\n    self.type_=self.type_.__bound__\n   elif self.type_.__constraints__:\n    self.type_=Union[self.type_.__constraints__]\n   else:\n    self.type_=Any\n  elif is_new_type(self.type_):\n   self.type_=new_type_supertype(self.type_)\n   \n  if self.type_ is Any or self.type_ is object:\n   if self.required is Undefined:\n    self.required=False\n   self.allow_none=True\n   return\n  elif self.type_ is Pattern or self.type_ is re.Pattern:\n  \n   return\n  elif is_literal_type(self.type_):\n   return\n  elif is_typeddict(self.type_):\n   return\n   \n  if is_finalvar(self.type_):\n   self.final=True\n   \n   if self.type_ is Final:\n    self.type_=Any\n   else:\n    self.type_=get_args(self.type_)[0]\n    \n   self._type_analysis()\n   return\n   \n  origin=get_origin(self.type_)\n  \n  if origin is Annotated or is_typeddict_special(origin):\n   self.type_=get_args(self.type_)[0]\n   self._type_analysis()\n   return\n   \n  if self.discriminator_key is not None and not is_union(origin):\n   raise TypeError('`discriminator` can only be used with `Union` type with more than one variant')\n   \n   \n  if origin is None or origin is CollectionsHashable:\n  \n  \n   if isinstance(self.type_,type)and isinstance(None,self.type_):\n    self.allow_none=True\n   return\n  elif origin is Callable:\n   return\n  elif is_union(origin):\n   types_=[]\n   for type_ in get_args(self.type_):\n    if is_none_type(type_)or type_ is Any or type_ is object:\n     if self.required is Undefined:\n      self.required=False\n     self.allow_none=True\n    if is_none_type(type_):\n     continue\n    types_.append(type_)\n    \n   if len(types_)==1:\n   \n    self.type_=types_[0]\n    \n    self.outer_type_=self.type_\n    \n    self._type_analysis()\n   else:\n    self.sub_fields=[self._create_sub_type(t,f'{self.name}_{display_as_type(t)}')for t in types_]\n    \n    if self.discriminator_key is not None:\n     self.prepare_discriminated_union_sub_fields()\n   return\n  elif issubclass(origin,Tuple):\n  \n   args=get_args(self.type_)\n   if not args:\n    self.type_=Any\n    self.shape=SHAPE_TUPLE_ELLIPSIS\n   elif len(args)==2 and args[1]is Ellipsis:\n    self.type_=args[0]\n    self.shape=SHAPE_TUPLE_ELLIPSIS\n    self.sub_fields=[self._create_sub_type(args[0],f'{self.name}_0')]\n   elif args ==((),):\n    self.shape=SHAPE_TUPLE\n    self.type_=Any\n    self.sub_fields=[]\n   else:\n    self.shape=SHAPE_TUPLE\n    self.sub_fields=[self._create_sub_type(t,f'{self.name}_{i}')for i,t in enumerate(args)]\n   return\n  elif issubclass(origin,List):\n  \n   get_validators=getattr(self.type_,'__get_validators__',None)\n   if get_validators:\n    self.class_validators.update(\n    {f'list_{i}':Validator(validator,pre=True)for i,validator in enumerate(get_validators())}\n    )\n    \n   self.type_=get_args(self.type_)[0]\n   self.shape=SHAPE_LIST\n  elif issubclass(origin,Set):\n  \n   get_validators=getattr(self.type_,'__get_validators__',None)\n   if get_validators:\n    self.class_validators.update(\n    {f'set_{i}':Validator(validator,pre=True)for i,validator in enumerate(get_validators())}\n    )\n    \n   self.type_=get_args(self.type_)[0]\n   self.shape=SHAPE_SET\n  elif issubclass(origin,FrozenSet):\n  \n   get_validators=getattr(self.type_,'__get_validators__',None)\n   if get_validators:\n    self.class_validators.update(\n    {f'frozenset_{i}':Validator(validator,pre=True)for i,validator in enumerate(get_validators())}\n    )\n    \n   self.type_=get_args(self.type_)[0]\n   self.shape=SHAPE_FROZENSET\n  elif issubclass(origin,Deque):\n   self.type_=get_args(self.type_)[0]\n   self.shape=SHAPE_DEQUE\n  elif issubclass(origin,Sequence):\n   self.type_=get_args(self.type_)[0]\n   self.shape=SHAPE_SEQUENCE\n   \n  elif origin is dict or origin is Dict:\n   self.key_field=self._create_sub_type(get_args(self.type_)[0],'key_'+self.name,for_keys=True)\n   self.type_=get_args(self.type_)[1]\n   self.shape=SHAPE_DICT\n  elif issubclass(origin,DefaultDict):\n   self.key_field=self._create_sub_type(get_args(self.type_)[0],'key_'+self.name,for_keys=True)\n   self.type_=get_args(self.type_)[1]\n   self.shape=SHAPE_DEFAULTDICT\n  elif issubclass(origin,Counter):\n   self.key_field=self._create_sub_type(get_args(self.type_)[0],'key_'+self.name,for_keys=True)\n   self.type_=int\n   self.shape=SHAPE_COUNTER\n  elif issubclass(origin,Mapping):\n   self.key_field=self._create_sub_type(get_args(self.type_)[0],'key_'+self.name,for_keys=True)\n   self.type_=get_args(self.type_)[1]\n   self.shape=SHAPE_MAPPING\n   \n   \n  elif origin in{Iterable,CollectionsIterable}:\n   self.type_=get_args(self.type_)[0]\n   self.shape=SHAPE_ITERABLE\n   self.sub_fields=[self._create_sub_type(self.type_,f'{self.name}_type')]\n  elif issubclass(origin,Type):\n   return\n  elif hasattr(origin,'__get_validators__')or self.model_config.arbitrary_types_allowed:\n  \n  \n   self.shape=SHAPE_GENERIC\n   self.sub_fields=[self._create_sub_type(t,f'{self.name}_{i}')for i,t in enumerate(get_args(self.type_))]\n   self.type_=origin\n   return\n  else:\n   raise TypeError(f'Fields of type \"{origin}\" are not supported.')\n   \n   \n  self.sub_fields=[self._create_sub_type(self.type_,'_'+self.name)]\n  \n def prepare_discriminated_union_sub_fields(self)->None:\n  ''\n\n\n  \n  assert self.discriminator_key is not None\n  \n  if self.type_.__class__ is DeferredType:\n   return\n   \n  assert self.sub_fields is not None\n  sub_fields_mapping:Dict[str,'ModelField']={}\n  all_aliases:Set[str]=set()\n  \n  for sub_field in self.sub_fields:\n   t=sub_field.type_\n   if t.__class__ is ForwardRef:\n   \n    return\n    \n   alias,discriminator_values=get_discriminator_alias_and_values(t,self.discriminator_key)\n   all_aliases.add(alias)\n   for discriminator_value in discriminator_values:\n    sub_fields_mapping[discriminator_value]=sub_field\n    \n  self.sub_fields_mapping=sub_fields_mapping\n  self.discriminator_alias=get_unique_discriminator_alias(all_aliases,self.discriminator_key)\n  \n def _create_sub_type(self,type_:Type[Any],name:str,*,for_keys:bool=False)->'ModelField':\n  if for_keys:\n   class_validators=None\n  else:\n  \n   class_validators={\n   k:Validator(\n   func=v.func,\n   pre=v.pre,\n   each_item=False,\n   always=v.always,\n   check_fields=v.check_fields,\n   skip_on_failure=v.skip_on_failure,\n   )\n   for k,v in self.class_validators.items()\n   if v.each_item\n   }\n   \n  field_info,_=self._get_field_info(name,type_,None,self.model_config)\n  \n  return self.__class__(\n  type_=type_,\n  name=name,\n  class_validators=class_validators,\n  model_config=self.model_config,\n  field_info=field_info,\n  )\n  \n def populate_validators(self)->None:\n  ''\n\n\n\n  \n  self.validate_always=getattr(self.type_,'validate_always',False)or any(\n  v.always for v in self.class_validators.values()\n  )\n  \n  class_validators_=self.class_validators.values()\n  if not self.sub_fields or self.shape ==SHAPE_GENERIC:\n   get_validators=getattr(self.type_,'__get_validators__',None)\n   v_funcs=(\n   *[v.func for v in class_validators_ if v.each_item and v.pre],\n   *(get_validators()if get_validators else list(find_validators(self.type_,self.model_config))),\n   *[v.func for v in class_validators_ if v.each_item and not v.pre],\n   )\n   self.validators=prep_validators(v_funcs)\n   \n  self.pre_validators=[]\n  self.post_validators=[]\n  \n  if self.field_info and self.field_info.const:\n   self.post_validators.append(make_generic_validator(constant_validator))\n   \n  if class_validators_:\n   self.pre_validators +=prep_validators(v.func for v in class_validators_ if not v.each_item and v.pre)\n   self.post_validators +=prep_validators(v.func for v in class_validators_ if not v.each_item and not v.pre)\n   \n  if self.parse_json:\n   self.pre_validators.append(make_generic_validator(validate_json))\n   \n  self.pre_validators=self.pre_validators or None\n  self.post_validators=self.post_validators or None\n  \n def validate(\n self,v:Any,values:Dict[str,Any],*,loc:'LocStr',cls:Optional['ModelOrDc']=None\n )->'ValidateReturn':\n  assert self.type_.__class__ is not DeferredType\n  \n  if self.type_.__class__ is ForwardRef:\n   assert cls is not None\n   raise ConfigError(\n   f'field \"{self.name}\" not yet prepared so type is still a ForwardRef, '\n   f'you might need to call {cls.__name__}.update_forward_refs().'\n   )\n   \n  errors:Optional['ErrorList']\n  if self.pre_validators:\n   v,errors=self._apply_validators(v,values,loc,cls,self.pre_validators)\n   if errors:\n    return v,errors\n    \n  if v is None:\n   if is_none_type(self.type_):\n   \n    pass\n   elif self.allow_none:\n    if self.post_validators:\n     return self._apply_validators(v,values,loc,cls,self.post_validators)\n    else:\n     return None,None\n   else:\n    return v,ErrorWrapper(NoneIsNotAllowedError(),loc)\n    \n  if self.shape ==SHAPE_SINGLETON:\n   v,errors=self._validate_singleton(v,values,loc,cls)\n  elif self.shape in MAPPING_LIKE_SHAPES:\n   v,errors=self._validate_mapping_like(v,values,loc,cls)\n  elif self.shape ==SHAPE_TUPLE:\n   v,errors=self._validate_tuple(v,values,loc,cls)\n  elif self.shape ==SHAPE_ITERABLE:\n   v,errors=self._validate_iterable(v,values,loc,cls)\n  elif self.shape ==SHAPE_GENERIC:\n   v,errors=self._apply_validators(v,values,loc,cls,self.validators)\n  else:\n  \n   v,errors=self._validate_sequence_like(v,values,loc,cls)\n   \n  if not errors and self.post_validators:\n   v,errors=self._apply_validators(v,values,loc,cls,self.post_validators)\n  return v,errors\n  \n def _validate_sequence_like(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc']\n )->'ValidateReturn':\n  ''\n\n\n\n  \n  if not sequence_like(v):\n   e:errors_.PydanticTypeError\n   if self.shape ==SHAPE_LIST:\n    e=errors_.ListError()\n   elif self.shape in(SHAPE_TUPLE,SHAPE_TUPLE_ELLIPSIS):\n    e=errors_.TupleError()\n   elif self.shape ==SHAPE_SET:\n    e=errors_.SetError()\n   elif self.shape ==SHAPE_FROZENSET:\n    e=errors_.FrozenSetError()\n   else:\n    e=errors_.SequenceError()\n   return v,ErrorWrapper(e,loc)\n   \n  loc=loc if isinstance(loc,tuple)else(loc,)\n  result=[]\n  errors:List[ErrorList]=[]\n  for i,v_ in enumerate(v):\n   v_loc=*loc,i\n   r,ee=self._validate_singleton(v_,values,v_loc,cls)\n   if ee:\n    errors.append(ee)\n   else:\n    result.append(r)\n    \n  if errors:\n   return v,errors\n   \n  converted:Union[List[Any],Set[Any],FrozenSet[Any],Tuple[Any,...],Iterator[Any],Deque[Any]]=result\n  \n  if self.shape ==SHAPE_SET:\n   converted=set(result)\n  elif self.shape ==SHAPE_FROZENSET:\n   converted=frozenset(result)\n  elif self.shape ==SHAPE_TUPLE_ELLIPSIS:\n   converted=tuple(result)\n  elif self.shape ==SHAPE_DEQUE:\n   converted=deque(result,maxlen=getattr(v,'maxlen',None))\n  elif self.shape ==SHAPE_SEQUENCE:\n   if isinstance(v,tuple):\n    converted=tuple(result)\n   elif isinstance(v,set):\n    converted=set(result)\n   elif isinstance(v,Generator):\n    converted=iter(result)\n   elif isinstance(v,deque):\n    converted=deque(result,maxlen=getattr(v,'maxlen',None))\n  return converted,None\n  \n def _validate_iterable(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc']\n )->'ValidateReturn':\n  ''\n\n\n\n  \n  \n  try:\n   iterable=iter(v)\n  except TypeError:\n   return v,ErrorWrapper(errors_.IterableError(),loc)\n  return iterable,None\n  \n def _validate_tuple(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc']\n )->'ValidateReturn':\n  e:Optional[Exception]=None\n  if not sequence_like(v):\n   e=errors_.TupleError()\n  else:\n   actual_length,expected_length=len(v),len(self.sub_fields)\n   if actual_length !=expected_length:\n    e=errors_.TupleLengthError(actual_length=actual_length,expected_length=expected_length)\n    \n  if e:\n   return v,ErrorWrapper(e,loc)\n   \n  loc=loc if isinstance(loc,tuple)else(loc,)\n  result=[]\n  errors:List[ErrorList]=[]\n  for i,(v_,field)in enumerate(zip(v,self.sub_fields)):\n   v_loc=*loc,i\n   r,ee=field.validate(v_,values,loc=v_loc,cls=cls)\n   if ee:\n    errors.append(ee)\n   else:\n    result.append(r)\n    \n  if errors:\n   return v,errors\n  else:\n   return tuple(result),None\n   \n def _validate_mapping_like(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc']\n )->'ValidateReturn':\n  try:\n   v_iter=dict_validator(v)\n  except TypeError as exc:\n   return v,ErrorWrapper(exc,loc)\n   \n  loc=loc if isinstance(loc,tuple)else(loc,)\n  result,errors={},[]\n  for k,v_ in v_iter.items():\n   v_loc=*loc,'__key__'\n   key_result,key_errors=self.key_field.validate(k,values,loc=v_loc,cls=cls)\n   if key_errors:\n    errors.append(key_errors)\n    continue\n    \n   v_loc=*loc,k\n   value_result,value_errors=self._validate_singleton(v_,values,v_loc,cls)\n   if value_errors:\n    errors.append(value_errors)\n    continue\n    \n   result[key_result]=value_result\n  if errors:\n   return v,errors\n  elif self.shape ==SHAPE_DICT:\n   return result,None\n  elif self.shape ==SHAPE_DEFAULTDICT:\n   return defaultdict(self.type_,result),None\n  elif self.shape ==SHAPE_COUNTER:\n   return CollectionCounter(result),None\n  else:\n   return self._get_mapping_value(v,result),None\n   \n def _get_mapping_value(self,original:T,converted:Dict[Any,Any])->Union[T,Dict[Any,Any]]:\n  ''\n\n\n  \n  original_cls=original.__class__\n  \n  if original_cls ==dict or original_cls ==Dict:\n   return converted\n  elif original_cls in{defaultdict,DefaultDict}:\n   return defaultdict(self.type_,converted)\n  else:\n   try:\n   \n    return original_cls(converted)\n   except TypeError:\n    raise RuntimeError(f'Could not convert dictionary to {original_cls.__name__ !r}')from None\n    \n def _validate_singleton(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc']\n )->'ValidateReturn':\n  if self.sub_fields:\n   if self.discriminator_key is not None:\n    return self._validate_discriminated_union(v,values,loc,cls)\n    \n   errors=[]\n   \n   if self.model_config.smart_union and is_union(get_origin(self.type_)):\n   \n   \n    for field in self.sub_fields:\n     if v.__class__ is field.outer_type_:\n      return v,None\n      \n      \n    for field in self.sub_fields:\n    \n    \n    \n    \n    \n    \n    \n     try:\n      if isinstance(v,field.outer_type_):\n       return v,None\n     except TypeError:\n     \n      if lenient_isinstance(v,get_origin(field.outer_type_)):\n       value,error=field.validate(v,values,loc=loc,cls=cls)\n       if not error:\n        return value,None\n        \n        \n        \n   for field in self.sub_fields:\n    value,error=field.validate(v,values,loc=loc,cls=cls)\n    if error:\n     errors.append(error)\n    else:\n     return value,None\n   return v,errors\n  else:\n   return self._apply_validators(v,values,loc,cls,self.validators)\n   \n def _validate_discriminated_union(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc']\n )->'ValidateReturn':\n  assert self.discriminator_key is not None\n  assert self.discriminator_alias is not None\n  \n  try:\n   try:\n    discriminator_value=v[self.discriminator_alias]\n   except KeyError:\n    if self.model_config.allow_population_by_field_name:\n     discriminator_value=v[self.discriminator_key]\n    else:\n     raise\n  except KeyError:\n   return v,ErrorWrapper(MissingDiscriminator(discriminator_key=self.discriminator_key),loc)\n  except TypeError:\n   try:\n   \n    discriminator_value=getattr(v,self.discriminator_key)\n   except(AttributeError,TypeError):\n    return v,ErrorWrapper(MissingDiscriminator(discriminator_key=self.discriminator_key),loc)\n    \n  if self.sub_fields_mapping is None:\n   assert cls is not None\n   raise ConfigError(\n   f'field \"{self.name}\" not yet prepared so type is still a ForwardRef, '\n   f'you might need to call {cls.__name__}.update_forward_refs().'\n   )\n   \n  try:\n   sub_field=self.sub_fields_mapping[discriminator_value]\n  except(KeyError,TypeError):\n  \n  \n   assert self.sub_fields_mapping is not None\n   return v,ErrorWrapper(\n   InvalidDiscriminator(\n   discriminator_key=self.discriminator_key,\n   discriminator_value=discriminator_value,\n   allowed_values=list(self.sub_fields_mapping),\n   ),\n   loc,\n   )\n  else:\n   if not isinstance(loc,tuple):\n    loc=(loc,)\n   return sub_field.validate(v,values,loc=(*loc,display_as_type(sub_field.type_)),cls=cls)\n   \n def _apply_validators(\n self,v:Any,values:Dict[str,Any],loc:'LocStr',cls:Optional['ModelOrDc'],validators:'ValidatorsList'\n )->'ValidateReturn':\n  for validator in validators:\n   try:\n    v=validator(cls,v,values,self,self.model_config)\n   except(ValueError,TypeError,AssertionError)as exc:\n    return v,ErrorWrapper(exc,loc)\n  return v,None\n  \n def is_complex(self)->bool:\n  ''\n\n  \n  from pydantic.v1.main import BaseModel\n  \n  return(\n  self.shape !=SHAPE_SINGLETON\n  or hasattr(self.type_,'__pydantic_model__')\n  or lenient_issubclass(self.type_,(BaseModel,list,set,frozenset,dict))\n  )\n  \n def _type_display(self)->PyObjectStr:\n  t=display_as_type(self.type_)\n  \n  if self.shape in MAPPING_LIKE_SHAPES:\n   t=f'Mapping[{display_as_type(self.key_field.type_)}, {t}]'\n  elif self.shape ==SHAPE_TUPLE:\n   t='Tuple[{}]'.format(', '.join(display_as_type(f.type_)for f in self.sub_fields))\n  elif self.shape ==SHAPE_GENERIC:\n   assert self.sub_fields\n   t='{}[{}]'.format(\n   display_as_type(self.type_),', '.join(display_as_type(f.type_)for f in self.sub_fields)\n   )\n  elif self.shape !=SHAPE_SINGLETON:\n   t=SHAPE_NAME_LOOKUP[self.shape].format(t)\n   \n  if self.allow_none and(self.shape !=SHAPE_SINGLETON or not self.sub_fields):\n   t=f'Optional[{t}]'\n  return PyObjectStr(t)\n  \n def __repr_args__(self)->'ReprArgs':\n  args=[('name',self.name),('type',self._type_display()),('required',self.required)]\n  \n  if not self.required:\n   if self.default_factory is not None:\n    args.append(('default_factory',f'<function {self.default_factory.__name__}>'))\n   else:\n    args.append(('default',self.default))\n    \n  if self.alt_alias:\n   args.append(('alias',self.alias))\n  return args\n  \n  \nclass ModelPrivateAttr(Representation):\n __slots__=('default','default_factory')\n \n def __init__(self,default:Any=Undefined,*,default_factory:Optional[NoArgAnyCallable]=None)->None:\n  self.default=default\n  self.default_factory=default_factory\n  \n def get_default(self)->Any:\n  return smart_deepcopy(self.default)if self.default_factory is None else self.default_factory()\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,self.__class__)and(self.default,self.default_factory)==(\n  other.default,\n  other.default_factory,\n  )\n  \n  \ndef PrivateAttr(\ndefault:Any=Undefined,\n*,\ndefault_factory:Optional[NoArgAnyCallable]=None,\n)->Any:\n ''\n\n\n\n\n\n\n\n\n\n \n if default is not Undefined and default_factory is not None:\n  raise ValueError('cannot specify both default and default_factory')\n  \n return ModelPrivateAttr(\n default,\n default_factory=default_factory,\n )\n \n \nclass DeferredType:\n ''\n\n \n \n \ndef is_finalvar_with_default_val(type_:Type[Any],val:Any)->bool:\n return is_finalvar(type_)and val is not Undefined and not isinstance(val,FieldInfo)\n", ["collections", "collections.abc", "copy", "pydantic.v1", "pydantic.v1.class_validators", "pydantic.v1.config", "pydantic.v1.error_wrappers", "pydantic.v1.errors", "pydantic.v1.main", "pydantic.v1.schema", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "pydantic.v1.validators", "re", "typing", "typing_extensions"]], "pydantic.v1.dataclasses": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport copy\nimport dataclasses\nimport sys\nfrom contextlib import contextmanager\nfrom functools import wraps\n\ntry:\n from functools import cached_property\nexcept ImportError:\n\n pass\n \nfrom typing import TYPE_CHECKING,Any,Callable,ClassVar,Dict,Generator,Optional,Type,TypeVar,Union,overload\n\nfrom typing_extensions import dataclass_transform\n\nfrom pydantic.v1.class_validators import gather_all_validators\nfrom pydantic.v1.config import BaseConfig,ConfigDict,Extra,get_config\nfrom pydantic.v1.error_wrappers import ValidationError\nfrom pydantic.v1.errors import DataclassTypeError\nfrom pydantic.v1.fields import Field,FieldInfo,Required,Undefined\nfrom pydantic.v1.main import create_model,validate_model\nfrom pydantic.v1.utils import ClassAttribute\n\nif TYPE_CHECKING:\n from pydantic.v1.main import BaseModel\n from pydantic.v1.typing import CallableGenerator,NoArgAnyCallable\n \n DataclassT=TypeVar('DataclassT',bound='Dataclass')\n \n DataclassClassOrWrapper=Union[Type['Dataclass'],'DataclassProxy']\n \n class Dataclass:\n \n  __dataclass_fields__:ClassVar[Dict[str,Any]]\n  __dataclass_params__:ClassVar[Any]\n  __post_init__:ClassVar[Callable[...,None]]\n  \n  \n  __pydantic_run_validation__:ClassVar[bool]\n  __post_init_post_parse__:ClassVar[Callable[...,None]]\n  __pydantic_initialised__:ClassVar[bool]\n  __pydantic_model__:ClassVar[Type[BaseModel]]\n  __pydantic_validate_values__:ClassVar[Callable[['Dataclass'],None]]\n  __pydantic_has_field_info_default__:ClassVar[bool]\n  \n  def __init__(self,*args:object,**kwargs:object)->None:\n   pass\n   \n  @classmethod\n  def __get_validators__(cls:Type['Dataclass'])->'CallableGenerator':\n   pass\n   \n  @classmethod\n  def __validate__(cls:Type['DataclassT'],v:Any)->'DataclassT':\n   pass\n   \n   \n__all__=[\n'dataclass',\n'set_validation',\n'create_pydantic_model_from_dataclass',\n'is_builtin_dataclass',\n'make_dataclass_validator',\n]\n\n_T=TypeVar('_T')\n\nif sys.version_info >=(3,10):\n\n @dataclass_transform(field_specifiers=(dataclasses.field,Field))\n @overload\n def dataclass(\n *,\n init:bool=True,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool=False,\n config:Union[ConfigDict,Type[object],None]=None,\n validate_on_init:Optional[bool]=None,\n use_proxy:Optional[bool]=None,\n kw_only:bool=...,\n )->Callable[[Type[_T]],'DataclassClassOrWrapper']:\n  ...\n  \n @dataclass_transform(field_specifiers=(dataclasses.field,Field))\n @overload\n def dataclass(\n _cls:Type[_T],\n *,\n init:bool=True,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool=False,\n config:Union[ConfigDict,Type[object],None]=None,\n validate_on_init:Optional[bool]=None,\n use_proxy:Optional[bool]=None,\n kw_only:bool=...,\n )->'DataclassClassOrWrapper':\n  ...\n  \nelse:\n\n @dataclass_transform(field_specifiers=(dataclasses.field,Field))\n @overload\n def dataclass(\n *,\n init:bool=True,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool=False,\n config:Union[ConfigDict,Type[object],None]=None,\n validate_on_init:Optional[bool]=None,\n use_proxy:Optional[bool]=None,\n )->Callable[[Type[_T]],'DataclassClassOrWrapper']:\n  ...\n  \n @dataclass_transform(field_specifiers=(dataclasses.field,Field))\n @overload\n def dataclass(\n _cls:Type[_T],\n *,\n init:bool=True,\n repr:bool=True,\n eq:bool=True,\n order:bool=False,\n unsafe_hash:bool=False,\n frozen:bool=False,\n config:Union[ConfigDict,Type[object],None]=None,\n validate_on_init:Optional[bool]=None,\n use_proxy:Optional[bool]=None,\n )->'DataclassClassOrWrapper':\n  ...\n  \n  \n@dataclass_transform(field_specifiers=(dataclasses.field,Field))\ndef dataclass(\n_cls:Optional[Type[_T]]=None,\n*,\ninit:bool=True,\nrepr:bool=True,\neq:bool=True,\norder:bool=False,\nunsafe_hash:bool=False,\nfrozen:bool=False,\nconfig:Union[ConfigDict,Type[object],None]=None,\nvalidate_on_init:Optional[bool]=None,\nuse_proxy:Optional[bool]=None,\nkw_only:bool=False,\n)->Union[Callable[[Type[_T]],'DataclassClassOrWrapper'],'DataclassClassOrWrapper']:\n ''\n\n\n\n\n \n the_config=get_config(config)\n \n def wrap(cls:Type[Any])->'DataclassClassOrWrapper':\n  should_use_proxy=(\n  use_proxy\n  if use_proxy is not None\n  else(\n  is_builtin_dataclass(cls)\n  and(cls.__bases__[0]is object or set(dir(cls))==set(dir(cls.__bases__[0])))\n  )\n  )\n  if should_use_proxy:\n   dc_cls_doc=''\n   dc_cls=DataclassProxy(cls)\n   default_validate_on_init=False\n  else:\n   dc_cls_doc=cls.__doc__ or ''\n   if sys.version_info >=(3,10):\n    dc_cls=dataclasses.dataclass(\n    cls,\n    init=init,\n    repr=repr,\n    eq=eq,\n    order=order,\n    unsafe_hash=unsafe_hash,\n    frozen=frozen,\n    kw_only=kw_only,\n    )\n   else:\n    dc_cls=dataclasses.dataclass(\n    cls,init=init,repr=repr,eq=eq,order=order,unsafe_hash=unsafe_hash,frozen=frozen\n    )\n   default_validate_on_init=True\n   \n  should_validate_on_init=default_validate_on_init if validate_on_init is None else validate_on_init\n  _add_pydantic_validation_attributes(cls,the_config,should_validate_on_init,dc_cls_doc)\n  dc_cls.__pydantic_model__.__try_update_forward_refs__(**{cls.__name__:cls})\n  return dc_cls\n  \n if _cls is None:\n  return wrap\n  \n return wrap(_cls)\n \n \n@contextmanager\ndef set_validation(cls:Type['DataclassT'],value:bool)->Generator[Type['DataclassT'],None,None]:\n original_run_validation=cls.__pydantic_run_validation__\n try:\n  cls.__pydantic_run_validation__=value\n  yield cls\n finally:\n  cls.__pydantic_run_validation__=original_run_validation\n  \n  \nclass DataclassProxy:\n __slots__='__dataclass__'\n \n def __init__(self,dc_cls:Type['Dataclass'])->None:\n  object.__setattr__(self,'__dataclass__',dc_cls)\n  \n def __call__(self,*args:Any,**kwargs:Any)->Any:\n  with set_validation(self.__dataclass__,True):\n   return self.__dataclass__(*args,**kwargs)\n   \n def __getattr__(self,name:str)->Any:\n  return getattr(self.__dataclass__,name)\n  \n def __setattr__(self,__name:str,__value:Any)->None:\n  return setattr(self.__dataclass__,__name,__value)\n  \n def __instancecheck__(self,instance:Any)->bool:\n  return isinstance(instance,self.__dataclass__)\n  \n def __copy__(self)->'DataclassProxy':\n  return DataclassProxy(copy.copy(self.__dataclass__))\n  \n def __deepcopy__(self,memo:Any)->'DataclassProxy':\n  return DataclassProxy(copy.deepcopy(self.__dataclass__,memo))\n  \n  \ndef _add_pydantic_validation_attributes(\ndc_cls:Type['Dataclass'],\nconfig:Type[BaseConfig],\nvalidate_on_init:bool,\ndc_cls_doc:str,\n)->None:\n ''\n\n\n\n \n init=dc_cls.__init__\n \n @wraps(init)\n def handle_extra_init(self:'Dataclass',*args:Any,**kwargs:Any)->None:\n  if config.extra ==Extra.ignore:\n   init(self,*args,**{k:v for k,v in kwargs.items()if k in self.__dataclass_fields__})\n   \n  elif config.extra ==Extra.allow:\n   for k,v in kwargs.items():\n    self.__dict__.setdefault(k,v)\n   init(self,*args,**{k:v for k,v in kwargs.items()if k in self.__dataclass_fields__})\n   \n  else:\n   init(self,*args,**kwargs)\n   \n if hasattr(dc_cls,'__post_init__'):\n  try:\n   post_init=dc_cls.__post_init__.__wrapped__\n  except AttributeError:\n   post_init=dc_cls.__post_init__\n   \n  @wraps(post_init)\n  def new_post_init(self:'Dataclass',*args:Any,**kwargs:Any)->None:\n   if config.post_init_call =='before_validation':\n    post_init(self,*args,**kwargs)\n    \n   if self.__class__.__pydantic_run_validation__:\n    self.__pydantic_validate_values__()\n    if hasattr(self,'__post_init_post_parse__'):\n     self.__post_init_post_parse__(*args,**kwargs)\n     \n   if config.post_init_call =='after_validation':\n    post_init(self,*args,**kwargs)\n    \n  setattr(dc_cls,'__init__',handle_extra_init)\n  setattr(dc_cls,'__post_init__',new_post_init)\n  \n else:\n \n  @wraps(init)\n  def new_init(self:'Dataclass',*args:Any,**kwargs:Any)->None:\n   handle_extra_init(self,*args,**kwargs)\n   \n   if self.__class__.__pydantic_run_validation__:\n    self.__pydantic_validate_values__()\n    \n   if hasattr(self,'__post_init_post_parse__'):\n   \n   \n   \n   \n    initvars_and_values:Dict[str,Any]={}\n    for i,f in enumerate(self.__class__.__dataclass_fields__.values()):\n     if f._field_type is dataclasses._FIELD_INITVAR:\n      try:\n      \n       initvars_and_values[f.name]=args[i]\n      except IndexError:\n       initvars_and_values[f.name]=kwargs.get(f.name,f.default)\n       \n    self.__post_init_post_parse__(**initvars_and_values)\n    \n  setattr(dc_cls,'__init__',new_init)\n  \n setattr(dc_cls,'__pydantic_run_validation__',ClassAttribute('__pydantic_run_validation__',validate_on_init))\n setattr(dc_cls,'__pydantic_initialised__',False)\n setattr(dc_cls,'__pydantic_model__',create_pydantic_model_from_dataclass(dc_cls,config,dc_cls_doc))\n setattr(dc_cls,'__pydantic_validate_values__',_dataclass_validate_values)\n setattr(dc_cls,'__validate__',classmethod(_validate_dataclass))\n setattr(dc_cls,'__get_validators__',classmethod(_get_validators))\n \n if dc_cls.__pydantic_model__.__config__.validate_assignment and not dc_cls.__dataclass_params__.frozen:\n  setattr(dc_cls,'__setattr__',_dataclass_validate_assignment_setattr)\n  \n  \ndef _get_validators(cls:'DataclassClassOrWrapper')->'CallableGenerator':\n yield cls.__validate__\n \n \ndef _validate_dataclass(cls:Type['DataclassT'],v:Any)->'DataclassT':\n with set_validation(cls,True):\n  if isinstance(v,cls):\n   v.__pydantic_validate_values__()\n   return v\n  elif isinstance(v,(list,tuple)):\n   return cls(*v)\n  elif isinstance(v,dict):\n   return cls(**v)\n  else:\n   raise DataclassTypeError(class_name=cls.__name__)\n   \n   \ndef create_pydantic_model_from_dataclass(\ndc_cls:Type['Dataclass'],\nconfig:Type[Any]=BaseConfig,\ndc_cls_doc:Optional[str]=None,\n)->Type['BaseModel']:\n field_definitions:Dict[str,Any]={}\n for field in dataclasses.fields(dc_cls):\n  default:Any=Undefined\n  default_factory:Optional['NoArgAnyCallable']=None\n  field_info:FieldInfo\n  \n  if field.default is not dataclasses.MISSING:\n   default=field.default\n  elif field.default_factory is not dataclasses.MISSING:\n   default_factory=field.default_factory\n  else:\n   default=Required\n   \n  if isinstance(default,FieldInfo):\n   field_info=default\n   dc_cls.__pydantic_has_field_info_default__=True\n  else:\n   field_info=Field(default=default,default_factory=default_factory,**field.metadata)\n   \n  field_definitions[field.name]=(field.type,field_info)\n  \n validators=gather_all_validators(dc_cls)\n model:Type['BaseModel']=create_model(\n dc_cls.__name__,\n __config__=config,\n __module__=dc_cls.__module__,\n __validators__=validators,\n __cls_kwargs__={'__resolve_forward_refs__':False},\n **field_definitions,\n )\n model.__doc__=dc_cls_doc if dc_cls_doc is not None else dc_cls.__doc__ or ''\n return model\n \n \nif sys.version_info >=(3,8):\n\n def _is_field_cached_property(obj:'Dataclass',k:str)->bool:\n  return isinstance(getattr(type(obj),k,None),cached_property)\n  \nelse:\n\n def _is_field_cached_property(obj:'Dataclass',k:str)->bool:\n  return False\n  \n  \ndef _dataclass_validate_values(self:'Dataclass')->None:\n\n\n if getattr(self,'__pydantic_initialised__'):\n  return\n if getattr(self,'__pydantic_has_field_info_default__',False):\n \n \n  input_data={\n  k:v\n  for k,v in self.__dict__.items()\n  if not(isinstance(v,FieldInfo)or _is_field_cached_property(self,k))\n  }\n else:\n  input_data={k:v for k,v in self.__dict__.items()if not _is_field_cached_property(self,k)}\n d,_,validation_error=validate_model(self.__pydantic_model__,input_data,cls=self.__class__)\n if validation_error:\n  raise validation_error\n self.__dict__.update(d)\n object.__setattr__(self,'__pydantic_initialised__',True)\n \n \ndef _dataclass_validate_assignment_setattr(self:'Dataclass',name:str,value:Any)->None:\n if self.__pydantic_initialised__:\n  d=dict(self.__dict__)\n  d.pop(name,None)\n  known_field=self.__pydantic_model__.__fields__.get(name,None)\n  if known_field:\n   value,error_=known_field.validate(value,d,loc=name,cls=self.__class__)\n   if error_:\n    raise ValidationError([error_],self.__class__)\n    \n object.__setattr__(self,name,value)\n \n \ndef is_builtin_dataclass(_cls:Type[Any])->bool:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return(\n dataclasses.is_dataclass(_cls)\n and not hasattr(_cls,'__pydantic_model__')\n and set(_cls.__dataclass_fields__).issuperset(set(getattr(_cls,'__annotations__',{})))\n )\n \n \ndef make_dataclass_validator(dc_cls:Type['Dataclass'],config:Type[BaseConfig])->'CallableGenerator':\n ''\n\n\n\n \n yield from _get_validators(dataclass(dc_cls,config=config,use_proxy=True))\n", ["contextlib", "copy", "dataclasses", "functools", "pydantic.v1.class_validators", "pydantic.v1.config", "pydantic.v1.error_wrappers", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.typing", "pydantic.v1.utils", "sys", "typing", "typing_extensions"]], "pydantic.v1.errors": [".py", "from decimal import Decimal\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING,Any,Callable,Sequence,Set,Tuple,Type,Union\n\nfrom pydantic.v1.typing import display_as_type\n\nif TYPE_CHECKING:\n from pydantic.v1.typing import DictStrAny\n \n \n__all__=(\n'PydanticTypeError',\n'PydanticValueError',\n'ConfigError',\n'MissingError',\n'ExtraError',\n'NoneIsNotAllowedError',\n'NoneIsAllowedError',\n'WrongConstantError',\n'NotNoneError',\n'BoolError',\n'BytesError',\n'DictError',\n'EmailError',\n'UrlError',\n'UrlSchemeError',\n'UrlSchemePermittedError',\n'UrlUserInfoError',\n'UrlHostError',\n'UrlHostTldError',\n'UrlPortError',\n'UrlExtraError',\n'EnumError',\n'IntEnumError',\n'EnumMemberError',\n'IntegerError',\n'FloatError',\n'PathError',\n'PathNotExistsError',\n'PathNotAFileError',\n'PathNotADirectoryError',\n'PyObjectError',\n'SequenceError',\n'ListError',\n'SetError',\n'FrozenSetError',\n'TupleError',\n'TupleLengthError',\n'ListMinLengthError',\n'ListMaxLengthError',\n'ListUniqueItemsError',\n'SetMinLengthError',\n'SetMaxLengthError',\n'FrozenSetMinLengthError',\n'FrozenSetMaxLengthError',\n'AnyStrMinLengthError',\n'AnyStrMaxLengthError',\n'StrError',\n'StrRegexError',\n'NumberNotGtError',\n'NumberNotGeError',\n'NumberNotLtError',\n'NumberNotLeError',\n'NumberNotMultipleError',\n'DecimalError',\n'DecimalIsNotFiniteError',\n'DecimalMaxDigitsError',\n'DecimalMaxPlacesError',\n'DecimalWholeDigitsError',\n'DateTimeError',\n'DateError',\n'DateNotInThePastError',\n'DateNotInTheFutureError',\n'TimeError',\n'DurationError',\n'HashableError',\n'UUIDError',\n'UUIDVersionError',\n'ArbitraryTypeError',\n'ClassError',\n'SubclassError',\n'JsonError',\n'JsonTypeError',\n'PatternError',\n'DataclassTypeError',\n'CallableError',\n'IPvAnyAddressError',\n'IPvAnyInterfaceError',\n'IPvAnyNetworkError',\n'IPv4AddressError',\n'IPv6AddressError',\n'IPv4NetworkError',\n'IPv6NetworkError',\n'IPv4InterfaceError',\n'IPv6InterfaceError',\n'ColorError',\n'StrictBoolError',\n'NotDigitError',\n'LuhnValidationError',\n'InvalidLengthForBrand',\n'InvalidByteSize',\n'InvalidByteSizeUnit',\n'MissingDiscriminator',\n'InvalidDiscriminator',\n)\n\n\ndef cls_kwargs(cls:Type['PydanticErrorMixin'],ctx:'DictStrAny')->'PydanticErrorMixin':\n ''\n\n\n\n\n\n \n return cls(**ctx)\n \n \nclass PydanticErrorMixin:\n code:str\n msg_template:str\n \n def __init__(self,**ctx:Any)->None:\n  self.__dict__=ctx\n  \n def __str__(self)->str:\n  return self.msg_template.format(**self.__dict__)\n  \n def __reduce__(self)->Tuple[Callable[...,'PydanticErrorMixin'],Tuple[Type['PydanticErrorMixin'],'DictStrAny']]:\n  return cls_kwargs,(self.__class__,self.__dict__)\n  \n  \nclass PydanticTypeError(PydanticErrorMixin,TypeError):\n pass\n \n \nclass PydanticValueError(PydanticErrorMixin,ValueError):\n pass\n \n \nclass ConfigError(RuntimeError):\n pass\n \n \nclass MissingError(PydanticValueError):\n msg_template='field required'\n \n \nclass ExtraError(PydanticValueError):\n msg_template='extra fields not permitted'\n \n \nclass NoneIsNotAllowedError(PydanticTypeError):\n code='none.not_allowed'\n msg_template='none is not an allowed value'\n \n \nclass NoneIsAllowedError(PydanticTypeError):\n code='none.allowed'\n msg_template='value is not none'\n \n \nclass WrongConstantError(PydanticValueError):\n code='const'\n \n def __str__(self)->str:\n  permitted=', '.join(repr(v)for v in self.permitted)\n  return f'unexpected value; permitted: {permitted}'\n  \n  \nclass NotNoneError(PydanticTypeError):\n code='not_none'\n msg_template='value is not None'\n \n \nclass BoolError(PydanticTypeError):\n msg_template='value could not be parsed to a boolean'\n \n \nclass BytesError(PydanticTypeError):\n msg_template='byte type expected'\n \n \nclass DictError(PydanticTypeError):\n msg_template='value is not a valid dict'\n \n \nclass EmailError(PydanticValueError):\n msg_template='value is not a valid email address'\n \n \nclass UrlError(PydanticValueError):\n code='url'\n \n \nclass UrlSchemeError(UrlError):\n code='url.scheme'\n msg_template='invalid or missing URL scheme'\n \n \nclass UrlSchemePermittedError(UrlError):\n code='url.scheme'\n msg_template='URL scheme not permitted'\n \n def __init__(self,allowed_schemes:Set[str]):\n  super().__init__(allowed_schemes=allowed_schemes)\n  \n  \nclass UrlUserInfoError(UrlError):\n code='url.userinfo'\n msg_template='userinfo required in URL but missing'\n \n \nclass UrlHostError(UrlError):\n code='url.host'\n msg_template='URL host invalid'\n \n \nclass UrlHostTldError(UrlError):\n code='url.host'\n msg_template='URL host invalid, top level domain required'\n \n \nclass UrlPortError(UrlError):\n code='url.port'\n msg_template='URL port invalid, port cannot exceed 65535'\n \n \nclass UrlExtraError(UrlError):\n code='url.extra'\n msg_template='URL invalid, extra characters found after valid URL: {extra!r}'\n \n \nclass EnumMemberError(PydanticTypeError):\n code='enum'\n \n def __str__(self)->str:\n  permitted=', '.join(repr(v.value)for v in self.enum_values)\n  return f'value is not a valid enumeration member; permitted: {permitted}'\n  \n  \nclass IntegerError(PydanticTypeError):\n msg_template='value is not a valid integer'\n \n \nclass FloatError(PydanticTypeError):\n msg_template='value is not a valid float'\n \n \nclass PathError(PydanticTypeError):\n msg_template='value is not a valid path'\n \n \nclass _PathValueError(PydanticValueError):\n def __init__(self,*,path:Path)->None:\n  super().__init__(path=str(path))\n  \n  \nclass PathNotExistsError(_PathValueError):\n code='path.not_exists'\n msg_template='file or directory at path \"{path}\" does not exist'\n \n \nclass PathNotAFileError(_PathValueError):\n code='path.not_a_file'\n msg_template='path \"{path}\" does not point to a file'\n \n \nclass PathNotADirectoryError(_PathValueError):\n code='path.not_a_directory'\n msg_template='path \"{path}\" does not point to a directory'\n \n \nclass PyObjectError(PydanticTypeError):\n msg_template='ensure this value contains valid import path or valid callable: {error_message}'\n \n \nclass SequenceError(PydanticTypeError):\n msg_template='value is not a valid sequence'\n \n \nclass IterableError(PydanticTypeError):\n msg_template='value is not a valid iterable'\n \n \nclass ListError(PydanticTypeError):\n msg_template='value is not a valid list'\n \n \nclass SetError(PydanticTypeError):\n msg_template='value is not a valid set'\n \n \nclass FrozenSetError(PydanticTypeError):\n msg_template='value is not a valid frozenset'\n \n \nclass DequeError(PydanticTypeError):\n msg_template='value is not a valid deque'\n \n \nclass TupleError(PydanticTypeError):\n msg_template='value is not a valid tuple'\n \n \nclass TupleLengthError(PydanticValueError):\n code='tuple.length'\n msg_template='wrong tuple length {actual_length}, expected {expected_length}'\n \n def __init__(self,*,actual_length:int,expected_length:int)->None:\n  super().__init__(actual_length=actual_length,expected_length=expected_length)\n  \n  \nclass ListMinLengthError(PydanticValueError):\n code='list.min_items'\n msg_template='ensure this value has at least {limit_value} items'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass ListMaxLengthError(PydanticValueError):\n code='list.max_items'\n msg_template='ensure this value has at most {limit_value} items'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass ListUniqueItemsError(PydanticValueError):\n code='list.unique_items'\n msg_template='the list has duplicated items'\n \n \nclass SetMinLengthError(PydanticValueError):\n code='set.min_items'\n msg_template='ensure this value has at least {limit_value} items'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass SetMaxLengthError(PydanticValueError):\n code='set.max_items'\n msg_template='ensure this value has at most {limit_value} items'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass FrozenSetMinLengthError(PydanticValueError):\n code='frozenset.min_items'\n msg_template='ensure this value has at least {limit_value} items'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass FrozenSetMaxLengthError(PydanticValueError):\n code='frozenset.max_items'\n msg_template='ensure this value has at most {limit_value} items'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass AnyStrMinLengthError(PydanticValueError):\n code='any_str.min_length'\n msg_template='ensure this value has at least {limit_value} characters'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass AnyStrMaxLengthError(PydanticValueError):\n code='any_str.max_length'\n msg_template='ensure this value has at most {limit_value} characters'\n \n def __init__(self,*,limit_value:int)->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass StrError(PydanticTypeError):\n msg_template='str type expected'\n \n \nclass StrRegexError(PydanticValueError):\n code='str.regex'\n msg_template='string does not match regex \"{pattern}\"'\n \n def __init__(self,*,pattern:str)->None:\n  super().__init__(pattern=pattern)\n  \n  \nclass _NumberBoundError(PydanticValueError):\n def __init__(self,*,limit_value:Union[int,float,Decimal])->None:\n  super().__init__(limit_value=limit_value)\n  \n  \nclass NumberNotGtError(_NumberBoundError):\n code='number.not_gt'\n msg_template='ensure this value is greater than {limit_value}'\n \n \nclass NumberNotGeError(_NumberBoundError):\n code='number.not_ge'\n msg_template='ensure this value is greater than or equal to {limit_value}'\n \n \nclass NumberNotLtError(_NumberBoundError):\n code='number.not_lt'\n msg_template='ensure this value is less than {limit_value}'\n \n \nclass NumberNotLeError(_NumberBoundError):\n code='number.not_le'\n msg_template='ensure this value is less than or equal to {limit_value}'\n \n \nclass NumberNotFiniteError(PydanticValueError):\n code='number.not_finite_number'\n msg_template='ensure this value is a finite number'\n \n \nclass NumberNotMultipleError(PydanticValueError):\n code='number.not_multiple'\n msg_template='ensure this value is a multiple of {multiple_of}'\n \n def __init__(self,*,multiple_of:Union[int,float,Decimal])->None:\n  super().__init__(multiple_of=multiple_of)\n  \n  \nclass DecimalError(PydanticTypeError):\n msg_template='value is not a valid decimal'\n \n \nclass DecimalIsNotFiniteError(PydanticValueError):\n code='decimal.not_finite'\n msg_template='value is not a valid decimal'\n \n \nclass DecimalMaxDigitsError(PydanticValueError):\n code='decimal.max_digits'\n msg_template='ensure that there are no more than {max_digits} digits in total'\n \n def __init__(self,*,max_digits:int)->None:\n  super().__init__(max_digits=max_digits)\n  \n  \nclass DecimalMaxPlacesError(PydanticValueError):\n code='decimal.max_places'\n msg_template='ensure that there are no more than {decimal_places} decimal places'\n \n def __init__(self,*,decimal_places:int)->None:\n  super().__init__(decimal_places=decimal_places)\n  \n  \nclass DecimalWholeDigitsError(PydanticValueError):\n code='decimal.whole_digits'\n msg_template='ensure that there are no more than {whole_digits} digits before the decimal point'\n \n def __init__(self,*,whole_digits:int)->None:\n  super().__init__(whole_digits=whole_digits)\n  \n  \nclass DateTimeError(PydanticValueError):\n msg_template='invalid datetime format'\n \n \nclass DateError(PydanticValueError):\n msg_template='invalid date format'\n \n \nclass DateNotInThePastError(PydanticValueError):\n code='date.not_in_the_past'\n msg_template='date is not in the past'\n \n \nclass DateNotInTheFutureError(PydanticValueError):\n code='date.not_in_the_future'\n msg_template='date is not in the future'\n \n \nclass TimeError(PydanticValueError):\n msg_template='invalid time format'\n \n \nclass DurationError(PydanticValueError):\n msg_template='invalid duration format'\n \n \nclass HashableError(PydanticTypeError):\n msg_template='value is not a valid hashable'\n \n \nclass UUIDError(PydanticTypeError):\n msg_template='value is not a valid uuid'\n \n \nclass UUIDVersionError(PydanticValueError):\n code='uuid.version'\n msg_template='uuid version {required_version} expected'\n \n def __init__(self,*,required_version:int)->None:\n  super().__init__(required_version=required_version)\n  \n  \nclass ArbitraryTypeError(PydanticTypeError):\n code='arbitrary_type'\n msg_template='instance of {expected_arbitrary_type} expected'\n \n def __init__(self,*,expected_arbitrary_type:Type[Any])->None:\n  super().__init__(expected_arbitrary_type=display_as_type(expected_arbitrary_type))\n  \n  \nclass ClassError(PydanticTypeError):\n code='class'\n msg_template='a class is expected'\n \n \nclass SubclassError(PydanticTypeError):\n code='subclass'\n msg_template='subclass of {expected_class} expected'\n \n def __init__(self,*,expected_class:Type[Any])->None:\n  super().__init__(expected_class=display_as_type(expected_class))\n  \n  \nclass JsonError(PydanticValueError):\n msg_template='Invalid JSON'\n \n \nclass JsonTypeError(PydanticTypeError):\n code='json'\n msg_template='JSON object must be str, bytes or bytearray'\n \n \nclass PatternError(PydanticValueError):\n code='regex_pattern'\n msg_template='Invalid regular expression'\n \n \nclass DataclassTypeError(PydanticTypeError):\n code='dataclass'\n msg_template='instance of {class_name}, tuple or dict expected'\n \n \nclass CallableError(PydanticTypeError):\n msg_template='{value} is not callable'\n \n \nclass EnumError(PydanticTypeError):\n code='enum_instance'\n msg_template='{value} is not a valid Enum instance'\n \n \nclass IntEnumError(PydanticTypeError):\n code='int_enum_instance'\n msg_template='{value} is not a valid IntEnum instance'\n \n \nclass IPvAnyAddressError(PydanticValueError):\n msg_template='value is not a valid IPv4 or IPv6 address'\n \n \nclass IPvAnyInterfaceError(PydanticValueError):\n msg_template='value is not a valid IPv4 or IPv6 interface'\n \n \nclass IPvAnyNetworkError(PydanticValueError):\n msg_template='value is not a valid IPv4 or IPv6 network'\n \n \nclass IPv4AddressError(PydanticValueError):\n msg_template='value is not a valid IPv4 address'\n \n \nclass IPv6AddressError(PydanticValueError):\n msg_template='value is not a valid IPv6 address'\n \n \nclass IPv4NetworkError(PydanticValueError):\n msg_template='value is not a valid IPv4 network'\n \n \nclass IPv6NetworkError(PydanticValueError):\n msg_template='value is not a valid IPv6 network'\n \n \nclass IPv4InterfaceError(PydanticValueError):\n msg_template='value is not a valid IPv4 interface'\n \n \nclass IPv6InterfaceError(PydanticValueError):\n msg_template='value is not a valid IPv6 interface'\n \n \nclass ColorError(PydanticValueError):\n msg_template='value is not a valid color: {reason}'\n \n \nclass StrictBoolError(PydanticValueError):\n msg_template='value is not a valid boolean'\n \n \nclass NotDigitError(PydanticValueError):\n code='payment_card_number.digits'\n msg_template='card number is not all digits'\n \n \nclass LuhnValidationError(PydanticValueError):\n code='payment_card_number.luhn_check'\n msg_template='card number is not luhn valid'\n \n \nclass InvalidLengthForBrand(PydanticValueError):\n code='payment_card_number.invalid_length_for_brand'\n msg_template='Length for a {brand} card must be {required_length}'\n \n \nclass InvalidByteSize(PydanticValueError):\n msg_template='could not parse value and unit from byte string'\n \n \nclass InvalidByteSizeUnit(PydanticValueError):\n msg_template='could not interpret byte unit: {unit}'\n \n \nclass MissingDiscriminator(PydanticValueError):\n code='discriminated_union.missing_discriminator'\n msg_template='Discriminator {discriminator_key!r} is missing in value'\n \n \nclass InvalidDiscriminator(PydanticValueError):\n code='discriminated_union.invalid_discriminator'\n msg_template=(\n 'No match for discriminator {discriminator_key!r} and value {discriminator_value!r} '\n '(allowed values: {allowed_values})'\n )\n \n def __init__(self,*,discriminator_key:str,discriminator_value:Any,allowed_values:Sequence[Any])->None:\n  super().__init__(\n  discriminator_key=discriminator_key,\n  discriminator_value=discriminator_value,\n  allowed_values=', '.join(map(repr,allowed_values)),\n  )\n", ["decimal", "pathlib", "pydantic.v1.typing", "typing"]], "pydantic.v1.typing": [".py", "import sys\nimport typing\nfrom collections.abc import Callable\nfrom os import PathLike\nfrom typing import(\nTYPE_CHECKING,\nAbstractSet,\nAny,\nCallable as TypingCallable,\nClassVar,\nDict,\nForwardRef,\nGenerator,\nIterable,\nList,\nMapping,\nNewType,\nOptional,\nSequence,\nSet,\nTuple,\nType,\nTypeVar,\nUnion,\n_eval_type,\ncast,\nget_type_hints,\n)\n\nfrom typing_extensions import(\nAnnotated,\nFinal,\nLiteral,\nNotRequired as TypedDictNotRequired,\nRequired as TypedDictRequired,\n)\n\ntry:\n from typing import _TypingBase as typing_base\nexcept ImportError:\n from typing import _Final as typing_base\n \ntry:\n from typing import GenericAlias as TypingGenericAlias\nexcept ImportError:\n\n TypingGenericAlias=()\n \ntry:\n from types import UnionType as TypesUnionType\nexcept ImportError:\n\n TypesUnionType=()\n \n \nif sys.version_info <(3,9):\n\n def evaluate_forwardref(type_:ForwardRef,globalns:Any,localns:Any)->Any:\n  return type_._evaluate(globalns,localns)\n  \nelse:\n\n def evaluate_forwardref(type_:ForwardRef,globalns:Any,localns:Any)->Any:\n \n \n \n \n  return cast(Any,type_)._evaluate(globalns,localns,recursive_guard=set())\n  \n  \nif sys.version_info <(3,9):\n\n\n\n get_all_type_hints=get_type_hints\n \nelse:\n\n def get_all_type_hints(obj:Any,globalns:Any=None,localns:Any=None)->Any:\n  return get_type_hints(obj,globalns,localns,include_extras=True)\n  \n  \n_T=TypeVar('_T')\n\nAnyCallable=TypingCallable[...,Any]\nNoArgAnyCallable=TypingCallable[[],Any]\n\n\nAnyArgTCallable=TypingCallable[...,_T]\n\n\n\n\nAnnotatedTypeNames={'AnnotatedMeta','_AnnotatedAlias'}\n\n\nLITERAL_TYPES:Set[Any]={Literal}\nif hasattr(typing,'Literal'):\n LITERAL_TYPES.add(typing.Literal)\n \n \nif sys.version_info <(3,8):\n\n def get_origin(t:Type[Any])->Optional[Type[Any]]:\n  if type(t).__name__ in AnnotatedTypeNames:\n  \n   return cast(Type[Any],Annotated)\n  return getattr(t,'__origin__',None)\n  \nelse:\n from typing import get_origin as _typing_get_origin\n \n def get_origin(tp:Type[Any])->Optional[Type[Any]]:\n  ''\n\n\n\n\n  \n  if type(tp).__name__ in AnnotatedTypeNames:\n   return cast(Type[Any],Annotated)\n  return _typing_get_origin(tp)or getattr(tp,'__origin__',None)\n  \n  \nif sys.version_info <(3,8):\n from typing import _GenericAlias\n \n def get_args(t:Type[Any])->Tuple[Any,...]:\n  ''\n\n\n\n  \n  if type(t).__name__ in AnnotatedTypeNames:\n   return t.__args__+t.__metadata__\n  if isinstance(t,_GenericAlias):\n   res=t.__args__\n   if t.__origin__ is Callable and res and res[0]is not Ellipsis:\n    res=(list(res[:-1]),res[-1])\n   return res\n  return getattr(t,'__args__',())\n  \nelse:\n from typing import get_args as _typing_get_args\n \n def _generic_get_args(tp:Type[Any])->Tuple[Any,...]:\n  ''\n\n\n\n\n  \n  if hasattr(tp,'_nparams'):\n   return(Any,)*tp._nparams\n   \n   \n   \n  try:\n   if tp ==Tuple[()]or sys.version_info >=(3,9)and tp ==tuple[()]:\n    return((),)\n    \n  except TypeError:\n   pass\n  return()\n  \n def get_args(tp:Type[Any])->Tuple[Any,...]:\n  ''\n\n\n\n\n\n\n\n\n  \n  if type(tp).__name__ in AnnotatedTypeNames:\n   return tp.__args__+tp.__metadata__\n   \n  return _typing_get_args(tp)or getattr(tp,'__args__',())or _generic_get_args(tp)\n  \n  \nif sys.version_info <(3,9):\n\n def convert_generics(tp:Type[Any])->Type[Any]:\n  ''\n\n\n\n\n  \n  return tp\n  \nelse:\n from typing import _UnionGenericAlias\n \n from typing_extensions import _AnnotatedAlias\n \n def convert_generics(tp:Type[Any])->Type[Any]:\n  ''\n\n\n\n\n\n\n\n  \n  origin=get_origin(tp)\n  if not origin or not hasattr(tp,'__args__'):\n   return tp\n   \n  args=get_args(tp)\n  \n  \n  if origin is Annotated:\n   return _AnnotatedAlias(convert_generics(args[0]),args[1:])\n   \n   \n  converted=tuple(\n  ForwardRef(arg)if isinstance(arg,str)and isinstance(tp,TypingGenericAlias)else convert_generics(arg)\n  for arg in args\n  )\n  \n  if converted ==args:\n   return tp\n  elif isinstance(tp,TypingGenericAlias):\n   return TypingGenericAlias(origin,converted)\n  elif isinstance(tp,TypesUnionType):\n  \n   return _UnionGenericAlias(origin,converted)\n  else:\n   try:\n    setattr(tp,'__args__',converted)\n   except AttributeError:\n    pass\n   return tp\n   \n   \nif sys.version_info <(3,10):\n\n def is_union(tp:Optional[Type[Any]])->bool:\n  return tp is Union\n  \n WithArgsTypes=(TypingGenericAlias,)\n \nelse:\n import types\n import typing\n \n def is_union(tp:Optional[Type[Any]])->bool:\n  return tp is Union or tp is types.UnionType\n  \n WithArgsTypes=(typing._GenericAlias,types.GenericAlias,types.UnionType)\n \n \nStrPath=Union[str,PathLike]\n\n\nif TYPE_CHECKING:\n from pydantic.v1.fields import ModelField\n \n TupleGenerator=Generator[Tuple[str,Any],None,None]\n DictStrAny=Dict[str,Any]\n DictAny=Dict[Any,Any]\n SetStr=Set[str]\n ListStr=List[str]\n IntStr=Union[int,str]\n AbstractSetIntStr=AbstractSet[IntStr]\n DictIntStrAny=Dict[IntStr,Any]\n MappingIntStrAny=Mapping[IntStr,Any]\n CallableGenerator=Generator[AnyCallable,None,None]\n ReprArgs=Sequence[Tuple[Optional[str],Any]]\n \n MYPY=False\n if MYPY:\n  AnyClassMethod=classmethod[Any]\n else:\n \n  AnyClassMethod=classmethod[Any,Any,Any]\n  \n__all__=(\n'AnyCallable',\n'NoArgAnyCallable',\n'NoneType',\n'is_none_type',\n'display_as_type',\n'resolve_annotations',\n'is_callable_type',\n'is_literal_type',\n'all_literal_values',\n'is_namedtuple',\n'is_typeddict',\n'is_typeddict_special',\n'is_new_type',\n'new_type_supertype',\n'is_classvar',\n'is_finalvar',\n'update_field_forward_refs',\n'update_model_forward_refs',\n'TupleGenerator',\n'DictStrAny',\n'DictAny',\n'SetStr',\n'ListStr',\n'IntStr',\n'AbstractSetIntStr',\n'DictIntStrAny',\n'CallableGenerator',\n'ReprArgs',\n'AnyClassMethod',\n'CallableGenerator',\n'WithArgsTypes',\n'get_args',\n'get_origin',\n'get_sub_types',\n'typing_base',\n'get_all_type_hints',\n'is_union',\n'StrPath',\n'MappingIntStrAny',\n)\n\n\nNoneType=None.__class__\n\n\nNONE_TYPES:Tuple[Any,Any,Any]=(None,NoneType,Literal[None])\n\n\nif sys.version_info <(3,8):\n\n\n\n\n\n\n def is_none_type(type_:Any)->bool:\n  return type_ in NONE_TYPES\n  \nelif sys.version_info[:2]==(3,8):\n\n def is_none_type(type_:Any)->bool:\n  for none_type in NONE_TYPES:\n   if type_ is none_type:\n    return True\n    \n    \n    \n  if is_literal_type(type_):\n   return all_literal_values(type_)==(None,)\n  return False\n  \nelse:\n\n def is_none_type(type_:Any)->bool:\n  return type_ in NONE_TYPES\n  \n  \ndef display_as_type(v:Type[Any])->str:\n if not isinstance(v,typing_base)and not isinstance(v,WithArgsTypes)and not isinstance(v,type):\n  v=v.__class__\n  \n if is_union(get_origin(v)):\n  return f'Union[{\", \".join(map(display_as_type,get_args(v)))}]'\n  \n if isinstance(v,WithArgsTypes):\n \n  return str(v).replace('typing.','')\n  \n try:\n  return v.__name__\n except AttributeError:\n \n  return str(v).replace('typing.','')\n  \n  \ndef resolve_annotations(raw_annotations:Dict[str,Type[Any]],module_name:Optional[str])->Dict[str,Type[Any]]:\n ''\n\n\n\n \n base_globals:Optional[Dict[str,Any]]=None\n if module_name:\n  try:\n   module=sys.modules[module_name]\n  except KeyError:\n  \n   pass\n  else:\n   base_globals=module.__dict__\n   \n annotations={}\n for name,value in raw_annotations.items():\n  if isinstance(value,str):\n   if(3,10)>sys.version_info >=(3,9,8)or sys.version_info >=(3,10,1):\n    value=ForwardRef(value,is_argument=False,is_class=True)\n   else:\n    value=ForwardRef(value,is_argument=False)\n  try:\n   if sys.version_info >=(3,13):\n    value=_eval_type(value,base_globals,None,type_params=())\n   else:\n    value=_eval_type(value,base_globals,None)\n  except NameError:\n  \n   pass\n  annotations[name]=value\n return annotations\n \n \ndef is_callable_type(type_:Type[Any])->bool:\n return type_ is Callable or get_origin(type_)is Callable\n \n \ndef is_literal_type(type_:Type[Any])->bool:\n return Literal is not None and get_origin(type_)in LITERAL_TYPES\n \n \ndef literal_values(type_:Type[Any])->Tuple[Any,...]:\n return get_args(type_)\n \n \ndef all_literal_values(type_:Type[Any])->Tuple[Any,...]:\n ''\n\n\n\n \n if not is_literal_type(type_):\n  return(type_,)\n  \n values=literal_values(type_)\n return tuple(x for value in values for x in all_literal_values(value))\n \n \ndef is_namedtuple(type_:Type[Any])->bool:\n ''\n\n\n \n from pydantic.v1.utils import lenient_issubclass\n \n return lenient_issubclass(type_,tuple)and hasattr(type_,'_fields')\n \n \ndef is_typeddict(type_:Type[Any])->bool:\n ''\n\n\n \n from pydantic.v1.utils import lenient_issubclass\n \n return lenient_issubclass(type_,dict)and hasattr(type_,'__total__')\n \n \ndef _check_typeddict_special(type_:Any)->bool:\n return type_ is TypedDictRequired or type_ is TypedDictNotRequired\n \n \ndef is_typeddict_special(type_:Any)->bool:\n ''\n\n \n return _check_typeddict_special(type_)or _check_typeddict_special(get_origin(type_))\n \n \ntest_type=NewType('test_type',str)\n\n\ndef is_new_type(type_:Type[Any])->bool:\n ''\n\n \n return isinstance(type_,test_type.__class__)and hasattr(type_,'__supertype__')\n \n \ndef new_type_supertype(type_:Type[Any])->Type[Any]:\n while hasattr(type_,'__supertype__'):\n  type_=type_.__supertype__\n return type_\n \n \ndef _check_classvar(v:Optional[Type[Any]])->bool:\n if v is None:\n  return False\n  \n return v.__class__ ==ClassVar.__class__ and getattr(v,'_name',None)=='ClassVar'\n \n \ndef _check_finalvar(v:Optional[Type[Any]])->bool:\n ''\n\n \n if v is None:\n  return False\n  \n return v.__class__ ==Final.__class__ and(sys.version_info <(3,8)or getattr(v,'_name',None)=='Final')\n \n \ndef is_classvar(ann_type:Type[Any])->bool:\n if _check_classvar(ann_type)or _check_classvar(get_origin(ann_type)):\n  return True\n  \n  \n  \n if ann_type.__class__ ==ForwardRef and ann_type.__forward_arg__.startswith('ClassVar['):\n  return True\n  \n return False\n \n \ndef is_finalvar(ann_type:Type[Any])->bool:\n return _check_finalvar(ann_type)or _check_finalvar(get_origin(ann_type))\n \n \ndef update_field_forward_refs(field:'ModelField',globalns:Any,localns:Any)->None:\n ''\n\n \n prepare=False\n if field.type_.__class__ ==ForwardRef:\n  prepare=True\n  field.type_=evaluate_forwardref(field.type_,globalns,localns or None)\n if field.outer_type_.__class__ ==ForwardRef:\n  prepare=True\n  field.outer_type_=evaluate_forwardref(field.outer_type_,globalns,localns or None)\n if prepare:\n  field.prepare()\n  \n if field.sub_fields:\n  for sub_f in field.sub_fields:\n   update_field_forward_refs(sub_f,globalns=globalns,localns=localns)\n   \n if field.discriminator_key is not None:\n  field.prepare_discriminated_union_sub_fields()\n  \n  \ndef update_model_forward_refs(\nmodel:Type[Any],\nfields:Iterable['ModelField'],\njson_encoders:Dict[Union[Type[Any],str,ForwardRef],AnyCallable],\nlocalns:'DictStrAny',\nexc_to_suppress:Tuple[Type[BaseException],...]=(),\n)->None:\n ''\n\n \n if model.__module__ in sys.modules:\n  globalns=sys.modules[model.__module__].__dict__.copy()\n else:\n  globalns={}\n  \n globalns.setdefault(model.__name__,model)\n \n for f in fields:\n  try:\n   update_field_forward_refs(f,globalns=globalns,localns=localns)\n  except exc_to_suppress:\n   pass\n   \n for key in set(json_encoders.keys()):\n  if isinstance(key,str):\n   fr:ForwardRef=ForwardRef(key)\n  elif isinstance(key,ForwardRef):\n   fr=key\n  else:\n   continue\n   \n  try:\n   new_key=evaluate_forwardref(fr,globalns,localns or None)\n  except exc_to_suppress:\n   continue\n   \n  json_encoders[new_key]=json_encoders.pop(key)\n  \n  \ndef get_class(type_:Type[Any])->Union[None,bool,Type[Any]]:\n ''\n\n\n \n if type_ is type:\n  return True\n  \n if get_origin(type_)is None:\n  return None\n  \n args=get_args(type_)\n if not args or not isinstance(args[0],type):\n  return True\n else:\n  return args[0]\n  \n  \ndef get_sub_types(tp:Any)->List[Any]:\n ''\n\n\n \n origin=get_origin(tp)\n if origin is Annotated:\n  return get_sub_types(get_args(tp)[0])\n elif is_union(origin):\n  return[x for t in get_args(tp)for x in get_sub_types(t)]\n else:\n  return[tp]\n", ["collections.abc", "os", "pydantic.v1.fields", "pydantic.v1.utils", "sys", "types", "typing", "typing_extensions"]], "pydantic.v1._hypothesis_plugin": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport contextlib\nimport datetime\nimport ipaddress\nimport json\nimport math\nfrom fractions import Fraction\nfrom typing import Callable,Dict,Type,Union,cast,overload\n\nimport hypothesis.strategies as st\n\nimport pydantic\nimport pydantic.color\nimport pydantic.types\nfrom pydantic.v1.utils import lenient_issubclass\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry:\n import email_validator\nexcept ImportError:\n pass\nelse:\n\n def is_valid_email(s:str)->bool:\n \n \n  try:\n   email_validator.validate_email(s,check_deliverability=False)\n   return True\n  except email_validator.EmailNotValidError:\n   return False\n   \n   \n   \n st.register_type_strategy(pydantic.EmailStr,st.emails().filter(is_valid_email))\n st.register_type_strategy(\n pydantic.NameEmail,\n st.builds(\n '{} <{}>'.format,\n st.from_regex('[A-Za-z0-9_]+( [A-Za-z0-9_]+){0,5}',fullmatch=True),\n st.emails().filter(is_valid_email),\n ),\n )\n \n \nst.register_type_strategy(\npydantic.PyObject,\nst.sampled_from(\n[cast(pydantic.PyObject,f'math.{name}')for name in sorted(vars(math))if not name.startswith('_')]\n),\n)\n\n\n_color_regexes=(\n'|'.join(\n(\npydantic.color.r_hex_short,\npydantic.color.r_hex_long,\npydantic.color.r_rgb,\npydantic.color.r_rgba,\npydantic.color.r_hsl,\npydantic.color.r_hsla,\n)\n)\n\n.replace(pydantic.color._r_sl,r'(?:(\\d\\d?(?:\\.\\d+)?|100(?:\\.0+)?)%)')\n.replace(pydantic.color._r_alpha,r'(?:(0(?:\\.\\d+)?|1(?:\\.0+)?|\\.\\d+|\\d{1,2}%))')\n.replace(pydantic.color._r_255,r'(?:((?:\\d|\\d\\d|[01]\\d\\d|2[0-4]\\d|25[0-4])(?:\\.\\d+)?|255(?:\\.0+)?))')\n)\nst.register_type_strategy(\npydantic.color.Color,\nst.one_of(\nst.sampled_from(sorted(pydantic.color.COLORS_BY_NAME)),\nst.tuples(\nst.integers(0,255),\nst.integers(0,255),\nst.integers(0,255),\nst.none()|st.floats(0,1)|st.floats(0,100).map('{}%'.format),\n),\nst.from_regex(_color_regexes,fullmatch=True),\n),\n)\n\n\n\n\n\ndef add_luhn_digit(card_number:str)->str:\n\n for digit in '0123456789':\n  with contextlib.suppress(Exception):\n   pydantic.PaymentCardNumber.validate_luhn_check_digit(card_number+digit)\n   return card_number+digit\n raise AssertionError('Unreachable')\n \n \ncard_patterns=(\n\n'4[0-9]{14}',\n'5[12345][0-9]{13}',\n'3[47][0-9]{12}',\n'[0-26-9][0-9]{10,17}',\n)\nst.register_type_strategy(\npydantic.PaymentCardNumber,\nst.from_regex('|'.join(card_patterns),fullmatch=True).map(add_luhn_digit),\n)\n\n\nst.register_type_strategy(pydantic.UUID1,st.uuids(version=1))\nst.register_type_strategy(pydantic.UUID3,st.uuids(version=3))\nst.register_type_strategy(pydantic.UUID4,st.uuids(version=4))\nst.register_type_strategy(pydantic.UUID5,st.uuids(version=5))\n\n\nst.register_type_strategy(pydantic.SecretBytes,st.binary().map(pydantic.SecretBytes))\nst.register_type_strategy(pydantic.SecretStr,st.text().map(pydantic.SecretStr))\n\n\nst.register_type_strategy(pydantic.IPvAnyAddress,st.ip_addresses())\nst.register_type_strategy(\npydantic.IPvAnyInterface,\nst.from_type(ipaddress.IPv4Interface)|st.from_type(ipaddress.IPv6Interface),\n)\nst.register_type_strategy(\npydantic.IPvAnyNetwork,\nst.from_type(ipaddress.IPv4Network)|st.from_type(ipaddress.IPv6Network),\n)\n\n\n\n\nst.register_type_strategy(pydantic.StrictBool,st.booleans())\nst.register_type_strategy(pydantic.StrictStr,st.text())\n\n\n\nst.register_type_strategy(pydantic.FutureDate,st.dates(min_value=datetime.date.today()+datetime.timedelta(days=1)))\nst.register_type_strategy(pydantic.PastDate,st.dates(max_value=datetime.date.today()-datetime.timedelta(days=1)))\n\n\n\n\n\n\n\nRESOLVERS:Dict[type,Callable[[type],st.SearchStrategy]]={}\n\n\n@overload\ndef _registered(typ:Type[pydantic.types.T])->Type[pydantic.types.T]:\n pass\n \n \n@overload\ndef _registered(typ:pydantic.types.ConstrainedNumberMeta)->pydantic.types.ConstrainedNumberMeta:\n pass\n \n \ndef _registered(\ntyp:Union[Type[pydantic.types.T],pydantic.types.ConstrainedNumberMeta]\n)->Union[Type[pydantic.types.T],pydantic.types.ConstrainedNumberMeta]:\n\n\n\n pydantic.types._DEFINED_TYPES.add(typ)\n for supertype,resolver in RESOLVERS.items():\n  if issubclass(typ,supertype):\n   st.register_type_strategy(typ,resolver(typ))\n   return typ\n raise NotImplementedError(f'Unknown type {typ !r} has no resolver to register')\n \n \ndef resolves(\ntyp:Union[type,pydantic.types.ConstrainedNumberMeta]\n)->Callable[[Callable[...,st.SearchStrategy]],Callable[...,st.SearchStrategy]]:\n def inner(f):\n  assert f not in RESOLVERS\n  RESOLVERS[typ]=f\n  return f\n  \n return inner\n \n \n \n \n \n@resolves(pydantic.JsonWrapper)\ndef resolve_json(cls):\n try:\n  inner=st.none()if cls.inner_type is None else st.from_type(cls.inner_type)\n except Exception:\n  finite=st.floats(allow_infinity=False,allow_nan=False)\n  inner=st.recursive(\n  base=st.one_of(st.none(),st.booleans(),st.integers(),finite,st.text()),\n  extend=lambda x:st.lists(x)|st.dictionaries(st.text(),x),\n  )\n inner_type=getattr(cls,'inner_type',None)\n return st.builds(\n cls.inner_type.json if lenient_issubclass(inner_type,pydantic.BaseModel)else json.dumps,\n inner,\n ensure_ascii=st.booleans(),\n indent=st.none()|st.integers(0,16),\n sort_keys=st.booleans(),\n )\n \n \n@resolves(pydantic.ConstrainedBytes)\ndef resolve_conbytes(cls):\n min_size=cls.min_length or 0\n max_size=cls.max_length\n if not cls.strip_whitespace:\n  return st.binary(min_size=min_size,max_size=max_size)\n  \n repeats='{{{},{}}}'.format(\n min_size -2 if min_size >2 else 0,\n max_size -2 if(max_size or 0)>2 else '',\n )\n if min_size >=2:\n  pattern=rf'\\W.{repeats}\\W'\n elif min_size ==1:\n  pattern=rf'\\W(.{repeats}\\W)?'\n else:\n  assert min_size ==0\n  pattern=rf'(\\W(.{repeats}\\W)?)?'\n return st.from_regex(pattern.encode(),fullmatch=True)\n \n \n@resolves(pydantic.ConstrainedDecimal)\ndef resolve_condecimal(cls):\n min_value=cls.ge\n max_value=cls.le\n if cls.gt is not None:\n  assert min_value is None,'Set `gt` or `ge`, but not both'\n  min_value=cls.gt\n if cls.lt is not None:\n  assert max_value is None,'Set `lt` or `le`, but not both'\n  max_value=cls.lt\n s=st.decimals(min_value,max_value,allow_nan=False,places=cls.decimal_places)\n if cls.lt is not None:\n  s=s.filter(lambda d:d <cls.lt)\n if cls.gt is not None:\n  s=s.filter(lambda d:cls.gt <d)\n return s\n \n \n@resolves(pydantic.ConstrainedFloat)\ndef resolve_confloat(cls):\n min_value=cls.ge\n max_value=cls.le\n exclude_min=False\n exclude_max=False\n \n if cls.gt is not None:\n  assert min_value is None,'Set `gt` or `ge`, but not both'\n  min_value=cls.gt\n  exclude_min=True\n if cls.lt is not None:\n  assert max_value is None,'Set `lt` or `le`, but not both'\n  max_value=cls.lt\n  exclude_max=True\n  \n if cls.multiple_of is None:\n  return st.floats(min_value,max_value,exclude_min=exclude_min,exclude_max=exclude_max,allow_nan=False)\n  \n if min_value is not None:\n  min_value=math.ceil(min_value /cls.multiple_of)\n  if exclude_min:\n   min_value=min_value+1\n if max_value is not None:\n  assert max_value >=cls.multiple_of,'Cannot build model with max value smaller than multiple of'\n  max_value=math.floor(max_value /cls.multiple_of)\n  if exclude_max:\n   max_value=max_value -1\n   \n return st.integers(min_value,max_value).map(lambda x:x *cls.multiple_of)\n \n \n@resolves(pydantic.ConstrainedInt)\ndef resolve_conint(cls):\n min_value=cls.ge\n max_value=cls.le\n if cls.gt is not None:\n  assert min_value is None,'Set `gt` or `ge`, but not both'\n  min_value=cls.gt+1\n if cls.lt is not None:\n  assert max_value is None,'Set `lt` or `le`, but not both'\n  max_value=cls.lt -1\n  \n if cls.multiple_of is None or cls.multiple_of ==1:\n  return st.integers(min_value,max_value)\n  \n  \n  \n if min_value is not None:\n  min_value=math.ceil(Fraction(min_value)/Fraction(cls.multiple_of))\n if max_value is not None:\n  max_value=math.floor(Fraction(max_value)/Fraction(cls.multiple_of))\n return st.integers(min_value,max_value).map(lambda x:x *cls.multiple_of)\n \n \n@resolves(pydantic.ConstrainedDate)\ndef resolve_condate(cls):\n if cls.ge is not None:\n  assert cls.gt is None,'Set `gt` or `ge`, but not both'\n  min_value=cls.ge\n elif cls.gt is not None:\n  min_value=cls.gt+datetime.timedelta(days=1)\n else:\n  min_value=datetime.date.min\n if cls.le is not None:\n  assert cls.lt is None,'Set `lt` or `le`, but not both'\n  max_value=cls.le\n elif cls.lt is not None:\n  max_value=cls.lt -datetime.timedelta(days=1)\n else:\n  max_value=datetime.date.max\n return st.dates(min_value,max_value)\n \n \n@resolves(pydantic.ConstrainedStr)\ndef resolve_constr(cls):\n min_size=cls.min_length or 0\n max_size=cls.max_length\n \n if cls.regex is None and not cls.strip_whitespace:\n  return st.text(min_size=min_size,max_size=max_size)\n  \n if cls.regex is not None:\n  strategy=st.from_regex(cls.regex)\n  if cls.strip_whitespace:\n   strategy=strategy.filter(lambda s:s ==s.strip())\n elif cls.strip_whitespace:\n  repeats='{{{},{}}}'.format(\n  min_size -2 if min_size >2 else 0,\n  max_size -2 if(max_size or 0)>2 else '',\n  )\n  if min_size >=2:\n   strategy=st.from_regex(rf'\\W.{repeats}\\W')\n  elif min_size ==1:\n   strategy=st.from_regex(rf'\\W(.{repeats}\\W)?')\n  else:\n   assert min_size ==0\n   strategy=st.from_regex(rf'(\\W(.{repeats}\\W)?)?')\n   \n if min_size ==0 and max_size is None:\n  return strategy\n elif max_size is None:\n  return strategy.filter(lambda s:min_size <=len(s))\n return strategy.filter(lambda s:min_size <=len(s)<=max_size)\n \n \n \nfor typ in list(pydantic.types._DEFINED_TYPES):\n _registered(typ)\npydantic.types._registered=_registered\nst.register_type_strategy(pydantic.Json,resolve_json)\n", ["contextlib", "datetime", "email_validator", "fractions", "hypothesis.strategies", "ipaddress", "json", "math", "pydantic", "pydantic.color", "pydantic.types", "pydantic.v1.utils", "typing"]], "pydantic.v1.annotated_types": [".py", "import sys\nfrom typing import TYPE_CHECKING,Any,Dict,FrozenSet,NamedTuple,Type\n\nfrom pydantic.v1.fields import Required\nfrom pydantic.v1.main import BaseModel,create_model\nfrom pydantic.v1.typing import is_typeddict,is_typeddict_special\n\nif TYPE_CHECKING:\n from typing_extensions import TypedDict\n \nif sys.version_info <(3,11):\n\n def is_legacy_typeddict(typeddict_cls:Type['TypedDict'])->bool:\n  return is_typeddict(typeddict_cls)and type(typeddict_cls).__module__ =='typing'\n  \nelse:\n\n def is_legacy_typeddict(_:Any)->Any:\n  return False\n  \n  \ndef create_model_from_typeddict(\n\ntypeddict_cls:Type['TypedDict'],\n**kwargs:Any,\n)->Type['BaseModel']:\n ''\n\n\n\n \n field_definitions:Dict[str,Any]\n \n \n if not hasattr(typeddict_cls,'__required_keys__'):\n  raise TypeError(\n  'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.9.2. '\n  'Without it, there is no way to differentiate required and optional fields when subclassed.'\n  )\n  \n if is_legacy_typeddict(typeddict_cls)and any(\n is_typeddict_special(t)for t in typeddict_cls.__annotations__.values()\n ):\n  raise TypeError(\n  'You should use `typing_extensions.TypedDict` instead of `typing.TypedDict` with Python < 3.11. '\n  'Without it, there is no way to reflect Required/NotRequired keys.'\n  )\n  \n required_keys:FrozenSet[str]=typeddict_cls.__required_keys__\n field_definitions={\n field_name:(field_type,Required if field_name in required_keys else None)\n for field_name,field_type in typeddict_cls.__annotations__.items()\n }\n \n return create_model(typeddict_cls.__name__,**kwargs,**field_definitions)\n \n \ndef create_model_from_namedtuple(namedtuple_cls:Type['NamedTuple'],**kwargs:Any)->Type['BaseModel']:\n ''\n\n\n\n\n \n \n namedtuple_annotations:Dict[str,Type[Any]]=getattr(namedtuple_cls,'__annotations__',None)or{\n k:Any for k in namedtuple_cls._fields\n }\n field_definitions:Dict[str,Any]={\n field_name:(field_type,Required)for field_name,field_type in namedtuple_annotations.items()\n }\n return create_model(namedtuple_cls.__name__,**kwargs,**field_definitions)\n", ["pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.typing", "sys", "typing", "typing_extensions"]], "pydantic.v1.color": [".py", "''\n\n\n\n\n\n\n\n\nimport math\nimport re\nfrom colorsys import hls_to_rgb,rgb_to_hls\nfrom typing import TYPE_CHECKING,Any,Dict,Optional,Tuple,Union,cast\n\nfrom pydantic.v1.errors import ColorError\nfrom pydantic.v1.utils import Representation,almost_equal_floats\n\nif TYPE_CHECKING:\n from pydantic.v1.typing import CallableGenerator,ReprArgs\n \nColorTuple=Union[Tuple[int,int,int],Tuple[int,int,int,float]]\nColorType=Union[ColorTuple,str]\nHslColorTuple=Union[Tuple[float,float,float],Tuple[float,float,float,float]]\n\n\nclass RGBA:\n ''\n\n \n \n __slots__='r','g','b','alpha','_tuple'\n \n def __init__(self,r:float,g:float,b:float,alpha:Optional[float]):\n  self.r=r\n  self.g=g\n  self.b=b\n  self.alpha=alpha\n  \n  self._tuple:Tuple[float,float,float,Optional[float]]=(r,g,b,alpha)\n  \n def __getitem__(self,item:Any)->Any:\n  return self._tuple[item]\n  \n  \n  \nr_hex_short=r'\\s*(?:#|0x)?([0-9a-f])([0-9a-f])([0-9a-f])([0-9a-f])?\\s*'\nr_hex_long=r'\\s*(?:#|0x)?([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})?\\s*'\n_r_255=r'(\\d{1,3}(?:\\.\\d+)?)'\n_r_comma=r'\\s*,\\s*'\nr_rgb=fr'\\s*rgb\\(\\s*{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_255}\\)\\s*'\n_r_alpha=r'(\\d(?:\\.\\d+)?|\\.\\d+|\\d{1,2}%)'\nr_rgba=fr'\\s*rgba\\(\\s*{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_255}{_r_comma}{_r_alpha}\\s*\\)\\s*'\n_r_h=r'(-?\\d+(?:\\.\\d+)?|-?\\.\\d+)(deg|rad|turn)?'\n_r_sl=r'(\\d{1,3}(?:\\.\\d+)?)%'\nr_hsl=fr'\\s*hsl\\(\\s*{_r_h}{_r_comma}{_r_sl}{_r_comma}{_r_sl}\\s*\\)\\s*'\nr_hsla=fr'\\s*hsl\\(\\s*{_r_h}{_r_comma}{_r_sl}{_r_comma}{_r_sl}{_r_comma}{_r_alpha}\\s*\\)\\s*'\n\n\nrepeat_colors={int(c *2,16)for c in '0123456789abcdef'}\nrads=2 *math.pi\n\n\nclass Color(Representation):\n __slots__='_original','_rgba'\n \n def __init__(self,value:ColorType)->None:\n  self._rgba:RGBA\n  self._original:ColorType\n  if isinstance(value,(tuple,list)):\n   self._rgba=parse_tuple(value)\n  elif isinstance(value,str):\n   self._rgba=parse_str(value)\n  elif isinstance(value,Color):\n   self._rgba=value._rgba\n   value=value._original\n  else:\n   raise ColorError(reason='value must be a tuple, list or string')\n   \n   \n  self._original=value\n  \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  field_schema.update(type='string',format='color')\n  \n def original(self)->ColorType:\n  ''\n\n  \n  return self._original\n  \n def as_named(self,*,fallback:bool=False)->str:\n  if self._rgba.alpha is None:\n   rgb=cast(Tuple[int,int,int],self.as_rgb_tuple())\n   try:\n    return COLORS_BY_VALUE[rgb]\n   except KeyError as e:\n    if fallback:\n     return self.as_hex()\n    else:\n     raise ValueError('no named color found, use fallback=True, as_hex() or as_rgb()')from e\n  else:\n   return self.as_hex()\n   \n def as_hex(self)->str:\n  ''\n\n\n  \n  values=[float_to_255(c)for c in self._rgba[:3]]\n  if self._rgba.alpha is not None:\n   values.append(float_to_255(self._rgba.alpha))\n   \n  as_hex=''.join(f'{v:02x}'for v in values)\n  if all(c in repeat_colors for c in values):\n   as_hex=''.join(as_hex[c]for c in range(0,len(as_hex),2))\n  return '#'+as_hex\n  \n def as_rgb(self)->str:\n  ''\n\n  \n  if self._rgba.alpha is None:\n   return f'rgb({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)})'\n  else:\n   return(\n   f'rgba({float_to_255(self._rgba.r)}, {float_to_255(self._rgba.g)}, {float_to_255(self._rgba.b)}, '\n   f'{round(self._alpha_float(),2)})'\n   )\n   \n def as_rgb_tuple(self,*,alpha:Optional[bool]=None)->ColorTuple:\n  ''\n\n\n\n\n\n\n\n  \n  r,g,b=(float_to_255(c)for c in self._rgba[:3])\n  if alpha is None:\n   if self._rgba.alpha is None:\n    return r,g,b\n   else:\n    return r,g,b,self._alpha_float()\n  elif alpha:\n   return r,g,b,self._alpha_float()\n  else:\n  \n   return r,g,b\n   \n def as_hsl(self)->str:\n  ''\n\n  \n  if self._rgba.alpha is None:\n   h,s,li=self.as_hsl_tuple(alpha=False)\n   return f'hsl({h *360:0.0f}, {s:0.0%}, {li:0.0%})'\n  else:\n   h,s,li,a=self.as_hsl_tuple(alpha=True)\n   return f'hsl({h *360:0.0f}, {s:0.0%}, {li:0.0%}, {round(a,2)})'\n   \n def as_hsl_tuple(self,*,alpha:Optional[bool]=None)->HslColorTuple:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  h,l,s=rgb_to_hls(self._rgba.r,self._rgba.g,self._rgba.b)\n  if alpha is None:\n   if self._rgba.alpha is None:\n    return h,s,l\n   else:\n    return h,s,l,self._alpha_float()\n  if alpha:\n   return h,s,l,self._alpha_float()\n  else:\n  \n   return h,s,l\n   \n def _alpha_float(self)->float:\n  return 1 if self._rgba.alpha is None else self._rgba.alpha\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls\n  \n def __str__(self)->str:\n  return self.as_named(fallback=True)\n  \n def __repr_args__(self)->'ReprArgs':\n  return[(None,self.as_named(fallback=True))]+[('rgb',self.as_rgb_tuple())]\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,Color)and self.as_rgb_tuple()==other.as_rgb_tuple()\n  \n def __hash__(self)->int:\n  return hash(self.as_rgb_tuple())\n  \n  \ndef parse_tuple(value:Tuple[Any,...])->RGBA:\n ''\n\n \n if len(value)==3:\n  r,g,b=(parse_color_value(v)for v in value)\n  return RGBA(r,g,b,None)\n elif len(value)==4:\n  r,g,b=(parse_color_value(v)for v in value[:3])\n  return RGBA(r,g,b,parse_float_alpha(value[3]))\n else:\n  raise ColorError(reason='tuples must have length 3 or 4')\n  \n  \ndef parse_str(value:str)->RGBA:\n ''\n\n\n\n\n\n\n \n value_lower=value.lower()\n try:\n  r,g,b=COLORS_BY_NAME[value_lower]\n except KeyError:\n  pass\n else:\n  return ints_to_rgba(r,g,b,None)\n  \n m=re.fullmatch(r_hex_short,value_lower)\n if m:\n  *rgb,a=m.groups()\n  r,g,b=(int(v *2,16)for v in rgb)\n  if a:\n   alpha:Optional[float]=int(a *2,16)/255\n  else:\n   alpha=None\n  return ints_to_rgba(r,g,b,alpha)\n  \n m=re.fullmatch(r_hex_long,value_lower)\n if m:\n  *rgb,a=m.groups()\n  r,g,b=(int(v,16)for v in rgb)\n  if a:\n   alpha=int(a,16)/255\n  else:\n   alpha=None\n  return ints_to_rgba(r,g,b,alpha)\n  \n m=re.fullmatch(r_rgb,value_lower)\n if m:\n  return ints_to_rgba(*m.groups(),None)\n  \n m=re.fullmatch(r_rgba,value_lower)\n if m:\n  return ints_to_rgba(*m.groups())\n  \n m=re.fullmatch(r_hsl,value_lower)\n if m:\n  h,h_units,s,l_=m.groups()\n  return parse_hsl(h,h_units,s,l_)\n  \n m=re.fullmatch(r_hsla,value_lower)\n if m:\n  h,h_units,s,l_,a=m.groups()\n  return parse_hsl(h,h_units,s,l_,parse_float_alpha(a))\n  \n raise ColorError(reason='string not recognised as a valid color')\n \n \ndef ints_to_rgba(r:Union[int,str],g:Union[int,str],b:Union[int,str],alpha:Optional[float])->RGBA:\n return RGBA(parse_color_value(r),parse_color_value(g),parse_color_value(b),parse_float_alpha(alpha))\n \n \ndef parse_color_value(value:Union[int,str],max_val:int=255)->float:\n ''\n\n\n \n try:\n  color=float(value)\n except ValueError:\n  raise ColorError(reason='color values must be a valid number')\n if 0 <=color <=max_val:\n  return color /max_val\n else:\n  raise ColorError(reason=f'color values must be in the range 0 to {max_val}')\n  \n  \ndef parse_float_alpha(value:Union[None,str,float,int])->Optional[float]:\n ''\n\n \n if value is None:\n  return None\n try:\n  if isinstance(value,str)and value.endswith('%'):\n   alpha=float(value[:-1])/100\n  else:\n   alpha=float(value)\n except ValueError:\n  raise ColorError(reason='alpha values must be a valid float')\n  \n if almost_equal_floats(alpha,1):\n  return None\n elif 0 <=alpha <=1:\n  return alpha\n else:\n  raise ColorError(reason='alpha values must be in the range 0 to 1')\n  \n  \ndef parse_hsl(h:str,h_units:str,sat:str,light:str,alpha:Optional[float]=None)->RGBA:\n ''\n\n \n s_value,l_value=parse_color_value(sat,100),parse_color_value(light,100)\n \n h_value=float(h)\n if h_units in{None,'deg'}:\n  h_value=h_value %360 /360\n elif h_units =='rad':\n  h_value=h_value %rads /rads\n else:\n \n  h_value=h_value %1\n  \n r,g,b=hls_to_rgb(h_value,l_value,s_value)\n return RGBA(r,g,b,alpha)\n \n \ndef float_to_255(c:float)->int:\n return int(round(c *255))\n \n \nCOLORS_BY_NAME={\n'aliceblue':(240,248,255),\n'antiquewhite':(250,235,215),\n'aqua':(0,255,255),\n'aquamarine':(127,255,212),\n'azure':(240,255,255),\n'beige':(245,245,220),\n'bisque':(255,228,196),\n'black':(0,0,0),\n'blanchedalmond':(255,235,205),\n'blue':(0,0,255),\n'blueviolet':(138,43,226),\n'brown':(165,42,42),\n'burlywood':(222,184,135),\n'cadetblue':(95,158,160),\n'chartreuse':(127,255,0),\n'chocolate':(210,105,30),\n'coral':(255,127,80),\n'cornflowerblue':(100,149,237),\n'cornsilk':(255,248,220),\n'crimson':(220,20,60),\n'cyan':(0,255,255),\n'darkblue':(0,0,139),\n'darkcyan':(0,139,139),\n'darkgoldenrod':(184,134,11),\n'darkgray':(169,169,169),\n'darkgreen':(0,100,0),\n'darkgrey':(169,169,169),\n'darkkhaki':(189,183,107),\n'darkmagenta':(139,0,139),\n'darkolivegreen':(85,107,47),\n'darkorange':(255,140,0),\n'darkorchid':(153,50,204),\n'darkred':(139,0,0),\n'darksalmon':(233,150,122),\n'darkseagreen':(143,188,143),\n'darkslateblue':(72,61,139),\n'darkslategray':(47,79,79),\n'darkslategrey':(47,79,79),\n'darkturquoise':(0,206,209),\n'darkviolet':(148,0,211),\n'deeppink':(255,20,147),\n'deepskyblue':(0,191,255),\n'dimgray':(105,105,105),\n'dimgrey':(105,105,105),\n'dodgerblue':(30,144,255),\n'firebrick':(178,34,34),\n'floralwhite':(255,250,240),\n'forestgreen':(34,139,34),\n'fuchsia':(255,0,255),\n'gainsboro':(220,220,220),\n'ghostwhite':(248,248,255),\n'gold':(255,215,0),\n'goldenrod':(218,165,32),\n'gray':(128,128,128),\n'green':(0,128,0),\n'greenyellow':(173,255,47),\n'grey':(128,128,128),\n'honeydew':(240,255,240),\n'hotpink':(255,105,180),\n'indianred':(205,92,92),\n'indigo':(75,0,130),\n'ivory':(255,255,240),\n'khaki':(240,230,140),\n'lavender':(230,230,250),\n'lavenderblush':(255,240,245),\n'lawngreen':(124,252,0),\n'lemonchiffon':(255,250,205),\n'lightblue':(173,216,230),\n'lightcoral':(240,128,128),\n'lightcyan':(224,255,255),\n'lightgoldenrodyellow':(250,250,210),\n'lightgray':(211,211,211),\n'lightgreen':(144,238,144),\n'lightgrey':(211,211,211),\n'lightpink':(255,182,193),\n'lightsalmon':(255,160,122),\n'lightseagreen':(32,178,170),\n'lightskyblue':(135,206,250),\n'lightslategray':(119,136,153),\n'lightslategrey':(119,136,153),\n'lightsteelblue':(176,196,222),\n'lightyellow':(255,255,224),\n'lime':(0,255,0),\n'limegreen':(50,205,50),\n'linen':(250,240,230),\n'magenta':(255,0,255),\n'maroon':(128,0,0),\n'mediumaquamarine':(102,205,170),\n'mediumblue':(0,0,205),\n'mediumorchid':(186,85,211),\n'mediumpurple':(147,112,219),\n'mediumseagreen':(60,179,113),\n'mediumslateblue':(123,104,238),\n'mediumspringgreen':(0,250,154),\n'mediumturquoise':(72,209,204),\n'mediumvioletred':(199,21,133),\n'midnightblue':(25,25,112),\n'mintcream':(245,255,250),\n'mistyrose':(255,228,225),\n'moccasin':(255,228,181),\n'navajowhite':(255,222,173),\n'navy':(0,0,128),\n'oldlace':(253,245,230),\n'olive':(128,128,0),\n'olivedrab':(107,142,35),\n'orange':(255,165,0),\n'orangered':(255,69,0),\n'orchid':(218,112,214),\n'palegoldenrod':(238,232,170),\n'palegreen':(152,251,152),\n'paleturquoise':(175,238,238),\n'palevioletred':(219,112,147),\n'papayawhip':(255,239,213),\n'peachpuff':(255,218,185),\n'peru':(205,133,63),\n'pink':(255,192,203),\n'plum':(221,160,221),\n'powderblue':(176,224,230),\n'purple':(128,0,128),\n'red':(255,0,0),\n'rosybrown':(188,143,143),\n'royalblue':(65,105,225),\n'saddlebrown':(139,69,19),\n'salmon':(250,128,114),\n'sandybrown':(244,164,96),\n'seagreen':(46,139,87),\n'seashell':(255,245,238),\n'sienna':(160,82,45),\n'silver':(192,192,192),\n'skyblue':(135,206,235),\n'slateblue':(106,90,205),\n'slategray':(112,128,144),\n'slategrey':(112,128,144),\n'snow':(255,250,250),\n'springgreen':(0,255,127),\n'steelblue':(70,130,180),\n'tan':(210,180,140),\n'teal':(0,128,128),\n'thistle':(216,191,216),\n'tomato':(255,99,71),\n'turquoise':(64,224,208),\n'violet':(238,130,238),\n'wheat':(245,222,179),\n'white':(255,255,255),\n'whitesmoke':(245,245,245),\n'yellow':(255,255,0),\n'yellowgreen':(154,205,50),\n}\n\nCOLORS_BY_VALUE={v:k for k,v in COLORS_BY_NAME.items()}\n", ["colorsys", "math", "pydantic.v1.errors", "pydantic.v1.typing", "pydantic.v1.utils", "re", "typing"]], "pydantic.v1.tools": [".py", "import json\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING,Any,Callable,Optional,Type,TypeVar,Union\n\nfrom pydantic.v1.parse import Protocol,load_file,load_str_bytes\nfrom pydantic.v1.types import StrBytes\nfrom pydantic.v1.typing import display_as_type\n\n__all__=('parse_file_as','parse_obj_as','parse_raw_as','schema_of','schema_json_of')\n\nNameFactory=Union[str,Callable[[Type[Any]],str]]\n\nif TYPE_CHECKING:\n from pydantic.v1.typing import DictStrAny\n \n \ndef _generate_parsing_type_name(type_:Any)->str:\n return f'ParsingModel[{display_as_type(type_)}]'\n \n \n@lru_cache(maxsize=2048)\ndef _get_parsing_type(type_:Any,*,type_name:Optional[NameFactory]=None)->Any:\n from pydantic.v1.main import create_model\n \n if type_name is None:\n  type_name=_generate_parsing_type_name\n if not isinstance(type_name,str):\n  type_name=type_name(type_)\n return create_model(type_name,__root__=(type_,...))\n \n \nT=TypeVar('T')\n\n\ndef parse_obj_as(type_:Type[T],obj:Any,*,type_name:Optional[NameFactory]=None)->T:\n model_type=_get_parsing_type(type_,type_name=type_name)\n return model_type(__root__=obj).__root__\n \n \ndef parse_file_as(\ntype_:Type[T],\npath:Union[str,Path],\n*,\ncontent_type:str=None,\nencoding:str='utf8',\nproto:Protocol=None,\nallow_pickle:bool=False,\njson_loads:Callable[[str],Any]=json.loads,\ntype_name:Optional[NameFactory]=None,\n)->T:\n obj=load_file(\n path,\n proto=proto,\n content_type=content_type,\n encoding=encoding,\n allow_pickle=allow_pickle,\n json_loads=json_loads,\n )\n return parse_obj_as(type_,obj,type_name=type_name)\n \n \ndef parse_raw_as(\ntype_:Type[T],\nb:StrBytes,\n*,\ncontent_type:str=None,\nencoding:str='utf8',\nproto:Protocol=None,\nallow_pickle:bool=False,\njson_loads:Callable[[str],Any]=json.loads,\ntype_name:Optional[NameFactory]=None,\n)->T:\n obj=load_str_bytes(\n b,\n proto=proto,\n content_type=content_type,\n encoding=encoding,\n allow_pickle=allow_pickle,\n json_loads=json_loads,\n )\n return parse_obj_as(type_,obj,type_name=type_name)\n \n \ndef schema_of(type_:Any,*,title:Optional[NameFactory]=None,**schema_kwargs:Any)->'DictStrAny':\n ''\n return _get_parsing_type(type_,type_name=title).schema(**schema_kwargs)\n \n \ndef schema_json_of(type_:Any,*,title:Optional[NameFactory]=None,**schema_json_kwargs:Any)->str:\n ''\n return _get_parsing_type(type_,type_name=title).schema_json(**schema_json_kwargs)\n", ["functools", "json", "pathlib", "pydantic.v1.main", "pydantic.v1.parse", "pydantic.v1.types", "pydantic.v1.typing", "typing"]], "pydantic.v1.error_wrappers": [".py", "import json\nfrom typing import TYPE_CHECKING,Any,Dict,Generator,List,Optional,Sequence,Tuple,Type,Union\n\nfrom pydantic.v1.json import pydantic_encoder\nfrom pydantic.v1.utils import Representation\n\nif TYPE_CHECKING:\n from typing_extensions import TypedDict\n \n from pydantic.v1.config import BaseConfig\n from pydantic.v1.types import ModelOrDc\n from pydantic.v1.typing import ReprArgs\n \n Loc=Tuple[Union[int,str],...]\n \n class _ErrorDictRequired(TypedDict):\n  loc:Loc\n  msg:str\n  type:str\n  \n class ErrorDict(_ErrorDictRequired,total=False):\n  ctx:Dict[str,Any]\n  \n  \n__all__='ErrorWrapper','ValidationError'\n\n\nclass ErrorWrapper(Representation):\n __slots__='exc','_loc'\n \n def __init__(self,exc:Exception,loc:Union[str,'Loc'])->None:\n  self.exc=exc\n  self._loc=loc\n  \n def loc_tuple(self)->'Loc':\n  if isinstance(self._loc,tuple):\n   return self._loc\n  else:\n   return(self._loc,)\n   \n def __repr_args__(self)->'ReprArgs':\n  return[('exc',self.exc),('loc',self.loc_tuple())]\n  \n  \n  \n  \nErrorList=Union[Sequence[Any],ErrorWrapper]\n\n\nclass ValidationError(Representation,ValueError):\n __slots__='raw_errors','model','_error_cache'\n \n def __init__(self,errors:Sequence[ErrorList],model:'ModelOrDc')->None:\n  self.raw_errors=errors\n  self.model=model\n  self._error_cache:Optional[List['ErrorDict']]=None\n  \n def errors(self)->List['ErrorDict']:\n  if self._error_cache is None:\n   try:\n    config=self.model.__config__\n   except AttributeError:\n    config=self.model.__pydantic_model__.__config__\n   self._error_cache=list(flatten_errors(self.raw_errors,config))\n  return self._error_cache\n  \n def json(self,*,indent:Union[None,int,str]=2)->str:\n  return json.dumps(self.errors(),indent=indent,default=pydantic_encoder)\n  \n def __str__(self)->str:\n  errors=self.errors()\n  no_errors=len(errors)\n  return(\n  f'{no_errors} validation error{\"\"if no_errors ==1 else \"s\"} for {self.model.__name__}\\n'\n  f'{display_errors(errors)}'\n  )\n  \n def __repr_args__(self)->'ReprArgs':\n  return[('model',self.model.__name__),('errors',self.errors())]\n  \n  \ndef display_errors(errors:List['ErrorDict'])->str:\n return '\\n'.join(f'{_display_error_loc(e)}\\n  {e[\"msg\"]} ({_display_error_type_and_ctx(e)})'for e in errors)\n \n \ndef _display_error_loc(error:'ErrorDict')->str:\n return ' -> '.join(str(e)for e in error['loc'])\n \n \ndef _display_error_type_and_ctx(error:'ErrorDict')->str:\n t='type='+error['type']\n ctx=error.get('ctx')\n if ctx:\n  return t+''.join(f'; {k}={v}'for k,v in ctx.items())\n else:\n  return t\n  \n  \ndef flatten_errors(\nerrors:Sequence[Any],config:Type['BaseConfig'],loc:Optional['Loc']=None\n)->Generator['ErrorDict',None,None]:\n for error in errors:\n  if isinstance(error,ErrorWrapper):\n   if loc:\n    error_loc=loc+error.loc_tuple()\n   else:\n    error_loc=error.loc_tuple()\n    \n   if isinstance(error.exc,ValidationError):\n    yield from flatten_errors(error.exc.raw_errors,config,error_loc)\n   else:\n    yield error_dict(error.exc,config,error_loc)\n  elif isinstance(error,list):\n   yield from flatten_errors(error,config,loc=loc)\n  else:\n   raise RuntimeError(f'Unknown error object: {error}')\n   \n   \ndef error_dict(exc:Exception,config:Type['BaseConfig'],loc:'Loc')->'ErrorDict':\n type_=get_exc_type(exc.__class__)\n msg_template=config.error_msg_templates.get(type_)or getattr(exc,'msg_template',None)\n ctx=exc.__dict__\n if msg_template:\n  msg=msg_template.format(**ctx)\n else:\n  msg=str(exc)\n  \n d:'ErrorDict'={'loc':loc,'msg':msg,'type':type_}\n \n if ctx:\n  d['ctx']=ctx\n  \n return d\n \n \n_EXC_TYPE_CACHE:Dict[Type[Exception],str]={}\n\n\ndef get_exc_type(cls:Type[Exception])->str:\n\n try:\n  return _EXC_TYPE_CACHE[cls]\n except KeyError:\n  r=_get_exc_type(cls)\n  _EXC_TYPE_CACHE[cls]=r\n  return r\n  \n  \ndef _get_exc_type(cls:Type[Exception])->str:\n if issubclass(cls,AssertionError):\n  return 'assertion_error'\n  \n base_name='type_error'if issubclass(cls,TypeError)else 'value_error'\n if cls in(TypeError,ValueError):\n \n  return base_name\n  \n  \n  \n code=getattr(cls,'code',None)or cls.__name__.replace('Error','').lower()\n return base_name+'.'+code\n", ["json", "pydantic.v1.config", "pydantic.v1.json", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "typing", "typing_extensions"]], "pydantic.v1.config": [".py", "import json\nfrom enum import Enum\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,ForwardRef,Optional,Tuple,Type,Union\n\nfrom typing_extensions import Literal,Protocol\n\nfrom pydantic.v1.typing import AnyArgTCallable,AnyCallable\nfrom pydantic.v1.utils import GetterDict\nfrom pydantic.v1.version import compiled\n\nif TYPE_CHECKING:\n from typing import overload\n \n from pydantic.v1.fields import ModelField\n from pydantic.v1.main import BaseModel\n \n ConfigType=Type['BaseConfig']\n \n class SchemaExtraCallable(Protocol):\n  @overload\n  def __call__(self,schema:Dict[str,Any])->None:\n   pass\n   \n  @overload\n  def __call__(self,schema:Dict[str,Any],model_class:Type[BaseModel])->None:\n   pass\n   \nelse:\n SchemaExtraCallable=Callable[...,None]\n \n__all__='BaseConfig','ConfigDict','get_config','Extra','inherit_config','prepare_config'\n\n\nclass Extra(str,Enum):\n allow='allow'\n ignore='ignore'\n forbid='forbid'\n \n \n \n \n \nif not compiled:\n from typing_extensions import TypedDict\n \n class ConfigDict(TypedDict,total=False):\n  title:Optional[str]\n  anystr_lower:bool\n  anystr_strip_whitespace:bool\n  min_anystr_length:int\n  max_anystr_length:Optional[int]\n  validate_all:bool\n  extra:Extra\n  allow_mutation:bool\n  frozen:bool\n  allow_population_by_field_name:bool\n  use_enum_values:bool\n  fields:Dict[str,Union[str,Dict[str,str]]]\n  validate_assignment:bool\n  error_msg_templates:Dict[str,str]\n  arbitrary_types_allowed:bool\n  orm_mode:bool\n  getter_dict:Type[GetterDict]\n  alias_generator:Optional[Callable[[str],str]]\n  keep_untouched:Tuple[type,...]\n  schema_extra:Union[Dict[str,object],'SchemaExtraCallable']\n  json_loads:Callable[[str],object]\n  json_dumps:AnyArgTCallable[str]\n  json_encoders:Dict[Type[object],AnyCallable]\n  underscore_attrs_are_private:bool\n  allow_inf_nan:bool\n  copy_on_model_validation:Literal['none','deep','shallow']\n  \n  post_init_call:Literal['before_validation','after_validation']\n  \nelse:\n ConfigDict=dict\n \n \nclass BaseConfig:\n title:Optional[str]=None\n anystr_lower:bool=False\n anystr_upper:bool=False\n anystr_strip_whitespace:bool=False\n min_anystr_length:int=0\n max_anystr_length:Optional[int]=None\n validate_all:bool=False\n extra:Extra=Extra.ignore\n allow_mutation:bool=True\n frozen:bool=False\n allow_population_by_field_name:bool=False\n use_enum_values:bool=False\n fields:Dict[str,Union[str,Dict[str,str]]]={}\n validate_assignment:bool=False\n error_msg_templates:Dict[str,str]={}\n arbitrary_types_allowed:bool=False\n orm_mode:bool=False\n getter_dict:Type[GetterDict]=GetterDict\n alias_generator:Optional[Callable[[str],str]]=None\n keep_untouched:Tuple[type,...]=()\n schema_extra:Union[Dict[str,Any],'SchemaExtraCallable']={}\n json_loads:Callable[[str],Any]=json.loads\n json_dumps:Callable[...,str]=json.dumps\n json_encoders:Dict[Union[Type[Any],str,ForwardRef],AnyCallable]={}\n underscore_attrs_are_private:bool=False\n allow_inf_nan:bool=True\n \n \n \n copy_on_model_validation:Literal['none','deep','shallow']='shallow'\n \n \n smart_union:bool=False\n \n post_init_call:Literal['before_validation','after_validation']='before_validation'\n \n @classmethod\n def get_field_info(cls,name:str)->Dict[str,Any]:\n  ''\n\n  \n  \n  fields_value=cls.fields.get(name)\n  \n  if isinstance(fields_value,str):\n   field_info:Dict[str,Any]={'alias':fields_value}\n  elif isinstance(fields_value,dict):\n   field_info=fields_value\n  else:\n   field_info={}\n   \n  if 'alias'in field_info:\n   field_info.setdefault('alias_priority',2)\n   \n  if field_info.get('alias_priority',0)<=1 and cls.alias_generator:\n   alias=cls.alias_generator(name)\n   if not isinstance(alias,str):\n    raise TypeError(f'Config.alias_generator must return str, not {alias.__class__}')\n   field_info.update(alias=alias,alias_priority=1)\n  return field_info\n  \n @classmethod\n def prepare_field(cls,field:'ModelField')->None:\n  ''\n\n  \n  pass\n  \n  \ndef get_config(config:Union[ConfigDict,Type[object],None])->Type[BaseConfig]:\n if config is None:\n  return BaseConfig\n  \n else:\n  config_dict=(\n  config\n  if isinstance(config,dict)\n  else{k:getattr(config,k)for k in dir(config)if not k.startswith('__')}\n  )\n  \n  class Config(BaseConfig):\n   ...\n   \n  for k,v in config_dict.items():\n   setattr(Config,k,v)\n  return Config\n  \n  \ndef inherit_config(self_config:'ConfigType',parent_config:'ConfigType',**namespace:Any)->'ConfigType':\n if not self_config:\n  base_classes:Tuple['ConfigType',...]=(parent_config,)\n elif self_config ==parent_config:\n  base_classes=(self_config,)\n else:\n  base_classes=self_config,parent_config\n  \n namespace['json_encoders']={\n **getattr(parent_config,'json_encoders',{}),\n **getattr(self_config,'json_encoders',{}),\n **namespace.get('json_encoders',{}),\n }\n \n return type('Config',base_classes,namespace)\n \n \ndef prepare_config(config:Type[BaseConfig],cls_name:str)->None:\n if not isinstance(config.extra,Extra):\n  try:\n   config.extra=Extra(config.extra)\n  except ValueError:\n   raise ValueError(f'\"{cls_name}\": {config.extra} is not a valid value for \"extra\"')\n", ["enum", "json", "pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.typing", "pydantic.v1.utils", "pydantic.v1.version", "typing", "typing_extensions"]], "pydantic.v1.datetime_parse": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport re\nfrom datetime import date,datetime,time,timedelta,timezone\nfrom typing import Dict,Optional,Type,Union\n\nfrom pydantic.v1 import errors\n\ndate_expr=r'(?P<year>\\d{4})-(?P<month>\\d{1,2})-(?P<day>\\d{1,2})'\ntime_expr=(\nr'(?P<hour>\\d{1,2}):(?P<minute>\\d{1,2})'\nr'(?::(?P<second>\\d{1,2})(?:\\.(?P<microsecond>\\d{1,6})\\d{0,6})?)?'\nr'(?P<tzinfo>Z|[+-]\\d{2}(?::?\\d{2})?)?$'\n)\n\ndate_re=re.compile(f'{date_expr}$')\ntime_re=re.compile(time_expr)\ndatetime_re=re.compile(f'{date_expr}[T ]{time_expr}')\n\nstandard_duration_re=re.compile(\nr'^'\nr'(?:(?P<days>-?\\d+) (days?, )?)?'\nr'((?:(?P<hours>-?\\d+):)(?=\\d+:\\d+))?'\nr'(?:(?P<minutes>-?\\d+):)?'\nr'(?P<seconds>-?\\d+)'\nr'(?:\\.(?P<microseconds>\\d{1,6})\\d{0,6})?'\nr'$'\n)\n\n\niso8601_duration_re=re.compile(\nr'^(?P<sign>[-+]?)'\nr'P'\nr'(?:(?P<days>\\d+(.\\d+)?)D)?'\nr'(?:T'\nr'(?:(?P<hours>\\d+(.\\d+)?)H)?'\nr'(?:(?P<minutes>\\d+(.\\d+)?)M)?'\nr'(?:(?P<seconds>\\d+(.\\d+)?)S)?'\nr')?'\nr'$'\n)\n\nEPOCH=datetime(1970,1,1)\n\n\nMS_WATERSHED=int(2e10)\n\nMAX_NUMBER=int(3e20)\nStrBytesIntFloat=Union[str,bytes,int,float]\n\n\ndef get_numeric(value:StrBytesIntFloat,native_expected_type:str)->Union[None,int,float]:\n if isinstance(value,(int,float)):\n  return value\n try:\n  return float(value)\n except ValueError:\n  return None\n except TypeError:\n  raise TypeError(f'invalid type; expected {native_expected_type}, string, bytes, int or float')\n  \n  \ndef from_unix_seconds(seconds:Union[int,float])->datetime:\n if seconds >MAX_NUMBER:\n  return datetime.max\n elif seconds <-MAX_NUMBER:\n  return datetime.min\n  \n while abs(seconds)>MS_WATERSHED:\n  seconds /=1000\n dt=EPOCH+timedelta(seconds=seconds)\n return dt.replace(tzinfo=timezone.utc)\n \n \ndef _parse_timezone(value:Optional[str],error:Type[Exception])->Union[None,int,timezone]:\n if value =='Z':\n  return timezone.utc\n elif value is not None:\n  offset_mins=int(value[-2:])if len(value)>3 else 0\n  offset=60 *int(value[1:3])+offset_mins\n  if value[0]=='-':\n   offset=-offset\n  try:\n   return timezone(timedelta(minutes=offset))\n  except ValueError:\n   raise error()\n else:\n  return None\n  \n  \ndef parse_date(value:Union[date,StrBytesIntFloat])->date:\n ''\n\n\n\n\n \n if isinstance(value,date):\n  if isinstance(value,datetime):\n   return value.date()\n  else:\n   return value\n   \n number=get_numeric(value,'date')\n if number is not None:\n  return from_unix_seconds(number).date()\n  \n if isinstance(value,bytes):\n  value=value.decode()\n  \n match=date_re.match(value)\n if match is None:\n  raise errors.DateError()\n  \n kw={k:int(v)for k,v in match.groupdict().items()}\n \n try:\n  return date(**kw)\n except ValueError:\n  raise errors.DateError()\n  \n  \ndef parse_time(value:Union[time,StrBytesIntFloat])->time:\n ''\n\n\n\n\n \n if isinstance(value,time):\n  return value\n  \n number=get_numeric(value,'time')\n if number is not None:\n  if number >=86400:\n  \n   raise errors.TimeError()\n  return(datetime.min+timedelta(seconds=number)).time()\n  \n if isinstance(value,bytes):\n  value=value.decode()\n  \n match=time_re.match(value)\n if match is None:\n  raise errors.TimeError()\n  \n kw=match.groupdict()\n if kw['microsecond']:\n  kw['microsecond']=kw['microsecond'].ljust(6,'0')\n  \n tzinfo=_parse_timezone(kw.pop('tzinfo'),errors.TimeError)\n kw_:Dict[str,Union[None,int,timezone]]={k:int(v)for k,v in kw.items()if v is not None}\n kw_['tzinfo']=tzinfo\n \n try:\n  return time(**kw_)\n except ValueError:\n  raise errors.TimeError()\n  \n  \ndef parse_datetime(value:Union[datetime,StrBytesIntFloat])->datetime:\n ''\n\n\n\n\n\n\n\n \n if isinstance(value,datetime):\n  return value\n  \n number=get_numeric(value,'datetime')\n if number is not None:\n  return from_unix_seconds(number)\n  \n if isinstance(value,bytes):\n  value=value.decode()\n  \n match=datetime_re.match(value)\n if match is None:\n  raise errors.DateTimeError()\n  \n kw=match.groupdict()\n if kw['microsecond']:\n  kw['microsecond']=kw['microsecond'].ljust(6,'0')\n  \n tzinfo=_parse_timezone(kw.pop('tzinfo'),errors.DateTimeError)\n kw_:Dict[str,Union[None,int,timezone]]={k:int(v)for k,v in kw.items()if v is not None}\n kw_['tzinfo']=tzinfo\n \n try:\n  return datetime(**kw_)\n except ValueError:\n  raise errors.DateTimeError()\n  \n  \ndef parse_duration(value:StrBytesIntFloat)->timedelta:\n ''\n\n\n\n\n\n \n if isinstance(value,timedelta):\n  return value\n  \n if isinstance(value,(int,float)):\n \n  value=f'{value:f}'\n elif isinstance(value,bytes):\n  value=value.decode()\n  \n try:\n  match=standard_duration_re.match(value)or iso8601_duration_re.match(value)\n except TypeError:\n  raise TypeError('invalid type; expected timedelta, string, bytes, int or float')\n  \n if not match:\n  raise errors.DurationError()\n  \n kw=match.groupdict()\n sign=-1 if kw.pop('sign','+')=='-'else 1\n if kw.get('microseconds'):\n  kw['microseconds']=kw['microseconds'].ljust(6,'0')\n  \n if kw.get('seconds')and kw.get('microseconds')and kw['seconds'].startswith('-'):\n  kw['microseconds']='-'+kw['microseconds']\n  \n kw_={k:float(v)for k,v in kw.items()if v is not None}\n \n return sign *timedelta(**kw_)\n", ["datetime", "pydantic.v1", "pydantic.v1.errors", "re", "typing"]], "pydantic.v1.utils": [".py", "import keyword\nimport warnings\nimport weakref\nfrom collections import OrderedDict,defaultdict,deque\nfrom copy import deepcopy\nfrom itertools import islice,zip_longest\nfrom types import BuiltinFunctionType,CodeType,FunctionType,GeneratorType,LambdaType,ModuleType\nfrom typing import(\nTYPE_CHECKING,\nAbstractSet,\nAny,\nCallable,\nCollection,\nDict,\nGenerator,\nIterable,\nIterator,\nList,\nMapping,\nNoReturn,\nOptional,\nSet,\nTuple,\nType,\nTypeVar,\nUnion,\n)\n\nfrom typing_extensions import Annotated\n\nfrom pydantic.v1.errors import ConfigError\nfrom pydantic.v1.typing import(\nNoneType,\nWithArgsTypes,\nall_literal_values,\ndisplay_as_type,\nget_args,\nget_origin,\nis_literal_type,\nis_union,\n)\nfrom pydantic.v1.version import version_info\n\nif TYPE_CHECKING:\n from inspect import Signature\n from pathlib import Path\n \n from pydantic.v1.config import BaseConfig\n from pydantic.v1.dataclasses import Dataclass\n from pydantic.v1.fields import ModelField\n from pydantic.v1.main import BaseModel\n from pydantic.v1.typing import AbstractSetIntStr,DictIntStrAny,IntStr,MappingIntStrAny,ReprArgs\n \n RichReprResult=Iterable[Union[Any,Tuple[Any],Tuple[str,Any],Tuple[str,Any,Any]]]\n \n__all__=(\n'import_string',\n'sequence_like',\n'validate_field_name',\n'lenient_isinstance',\n'lenient_issubclass',\n'in_ipython',\n'is_valid_identifier',\n'deep_update',\n'update_not_none',\n'almost_equal_floats',\n'get_model',\n'to_camel',\n'to_lower_camel',\n'is_valid_field',\n'smart_deepcopy',\n'PyObjectStr',\n'Representation',\n'GetterDict',\n'ValueItems',\n'version_info',\n'ClassAttribute',\n'path_type',\n'ROOT_KEY',\n'get_unique_discriminator_alias',\n'get_discriminator_alias_and_values',\n'DUNDER_ATTRIBUTES',\n)\n\nROOT_KEY='__root__'\n\nIMMUTABLE_NON_COLLECTIONS_TYPES:Set[Type[Any]]={\nint,\nfloat,\ncomplex,\nstr,\nbool,\nbytes,\ntype,\nNoneType,\nFunctionType,\nBuiltinFunctionType,\nLambdaType,\nweakref.ref,\nCodeType,\n\n\n\nModuleType,\nNotImplemented.__class__,\nEllipsis.__class__,\n}\n\n\nBUILTIN_COLLECTIONS:Set[Type[Any]]={\nlist,\nset,\ntuple,\nfrozenset,\ndict,\nOrderedDict,\ndefaultdict,\ndeque,\n}\n\n\ndef import_string(dotted_path:str)->Any:\n ''\n\n\n \n from importlib import import_module\n \n try:\n  module_path,class_name=dotted_path.strip(' ').rsplit('.',1)\n except ValueError as e:\n  raise ImportError(f'\"{dotted_path}\" doesn\\'t look like a module path')from e\n  \n module=import_module(module_path)\n try:\n  return getattr(module,class_name)\n except AttributeError as e:\n  raise ImportError(f'Module \"{module_path}\" does not define a \"{class_name}\" attribute')from e\n  \n  \ndef truncate(v:Union[str],*,max_len:int=80)->str:\n ''\n\n \n warnings.warn('`truncate` is no-longer used by pydantic and is deprecated',DeprecationWarning)\n if isinstance(v,str)and len(v)>(max_len -2):\n \n  return(v[:(max_len -3)]+'\u2026').__repr__()\n try:\n  v=v.__repr__()\n except TypeError:\n  v=v.__class__.__repr__(v)\n if len(v)>max_len:\n  v=v[:max_len -1]+'\u2026'\n return v\n \n \ndef sequence_like(v:Any)->bool:\n return isinstance(v,(list,tuple,set,frozenset,GeneratorType,deque))\n \n \ndef validate_field_name(bases:List[Type['BaseModel']],field_name:str)->None:\n ''\n\n \n for base in bases:\n  if getattr(base,field_name,None):\n   raise NameError(\n   f'Field name \"{field_name}\" shadows a BaseModel attribute; '\n   f'use a different field name with \"alias=\\'{field_name}\\'\".'\n   )\n   \n   \ndef lenient_isinstance(o:Any,class_or_tuple:Union[Type[Any],Tuple[Type[Any],...],None])->bool:\n try:\n  return isinstance(o,class_or_tuple)\n except TypeError:\n  return False\n  \n  \ndef lenient_issubclass(cls:Any,class_or_tuple:Union[Type[Any],Tuple[Type[Any],...],None])->bool:\n try:\n  return isinstance(cls,type)and issubclass(cls,class_or_tuple)\n except TypeError:\n  if isinstance(cls,WithArgsTypes):\n   return False\n  raise\n  \n  \ndef in_ipython()->bool:\n ''\n\n \n try:\n  eval('__IPYTHON__')\n except NameError:\n  return False\n else:\n  return True\n  \n  \ndef is_valid_identifier(identifier:str)->bool:\n ''\n\n\n\n \n return identifier.isidentifier()and not keyword.iskeyword(identifier)\n \n \nKeyType=TypeVar('KeyType')\n\n\ndef deep_update(mapping:Dict[KeyType,Any],*updating_mappings:Dict[KeyType,Any])->Dict[KeyType,Any]:\n updated_mapping=mapping.copy()\n for updating_mapping in updating_mappings:\n  for k,v in updating_mapping.items():\n   if k in updated_mapping and isinstance(updated_mapping[k],dict)and isinstance(v,dict):\n    updated_mapping[k]=deep_update(updated_mapping[k],v)\n   else:\n    updated_mapping[k]=v\n return updated_mapping\n \n \ndef update_not_none(mapping:Dict[Any,Any],**update:Any)->None:\n mapping.update({k:v for k,v in update.items()if v is not None})\n \n \ndef almost_equal_floats(value_1:float,value_2:float,*,delta:float=1e-8)->bool:\n ''\n\n \n return abs(value_1 -value_2)<=delta\n \n \ndef generate_model_signature(\ninit:Callable[...,None],fields:Dict[str,'ModelField'],config:Type['BaseConfig']\n)->'Signature':\n ''\n\n \n from inspect import Parameter,Signature,signature\n \n from pydantic.v1.config import Extra\n \n present_params=signature(init).parameters.values()\n merged_params:Dict[str,Parameter]={}\n var_kw=None\n use_var_kw=False\n \n for param in islice(present_params,1,None):\n  if param.kind is param.VAR_KEYWORD:\n   var_kw=param\n   continue\n  merged_params[param.name]=param\n  \n if var_kw:\n  allow_names=config.allow_population_by_field_name\n  for field_name,field in fields.items():\n   param_name=field.alias\n   if field_name in merged_params or param_name in merged_params:\n    continue\n   elif not is_valid_identifier(param_name):\n    if allow_names and is_valid_identifier(field_name):\n     param_name=field_name\n    else:\n     use_var_kw=True\n     continue\n     \n     \n   kwargs={'default':field.default}if not field.required else{}\n   merged_params[param_name]=Parameter(\n   param_name,Parameter.KEYWORD_ONLY,annotation=field.annotation,**kwargs\n   )\n   \n if config.extra is Extra.allow:\n  use_var_kw=True\n  \n if var_kw and use_var_kw:\n \n \n  default_model_signature=[\n  ('__pydantic_self__',Parameter.POSITIONAL_OR_KEYWORD),\n  ('data',Parameter.VAR_KEYWORD),\n  ]\n  if[(p.name,p.kind)for p in present_params]==default_model_signature:\n  \n   var_kw_name='extra_data'\n  else:\n  \n   var_kw_name=var_kw.name\n   \n   \n  while var_kw_name in fields:\n   var_kw_name +='_'\n  merged_params[var_kw_name]=var_kw.replace(name=var_kw_name)\n  \n return Signature(parameters=list(merged_params.values()),return_annotation=None)\n \n \ndef get_model(obj:Union[Type['BaseModel'],Type['Dataclass']])->Type['BaseModel']:\n from pydantic.v1.main import BaseModel\n \n try:\n  model_cls=obj.__pydantic_model__\n except AttributeError:\n  model_cls=obj\n  \n if not issubclass(model_cls,BaseModel):\n  raise TypeError('Unsupported type, must be either BaseModel or dataclass')\n return model_cls\n \n \ndef to_camel(string:str)->str:\n return ''.join(word.capitalize()for word in string.split('_'))\n \n \ndef to_lower_camel(string:str)->str:\n if len(string)>=1:\n  pascal_string=to_camel(string)\n  return pascal_string[0].lower()+pascal_string[1:]\n return string.lower()\n \n \nT=TypeVar('T')\n\n\ndef unique_list(\ninput_list:Union[List[T],Tuple[T,...]],\n*,\nname_factory:Callable[[T],str]=str,\n)->List[T]:\n ''\n\n\n\n \n result:List[T]=[]\n result_names:List[str]=[]\n for v in input_list:\n  v_name=name_factory(v)\n  if v_name not in result_names:\n   result_names.append(v_name)\n   result.append(v)\n  else:\n   result[result_names.index(v_name)]=v\n   \n return result\n \n \nclass PyObjectStr(str):\n ''\n\n\n \n \n def __repr__(self)->str:\n  return str(self)\n  \n  \nclass Representation:\n ''\n\n\n\n\n \n \n __slots__:Tuple[str,...]=tuple()\n \n def __repr_args__(self)->'ReprArgs':\n  ''\n\n\n\n\n\n  \n  attrs=((s,getattr(self,s))for s in self.__slots__)\n  return[(a,v)for a,v in attrs if v is not None]\n  \n def __repr_name__(self)->str:\n  ''\n\n  \n  return self.__class__.__name__\n  \n def __repr_str__(self,join_str:str)->str:\n  return join_str.join(repr(v)if a is None else f'{a}={v !r}'for a,v in self.__repr_args__())\n  \n def __pretty__(self,fmt:Callable[[Any],Any],**kwargs:Any)->Generator[Any,None,None]:\n  ''\n\n  \n  yield self.__repr_name__()+'('\n  yield 1\n  for name,value in self.__repr_args__():\n   if name is not None:\n    yield name+'='\n   yield fmt(value)\n   yield ','\n   yield 0\n  yield -1\n  yield ')'\n  \n def __str__(self)->str:\n  return self.__repr_str__(' ')\n  \n def __repr__(self)->str:\n  return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n  \n def __rich_repr__(self)->'RichReprResult':\n  ''\n  for name,field_repr in self.__repr_args__():\n   if name is None:\n    yield field_repr\n   else:\n    yield name,field_repr\n    \n    \nclass GetterDict(Representation):\n ''\n\n\n\n \n \n __slots__=('_obj',)\n \n def __init__(self,obj:Any):\n  self._obj=obj\n  \n def __getitem__(self,key:str)->Any:\n  try:\n   return getattr(self._obj,key)\n  except AttributeError as e:\n   raise KeyError(key)from e\n   \n def get(self,key:Any,default:Any=None)->Any:\n  return getattr(self._obj,key,default)\n  \n def extra_keys(self)->Set[Any]:\n  ''\n\n  \n  return set()\n  \n def keys(self)->List[Any]:\n  ''\n\n\n  \n  return list(self)\n  \n def values(self)->List[Any]:\n  return[self[k]for k in self]\n  \n def items(self)->Iterator[Tuple[str,Any]]:\n  for k in self:\n   yield k,self.get(k)\n   \n def __iter__(self)->Iterator[str]:\n  for name in dir(self._obj):\n   if not name.startswith('_'):\n    yield name\n    \n def __len__(self)->int:\n  return sum(1 for _ in self)\n  \n def __contains__(self,item:Any)->bool:\n  return item in self.keys()\n  \n def __eq__(self,other:Any)->bool:\n  return dict(self)==dict(other.items())\n  \n def __repr_args__(self)->'ReprArgs':\n  return[(None,dict(self))]\n  \n def __repr_name__(self)->str:\n  return f'GetterDict[{display_as_type(self._obj)}]'\n  \n  \nclass ValueItems(Representation):\n ''\n\n \n \n __slots__=('_items','_type')\n \n def __init__(self,value:Any,items:Union['AbstractSetIntStr','MappingIntStrAny'])->None:\n  items=self._coerce_items(items)\n  \n  if isinstance(value,(list,tuple)):\n   items=self._normalize_indexes(items,len(value))\n   \n  self._items:'MappingIntStrAny'=items\n  \n def is_excluded(self,item:Any)->bool:\n  ''\n\n\n\n  \n  return self.is_true(self._items.get(item))\n  \n def is_included(self,item:Any)->bool:\n  ''\n\n\n\n  \n  return item in self._items\n  \n def for_element(self,e:'IntStr')->Optional[Union['AbstractSetIntStr','MappingIntStrAny']]:\n  ''\n\n\n  \n  \n  item=self._items.get(e)\n  return item if not self.is_true(item)else None\n  \n def _normalize_indexes(self,items:'MappingIntStrAny',v_length:int)->'DictIntStrAny':\n  ''\n\n\n\n\n\n\n\n  \n  \n  normalized_items:'DictIntStrAny'={}\n  all_items=None\n  for i,v in items.items():\n   if not(isinstance(v,Mapping)or isinstance(v,AbstractSet)or self.is_true(v)):\n    raise TypeError(f'Unexpected type of exclude value for index \"{i}\" {v.__class__}')\n   if i =='__all__':\n    all_items=self._coerce_value(v)\n    continue\n   if not isinstance(i,int):\n    raise TypeError(\n    'Excluding fields from a sequence of sub-models or dicts must be performed index-wise: '\n    'expected integer keys or keyword \"__all__\"'\n    )\n   normalized_i=v_length+i if i <0 else i\n   normalized_items[normalized_i]=self.merge(v,normalized_items.get(normalized_i))\n   \n  if not all_items:\n   return normalized_items\n  if self.is_true(all_items):\n   for i in range(v_length):\n    normalized_items.setdefault(i,...)\n   return normalized_items\n  for i in range(v_length):\n   normalized_item=normalized_items.setdefault(i,{})\n   if not self.is_true(normalized_item):\n    normalized_items[i]=self.merge(all_items,normalized_item)\n  return normalized_items\n  \n @classmethod\n def merge(cls,base:Any,override:Any,intersect:bool=False)->Any:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  override=cls._coerce_value(override)\n  base=cls._coerce_value(base)\n  if override is None:\n   return base\n  if cls.is_true(base)or base is None:\n   return override\n  if cls.is_true(override):\n   return base if intersect else override\n   \n   \n  if intersect:\n   merge_keys=[k for k in base if k in override]+[k for k in override if k in base]\n  else:\n   merge_keys=list(base)+[k for k in override if k not in base]\n   \n  merged:'DictIntStrAny'={}\n  for k in merge_keys:\n   merged_item=cls.merge(base.get(k),override.get(k),intersect=intersect)\n   if merged_item is not None:\n    merged[k]=merged_item\n    \n  return merged\n  \n @staticmethod\n def _coerce_items(items:Union['AbstractSetIntStr','MappingIntStrAny'])->'MappingIntStrAny':\n  if isinstance(items,Mapping):\n   pass\n  elif isinstance(items,AbstractSet):\n   items=dict.fromkeys(items,...)\n  else:\n   class_name=getattr(items,'__class__','???')\n   assert_never(\n   items,\n   f'Unexpected type of exclude value {class_name}',\n   )\n  return items\n  \n @classmethod\n def _coerce_value(cls,value:Any)->Any:\n  if value is None or cls.is_true(value):\n   return value\n  return cls._coerce_items(value)\n  \n @staticmethod\n def is_true(v:Any)->bool:\n  return v is True or v is ...\n  \n def __repr_args__(self)->'ReprArgs':\n  return[(None,self._items)]\n  \n  \nclass ClassAttribute:\n ''\n\n \n \n __slots__=(\n 'name',\n 'value',\n )\n \n def __init__(self,name:str,value:Any)->None:\n  self.name=name\n  self.value=value\n  \n def __get__(self,instance:Any,owner:Type[Any])->None:\n  if instance is None:\n   return self.value\n  raise AttributeError(f'{self.name !r} attribute of {owner.__name__ !r} is class-only')\n  \n  \npath_types={\n'is_dir':'directory',\n'is_file':'file',\n'is_mount':'mount point',\n'is_symlink':'symlink',\n'is_block_device':'block device',\n'is_char_device':'char device',\n'is_fifo':'FIFO',\n'is_socket':'socket',\n}\n\n\ndef path_type(p:'Path')->str:\n ''\n\n \n assert p.exists(),'path does not exist'\n for method,name in path_types.items():\n  if getattr(p,method)():\n   return name\n   \n return 'unknown'\n \n \nObj=TypeVar('Obj')\n\n\ndef smart_deepcopy(obj:Obj)->Obj:\n ''\n\n\n\n \n \n obj_type=obj.__class__\n if obj_type in IMMUTABLE_NON_COLLECTIONS_TYPES:\n  return obj\n try:\n  if not obj and obj_type in BUILTIN_COLLECTIONS:\n  \n   return obj if obj_type is tuple else obj.copy()\n except(TypeError,ValueError,RuntimeError):\n \n  pass\n  \n return deepcopy(obj)\n \n \ndef is_valid_field(name:str)->bool:\n if not name.startswith('_'):\n  return True\n return ROOT_KEY ==name\n \n \nDUNDER_ATTRIBUTES={\n'__annotations__',\n'__classcell__',\n'__doc__',\n'__module__',\n'__orig_bases__',\n'__orig_class__',\n'__qualname__',\n}\n\n\ndef is_valid_private_name(name:str)->bool:\n return not is_valid_field(name)and name not in DUNDER_ATTRIBUTES\n \n \n_EMPTY=object()\n\n\ndef all_identical(left:Iterable[Any],right:Iterable[Any])->bool:\n ''\n\n\n\n\n\n\n\n \n for left_item,right_item in zip_longest(left,right,fillvalue=_EMPTY):\n  if left_item is not right_item:\n   return False\n return True\n \n \ndef assert_never(obj:NoReturn,msg:str)->NoReturn:\n ''\n\n\n\n\n \n raise TypeError(msg)\n \n \ndef get_unique_discriminator_alias(all_aliases:Collection[str],discriminator_key:str)->str:\n ''\n unique_aliases=set(all_aliases)\n if len(unique_aliases)>1:\n  raise ConfigError(\n  f'Aliases for discriminator {discriminator_key !r} must be the same (got {\", \".join(sorted(all_aliases))})'\n  )\n return unique_aliases.pop()\n \n \ndef get_discriminator_alias_and_values(tp:Any,discriminator_key:str)->Tuple[str,Tuple[str,...]]:\n ''\n\n\n \n is_root_model=getattr(tp,'__custom_root_type__',False)\n \n if get_origin(tp)is Annotated:\n  tp=get_args(tp)[0]\n  \n if hasattr(tp,'__pydantic_model__'):\n  tp=tp.__pydantic_model__\n  \n if is_union(get_origin(tp)):\n  alias,all_values=_get_union_alias_and_all_values(tp,discriminator_key)\n  return alias,tuple(v for values in all_values for v in values)\n elif is_root_model:\n  union_type=tp.__fields__[ROOT_KEY].type_\n  alias,all_values=_get_union_alias_and_all_values(union_type,discriminator_key)\n  \n  if len(set(all_values))>1:\n   raise ConfigError(\n   f'Field {discriminator_key !r} is not the same for all submodels of {display_as_type(tp)!r}'\n   )\n   \n  return alias,all_values[0]\n  \n else:\n  try:\n   t_discriminator_type=tp.__fields__[discriminator_key].type_\n  except AttributeError as e:\n   raise TypeError(f'Type {tp.__name__ !r} is not a valid `BaseModel` or `dataclass`')from e\n  except KeyError as e:\n   raise ConfigError(f'Model {tp.__name__ !r} needs a discriminator field for key {discriminator_key !r}')from e\n   \n  if not is_literal_type(t_discriminator_type):\n   raise ConfigError(f'Field {discriminator_key !r} of model {tp.__name__ !r} needs to be a `Literal`')\n   \n  return tp.__fields__[discriminator_key].alias,all_literal_values(t_discriminator_type)\n  \n  \ndef _get_union_alias_and_all_values(\nunion_type:Type[Any],discriminator_key:str\n)->Tuple[str,Tuple[Tuple[str,...],...]]:\n zipped_aliases_values=[get_discriminator_alias_and_values(t,discriminator_key)for t in get_args(union_type)]\n \n all_aliases,all_values=zip(*zipped_aliases_values)\n return get_unique_discriminator_alias(all_aliases,discriminator_key),all_values\n", ["collections", "copy", "importlib", "inspect", "itertools", "keyword", "pathlib", "pydantic.v1.config", "pydantic.v1.dataclasses", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.main", "pydantic.v1.typing", "pydantic.v1.version", "types", "typing", "typing_extensions", "warnings", "weakref"]], "pydantic.v1.validators": [".py", "import math\nimport re\nfrom collections import OrderedDict,deque\nfrom collections.abc import Hashable as CollectionsHashable\nfrom datetime import date,datetime,time,timedelta\nfrom decimal import Decimal,DecimalException\nfrom enum import Enum,IntEnum\nfrom ipaddress import IPv4Address,IPv4Interface,IPv4Network,IPv6Address,IPv6Interface,IPv6Network\nfrom pathlib import Path\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCallable,\nDeque,\nDict,\nForwardRef,\nFrozenSet,\nGenerator,\nHashable,\nList,\nNamedTuple,\nPattern,\nSet,\nTuple,\nType,\nTypeVar,\nUnion,\n)\nfrom uuid import UUID\nfrom warnings import warn\n\nfrom pydantic.v1 import errors\nfrom pydantic.v1.datetime_parse import parse_date,parse_datetime,parse_duration,parse_time\nfrom pydantic.v1.typing import(\nAnyCallable,\nall_literal_values,\ndisplay_as_type,\nget_class,\nis_callable_type,\nis_literal_type,\nis_namedtuple,\nis_none_type,\nis_typeddict,\n)\nfrom pydantic.v1.utils import almost_equal_floats,lenient_issubclass,sequence_like\n\nif TYPE_CHECKING:\n from typing_extensions import Literal,TypedDict\n \n from pydantic.v1.config import BaseConfig\n from pydantic.v1.fields import ModelField\n from pydantic.v1.types import ConstrainedDecimal,ConstrainedFloat,ConstrainedInt\n \n ConstrainedNumber=Union[ConstrainedDecimal,ConstrainedFloat,ConstrainedInt]\n AnyOrderedDict=OrderedDict[Any,Any]\n Number=Union[int,float,Decimal]\n StrBytes=Union[str,bytes]\n \n \ndef str_validator(v:Any)->Union[str]:\n if isinstance(v,str):\n  if isinstance(v,Enum):\n   return v.value\n  else:\n   return v\n elif isinstance(v,(float,int,Decimal)):\n \n  return str(v)\n elif isinstance(v,(bytes,bytearray)):\n  return v.decode()\n else:\n  raise errors.StrError()\n  \n  \ndef strict_str_validator(v:Any)->Union[str]:\n if isinstance(v,str)and not isinstance(v,Enum):\n  return v\n raise errors.StrError()\n \n \ndef bytes_validator(v:Any)->Union[bytes]:\n if isinstance(v,bytes):\n  return v\n elif isinstance(v,bytearray):\n  return bytes(v)\n elif isinstance(v,str):\n  return v.encode()\n elif isinstance(v,(float,int,Decimal)):\n  return str(v).encode()\n else:\n  raise errors.BytesError()\n  \n  \ndef strict_bytes_validator(v:Any)->Union[bytes]:\n if isinstance(v,bytes):\n  return v\n elif isinstance(v,bytearray):\n  return bytes(v)\n else:\n  raise errors.BytesError()\n  \n  \nBOOL_FALSE={0,'0','off','f','false','n','no'}\nBOOL_TRUE={1,'1','on','t','true','y','yes'}\n\n\ndef bool_validator(v:Any)->bool:\n if v is True or v is False:\n  return v\n if isinstance(v,bytes):\n  v=v.decode()\n if isinstance(v,str):\n  v=v.lower()\n try:\n  if v in BOOL_TRUE:\n   return True\n  if v in BOOL_FALSE:\n   return False\n except TypeError:\n  raise errors.BoolError()\n raise errors.BoolError()\n \n \n \nmax_str_int=4_300\n\n\ndef int_validator(v:Any)->int:\n if isinstance(v,int)and not(v is True or v is False):\n  return v\n  \n  \n  \n  \n  \n  \n  \n if isinstance(v,(str,bytes,bytearray))and len(v)>max_str_int:\n  raise errors.IntegerError()\n  \n try:\n  return int(v)\n except(TypeError,ValueError,OverflowError):\n  raise errors.IntegerError()\n  \n  \ndef strict_int_validator(v:Any)->int:\n if isinstance(v,int)and not(v is True or v is False):\n  return v\n raise errors.IntegerError()\n \n \ndef float_validator(v:Any)->float:\n if isinstance(v,float):\n  return v\n  \n try:\n  return float(v)\n except(TypeError,ValueError):\n  raise errors.FloatError()\n  \n  \ndef strict_float_validator(v:Any)->float:\n if isinstance(v,float):\n  return v\n raise errors.FloatError()\n \n \ndef float_finite_validator(v:'Number',field:'ModelField',config:'BaseConfig')->'Number':\n allow_inf_nan=getattr(field.type_,'allow_inf_nan',None)\n if allow_inf_nan is None:\n  allow_inf_nan=config.allow_inf_nan\n  \n if allow_inf_nan is False and(math.isnan(v)or math.isinf(v)):\n  raise errors.NumberNotFiniteError()\n return v\n \n \ndef number_multiple_validator(v:'Number',field:'ModelField')->'Number':\n field_type:ConstrainedNumber=field.type_\n if field_type.multiple_of is not None:\n  mod=float(v)/float(field_type.multiple_of)%1\n  if not almost_equal_floats(mod,0.0)and not almost_equal_floats(mod,1.0):\n   raise errors.NumberNotMultipleError(multiple_of=field_type.multiple_of)\n return v\n \n \ndef number_size_validator(v:'Number',field:'ModelField')->'Number':\n field_type:ConstrainedNumber=field.type_\n if field_type.gt is not None and not v >field_type.gt:\n  raise errors.NumberNotGtError(limit_value=field_type.gt)\n elif field_type.ge is not None and not v >=field_type.ge:\n  raise errors.NumberNotGeError(limit_value=field_type.ge)\n  \n if field_type.lt is not None and not v <field_type.lt:\n  raise errors.NumberNotLtError(limit_value=field_type.lt)\n if field_type.le is not None and not v <=field_type.le:\n  raise errors.NumberNotLeError(limit_value=field_type.le)\n  \n return v\n \n \ndef constant_validator(v:'Any',field:'ModelField')->'Any':\n ''\n\n\n\n\n \n if v !=field.default:\n  raise errors.WrongConstantError(given=v,permitted=[field.default])\n  \n return v\n \n \ndef anystr_length_validator(v:'StrBytes',config:'BaseConfig')->'StrBytes':\n v_len=len(v)\n \n min_length=config.min_anystr_length\n if v_len <min_length:\n  raise errors.AnyStrMinLengthError(limit_value=min_length)\n  \n max_length=config.max_anystr_length\n if max_length is not None and v_len >max_length:\n  raise errors.AnyStrMaxLengthError(limit_value=max_length)\n  \n return v\n \n \ndef anystr_strip_whitespace(v:'StrBytes')->'StrBytes':\n return v.strip()\n \n \ndef anystr_upper(v:'StrBytes')->'StrBytes':\n return v.upper()\n \n \ndef anystr_lower(v:'StrBytes')->'StrBytes':\n return v.lower()\n \n \ndef ordered_dict_validator(v:Any)->'AnyOrderedDict':\n if isinstance(v,OrderedDict):\n  return v\n  \n try:\n  return OrderedDict(v)\n except(TypeError,ValueError):\n  raise errors.DictError()\n  \n  \ndef dict_validator(v:Any)->Dict[Any,Any]:\n if isinstance(v,dict):\n  return v\n  \n try:\n  return dict(v)\n except(TypeError,ValueError):\n  raise errors.DictError()\n  \n  \ndef list_validator(v:Any)->List[Any]:\n if isinstance(v,list):\n  return v\n elif sequence_like(v):\n  return list(v)\n else:\n  raise errors.ListError()\n  \n  \ndef tuple_validator(v:Any)->Tuple[Any,...]:\n if isinstance(v,tuple):\n  return v\n elif sequence_like(v):\n  return tuple(v)\n else:\n  raise errors.TupleError()\n  \n  \ndef set_validator(v:Any)->Set[Any]:\n if isinstance(v,set):\n  return v\n elif sequence_like(v):\n  return set(v)\n else:\n  raise errors.SetError()\n  \n  \ndef frozenset_validator(v:Any)->FrozenSet[Any]:\n if isinstance(v,frozenset):\n  return v\n elif sequence_like(v):\n  return frozenset(v)\n else:\n  raise errors.FrozenSetError()\n  \n  \ndef deque_validator(v:Any)->Deque[Any]:\n if isinstance(v,deque):\n  return v\n elif sequence_like(v):\n  return deque(v)\n else:\n  raise errors.DequeError()\n  \n  \ndef enum_member_validator(v:Any,field:'ModelField',config:'BaseConfig')->Enum:\n try:\n  enum_v=field.type_(v)\n except ValueError:\n \n  raise errors.EnumMemberError(enum_values=list(field.type_))\n return enum_v.value if config.use_enum_values else enum_v\n \n \ndef uuid_validator(v:Any,field:'ModelField')->UUID:\n try:\n  if isinstance(v,str):\n   v=UUID(v)\n  elif isinstance(v,(bytes,bytearray)):\n   try:\n    v=UUID(v.decode())\n   except ValueError:\n   \n   \n    v=UUID(bytes=v)\n except ValueError:\n  raise errors.UUIDError()\n  \n if not isinstance(v,UUID):\n  raise errors.UUIDError()\n  \n required_version=getattr(field.type_,'_required_version',None)\n if required_version and v.version !=required_version:\n  raise errors.UUIDVersionError(required_version=required_version)\n  \n return v\n \n \ndef decimal_validator(v:Any)->Decimal:\n if isinstance(v,Decimal):\n  return v\n elif isinstance(v,(bytes,bytearray)):\n  v=v.decode()\n  \n v=str(v).strip()\n \n try:\n  v=Decimal(v)\n except DecimalException:\n  raise errors.DecimalError()\n  \n if not v.is_finite():\n  raise errors.DecimalIsNotFiniteError()\n  \n return v\n \n \ndef hashable_validator(v:Any)->Hashable:\n if isinstance(v,Hashable):\n  return v\n  \n raise errors.HashableError()\n \n \ndef ip_v4_address_validator(v:Any)->IPv4Address:\n if isinstance(v,IPv4Address):\n  return v\n  \n try:\n  return IPv4Address(v)\n except ValueError:\n  raise errors.IPv4AddressError()\n  \n  \ndef ip_v6_address_validator(v:Any)->IPv6Address:\n if isinstance(v,IPv6Address):\n  return v\n  \n try:\n  return IPv6Address(v)\n except ValueError:\n  raise errors.IPv6AddressError()\n  \n  \ndef ip_v4_network_validator(v:Any)->IPv4Network:\n ''\n\n\n\n\n \n if isinstance(v,IPv4Network):\n  return v\n  \n try:\n  return IPv4Network(v)\n except ValueError:\n  raise errors.IPv4NetworkError()\n  \n  \ndef ip_v6_network_validator(v:Any)->IPv6Network:\n ''\n\n\n\n\n \n if isinstance(v,IPv6Network):\n  return v\n  \n try:\n  return IPv6Network(v)\n except ValueError:\n  raise errors.IPv6NetworkError()\n  \n  \ndef ip_v4_interface_validator(v:Any)->IPv4Interface:\n if isinstance(v,IPv4Interface):\n  return v\n  \n try:\n  return IPv4Interface(v)\n except ValueError:\n  raise errors.IPv4InterfaceError()\n  \n  \ndef ip_v6_interface_validator(v:Any)->IPv6Interface:\n if isinstance(v,IPv6Interface):\n  return v\n  \n try:\n  return IPv6Interface(v)\n except ValueError:\n  raise errors.IPv6InterfaceError()\n  \n  \ndef path_validator(v:Any)->Path:\n if isinstance(v,Path):\n  return v\n  \n try:\n  return Path(v)\n except TypeError:\n  raise errors.PathError()\n  \n  \ndef path_exists_validator(v:Any)->Path:\n if not v.exists():\n  raise errors.PathNotExistsError(path=v)\n  \n return v\n \n \ndef callable_validator(v:Any)->AnyCallable:\n ''\n\n\n\n \n if callable(v):\n  return v\n  \n raise errors.CallableError(value=v)\n \n \ndef enum_validator(v:Any)->Enum:\n if isinstance(v,Enum):\n  return v\n  \n raise errors.EnumError(value=v)\n \n \ndef int_enum_validator(v:Any)->IntEnum:\n if isinstance(v,IntEnum):\n  return v\n  \n raise errors.IntEnumError(value=v)\n \n \ndef make_literal_validator(type_:Any)->Callable[[Any],Any]:\n permitted_choices=all_literal_values(type_)\n \n \n \n \n allowed_choices={v:v for v in permitted_choices}\n \n def literal_validator(v:Any)->Any:\n  try:\n   return allowed_choices[v]\n  except(KeyError,TypeError):\n   raise errors.WrongConstantError(given=v,permitted=permitted_choices)\n   \n return literal_validator\n \n \ndef constr_length_validator(v:'StrBytes',field:'ModelField',config:'BaseConfig')->'StrBytes':\n v_len=len(v)\n \n min_length=field.type_.min_length if field.type_.min_length is not None else config.min_anystr_length\n if v_len <min_length:\n  raise errors.AnyStrMinLengthError(limit_value=min_length)\n  \n max_length=field.type_.max_length if field.type_.max_length is not None else config.max_anystr_length\n if max_length is not None and v_len >max_length:\n  raise errors.AnyStrMaxLengthError(limit_value=max_length)\n  \n return v\n \n \ndef constr_strip_whitespace(v:'StrBytes',field:'ModelField',config:'BaseConfig')->'StrBytes':\n strip_whitespace=field.type_.strip_whitespace or config.anystr_strip_whitespace\n if strip_whitespace:\n  v=v.strip()\n  \n return v\n \n \ndef constr_upper(v:'StrBytes',field:'ModelField',config:'BaseConfig')->'StrBytes':\n upper=field.type_.to_upper or config.anystr_upper\n if upper:\n  v=v.upper()\n  \n return v\n \n \ndef constr_lower(v:'StrBytes',field:'ModelField',config:'BaseConfig')->'StrBytes':\n lower=field.type_.to_lower or config.anystr_lower\n if lower:\n  v=v.lower()\n return v\n \n \ndef validate_json(v:Any,config:'BaseConfig')->Any:\n if v is None:\n \n  return v\n try:\n  return config.json_loads(v)\n except ValueError:\n  raise errors.JsonError()\n except TypeError:\n  raise errors.JsonTypeError()\n  \n  \nT=TypeVar('T')\n\n\ndef make_arbitrary_type_validator(type_:Type[T])->Callable[[T],T]:\n def arbitrary_type_validator(v:Any)->T:\n  if isinstance(v,type_):\n   return v\n  raise errors.ArbitraryTypeError(expected_arbitrary_type=type_)\n  \n return arbitrary_type_validator\n \n \ndef make_class_validator(type_:Type[T])->Callable[[Any],Type[T]]:\n def class_validator(v:Any)->Type[T]:\n  if lenient_issubclass(v,type_):\n   return v\n  raise errors.SubclassError(expected_class=type_)\n  \n return class_validator\n \n \ndef any_class_validator(v:Any)->Type[T]:\n if isinstance(v,type):\n  return v\n raise errors.ClassError()\n \n \ndef none_validator(v:Any)->'Literal[None]':\n if v is None:\n  return v\n raise errors.NotNoneError()\n \n \ndef pattern_validator(v:Any)->Pattern[str]:\n if isinstance(v,Pattern):\n  return v\n  \n str_value=str_validator(v)\n \n try:\n  return re.compile(str_value)\n except re.error:\n  raise errors.PatternError()\n  \n  \nNamedTupleT=TypeVar('NamedTupleT',bound=NamedTuple)\n\n\ndef make_namedtuple_validator(\nnamedtuple_cls:Type[NamedTupleT],config:Type['BaseConfig']\n)->Callable[[Tuple[Any,...]],NamedTupleT]:\n from pydantic.v1.annotated_types import create_model_from_namedtuple\n \n NamedTupleModel=create_model_from_namedtuple(\n namedtuple_cls,\n __config__=config,\n __module__=namedtuple_cls.__module__,\n )\n namedtuple_cls.__pydantic_model__=NamedTupleModel\n \n def namedtuple_validator(values:Tuple[Any,...])->NamedTupleT:\n  annotations=NamedTupleModel.__annotations__\n  \n  if len(values)>len(annotations):\n   raise errors.ListMaxLengthError(limit_value=len(annotations))\n   \n  dict_values:Dict[str,Any]=dict(zip(annotations,values))\n  validated_dict_values:Dict[str,Any]=dict(NamedTupleModel(**dict_values))\n  return namedtuple_cls(**validated_dict_values)\n  \n return namedtuple_validator\n \n \ndef make_typeddict_validator(\ntypeddict_cls:Type['TypedDict'],config:Type['BaseConfig']\n)->Callable[[Any],Dict[str,Any]]:\n from pydantic.v1.annotated_types import create_model_from_typeddict\n \n TypedDictModel=create_model_from_typeddict(\n typeddict_cls,\n __config__=config,\n __module__=typeddict_cls.__module__,\n )\n typeddict_cls.__pydantic_model__=TypedDictModel\n \n def typeddict_validator(values:'TypedDict')->Dict[str,Any]:\n  return TypedDictModel.parse_obj(values).dict(exclude_unset=True)\n  \n return typeddict_validator\n \n \nclass IfConfig:\n def __init__(self,validator:AnyCallable,*config_attr_names:str,ignored_value:Any=False)->None:\n  self.validator=validator\n  self.config_attr_names=config_attr_names\n  self.ignored_value=ignored_value\n  \n def check(self,config:Type['BaseConfig'])->bool:\n  return any(getattr(config,name)not in{None,self.ignored_value}for name in self.config_attr_names)\n  \n  \n  \n  \n_VALIDATORS:List[Tuple[Type[Any],List[Any]]]=[\n(IntEnum,[int_validator,enum_member_validator]),\n(Enum,[enum_member_validator]),\n(\nstr,\n[\nstr_validator,\nIfConfig(anystr_strip_whitespace,'anystr_strip_whitespace'),\nIfConfig(anystr_upper,'anystr_upper'),\nIfConfig(anystr_lower,'anystr_lower'),\nIfConfig(anystr_length_validator,'min_anystr_length','max_anystr_length'),\n],\n),\n(\nbytes,\n[\nbytes_validator,\nIfConfig(anystr_strip_whitespace,'anystr_strip_whitespace'),\nIfConfig(anystr_upper,'anystr_upper'),\nIfConfig(anystr_lower,'anystr_lower'),\nIfConfig(anystr_length_validator,'min_anystr_length','max_anystr_length'),\n],\n),\n(bool,[bool_validator]),\n(int,[int_validator]),\n(float,[float_validator,IfConfig(float_finite_validator,'allow_inf_nan',ignored_value=True)]),\n(Path,[path_validator]),\n(datetime,[parse_datetime]),\n(date,[parse_date]),\n(time,[parse_time]),\n(timedelta,[parse_duration]),\n(OrderedDict,[ordered_dict_validator]),\n(dict,[dict_validator]),\n(list,[list_validator]),\n(tuple,[tuple_validator]),\n(set,[set_validator]),\n(frozenset,[frozenset_validator]),\n(deque,[deque_validator]),\n(UUID,[uuid_validator]),\n(Decimal,[decimal_validator]),\n(IPv4Interface,[ip_v4_interface_validator]),\n(IPv6Interface,[ip_v6_interface_validator]),\n(IPv4Address,[ip_v4_address_validator]),\n(IPv6Address,[ip_v6_address_validator]),\n(IPv4Network,[ip_v4_network_validator]),\n(IPv6Network,[ip_v6_network_validator]),\n]\n\n\ndef find_validators(\ntype_:Type[Any],config:Type['BaseConfig']\n)->Generator[AnyCallable,None,None]:\n from pydantic.v1.dataclasses import is_builtin_dataclass,make_dataclass_validator\n \n if type_ is Any or type_ is object:\n  return\n type_type=type_.__class__\n if type_type ==ForwardRef or type_type ==TypeVar:\n  return\n  \n if is_none_type(type_):\n  yield none_validator\n  return\n if type_ is Pattern or type_ is re.Pattern:\n  yield pattern_validator\n  return\n if type_ is Hashable or type_ is CollectionsHashable:\n  yield hashable_validator\n  return\n if is_callable_type(type_):\n  yield callable_validator\n  return\n if is_literal_type(type_):\n  yield make_literal_validator(type_)\n  return\n if is_builtin_dataclass(type_):\n  yield from make_dataclass_validator(type_,config)\n  return\n if type_ is Enum:\n  yield enum_validator\n  return\n if type_ is IntEnum:\n  yield int_enum_validator\n  return\n if is_namedtuple(type_):\n  yield tuple_validator\n  yield make_namedtuple_validator(type_,config)\n  return\n if is_typeddict(type_):\n  yield make_typeddict_validator(type_,config)\n  return\n  \n class_=get_class(type_)\n if class_ is not None:\n  if class_ is not Any and isinstance(class_,type):\n   yield make_class_validator(class_)\n  else:\n   yield any_class_validator\n  return\n  \n for val_type,validators in _VALIDATORS:\n  try:\n   if issubclass(type_,val_type):\n    for v in validators:\n     if isinstance(v,IfConfig):\n      if v.check(config):\n       yield v.validator\n     else:\n      yield v\n    return\n  except TypeError:\n   raise RuntimeError(f'error checking inheritance of {type_ !r} (type: {display_as_type(type_)})')\n   \n if config.arbitrary_types_allowed:\n  yield make_arbitrary_type_validator(type_)\n else:\n  if hasattr(type_,'__pydantic_core_schema__'):\n   warn(f'Mixing V1 and V2 models is not supported. `{type_.__name__}` is a V2 model.',UserWarning)\n  raise RuntimeError(f'no validator found for {type_}, see `arbitrary_types_allowed` in Config')\n", ["collections", "collections.abc", "datetime", "decimal", "enum", "ipaddress", "math", "pathlib", "pydantic.v1", "pydantic.v1.annotated_types", "pydantic.v1.config", "pydantic.v1.dataclasses", "pydantic.v1.datetime_parse", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.types", "pydantic.v1.typing", "pydantic.v1.utils", "re", "typing", "typing_extensions", "uuid", "warnings"]], "pydantic.v1.mypy": [".py", "import sys\nfrom configparser import ConfigParser\nfrom typing import Any,Callable,Dict,List,Optional,Set,Tuple,Type as TypingType,Union\n\nfrom mypy.errorcodes import ErrorCode\nfrom mypy.nodes import(\nARG_NAMED,\nARG_NAMED_OPT,\nARG_OPT,\nARG_POS,\nARG_STAR2,\nMDEF,\nArgument,\nAssignmentStmt,\nBlock,\nCallExpr,\nClassDef,\nContext,\nDecorator,\nEllipsisExpr,\nFuncBase,\nFuncDef,\nJsonDict,\nMemberExpr,\nNameExpr,\nPassStmt,\nPlaceholderNode,\nRefExpr,\nStrExpr,\nSymbolNode,\nSymbolTableNode,\nTempNode,\nTypeInfo,\nTypeVarExpr,\nVar,\n)\nfrom mypy.options import Options\nfrom mypy.plugin import(\nCheckerPluginInterface,\nClassDefContext,\nFunctionContext,\nMethodContext,\nPlugin,\nReportConfigContext,\nSemanticAnalyzerPluginInterface,\n)\nfrom mypy.plugins import dataclasses\nfrom mypy.semanal import set_callable_name\nfrom mypy.server.trigger import make_wildcard_trigger\nfrom mypy.types import(\nAnyType,\nCallableType,\nInstance,\nNoneType,\nOverloaded,\nProperType,\nType,\nTypeOfAny,\nTypeType,\nTypeVarId,\nTypeVarType,\nUnionType,\nget_proper_type,\n)\nfrom mypy.typevars import fill_typevars\nfrom mypy.util import get_unique_redefinition_name\nfrom mypy.version import __version__ as mypy_version\n\nfrom pydantic.v1.utils import is_valid_field\n\ntry:\n from mypy.types import TypeVarDef\nexcept ImportError:\n\n from mypy.types import TypeVarType as TypeVarDef\n \nCONFIGFILE_KEY='pydantic-mypy'\nMETADATA_KEY='pydantic-mypy-metadata'\n_NAMESPACE=__name__[:-5]\nBASEMODEL_FULLNAME=f'{_NAMESPACE}.main.BaseModel'\nBASESETTINGS_FULLNAME=f'{_NAMESPACE}.env_settings.BaseSettings'\nMODEL_METACLASS_FULLNAME=f'{_NAMESPACE}.main.ModelMetaclass'\nFIELD_FULLNAME=f'{_NAMESPACE}.fields.Field'\nDATACLASS_FULLNAME=f'{_NAMESPACE}.dataclasses.dataclass'\n\n\ndef parse_mypy_version(version:str)->Tuple[int,...]:\n return tuple(map(int,version.partition('+')[0].split('.')))\n \n \nMYPY_VERSION_TUPLE=parse_mypy_version(mypy_version)\nBUILTINS_NAME='builtins'if MYPY_VERSION_TUPLE >=(0,930)else '__builtins__'\n\n\n__version__=2\n\n\ndef plugin(version:str)->'TypingType[Plugin]':\n ''\n\n\n\n\n \n return PydanticPlugin\n \n \nclass PydanticPlugin(Plugin):\n def __init__(self,options:Options)->None:\n  self.plugin_config=PydanticPluginConfig(options)\n  self._plugin_data=self.plugin_config.to_data()\n  super().__init__(options)\n  \n def get_base_class_hook(self,fullname:str)->'Optional[Callable[[ClassDefContext], None]]':\n  sym=self.lookup_fully_qualified(fullname)\n  if sym and isinstance(sym.node,TypeInfo):\n  \n   if any(get_fullname(base)==BASEMODEL_FULLNAME for base in sym.node.mro):\n    return self._pydantic_model_class_maker_callback\n  return None\n  \n def get_metaclass_hook(self,fullname:str)->Optional[Callable[[ClassDefContext],None]]:\n  if fullname ==MODEL_METACLASS_FULLNAME:\n   return self._pydantic_model_metaclass_marker_callback\n  return None\n  \n def get_function_hook(self,fullname:str)->'Optional[Callable[[FunctionContext], Type]]':\n  sym=self.lookup_fully_qualified(fullname)\n  if sym and sym.fullname ==FIELD_FULLNAME:\n   return self._pydantic_field_callback\n  return None\n  \n def get_method_hook(self,fullname:str)->Optional[Callable[[MethodContext],Type]]:\n  if fullname.endswith('.from_orm'):\n   return from_orm_callback\n  return None\n  \n def get_class_decorator_hook(self,fullname:str)->Optional[Callable[[ClassDefContext],None]]:\n  ''\n\n\n  \n  if fullname ==DATACLASS_FULLNAME and MYPY_VERSION_TUPLE <(1,1):\n   return dataclasses.dataclass_class_maker_callback\n  return None\n  \n def report_config_data(self,ctx:ReportConfigContext)->Dict[str,Any]:\n  ''\n\n\n  \n  return self._plugin_data\n  \n def _pydantic_model_class_maker_callback(self,ctx:ClassDefContext)->None:\n  transformer=PydanticModelTransformer(ctx,self.plugin_config)\n  transformer.transform()\n  \n def _pydantic_model_metaclass_marker_callback(self,ctx:ClassDefContext)->None:\n  ''\n\n\n\n  \n  if self.plugin_config.debug_dataclass_transform:\n   return\n  info_metaclass=ctx.cls.info.declared_metaclass\n  assert info_metaclass,\"callback not passed from 'get_metaclass_hook'\"\n  if getattr(info_metaclass.type,'dataclass_transform_spec',None):\n   info_metaclass.type.dataclass_transform_spec=None\n   \n def _pydantic_field_callback(self,ctx:FunctionContext)->'Type':\n  ''\n\n\n\n\n\n\n  \n  default_any_type=ctx.default_return_type\n  \n  assert ctx.callee_arg_names[0]=='default','\"default\" is no longer first argument in Field()'\n  assert ctx.callee_arg_names[1]=='default_factory','\"default_factory\" is no longer second argument in Field()'\n  default_args=ctx.args[0]\n  default_factory_args=ctx.args[1]\n  \n  if default_args and default_factory_args:\n   error_default_and_default_factory_specified(ctx.api,ctx.context)\n   return default_any_type\n   \n  if default_args:\n   default_type=ctx.arg_types[0][0]\n   default_arg=default_args[0]\n   \n   \n   if not isinstance(default_arg,EllipsisExpr):\n    return default_type\n    \n  elif default_factory_args:\n   default_factory_type=ctx.arg_types[1][0]\n   \n   \n   \n   if isinstance(default_factory_type,Overloaded):\n    if MYPY_VERSION_TUPLE >(0,910):\n     default_factory_type=default_factory_type.items[0]\n    else:\n    \n     default_factory_type=default_factory_type.items()[0]\n     \n   if isinstance(default_factory_type,CallableType):\n    ret_type=default_factory_type.ret_type\n    \n    \n    args=getattr(ret_type,'args',None)\n    if args:\n     if all(isinstance(arg,TypeVarType)for arg in args):\n     \n      ret_type.args=tuple(default_any_type for _ in args)\n    return ret_type\n    \n  return default_any_type\n  \n  \nclass PydanticPluginConfig:\n __slots__=(\n 'init_forbid_extra',\n 'init_typed',\n 'warn_required_dynamic_aliases',\n 'warn_untyped_fields',\n 'debug_dataclass_transform',\n )\n init_forbid_extra:bool\n init_typed:bool\n warn_required_dynamic_aliases:bool\n warn_untyped_fields:bool\n debug_dataclass_transform:bool\n \n def __init__(self,options:Options)->None:\n  if options.config_file is None:\n   return\n   \n  toml_config=parse_toml(options.config_file)\n  if toml_config is not None:\n   config=toml_config.get('tool',{}).get('pydantic-mypy',{})\n   for key in self.__slots__:\n    setting=config.get(key,False)\n    if not isinstance(setting,bool):\n     raise ValueError(f'Configuration value must be a boolean for key: {key}')\n    setattr(self,key,setting)\n  else:\n   plugin_config=ConfigParser()\n   plugin_config.read(options.config_file)\n   for key in self.__slots__:\n    setting=plugin_config.getboolean(CONFIGFILE_KEY,key,fallback=False)\n    setattr(self,key,setting)\n    \n def to_data(self)->Dict[str,Any]:\n  return{key:getattr(self,key)for key in self.__slots__}\n  \n  \ndef from_orm_callback(ctx:MethodContext)->Type:\n ''\n\n \n model_type:Instance\n ctx_type=ctx.type\n if isinstance(ctx_type,TypeType):\n  ctx_type=ctx_type.item\n if isinstance(ctx_type,CallableType)and isinstance(ctx_type.ret_type,Instance):\n  model_type=ctx_type.ret_type\n elif isinstance(ctx_type,Instance):\n  model_type=ctx_type\n else:\n  detail=f'ctx.type: {ctx_type} (of type {ctx_type.__class__.__name__})'\n  error_unexpected_behavior(detail,ctx.api,ctx.context)\n  return ctx.default_return_type\n pydantic_metadata=model_type.type.metadata.get(METADATA_KEY)\n if pydantic_metadata is None:\n  return ctx.default_return_type\n orm_mode=pydantic_metadata.get('config',{}).get('orm_mode')\n if orm_mode is not True:\n  error_from_orm(get_name(model_type.type),ctx.api,ctx.context)\n return ctx.default_return_type\n \n \nclass PydanticModelTransformer:\n tracked_config_fields:Set[str]={\n 'extra',\n 'allow_mutation',\n 'frozen',\n 'orm_mode',\n 'allow_population_by_field_name',\n 'alias_generator',\n }\n \n def __init__(self,ctx:ClassDefContext,plugin_config:PydanticPluginConfig)->None:\n  self._ctx=ctx\n  self.plugin_config=plugin_config\n  \n def transform(self)->None:\n  ''\n\n\n\n\n\n\n\n  \n  ctx=self._ctx\n  info=ctx.cls.info\n  \n  self.adjust_validator_signatures()\n  config=self.collect_config()\n  fields=self.collect_fields(config)\n  is_settings=any(get_fullname(base)==BASESETTINGS_FULLNAME for base in info.mro[:-1])\n  self.add_initializer(fields,config,is_settings)\n  self.add_construct_method(fields)\n  self.set_frozen(fields,frozen=config.allow_mutation is False or config.frozen is True)\n  info.metadata[METADATA_KEY]={\n  'fields':{field.name:field.serialize()for field in fields},\n  'config':config.set_values_dict(),\n  }\n  \n def adjust_validator_signatures(self)->None:\n  ''\n\n\n\n\n\n  \n  for name,sym in self._ctx.cls.info.names.items():\n   if isinstance(sym.node,Decorator):\n    first_dec=sym.node.original_decorators[0]\n    if(\n    isinstance(first_dec,CallExpr)\n    and isinstance(first_dec.callee,NameExpr)\n    and first_dec.callee.fullname ==f'{_NAMESPACE}.class_validators.validator'\n    ):\n     sym.node.func.is_class=True\n     \n def collect_config(self)->'ModelConfigData':\n  ''\n\n  \n  ctx=self._ctx\n  cls=ctx.cls\n  config=ModelConfigData()\n  for stmt in cls.defs.body:\n   if not isinstance(stmt,ClassDef):\n    continue\n   if stmt.name =='Config':\n    for substmt in stmt.defs.body:\n     if not isinstance(substmt,AssignmentStmt):\n      continue\n     config.update(self.get_config_update(substmt))\n    if(\n    config.has_alias_generator\n    and not config.allow_population_by_field_name\n    and self.plugin_config.warn_required_dynamic_aliases\n    ):\n     error_required_dynamic_aliases(ctx.api,stmt)\n  for info in cls.info.mro[1:]:\n   if METADATA_KEY not in info.metadata:\n    continue\n    \n    \n   ctx.api.add_plugin_dependency(make_wildcard_trigger(get_fullname(info)))\n   for name,value in info.metadata[METADATA_KEY]['config'].items():\n    config.setdefault(name,value)\n  return config\n  \n def collect_fields(self,model_config:'ModelConfigData')->List['PydanticModelField']:\n  ''\n\n  \n  \n  ctx=self._ctx\n  cls=self._ctx.cls\n  fields=[]\n  known_fields=set()\n  for stmt in cls.defs.body:\n   if not isinstance(stmt,AssignmentStmt):\n    continue\n    \n   lhs=stmt.lvalues[0]\n   if not isinstance(lhs,NameExpr)or not is_valid_field(lhs.name):\n    continue\n    \n   if not stmt.new_syntax and self.plugin_config.warn_untyped_fields:\n    error_untyped_fields(ctx.api,stmt)\n    \n    \n    \n    \n   sym=cls.info.names.get(lhs.name)\n   if sym is None:\n   \n   \n    continue\n    \n   node=sym.node\n   if isinstance(node,PlaceholderNode):\n   \n   \n   \n    continue\n   if not isinstance(node,Var):\n   \n   \n    continue\n    \n    \n   if node.is_classvar:\n    continue\n    \n   is_required=self.get_is_required(cls,stmt,lhs)\n   alias,has_dynamic_alias=self.get_alias_info(stmt)\n   if(\n   has_dynamic_alias\n   and not model_config.allow_population_by_field_name\n   and self.plugin_config.warn_required_dynamic_aliases\n   ):\n    error_required_dynamic_aliases(ctx.api,stmt)\n   fields.append(\n   PydanticModelField(\n   name=lhs.name,\n   is_required=is_required,\n   alias=alias,\n   has_dynamic_alias=has_dynamic_alias,\n   line=stmt.line,\n   column=stmt.column,\n   )\n   )\n   known_fields.add(lhs.name)\n  all_fields=fields.copy()\n  for info in cls.info.mro[1:]:\n   if METADATA_KEY not in info.metadata:\n    continue\n    \n   superclass_fields=[]\n   \n   ctx.api.add_plugin_dependency(make_wildcard_trigger(get_fullname(info)))\n   \n   for name,data in info.metadata[METADATA_KEY]['fields'].items():\n    if name not in known_fields:\n     field=PydanticModelField.deserialize(info,data)\n     known_fields.add(name)\n     superclass_fields.append(field)\n    else:\n     (field,)=(a for a in all_fields if a.name ==name)\n     all_fields.remove(field)\n     superclass_fields.append(field)\n   all_fields=superclass_fields+all_fields\n  return all_fields\n  \n def add_initializer(self,fields:List['PydanticModelField'],config:'ModelConfigData',is_settings:bool)->None:\n  ''\n\n\n\n  \n  ctx=self._ctx\n  typed=self.plugin_config.init_typed\n  use_alias=config.allow_population_by_field_name is not True\n  force_all_optional=is_settings or bool(\n  config.has_alias_generator and not config.allow_population_by_field_name\n  )\n  init_arguments=self.get_field_arguments(\n  fields,typed=typed,force_all_optional=force_all_optional,use_alias=use_alias\n  )\n  if not self.should_init_forbid_extra(fields,config):\n   var=Var('kwargs')\n   init_arguments.append(Argument(var,AnyType(TypeOfAny.explicit),None,ARG_STAR2))\n   \n  if '__init__'not in ctx.cls.info.names:\n   add_method(ctx,'__init__',init_arguments,NoneType())\n   \n def add_construct_method(self,fields:List['PydanticModelField'])->None:\n  ''\n\n\n\n\n  \n  ctx=self._ctx\n  set_str=ctx.api.named_type(f'{BUILTINS_NAME}.set',[ctx.api.named_type(f'{BUILTINS_NAME}.str')])\n  optional_set_str=UnionType([set_str,NoneType()])\n  fields_set_argument=Argument(Var('_fields_set',optional_set_str),optional_set_str,None,ARG_OPT)\n  construct_arguments=self.get_field_arguments(fields,typed=True,force_all_optional=False,use_alias=False)\n  construct_arguments=[fields_set_argument]+construct_arguments\n  \n  obj_type=ctx.api.named_type(f'{BUILTINS_NAME}.object')\n  self_tvar_name='_PydanticBaseModel'\n  tvar_fullname=ctx.cls.fullname+'.'+self_tvar_name\n  if MYPY_VERSION_TUPLE >=(1,4):\n   tvd=TypeVarType(\n   self_tvar_name,\n   tvar_fullname,\n   (\n   TypeVarId(-1,namespace=ctx.cls.fullname+'.construct')\n   if MYPY_VERSION_TUPLE >=(1,11)\n   else TypeVarId(-1)\n   ),\n   [],\n   obj_type,\n   AnyType(TypeOfAny.from_omitted_generics),\n   )\n   self_tvar_expr=TypeVarExpr(\n   self_tvar_name,\n   tvar_fullname,\n   [],\n   obj_type,\n   AnyType(TypeOfAny.from_omitted_generics),\n   )\n  else:\n   tvd=TypeVarDef(self_tvar_name,tvar_fullname,-1,[],obj_type)\n   self_tvar_expr=TypeVarExpr(self_tvar_name,tvar_fullname,[],obj_type)\n  ctx.cls.info.names[self_tvar_name]=SymbolTableNode(MDEF,self_tvar_expr)\n  \n  \n  if isinstance(tvd,TypeVarType):\n   self_type=tvd\n  else:\n   self_type=TypeVarType(tvd)\n   \n  add_method(\n  ctx,\n  'construct',\n  construct_arguments,\n  return_type=self_type,\n  self_type=self_type,\n  tvar_def=tvd,\n  is_classmethod=True,\n  )\n  \n def set_frozen(self,fields:List['PydanticModelField'],frozen:bool)->None:\n  ''\n\n\n\n  \n  ctx=self._ctx\n  info=ctx.cls.info\n  for field in fields:\n   sym_node=info.names.get(field.name)\n   if sym_node is not None:\n    var=sym_node.node\n    if isinstance(var,Var):\n     var.is_property=frozen\n    elif isinstance(var,PlaceholderNode)and not ctx.api.final_iteration:\n    \n     ctx.api.defer()\n    else:\n    \n     try:\n      var_str=str(var)\n     except TypeError:\n     \n      var_str=repr(var)\n     detail=f'sym_node.node: {var_str} (of type {var.__class__})'\n     error_unexpected_behavior(detail,ctx.api,ctx.cls)\n   else:\n    var=field.to_var(info,use_alias=False)\n    var.info=info\n    var.is_property=frozen\n    var._fullname=get_fullname(info)+'.'+get_name(var)\n    info.names[get_name(var)]=SymbolTableNode(MDEF,var)\n    \n def get_config_update(self,substmt:AssignmentStmt)->Optional['ModelConfigData']:\n  ''\n\n\n\n  \n  lhs=substmt.lvalues[0]\n  if not(isinstance(lhs,NameExpr)and lhs.name in self.tracked_config_fields):\n   return None\n  if lhs.name =='extra':\n   if isinstance(substmt.rvalue,StrExpr):\n    forbid_extra=substmt.rvalue.value =='forbid'\n   elif isinstance(substmt.rvalue,MemberExpr):\n    forbid_extra=substmt.rvalue.name =='forbid'\n   else:\n    error_invalid_config_value(lhs.name,self._ctx.api,substmt)\n    return None\n   return ModelConfigData(forbid_extra=forbid_extra)\n  if lhs.name =='alias_generator':\n   has_alias_generator=True\n   if isinstance(substmt.rvalue,NameExpr)and substmt.rvalue.fullname =='builtins.None':\n    has_alias_generator=False\n   return ModelConfigData(has_alias_generator=has_alias_generator)\n  if isinstance(substmt.rvalue,NameExpr)and substmt.rvalue.fullname in('builtins.True','builtins.False'):\n   return ModelConfigData(**{lhs.name:substmt.rvalue.fullname =='builtins.True'})\n  error_invalid_config_value(lhs.name,self._ctx.api,substmt)\n  return None\n  \n @staticmethod\n def get_is_required(cls:ClassDef,stmt:AssignmentStmt,lhs:NameExpr)->bool:\n  ''\n\n  \n  expr=stmt.rvalue\n  if isinstance(expr,TempNode):\n  \n   value_type=get_proper_type(cls.info[lhs.name].type)\n   return not PydanticModelTransformer.type_has_implicit_default(value_type)\n  if isinstance(expr,CallExpr)and isinstance(expr.callee,RefExpr)and expr.callee.fullname ==FIELD_FULLNAME:\n  \n  \n  \n   for arg,name in zip(expr.args,expr.arg_names):\n   \n    if name is None or name =='default':\n     return arg.__class__ is EllipsisExpr\n    if name =='default_factory':\n     return False\n     \n   value_type=get_proper_type(cls.info[lhs.name].type)\n   return not PydanticModelTransformer.type_has_implicit_default(value_type)\n   \n  return isinstance(expr,EllipsisExpr)\n  \n @staticmethod\n def type_has_implicit_default(type_:Optional[ProperType])->bool:\n  ''\n\n\n\n  \n  if isinstance(type_,AnyType):\n  \n   return True\n  if isinstance(type_,UnionType)and any(\n  isinstance(item,NoneType)or isinstance(item,AnyType)for item in type_.items\n  ):\n  \n   return True\n  return False\n  \n @staticmethod\n def get_alias_info(stmt:AssignmentStmt)->Tuple[Optional[str],bool]:\n  ''\n\n\n\n\n  \n  expr=stmt.rvalue\n  if isinstance(expr,TempNode):\n  \n   return None,False\n   \n  if not(\n  isinstance(expr,CallExpr)and isinstance(expr.callee,RefExpr)and expr.callee.fullname ==FIELD_FULLNAME\n  ):\n  \n   return None,False\n   \n  for i,arg_name in enumerate(expr.arg_names):\n   if arg_name !='alias':\n    continue\n   arg=expr.args[i]\n   if isinstance(arg,StrExpr):\n    return arg.value,False\n   else:\n    return None,True\n  return None,False\n  \n def get_field_arguments(\n self,fields:List['PydanticModelField'],typed:bool,force_all_optional:bool,use_alias:bool\n )->List[Argument]:\n  ''\n\n\n\n  \n  info=self._ctx.cls.info\n  arguments=[\n  field.to_argument(info,typed=typed,force_optional=force_all_optional,use_alias=use_alias)\n  for field in fields\n  if not(use_alias and field.has_dynamic_alias)\n  ]\n  return arguments\n  \n def should_init_forbid_extra(self,fields:List['PydanticModelField'],config:'ModelConfigData')->bool:\n  ''\n\n\n\n\n  \n  if not config.allow_population_by_field_name:\n   if self.is_dynamic_alias_present(fields,bool(config.has_alias_generator)):\n    return False\n  if config.forbid_extra:\n   return True\n  return self.plugin_config.init_forbid_extra\n  \n @staticmethod\n def is_dynamic_alias_present(fields:List['PydanticModelField'],has_alias_generator:bool)->bool:\n  ''\n\n\n  \n  for field in fields:\n   if field.has_dynamic_alias:\n    return True\n  if has_alias_generator:\n   for field in fields:\n    if field.alias is None:\n     return True\n  return False\n  \n  \nclass PydanticModelField:\n def __init__(\n self,name:str,is_required:bool,alias:Optional[str],has_dynamic_alias:bool,line:int,column:int\n ):\n  self.name=name\n  self.is_required=is_required\n  self.alias=alias\n  self.has_dynamic_alias=has_dynamic_alias\n  self.line=line\n  self.column=column\n  \n def to_var(self,info:TypeInfo,use_alias:bool)->Var:\n  name=self.name\n  if use_alias and self.alias is not None:\n   name=self.alias\n  return Var(name,info[self.name].type)\n  \n def to_argument(self,info:TypeInfo,typed:bool,force_optional:bool,use_alias:bool)->Argument:\n  if typed and info[self.name].type is not None:\n   type_annotation=info[self.name].type\n  else:\n   type_annotation=AnyType(TypeOfAny.explicit)\n  return Argument(\n  variable=self.to_var(info,use_alias),\n  type_annotation=type_annotation,\n  initializer=None,\n  kind=ARG_NAMED_OPT if force_optional or not self.is_required else ARG_NAMED,\n  )\n  \n def serialize(self)->JsonDict:\n  return self.__dict__\n  \n @classmethod\n def deserialize(cls,info:TypeInfo,data:JsonDict)->'PydanticModelField':\n  return cls(**data)\n  \n  \nclass ModelConfigData:\n def __init__(\n self,\n forbid_extra:Optional[bool]=None,\n allow_mutation:Optional[bool]=None,\n frozen:Optional[bool]=None,\n orm_mode:Optional[bool]=None,\n allow_population_by_field_name:Optional[bool]=None,\n has_alias_generator:Optional[bool]=None,\n ):\n  self.forbid_extra=forbid_extra\n  self.allow_mutation=allow_mutation\n  self.frozen=frozen\n  self.orm_mode=orm_mode\n  self.allow_population_by_field_name=allow_population_by_field_name\n  self.has_alias_generator=has_alias_generator\n  \n def set_values_dict(self)->Dict[str,Any]:\n  return{k:v for k,v in self.__dict__.items()if v is not None}\n  \n def update(self,config:Optional['ModelConfigData'])->None:\n  if config is None:\n   return\n  for k,v in config.set_values_dict().items():\n   setattr(self,k,v)\n   \n def setdefault(self,key:str,value:Any)->None:\n  if getattr(self,key)is None:\n   setattr(self,key,value)\n   \n   \nERROR_ORM=ErrorCode('pydantic-orm','Invalid from_orm call','Pydantic')\nERROR_CONFIG=ErrorCode('pydantic-config','Invalid config value','Pydantic')\nERROR_ALIAS=ErrorCode('pydantic-alias','Dynamic alias disallowed','Pydantic')\nERROR_UNEXPECTED=ErrorCode('pydantic-unexpected','Unexpected behavior','Pydantic')\nERROR_UNTYPED=ErrorCode('pydantic-field','Untyped field disallowed','Pydantic')\nERROR_FIELD_DEFAULTS=ErrorCode('pydantic-field','Invalid Field defaults','Pydantic')\n\n\ndef error_from_orm(model_name:str,api:CheckerPluginInterface,context:Context)->None:\n api.fail(f'\"{model_name}\" does not have orm_mode=True',context,code=ERROR_ORM)\n \n \ndef error_invalid_config_value(name:str,api:SemanticAnalyzerPluginInterface,context:Context)->None:\n api.fail(f'Invalid value for \"Config.{name}\"',context,code=ERROR_CONFIG)\n \n \ndef error_required_dynamic_aliases(api:SemanticAnalyzerPluginInterface,context:Context)->None:\n api.fail('Required dynamic aliases disallowed',context,code=ERROR_ALIAS)\n \n \ndef error_unexpected_behavior(\ndetail:str,api:Union[CheckerPluginInterface,SemanticAnalyzerPluginInterface],context:Context\n)->None:\n\n link='https://github.com/pydantic/pydantic/issues/new/choose'\n full_message=f'The pydantic mypy plugin ran into unexpected behavior: {detail}\\n'\n full_message +=f'Please consider reporting this bug at {link} so we can try to fix it!'\n api.fail(full_message,context,code=ERROR_UNEXPECTED)\n \n \ndef error_untyped_fields(api:SemanticAnalyzerPluginInterface,context:Context)->None:\n api.fail('Untyped fields disallowed',context,code=ERROR_UNTYPED)\n \n \ndef error_default_and_default_factory_specified(api:CheckerPluginInterface,context:Context)->None:\n api.fail('Field default and default_factory cannot be specified together',context,code=ERROR_FIELD_DEFAULTS)\n \n \ndef add_method(\nctx:ClassDefContext,\nname:str,\nargs:List[Argument],\nreturn_type:Type,\nself_type:Optional[Type]=None,\ntvar_def:Optional[TypeVarDef]=None,\nis_classmethod:bool=False,\nis_new:bool=False,\n\n)->None:\n ''\n\n\n\n \n info=ctx.cls.info\n \n \n \n if name in info.names:\n  sym=info.names[name]\n  if sym.plugin_generated and isinstance(sym.node,FuncDef):\n   ctx.cls.defs.body.remove(sym.node)\n   \n self_type=self_type or fill_typevars(info)\n if is_classmethod or is_new:\n  first=[Argument(Var('_cls'),TypeType.make_normalized(self_type),None,ARG_POS)]\n  \n  \n else:\n  self_type=self_type or fill_typevars(info)\n  first=[Argument(Var('__pydantic_self__'),self_type,None,ARG_POS)]\n args=first+args\n arg_types,arg_names,arg_kinds=[],[],[]\n for arg in args:\n  assert arg.type_annotation,'All arguments must be fully typed.'\n  arg_types.append(arg.type_annotation)\n  arg_names.append(get_name(arg.variable))\n  arg_kinds.append(arg.kind)\n  \n function_type=ctx.api.named_type(f'{BUILTINS_NAME}.function')\n signature=CallableType(arg_types,arg_kinds,arg_names,return_type,function_type)\n if tvar_def:\n  signature.variables=[tvar_def]\n  \n func=FuncDef(name,args,Block([PassStmt()]))\n func.info=info\n func.type=set_callable_name(signature,func)\n func.is_class=is_classmethod\n \n func._fullname=get_fullname(info)+'.'+name\n func.line=info.line\n \n \n \n if name in info.names:\n \n  r_name=get_unique_redefinition_name(name,info.names)\n  info.names[r_name]=info.names[name]\n  \n if is_classmethod:\n  func.is_decorated=True\n  v=Var(name,func.type)\n  v.info=info\n  v._fullname=func._fullname\n  \n  v.is_classmethod=True\n  dec=Decorator(func,[NameExpr('classmethod')],v)\n  \n  \n  \n  \n  dec.line=info.line\n  sym=SymbolTableNode(MDEF,dec)\n else:\n  sym=SymbolTableNode(MDEF,func)\n sym.plugin_generated=True\n \n info.names[name]=sym\n info.defn.defs.body.append(func)\n \n \ndef get_fullname(x:Union[FuncBase,SymbolNode])->str:\n ''\n\n \n fn=x.fullname\n if callable(fn):\n  return fn()\n return fn\n \n \ndef get_name(x:Union[FuncBase,SymbolNode])->str:\n ''\n\n \n fn=x.name\n if callable(fn):\n  return fn()\n return fn\n \n \ndef parse_toml(config_file:str)->Optional[Dict[str,Any]]:\n if not config_file.endswith('.toml'):\n  return None\n  \n read_mode='rb'\n if sys.version_info >=(3,11):\n  import tomllib as toml_\n else:\n  try:\n   import tomli as toml_\n  except ImportError:\n  \n   read_mode='r'\n   try:\n    import toml as toml_\n   except ImportError:\n    import warnings\n    \n    warnings.warn('No TOML parser installed, cannot read configuration from `pyproject.toml`.')\n    return None\n    \n with open(config_file,read_mode)as rf:\n  return toml_.load(rf)\n", ["configparser", "mypy.errorcodes", "mypy.nodes", "mypy.options", "mypy.plugin", "mypy.plugins", "mypy.semanal", "mypy.server.trigger", "mypy.types", "mypy.typevars", "mypy.util", "mypy.version", "pydantic.v1.utils", "sys", "toml", "tomli", "tomllib", "typing", "warnings"]], "pydantic.v1.networks": [".py", "import re\nfrom ipaddress import(\nIPv4Address,\nIPv4Interface,\nIPv4Network,\nIPv6Address,\nIPv6Interface,\nIPv6Network,\n_BaseAddress,\n_BaseNetwork,\n)\nfrom typing import(\nTYPE_CHECKING,\nAny,\nCollection,\nDict,\nGenerator,\nList,\nMatch,\nOptional,\nPattern,\nSet,\nTuple,\nType,\nUnion,\ncast,\nno_type_check,\n)\n\nfrom pydantic.v1 import errors\nfrom pydantic.v1.utils import Representation,update_not_none\nfrom pydantic.v1.validators import constr_length_validator,str_validator\n\nif TYPE_CHECKING:\n import email_validator\n from typing_extensions import TypedDict\n \n from pydantic.v1.config import BaseConfig\n from pydantic.v1.fields import ModelField\n from pydantic.v1.typing import AnyCallable\n \n CallableGenerator=Generator[AnyCallable,None,None]\n \n class Parts(TypedDict,total=False):\n  scheme:str\n  user:Optional[str]\n  password:Optional[str]\n  ipv4:Optional[str]\n  ipv6:Optional[str]\n  domain:Optional[str]\n  port:Optional[str]\n  path:Optional[str]\n  query:Optional[str]\n  fragment:Optional[str]\n  \n class HostParts(TypedDict,total=False):\n  host:str\n  tld:Optional[str]\n  host_type:Optional[str]\n  port:Optional[str]\n  rebuild:bool\n  \nelse:\n email_validator=None\n \n class Parts(dict):\n  pass\n  \n  \nNetworkType=Union[str,bytes,int,Tuple[Union[str,bytes,int],Union[str,int]]]\n\n__all__=[\n'AnyUrl',\n'AnyHttpUrl',\n'FileUrl',\n'HttpUrl',\n'stricturl',\n'EmailStr',\n'NameEmail',\n'IPvAnyAddress',\n'IPvAnyInterface',\n'IPvAnyNetwork',\n'PostgresDsn',\n'CockroachDsn',\n'AmqpDsn',\n'RedisDsn',\n'MongoDsn',\n'KafkaDsn',\n'validate_email',\n]\n\n_url_regex_cache=None\n_multi_host_url_regex_cache=None\n_ascii_domain_regex_cache=None\n_int_domain_regex_cache=None\n_host_regex_cache=None\n\n_host_regex=(\nr'(?:'\nr'(?P<ipv4>(?:\\d{1,3}\\.){3}\\d{1,3})(?=$|[/:#?])|'\nr'(?P<ipv6>\\[[A-F0-9]*:[A-F0-9:]+\\])(?=$|[/:#?])|'\nr'(?P<domain>[^\\s/:?#]+)'\nr')?'\nr'(?::(?P<port>\\d+))?'\n)\n_scheme_regex=r'(?:(?P<scheme>[a-z][a-z0-9+\\-.]+)://)?'\n_user_info_regex=r'(?:(?P<user>[^\\s:/]*)(?::(?P<password>[^\\s/]*))?@)?'\n_path_regex=r'(?P<path>/[^\\s?#]*)?'\n_query_regex=r'(?:\\?(?P<query>[^\\s#]*))?'\n_fragment_regex=r'(?:#(?P<fragment>[^\\s#]*))?'\n\n\ndef url_regex()->Pattern[str]:\n global _url_regex_cache\n if _url_regex_cache is None:\n  _url_regex_cache=re.compile(\n  rf'{_scheme_regex}{_user_info_regex}{_host_regex}{_path_regex}{_query_regex}{_fragment_regex}',\n  re.IGNORECASE,\n  )\n return _url_regex_cache\n \n \ndef multi_host_url_regex()->Pattern[str]:\n ''\n\n\n\n\n \n global _multi_host_url_regex_cache\n if _multi_host_url_regex_cache is None:\n  _multi_host_url_regex_cache=re.compile(\n  rf'{_scheme_regex}{_user_info_regex}'\n  r'(?P<hosts>([^/]*))'\n  rf'{_path_regex}{_query_regex}{_fragment_regex}',\n  re.IGNORECASE,\n  )\n return _multi_host_url_regex_cache\n \n \ndef ascii_domain_regex()->Pattern[str]:\n global _ascii_domain_regex_cache\n if _ascii_domain_regex_cache is None:\n  ascii_chunk=r'[_0-9a-z](?:[-_0-9a-z]{0,61}[_0-9a-z])?'\n  ascii_domain_ending=r'(?P<tld>\\.[a-z]{2,63})?\\.?'\n  _ascii_domain_regex_cache=re.compile(\n  fr'(?:{ascii_chunk}\\.)*?{ascii_chunk}{ascii_domain_ending}',re.IGNORECASE\n  )\n return _ascii_domain_regex_cache\n \n \ndef int_domain_regex()->Pattern[str]:\n global _int_domain_regex_cache\n if _int_domain_regex_cache is None:\n  int_chunk=r'[_0-9a-\\U00040000](?:[-_0-9a-\\U00040000]{0,61}[_0-9a-\\U00040000])?'\n  int_domain_ending=r'(?P<tld>(\\.[^\\W\\d_]{2,63})|(\\.(?:xn--)[_0-9a-z-]{2,63}))?\\.?'\n  _int_domain_regex_cache=re.compile(fr'(?:{int_chunk}\\.)*?{int_chunk}{int_domain_ending}',re.IGNORECASE)\n return _int_domain_regex_cache\n \n \ndef host_regex()->Pattern[str]:\n global _host_regex_cache\n if _host_regex_cache is None:\n  _host_regex_cache=re.compile(\n  _host_regex,\n  re.IGNORECASE,\n  )\n return _host_regex_cache\n \n \nclass AnyUrl(str):\n strip_whitespace=True\n min_length=1\n max_length=2 **16\n allowed_schemes:Optional[Collection[str]]=None\n tld_required:bool=False\n user_required:bool=False\n host_required:bool=True\n hidden_parts:Set[str]=set()\n \n __slots__=('scheme','user','password','host','tld','host_type','port','path','query','fragment')\n \n @no_type_check\n def __new__(cls,url:Optional[str],**kwargs)->object:\n  return str.__new__(cls,cls.build(**kwargs)if url is None else url)\n  \n def __init__(\n self,\n url:str,\n *,\n scheme:str,\n user:Optional[str]=None,\n password:Optional[str]=None,\n host:Optional[str]=None,\n tld:Optional[str]=None,\n host_type:str='domain',\n port:Optional[str]=None,\n path:Optional[str]=None,\n query:Optional[str]=None,\n fragment:Optional[str]=None,\n )->None:\n  str.__init__(url)\n  self.scheme=scheme\n  self.user=user\n  self.password=password\n  self.host=host\n  self.tld=tld\n  self.host_type=host_type\n  self.port=port\n  self.path=path\n  self.query=query\n  self.fragment=fragment\n  \n @classmethod\n def build(\n cls,\n *,\n scheme:str,\n user:Optional[str]=None,\n password:Optional[str]=None,\n host:str,\n port:Optional[str]=None,\n path:Optional[str]=None,\n query:Optional[str]=None,\n fragment:Optional[str]=None,\n **_kwargs:str,\n )->str:\n  parts=Parts(\n  scheme=scheme,\n  user=user,\n  password=password,\n  host=host,\n  port=port,\n  path=path,\n  query=query,\n  fragment=fragment,\n  **_kwargs,\n  )\n  \n  url=scheme+'://'\n  if user:\n   url +=user\n  if password:\n   url +=':'+password\n  if user or password:\n   url +='@'\n  url +=host\n  if port and('port'not in cls.hidden_parts or cls.get_default_parts(parts).get('port')!=port):\n   url +=':'+port\n  if path:\n   url +=path\n  if query:\n   url +='?'+query\n  if fragment:\n   url +='#'+fragment\n  return url\n  \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  update_not_none(field_schema,minLength=cls.min_length,maxLength=cls.max_length,format='uri')\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:Any,field:'ModelField',config:'BaseConfig')->'AnyUrl':\n  if value.__class__ ==cls:\n   return value\n  value=str_validator(value)\n  if cls.strip_whitespace:\n   value=value.strip()\n  url:str=cast(str,constr_length_validator(value,field,config))\n  \n  m=cls._match_url(url)\n  \n  assert m,'URL regex failed unexpectedly'\n  \n  original_parts=cast('Parts',m.groupdict())\n  parts=cls.apply_default_parts(original_parts)\n  parts=cls.validate_parts(parts)\n  \n  if m.end()!=len(url):\n   raise errors.UrlExtraError(extra=url[m.end():])\n   \n  return cls._build_url(m,url,parts)\n  \n @classmethod\n def _build_url(cls,m:Match[str],url:str,parts:'Parts')->'AnyUrl':\n  ''\n\n\n  \n  host,tld,host_type,rebuild=cls.validate_host(parts)\n  \n  return cls(\n  None if rebuild else url,\n  scheme=parts['scheme'],\n  user=parts['user'],\n  password=parts['password'],\n  host=host,\n  tld=tld,\n  host_type=host_type,\n  port=parts['port'],\n  path=parts['path'],\n  query=parts['query'],\n  fragment=parts['fragment'],\n  )\n  \n @staticmethod\n def _match_url(url:str)->Optional[Match[str]]:\n  return url_regex().match(url)\n  \n @staticmethod\n def _validate_port(port:Optional[str])->None:\n  if port is not None and int(port)>65_535:\n   raise errors.UrlPortError()\n   \n @classmethod\n def validate_parts(cls,parts:'Parts',validate_port:bool=True)->'Parts':\n  ''\n\n\n  \n  scheme=parts['scheme']\n  if scheme is None:\n   raise errors.UrlSchemeError()\n   \n  if cls.allowed_schemes and scheme.lower()not in cls.allowed_schemes:\n   raise errors.UrlSchemePermittedError(set(cls.allowed_schemes))\n   \n  if validate_port:\n   cls._validate_port(parts['port'])\n   \n  user=parts['user']\n  if cls.user_required and user is None:\n   raise errors.UrlUserInfoError()\n   \n  return parts\n  \n @classmethod\n def validate_host(cls,parts:'Parts')->Tuple[str,Optional[str],str,bool]:\n  tld,host_type,rebuild=None,None,False\n  for f in('domain','ipv4','ipv6'):\n   host=parts[f]\n   if host:\n    host_type=f\n    break\n    \n  if host is None:\n   if cls.host_required:\n    raise errors.UrlHostError()\n  elif host_type =='domain':\n   is_international=False\n   d=ascii_domain_regex().fullmatch(host)\n   if d is None:\n    d=int_domain_regex().fullmatch(host)\n    if d is None:\n     raise errors.UrlHostError()\n    is_international=True\n    \n   tld=d.group('tld')\n   if tld is None and not is_international:\n    d=int_domain_regex().fullmatch(host)\n    assert d is not None\n    tld=d.group('tld')\n    is_international=True\n    \n   if tld is not None:\n    tld=tld[1:]\n   elif cls.tld_required:\n    raise errors.UrlHostTldError()\n    \n   if is_international:\n    host_type='int_domain'\n    rebuild=True\n    host=host.encode('idna').decode('ascii')\n    if tld is not None:\n     tld=tld.encode('idna').decode('ascii')\n     \n  return host,tld,host_type,rebuild\n  \n @staticmethod\n def get_default_parts(parts:'Parts')->'Parts':\n  return{}\n  \n @classmethod\n def apply_default_parts(cls,parts:'Parts')->'Parts':\n  for key,value in cls.get_default_parts(parts).items():\n   if not parts[key]:\n    parts[key]=value\n  return parts\n  \n def __repr__(self)->str:\n  extra=', '.join(f'{n}={getattr(self,n)!r}'for n in self.__slots__ if getattr(self,n)is not None)\n  return f'{self.__class__.__name__}({super().__repr__()}, {extra})'\n  \n  \nclass AnyHttpUrl(AnyUrl):\n allowed_schemes={'http','https'}\n \n __slots__=()\n \n \nclass HttpUrl(AnyHttpUrl):\n tld_required=True\n \n max_length=2083\n hidden_parts={'port'}\n \n @staticmethod\n def get_default_parts(parts:'Parts')->'Parts':\n  return{'port':'80'if parts['scheme']=='http'else '443'}\n  \n  \nclass FileUrl(AnyUrl):\n allowed_schemes={'file'}\n host_required=False\n \n __slots__=()\n \n \nclass MultiHostDsn(AnyUrl):\n __slots__=AnyUrl.__slots__+('hosts',)\n \n def __init__(self,*args:Any,hosts:Optional[List['HostParts']]=None,**kwargs:Any):\n  super().__init__(*args,**kwargs)\n  self.hosts=hosts\n  \n @staticmethod\n def _match_url(url:str)->Optional[Match[str]]:\n  return multi_host_url_regex().match(url)\n  \n @classmethod\n def validate_parts(cls,parts:'Parts',validate_port:bool=True)->'Parts':\n  return super().validate_parts(parts,validate_port=False)\n  \n @classmethod\n def _build_url(cls,m:Match[str],url:str,parts:'Parts')->'MultiHostDsn':\n  hosts_parts:List['HostParts']=[]\n  host_re=host_regex()\n  for host in m.groupdict()['hosts'].split(','):\n   d:Parts=host_re.match(host).groupdict()\n   host,tld,host_type,rebuild=cls.validate_host(d)\n   port=d.get('port')\n   cls._validate_port(port)\n   hosts_parts.append(\n   {\n   'host':host,\n   'host_type':host_type,\n   'tld':tld,\n   'rebuild':rebuild,\n   'port':port,\n   }\n   )\n   \n  if len(hosts_parts)>1:\n   return cls(\n   None if any([hp['rebuild']for hp in hosts_parts])else url,\n   scheme=parts['scheme'],\n   user=parts['user'],\n   password=parts['password'],\n   path=parts['path'],\n   query=parts['query'],\n   fragment=parts['fragment'],\n   host_type=None,\n   hosts=hosts_parts,\n   )\n  else:\n  \n   host_part=hosts_parts[0]\n   return cls(\n   None if host_part['rebuild']else url,\n   scheme=parts['scheme'],\n   user=parts['user'],\n   password=parts['password'],\n   host=host_part['host'],\n   tld=host_part['tld'],\n   host_type=host_part['host_type'],\n   port=host_part.get('port'),\n   path=parts['path'],\n   query=parts['query'],\n   fragment=parts['fragment'],\n   )\n   \n   \nclass PostgresDsn(MultiHostDsn):\n allowed_schemes={\n 'postgres',\n 'postgresql',\n 'postgresql+asyncpg',\n 'postgresql+pg8000',\n 'postgresql+psycopg',\n 'postgresql+psycopg2',\n 'postgresql+psycopg2cffi',\n 'postgresql+py-postgresql',\n 'postgresql+pygresql',\n }\n user_required=True\n \n __slots__=()\n \n \nclass CockroachDsn(AnyUrl):\n allowed_schemes={\n 'cockroachdb',\n 'cockroachdb+psycopg2',\n 'cockroachdb+asyncpg',\n }\n user_required=True\n \n \nclass AmqpDsn(AnyUrl):\n allowed_schemes={'amqp','amqps'}\n host_required=False\n \n \nclass RedisDsn(AnyUrl):\n __slots__=()\n allowed_schemes={'redis','rediss'}\n host_required=False\n \n @staticmethod\n def get_default_parts(parts:'Parts')->'Parts':\n  return{\n  'domain':'localhost'if not(parts['ipv4']or parts['ipv6'])else '',\n  'port':'6379',\n  'path':'/0',\n  }\n  \n  \nclass MongoDsn(AnyUrl):\n allowed_schemes={'mongodb'}\n \n \n @staticmethod\n def get_default_parts(parts:'Parts')->'Parts':\n  return{\n  'port':'27017',\n  }\n  \n  \nclass KafkaDsn(AnyUrl):\n allowed_schemes={'kafka'}\n \n @staticmethod\n def get_default_parts(parts:'Parts')->'Parts':\n  return{\n  'domain':'localhost',\n  'port':'9092',\n  }\n  \n  \ndef stricturl(\n*,\nstrip_whitespace:bool=True,\nmin_length:int=1,\nmax_length:int=2 **16,\ntld_required:bool=True,\nhost_required:bool=True,\nallowed_schemes:Optional[Collection[str]]=None,\n)->Type[AnyUrl]:\n\n namespace=dict(\n strip_whitespace=strip_whitespace,\n min_length=min_length,\n max_length=max_length,\n tld_required=tld_required,\n host_required=host_required,\n allowed_schemes=allowed_schemes,\n )\n return type('UrlValue',(AnyUrl,),namespace)\n \n \ndef import_email_validator()->None:\n global email_validator\n try:\n  import email_validator\n except ImportError as e:\n  raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')from e\n  \n  \nclass EmailStr(str):\n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  field_schema.update(type='string',format='email')\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n \n  import_email_validator()\n  \n  yield str_validator\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:Union[str])->str:\n  return validate_email(value)[1]\n  \n  \nclass NameEmail(Representation):\n __slots__='name','email'\n \n def __init__(self,name:str,email:str):\n  self.name=name\n  self.email=email\n  \n def __eq__(self,other:Any)->bool:\n  return isinstance(other,NameEmail)and(self.name,self.email)==(other.name,other.email)\n  \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  field_schema.update(type='string',format='name-email')\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  import_email_validator()\n  \n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:Any)->'NameEmail':\n  if value.__class__ ==cls:\n   return value\n  value=str_validator(value)\n  return cls(*validate_email(value))\n  \n def __str__(self)->str:\n  return f'{self.name} <{self.email}>'\n  \n  \nclass IPvAnyAddress(_BaseAddress):\n __slots__=()\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  field_schema.update(type='string',format='ipvanyaddress')\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:Union[str,bytes,int])->Union[IPv4Address,IPv6Address]:\n  try:\n   return IPv4Address(value)\n  except ValueError:\n   pass\n   \n  try:\n   return IPv6Address(value)\n  except ValueError:\n   raise errors.IPvAnyAddressError()\n   \n   \nclass IPvAnyInterface(_BaseAddress):\n __slots__=()\n \n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  field_schema.update(type='string',format='ipvanyinterface')\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:NetworkType)->Union[IPv4Interface,IPv6Interface]:\n  try:\n   return IPv4Interface(value)\n  except ValueError:\n   pass\n   \n  try:\n   return IPv6Interface(value)\n  except ValueError:\n   raise errors.IPvAnyInterfaceError()\n   \n   \nclass IPvAnyNetwork(_BaseNetwork):\n @classmethod\n def __modify_schema__(cls,field_schema:Dict[str,Any])->None:\n  field_schema.update(type='string',format='ipvanynetwork')\n  \n @classmethod\n def __get_validators__(cls)->'CallableGenerator':\n  yield cls.validate\n  \n @classmethod\n def validate(cls,value:NetworkType)->Union[IPv4Network,IPv6Network]:\n \n \n  try:\n   return IPv4Network(value)\n  except ValueError:\n   pass\n   \n  try:\n   return IPv6Network(value)\n  except ValueError:\n   raise errors.IPvAnyNetworkError()\n   \n   \npretty_email_regex=re.compile(r'([\\w ]*?) *<(.*)> *')\nMAX_EMAIL_LENGTH=2048\n''\n\n\n\n\ndef validate_email(value:Union[str])->Tuple[str,str]:\n ''\n\n\n\n\n\n \n if email_validator is None:\n  import_email_validator()\n  \n if len(value)>MAX_EMAIL_LENGTH:\n  raise errors.EmailError()\n  \n m=pretty_email_regex.fullmatch(value)\n name:Union[str,None]=None\n if m:\n  name,value=m.groups()\n email=value.strip()\n try:\n  parts=email_validator.validate_email(email,check_deliverability=False)\n except email_validator.EmailNotValidError as e:\n  raise errors.EmailError from e\n  \n if hasattr(parts,'normalized'):\n \n  email=parts.normalized\n  assert email is not None\n  name=name or parts.local_part\n  return name,email\n else:\n \n  at_index=email.index('@')\n  local_part=email[:at_index]\n  global_part=email[at_index:].lower()\n  \n  return name or local_part,local_part+global_part\n", ["email_validator", "ipaddress", "pydantic.v1", "pydantic.v1.config", "pydantic.v1.errors", "pydantic.v1.fields", "pydantic.v1.typing", "pydantic.v1.utils", "pydantic.v1.validators", "re", "typing", "typing_extensions"]], "pydantic.plugin": [".py", "''\n\n\n\n\nfrom __future__ import annotations\n\nfrom typing import Any,Callable,NamedTuple\n\nfrom pydantic_core import CoreConfig,CoreSchema,ValidationError\nfrom typing_extensions import Literal,Protocol,TypeAlias\n\n__all__=(\n'PydanticPluginProtocol',\n'BaseValidateHandlerProtocol',\n'ValidatePythonHandlerProtocol',\n'ValidateJsonHandlerProtocol',\n'ValidateStringsHandlerProtocol',\n'NewSchemaReturns',\n'SchemaTypePath',\n'SchemaKind',\n)\n\nNewSchemaReturns:TypeAlias='tuple[ValidatePythonHandlerProtocol | None, ValidateJsonHandlerProtocol | None, ValidateStringsHandlerProtocol | None]'\n\n\nclass SchemaTypePath(NamedTuple):\n ''\n \n module:str\n name:str\n \n \nSchemaKind:TypeAlias=Literal['BaseModel','TypeAdapter','dataclass','create_model','validate_call']\n\n\nclass PydanticPluginProtocol(Protocol):\n ''\n \n def new_schema_validator(\n self,\n schema:CoreSchema,\n schema_type:Any,\n schema_type_path:SchemaTypePath,\n schema_kind:SchemaKind,\n config:CoreConfig |None,\n plugin_settings:dict[str,object],\n )->tuple[\n ValidatePythonHandlerProtocol |None,ValidateJsonHandlerProtocol |None,ValidateStringsHandlerProtocol |None\n ]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError('Pydantic plugins should implement `new_schema_validator`.')\n  \n  \nclass BaseValidateHandlerProtocol(Protocol):\n ''\n\n\n\n \n \n on_enter:Callable[...,None]\n '' \n \n def on_success(self,result:Any)->None:\n  ''\n\n\n\n  \n  return\n  \n def on_error(self,error:ValidationError)->None:\n  ''\n\n\n\n  \n  return\n  \n def on_exception(self,exception:Exception)->None:\n  ''\n\n\n\n  \n  return\n  \n  \nclass ValidatePythonHandlerProtocol(BaseValidateHandlerProtocol,Protocol):\n ''\n \n def on_enter(\n self,\n input:Any,\n *,\n strict:bool |None=None,\n from_attributes:bool |None=None,\n context:dict[str,Any]|None=None,\n self_instance:Any |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n  \n  pass\n  \n  \nclass ValidateJsonHandlerProtocol(BaseValidateHandlerProtocol,Protocol):\n ''\n \n def on_enter(\n self,\n input:str |bytes |bytearray,\n *,\n strict:bool |None=None,\n context:dict[str,Any]|None=None,\n self_instance:Any |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n  \n  pass\n  \n  \nStringInput:TypeAlias='dict[str, StringInput]'\n\n\nclass ValidateStringsHandlerProtocol(BaseValidateHandlerProtocol,Protocol):\n ''\n \n def on_enter(\n self,input:StringInput,*,strict:bool |None=None,context:dict[str,Any]|None=None\n )->None:\n  ''\n\n\n\n\n\n  \n  pass\n", ["__future__", "pydantic_core", "typing", "typing_extensions"], 1], "pydantic.plugin._loader": [".py", "from __future__ import annotations\n\nimport importlib.metadata as importlib_metadata\nimport os\nimport warnings\nfrom typing import TYPE_CHECKING,Final,Iterable\n\nif TYPE_CHECKING:\n from. import PydanticPluginProtocol\n \n \nPYDANTIC_ENTRY_POINT_GROUP:Final[str]='pydantic'\n\n\n_plugins:dict[str,PydanticPluginProtocol]|None=None\n\n\n_loading_plugins:bool=False\n\n\ndef get_plugins()->Iterable[PydanticPluginProtocol]:\n ''\n\n\n \n disabled_plugins=os.getenv('PYDANTIC_DISABLE_PLUGINS')\n global _plugins,_loading_plugins\n if _loading_plugins:\n \n  return()\n elif disabled_plugins in('__all__','1','true'):\n  return()\n elif _plugins is None:\n  _plugins={}\n  \n  _loading_plugins=True\n  try:\n   for dist in importlib_metadata.distributions():\n    for entry_point in dist.entry_points:\n     if entry_point.group !=PYDANTIC_ENTRY_POINT_GROUP:\n      continue\n     if entry_point.value in _plugins:\n      continue\n     if disabled_plugins is not None and entry_point.name in disabled_plugins.split(','):\n      continue\n     try:\n      _plugins[entry_point.value]=entry_point.load()\n     except(ImportError,AttributeError)as e:\n      warnings.warn(\n      f'{e.__class__.__name__} while loading the `{entry_point.name}` Pydantic plugin, '\n      f'this plugin will not be installed.\\n\\n{e !r}'\n      )\n  finally:\n   _loading_plugins=False\n   \n return _plugins.values()\n", ["__future__", "importlib.metadata", "os", "pydantic.plugin", "typing", "warnings"]], "pydantic.plugin._schema_validator": [".py", "''\n\nfrom __future__ import annotations\n\nimport functools\nfrom typing import TYPE_CHECKING,Any,Callable,Iterable,TypeVar\n\nfrom pydantic_core import CoreConfig,CoreSchema,SchemaValidator,ValidationError\nfrom typing_extensions import Literal,ParamSpec\n\nif TYPE_CHECKING:\n from. import BaseValidateHandlerProtocol,PydanticPluginProtocol,SchemaKind,SchemaTypePath\n \n \nP=ParamSpec('P')\nR=TypeVar('R')\nEvent=Literal['on_validate_python','on_validate_json','on_validate_strings']\nevents:list[Event]=list(Event.__args__)\n\n\ndef create_schema_validator(\nschema:CoreSchema,\nschema_type:Any,\nschema_type_module:str,\nschema_type_name:str,\nschema_kind:SchemaKind,\nconfig:CoreConfig |None=None,\nplugin_settings:dict[str,Any]|None=None,\n)->SchemaValidator |PluggableSchemaValidator:\n ''\n\n\n\n \n from. import SchemaTypePath\n from._loader import get_plugins\n \n plugins=get_plugins()\n if plugins:\n  return PluggableSchemaValidator(\n  schema,\n  schema_type,\n  SchemaTypePath(schema_type_module,schema_type_name),\n  schema_kind,\n  config,\n  plugins,\n  plugin_settings or{},\n  )\n else:\n  return SchemaValidator(schema,config)\n  \n  \nclass PluggableSchemaValidator:\n ''\n \n __slots__='_schema_validator','validate_json','validate_python','validate_strings'\n \n def __init__(\n self,\n schema:CoreSchema,\n schema_type:Any,\n schema_type_path:SchemaTypePath,\n schema_kind:SchemaKind,\n config:CoreConfig |None,\n plugins:Iterable[PydanticPluginProtocol],\n plugin_settings:dict[str,Any],\n )->None:\n  self._schema_validator=SchemaValidator(schema,config)\n  \n  python_event_handlers:list[BaseValidateHandlerProtocol]=[]\n  json_event_handlers:list[BaseValidateHandlerProtocol]=[]\n  strings_event_handlers:list[BaseValidateHandlerProtocol]=[]\n  for plugin in plugins:\n   try:\n    p,j,s=plugin.new_schema_validator(\n    schema,schema_type,schema_type_path,schema_kind,config,plugin_settings\n    )\n   except TypeError as e:\n    raise TypeError(f'Error using plugin `{plugin.__module__}:{plugin.__class__.__name__}`: {e}')from e\n   if p is not None:\n    python_event_handlers.append(p)\n   if j is not None:\n    json_event_handlers.append(j)\n   if s is not None:\n    strings_event_handlers.append(s)\n    \n  self.validate_python=build_wrapper(self._schema_validator.validate_python,python_event_handlers)\n  self.validate_json=build_wrapper(self._schema_validator.validate_json,json_event_handlers)\n  self.validate_strings=build_wrapper(self._schema_validator.validate_strings,strings_event_handlers)\n  \n def __getattr__(self,name:str)->Any:\n  return getattr(self._schema_validator,name)\n  \n  \ndef build_wrapper(func:Callable[P,R],event_handlers:list[BaseValidateHandlerProtocol])->Callable[P,R]:\n if not event_handlers:\n  return func\n else:\n  on_enters=tuple(h.on_enter for h in event_handlers if filter_handlers(h,'on_enter'))\n  on_successes=tuple(h.on_success for h in event_handlers if filter_handlers(h,'on_success'))\n  on_errors=tuple(h.on_error for h in event_handlers if filter_handlers(h,'on_error'))\n  on_exceptions=tuple(h.on_exception for h in event_handlers if filter_handlers(h,'on_exception'))\n  \n  @functools.wraps(func)\n  def wrapper(*args:P.args,**kwargs:P.kwargs)->R:\n   for on_enter_handler in on_enters:\n    on_enter_handler(*args,**kwargs)\n    \n   try:\n    result=func(*args,**kwargs)\n   except ValidationError as error:\n    for on_error_handler in on_errors:\n     on_error_handler(error)\n    raise\n   except Exception as exception:\n    for on_exception_handler in on_exceptions:\n     on_exception_handler(exception)\n    raise\n   else:\n    for on_success_handler in on_successes:\n     on_success_handler(result)\n    return result\n    \n  return wrapper\n  \n  \ndef filter_handlers(handler_cls:BaseValidateHandlerProtocol,method_name:str)->bool:\n ''\n\n \n handler=getattr(handler_cls,method_name,None)\n if handler is None:\n  return False\n elif handler.__module__ =='pydantic.plugin':\n \n \n  return False\n else:\n  return True\n", ["__future__", "functools", "pydantic.plugin", "pydantic.plugin._loader", "pydantic_core", "typing", "typing_extensions"]], "sniffio": [".py", "''\n\n__all__=[\n\"current_async_library\",\n\"AsyncLibraryNotFoundError\",\n\"current_async_library_cvar\",\n\"thread_local\",\n]\n\nfrom._version import __version__\n\nfrom._impl import(\ncurrent_async_library,\nAsyncLibraryNotFoundError,\ncurrent_async_library_cvar,\nthread_local,\n)\n", ["sniffio._impl", "sniffio._version"], 1], "sniffio._impl": [".py", "from contextvars import ContextVar\nfrom typing import Optional\nimport sys\nimport threading\n\ncurrent_async_library_cvar=ContextVar(\n\"current_async_library_cvar\",default=None\n)\n\n\nclass _ThreadLocal(threading.local):\n\n\n\n name=None\n \n \nthread_local=_ThreadLocal()\n\n\nclass AsyncLibraryNotFoundError(RuntimeError):\n pass\n \n \ndef current_async_library()->str:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n value=thread_local.name\n if value is not None:\n  return value\n  \n value=current_async_library_cvar.get()\n if value is not None:\n  return value\n  \n  \n if \"asyncio\"in sys.modules:\n  import asyncio\n  try:\n   current_task=asyncio.current_task\n  except AttributeError:\n   current_task=asyncio.Task.current_task\n  try:\n   if current_task()is not None:\n    return \"asyncio\"\n  except RuntimeError:\n   pass\n   \n   \n if 'curio'in sys.modules:\n  from curio.meta import curio_running\n  if curio_running():\n   return 'curio'\n   \n raise AsyncLibraryNotFoundError(\n \"unknown async library, or not in async context\"\n )\n", ["asyncio", "contextvars", "curio.meta", "sys", "threading", "typing"]], "sniffio._version": [".py", "\n\n__version__=\"1.3.1\"\n", []], "sniffio._tests": [".py", "", [], 1], "sniffio._tests.test_sniffio": [".py", "import os\nimport sys\n\nimport pytest\n\nfrom.. import(\ncurrent_async_library,AsyncLibraryNotFoundError,\ncurrent_async_library_cvar,thread_local\n)\n\n\ndef test_basics_cvar():\n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n token=current_async_library_cvar.set(\"generic-lib\")\n try:\n  assert current_async_library()==\"generic-lib\"\n finally:\n  current_async_library_cvar.reset(token)\n  \n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n  \ndef test_basics_tlocal():\n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n old_name,thread_local.name=thread_local.name,\"generic-lib\"\n try:\n  assert current_async_library()==\"generic-lib\"\n finally:\n  thread_local.name=old_name\n  \n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n  \ndef test_asyncio():\n import asyncio\n \n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n ran=[]\n \n async def this_is_asyncio():\n  assert current_async_library()==\"asyncio\"\n  \n  assert current_async_library()==\"asyncio\"\n  ran.append(True)\n  \n asyncio.run(this_is_asyncio())\n assert ran ==[True]\n \n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n  \n@pytest.mark.skipif(\nsys.version_info >=(3,12),\nreason=\n\"curio broken on 3.12 (https://github.com/python-trio/sniffio/pull/42)\",\n)\ndef test_curio():\n import curio\n \n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n  \n ran=[]\n \n async def this_is_curio():\n  assert current_async_library()==\"curio\"\n  \n  assert current_async_library()==\"curio\"\n  ran.append(True)\n  \n curio.run(this_is_curio)\n assert ran ==[True]\n \n with pytest.raises(AsyncLibraryNotFoundError):\n  current_async_library()\n", ["asyncio", "curio", "os", "pytest", "sniffio", "sys"]], "anyio": [".py", "from __future__ import annotations\n\nfrom._core._eventloop import current_time as current_time\nfrom._core._eventloop import get_all_backends as get_all_backends\nfrom._core._eventloop import get_cancelled_exc_class as get_cancelled_exc_class\nfrom._core._eventloop import run as run\nfrom._core._eventloop import sleep as sleep\nfrom._core._eventloop import sleep_forever as sleep_forever\nfrom._core._eventloop import sleep_until as sleep_until\nfrom._core._exceptions import BrokenResourceError as BrokenResourceError\nfrom._core._exceptions import BrokenWorkerProcess as BrokenWorkerProcess\nfrom._core._exceptions import BusyResourceError as BusyResourceError\nfrom._core._exceptions import ClosedResourceError as ClosedResourceError\nfrom._core._exceptions import DelimiterNotFound as DelimiterNotFound\nfrom._core._exceptions import EndOfStream as EndOfStream\nfrom._core._exceptions import IncompleteRead as IncompleteRead\nfrom._core._exceptions import TypedAttributeLookupError as TypedAttributeLookupError\nfrom._core._exceptions import WouldBlock as WouldBlock\nfrom._core._fileio import AsyncFile as AsyncFile\nfrom._core._fileio import Path as Path\nfrom._core._fileio import open_file as open_file\nfrom._core._fileio import wrap_file as wrap_file\nfrom._core._resources import aclose_forcefully as aclose_forcefully\nfrom._core._signals import open_signal_receiver as open_signal_receiver\nfrom._core._sockets import connect_tcp as connect_tcp\nfrom._core._sockets import connect_unix as connect_unix\nfrom._core._sockets import create_connected_udp_socket as create_connected_udp_socket\nfrom._core._sockets import(\ncreate_connected_unix_datagram_socket as create_connected_unix_datagram_socket,\n)\nfrom._core._sockets import create_tcp_listener as create_tcp_listener\nfrom._core._sockets import create_udp_socket as create_udp_socket\nfrom._core._sockets import create_unix_datagram_socket as create_unix_datagram_socket\nfrom._core._sockets import create_unix_listener as create_unix_listener\nfrom._core._sockets import getaddrinfo as getaddrinfo\nfrom._core._sockets import getnameinfo as getnameinfo\nfrom._core._sockets import wait_readable as wait_readable\nfrom._core._sockets import wait_socket_readable as wait_socket_readable\nfrom._core._sockets import wait_socket_writable as wait_socket_writable\nfrom._core._sockets import wait_writable as wait_writable\nfrom._core._streams import create_memory_object_stream as create_memory_object_stream\nfrom._core._subprocesses import open_process as open_process\nfrom._core._subprocesses import run_process as run_process\nfrom._core._synchronization import CapacityLimiter as CapacityLimiter\nfrom._core._synchronization import(\nCapacityLimiterStatistics as CapacityLimiterStatistics,\n)\nfrom._core._synchronization import Condition as Condition\nfrom._core._synchronization import ConditionStatistics as ConditionStatistics\nfrom._core._synchronization import Event as Event\nfrom._core._synchronization import EventStatistics as EventStatistics\nfrom._core._synchronization import Lock as Lock\nfrom._core._synchronization import LockStatistics as LockStatistics\nfrom._core._synchronization import ResourceGuard as ResourceGuard\nfrom._core._synchronization import Semaphore as Semaphore\nfrom._core._synchronization import SemaphoreStatistics as SemaphoreStatistics\nfrom._core._tasks import TASK_STATUS_IGNORED as TASK_STATUS_IGNORED\nfrom._core._tasks import CancelScope as CancelScope\nfrom._core._tasks import create_task_group as create_task_group\nfrom._core._tasks import current_effective_deadline as current_effective_deadline\nfrom._core._tasks import fail_after as fail_after\nfrom._core._tasks import move_on_after as move_on_after\nfrom._core._testing import TaskInfo as TaskInfo\nfrom._core._testing import get_current_task as get_current_task\nfrom._core._testing import get_running_tasks as get_running_tasks\nfrom._core._testing import wait_all_tasks_blocked as wait_all_tasks_blocked\nfrom._core._typedattr import TypedAttributeProvider as TypedAttributeProvider\nfrom._core._typedattr import TypedAttributeSet as TypedAttributeSet\nfrom._core._typedattr import typed_attribute as typed_attribute\n\n\nfor __value in list(locals().values()):\n if getattr(__value,\"__module__\",\"\").startswith(\"anyio.\"):\n  __value.__module__=__name__\n  \ndel __value\n", ["__future__", "anyio._core._eventloop", "anyio._core._exceptions", "anyio._core._fileio", "anyio._core._resources", "anyio._core._signals", "anyio._core._sockets", "anyio._core._streams", "anyio._core._subprocesses", "anyio._core._synchronization", "anyio._core._tasks", "anyio._core._testing", "anyio._core._typedattr"], 1], "anyio.to_thread": [".py", "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Callable\nfrom typing import TypeVar\nfrom warnings import warn\n\nfrom._core._eventloop import get_async_backend\nfrom.abc import CapacityLimiter\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nT_Retval=TypeVar(\"T_Retval\")\nPosArgsT=TypeVarTuple(\"PosArgsT\")\n\n\nasync def run_sync(\nfunc:Callable[[Unpack[PosArgsT]],T_Retval],\n*args:Unpack[PosArgsT],\nabandon_on_cancel:bool=False,\ncancellable:bool |None=None,\nlimiter:CapacityLimiter |None=None,\n)->T_Retval:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if cancellable is not None:\n  abandon_on_cancel=cancellable\n  warn(\n  \"The `cancellable=` keyword argument to `anyio.to_thread.run_sync` is \"\n  \"deprecated since AnyIO 4.1.0; use `abandon_on_cancel=` instead\",\n  DeprecationWarning,\n  stacklevel=2,\n  )\n  \n return await get_async_backend().run_sync_in_worker_thread(\n func,args,abandon_on_cancel=abandon_on_cancel,limiter=limiter\n )\n \n \ndef current_default_thread_limiter()->CapacityLimiter:\n ''\n\n\n\n\n\n \n return get_async_backend().current_default_thread_limiter()\n", ["__future__", "anyio._core._eventloop", "anyio.abc", "collections.abc", "sys", "typing", "typing_extensions", "warnings"]], "anyio.lowlevel": [".py", "from __future__ import annotations\n\nimport enum\nfrom dataclasses import dataclass\nfrom typing import Any,Generic,Literal,TypeVar,overload\nfrom weakref import WeakKeyDictionary\n\nfrom._core._eventloop import get_async_backend\n\nT=TypeVar(\"T\")\nD=TypeVar(\"D\")\n\n\nasync def checkpoint()->None:\n ''\n\n\n\n\n\n\n\n\n\n\n \n await get_async_backend().checkpoint()\n \n \nasync def checkpoint_if_cancelled()->None:\n ''\n\n\n\n\n\n\n \n await get_async_backend().checkpoint_if_cancelled()\n \n \nasync def cancel_shielded_checkpoint()->None:\n ''\n\n\n\n\n\n\n\n\n\n\n \n await get_async_backend().cancel_shielded_checkpoint()\n \n \ndef current_token()->object:\n ''\n\n\n\n \n return get_async_backend().current_token()\n \n \n_run_vars:WeakKeyDictionary[Any,dict[str,Any]]=WeakKeyDictionary()\n_token_wrappers:dict[Any,_TokenWrapper]={}\n\n\n@dataclass(frozen=True)\nclass _TokenWrapper:\n __slots__=\"_token\",\"__weakref__\"\n _token:object\n \n \nclass _NoValueSet(enum.Enum):\n NO_VALUE_SET=enum.auto()\n \n \nclass RunvarToken(Generic[T]):\n __slots__=\"_var\",\"_value\",\"_redeemed\"\n \n def __init__(self,var:RunVar[T],value:T |Literal[_NoValueSet.NO_VALUE_SET]):\n  self._var=var\n  self._value:T |Literal[_NoValueSet.NO_VALUE_SET]=value\n  self._redeemed=False\n  \n  \nclass RunVar(Generic[T]):\n ''\n\n \n \n __slots__=\"_name\",\"_default\"\n \n NO_VALUE_SET:Literal[_NoValueSet.NO_VALUE_SET]=_NoValueSet.NO_VALUE_SET\n \n _token_wrappers:set[_TokenWrapper]=set()\n \n def __init__(\n self,name:str,default:T |Literal[_NoValueSet.NO_VALUE_SET]=NO_VALUE_SET\n ):\n  self._name=name\n  self._default=default\n  \n @property\n def _current_vars(self)->dict[str,T]:\n  token=current_token()\n  try:\n   return _run_vars[token]\n  except KeyError:\n   run_vars=_run_vars[token]={}\n   return run_vars\n   \n @overload\n def get(self,default:D)->T |D:...\n \n @overload\n def get(self)->T:...\n \n def get(\n self,default:D |Literal[_NoValueSet.NO_VALUE_SET]=NO_VALUE_SET\n )->T |D:\n  try:\n   return self._current_vars[self._name]\n  except KeyError:\n   if default is not RunVar.NO_VALUE_SET:\n    return default\n   elif self._default is not RunVar.NO_VALUE_SET:\n    return self._default\n    \n  raise LookupError(\n  f'Run variable \"{self._name}\" has no value and no default set'\n  )\n  \n def set(self,value:T)->RunvarToken[T]:\n  current_vars=self._current_vars\n  token=RunvarToken(self,current_vars.get(self._name,RunVar.NO_VALUE_SET))\n  current_vars[self._name]=value\n  return token\n  \n def reset(self,token:RunvarToken[T])->None:\n  if token._var is not self:\n   raise ValueError(\"This token does not belong to this RunVar\")\n   \n  if token._redeemed:\n   raise ValueError(\"This token has already been used\")\n   \n  if token._value is _NoValueSet.NO_VALUE_SET:\n   try:\n    del self._current_vars[self._name]\n   except KeyError:\n    pass\n  else:\n   self._current_vars[self._name]=token._value\n   \n  token._redeemed=True\n  \n def __repr__(self)->str:\n  return f\"<RunVar name={self._name !r}>\"\n", ["__future__", "anyio._core._eventloop", "dataclasses", "enum", "typing", "weakref"]], "anyio.from_thread": [".py", "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Awaitable,Callable,Generator\nfrom concurrent.futures import Future\nfrom contextlib import(\nAbstractAsyncContextManager,\nAbstractContextManager,\ncontextmanager,\n)\nfrom dataclasses import dataclass,field\nfrom inspect import isawaitable\nfrom threading import Lock,Thread,get_ident\nfrom types import TracebackType\nfrom typing import(\nAny,\nGeneric,\nTypeVar,\ncast,\noverload,\n)\n\nfrom._core import _eventloop\nfrom._core._eventloop import get_async_backend,get_cancelled_exc_class,threadlocals\nfrom._core._synchronization import Event\nfrom._core._tasks import CancelScope,create_task_group\nfrom.abc import AsyncBackend\nfrom.abc._tasks import TaskStatus\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nT_Retval=TypeVar(\"T_Retval\")\nT_co=TypeVar(\"T_co\",covariant=True)\nPosArgsT=TypeVarTuple(\"PosArgsT\")\n\n\ndef run(\nfunc:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],*args:Unpack[PosArgsT]\n)->T_Retval:\n ''\n\n\n\n\n\n\n \n try:\n  async_backend=threadlocals.current_async_backend\n  token=threadlocals.current_token\n except AttributeError:\n  raise RuntimeError(\n  \"This function can only be run from an AnyIO worker thread\"\n  )from None\n  \n return async_backend.run_async_from_thread(func,args,token=token)\n \n \ndef run_sync(\nfunc:Callable[[Unpack[PosArgsT]],T_Retval],*args:Unpack[PosArgsT]\n)->T_Retval:\n ''\n\n\n\n\n\n\n \n try:\n  async_backend=threadlocals.current_async_backend\n  token=threadlocals.current_token\n except AttributeError:\n  raise RuntimeError(\n  \"This function can only be run from an AnyIO worker thread\"\n  )from None\n  \n return async_backend.run_sync_from_thread(func,args,token=token)\n \n \nclass _BlockingAsyncContextManager(Generic[T_co],AbstractContextManager):\n _enter_future:Future[T_co]\n _exit_future:Future[bool |None]\n _exit_event:Event\n _exit_exc_info:tuple[\n type[BaseException]|None,BaseException |None,TracebackType |None\n ]=(None,None,None)\n \n def __init__(\n self,async_cm:AbstractAsyncContextManager[T_co],portal:BlockingPortal\n ):\n  self._async_cm=async_cm\n  self._portal=portal\n  \n async def run_async_cm(self)->bool |None:\n  try:\n   self._exit_event=Event()\n   value=await self._async_cm.__aenter__()\n  except BaseException as exc:\n   self._enter_future.set_exception(exc)\n   raise\n  else:\n   self._enter_future.set_result(value)\n   \n  try:\n  \n  \n  \n  \n   await self._exit_event.wait()\n  finally:\n  \n  \n  \n   result=await self._async_cm.__aexit__(*self._exit_exc_info)\n   return result\n   \n def __enter__(self)->T_co:\n  self._enter_future=Future()\n  self._exit_future=self._portal.start_task_soon(self.run_async_cm)\n  return self._enter_future.result()\n  \n def __exit__(\n self,\n __exc_type:type[BaseException]|None,\n __exc_value:BaseException |None,\n __traceback:TracebackType |None,\n )->bool |None:\n  self._exit_exc_info=__exc_type,__exc_value,__traceback\n  self._portal.call(self._exit_event.set)\n  return self._exit_future.result()\n  \n  \nclass _BlockingPortalTaskStatus(TaskStatus):\n def __init__(self,future:Future):\n  self._future=future\n  \n def started(self,value:object=None)->None:\n  self._future.set_result(value)\n  \n  \nclass BlockingPortal:\n ''\n \n def __new__(cls)->BlockingPortal:\n  return get_async_backend().create_blocking_portal()\n  \n def __init__(self)->None:\n  self._event_loop_thread_id:int |None=get_ident()\n  self._stop_event=Event()\n  self._task_group=create_task_group()\n  self._cancelled_exc_class=get_cancelled_exc_class()\n  \n async def __aenter__(self)->BlockingPortal:\n  await self._task_group.__aenter__()\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  await self.stop()\n  return await self._task_group.__aexit__(exc_type,exc_val,exc_tb)\n  \n def _check_running(self)->None:\n  if self._event_loop_thread_id is None:\n   raise RuntimeError(\"This portal is not running\")\n  if self._event_loop_thread_id ==get_ident():\n   raise RuntimeError(\n   \"This method cannot be called from the event loop thread\"\n   )\n   \n async def sleep_until_stopped(self)->None:\n  ''\n  await self._stop_event.wait()\n  \n async def stop(self,cancel_remaining:bool=False)->None:\n  ''\n\n\n\n\n\n\n\n\n  \n  self._event_loop_thread_id=None\n  self._stop_event.set()\n  if cancel_remaining:\n   self._task_group.cancel_scope.cancel()\n   \n async def _call_func(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]|T_Retval],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n future:Future[T_Retval],\n )->None:\n  def callback(f:Future[T_Retval])->None:\n   if f.cancelled()and self._event_loop_thread_id not in(\n   None,\n   get_ident(),\n   ):\n    self.call(scope.cancel)\n    \n  try:\n   retval_or_awaitable=func(*args,**kwargs)\n   if isawaitable(retval_or_awaitable):\n    with CancelScope()as scope:\n     if future.cancelled():\n      scope.cancel()\n     else:\n      future.add_done_callback(callback)\n      \n     retval=await retval_or_awaitable\n   else:\n    retval=retval_or_awaitable\n  except self._cancelled_exc_class:\n   future.cancel()\n   future.set_running_or_notify_cancel()\n  except BaseException as exc:\n   if not future.cancelled():\n    future.set_exception(exc)\n    \n    \n   if not isinstance(exc,Exception):\n    raise\n  else:\n   if not future.cancelled():\n    future.set_result(retval)\n  finally:\n   scope=None\n   \n def _spawn_task_from_thread(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]|T_Retval],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n name:object,\n future:Future[T_Retval],\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n @overload\n def call(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n *args:Unpack[PosArgsT],\n )->T_Retval:...\n \n @overload\n def call(\n self,func:Callable[[Unpack[PosArgsT]],T_Retval],*args:Unpack[PosArgsT]\n )->T_Retval:...\n \n def call(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]|T_Retval],\n *args:Unpack[PosArgsT],\n )->T_Retval:\n  ''\n\n\n\n\n\n\n\n\n  \n  return cast(T_Retval,self.start_task_soon(func,*args).result())\n  \n @overload\n def start_task_soon(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n *args:Unpack[PosArgsT],\n name:object=None,\n )->Future[T_Retval]:...\n \n @overload\n def start_task_soon(\n self,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n *args:Unpack[PosArgsT],\n name:object=None,\n )->Future[T_Retval]:...\n \n def start_task_soon(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]|T_Retval],\n *args:Unpack[PosArgsT],\n name:object=None,\n )->Future[T_Retval]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  self._check_running()\n  f:Future[T_Retval]=Future()\n  self._spawn_task_from_thread(func,args,{},name,f)\n  return f\n  \n def start_task(\n self,\n func:Callable[...,Awaitable[T_Retval]],\n *args:object,\n name:object=None,\n )->tuple[Future[T_Retval],Any]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  def task_done(future:Future[T_Retval])->None:\n   if not task_status_future.done():\n    if future.cancelled():\n     task_status_future.cancel()\n    elif future.exception():\n     task_status_future.set_exception(future.exception())\n    else:\n     exc=RuntimeError(\n     \"Task exited without calling task_status.started()\"\n     )\n     task_status_future.set_exception(exc)\n     \n  self._check_running()\n  task_status_future:Future=Future()\n  task_status=_BlockingPortalTaskStatus(task_status_future)\n  f:Future=Future()\n  f.add_done_callback(task_done)\n  self._spawn_task_from_thread(func,args,{\"task_status\":task_status},name,f)\n  return f,task_status_future.result()\n  \n def wrap_async_context_manager(\n self,cm:AbstractAsyncContextManager[T_co]\n )->AbstractContextManager[T_co]:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return _BlockingAsyncContextManager(cm,self)\n  \n  \n@dataclass\nclass BlockingPortalProvider:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n backend:str=\"asyncio\"\n backend_options:dict[str,Any]|None=None\n _lock:Lock=field(init=False,default_factory=Lock)\n _leases:int=field(init=False,default=0)\n _portal:BlockingPortal=field(init=False)\n _portal_cm:AbstractContextManager[BlockingPortal]|None=field(\n init=False,default=None\n )\n \n def __enter__(self)->BlockingPortal:\n  with self._lock:\n   if self._portal_cm is None:\n    self._portal_cm=start_blocking_portal(\n    self.backend,self.backend_options\n    )\n    self._portal=self._portal_cm.__enter__()\n    \n   self._leases +=1\n   return self._portal\n   \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  portal_cm:AbstractContextManager[BlockingPortal]|None=None\n  with self._lock:\n   assert self._portal_cm\n   assert self._leases >0\n   self._leases -=1\n   if not self._leases:\n    portal_cm=self._portal_cm\n    self._portal_cm=None\n    del self._portal\n    \n  if portal_cm:\n   portal_cm.__exit__(None,None,None)\n   \n   \n@contextmanager\ndef start_blocking_portal(\nbackend:str=\"asyncio\",backend_options:dict[str,Any]|None=None\n)->Generator[BlockingPortal,Any,None]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n \n \n async def run_portal()->None:\n  async with BlockingPortal()as portal_:\n   future.set_result(portal_)\n   await portal_.sleep_until_stopped()\n   \n def run_blocking_portal()->None:\n  if future.set_running_or_notify_cancel():\n   try:\n    _eventloop.run(\n    run_portal,backend=backend,backend_options=backend_options\n    )\n   except BaseException as exc:\n    if not future.done():\n     future.set_exception(exc)\n     \n future:Future[BlockingPortal]=Future()\n thread=Thread(target=run_blocking_portal,daemon=True)\n thread.start()\n try:\n  cancel_remaining_tasks=False\n  portal=future.result()\n  try:\n   yield portal\n  except BaseException:\n   cancel_remaining_tasks=True\n   raise\n  finally:\n   try:\n    portal.call(portal.stop,cancel_remaining_tasks)\n   except RuntimeError:\n    pass\n finally:\n  thread.join()\n  \n  \ndef check_cancelled()->None:\n ''\n\n\n\n\n\n\n\n\n\n \n try:\n  async_backend:AsyncBackend=threadlocals.current_async_backend\n except AttributeError:\n  raise RuntimeError(\n  \"This function can only be run from an AnyIO worker thread\"\n  )from None\n  \n async_backend.check_cancelled()\n", ["__future__", "anyio._core", "anyio._core._eventloop", "anyio._core._synchronization", "anyio._core._tasks", "anyio.abc", "anyio.abc._tasks", "collections.abc", "concurrent.futures", "contextlib", "dataclasses", "inspect", "sys", "threading", "types", "typing", "typing_extensions"]], "anyio.to_process": [".py", "from __future__ import annotations\n\nimport os\nimport pickle\nimport subprocess\nimport sys\nfrom collections import deque\nfrom collections.abc import Callable\nfrom importlib.util import module_from_spec,spec_from_file_location\nfrom typing import TypeVar,cast\n\nfrom._core._eventloop import current_time,get_async_backend,get_cancelled_exc_class\nfrom._core._exceptions import BrokenWorkerProcess\nfrom._core._subprocesses import open_process\nfrom._core._synchronization import CapacityLimiter\nfrom._core._tasks import CancelScope,fail_after\nfrom.abc import ByteReceiveStream,ByteSendStream,Process\nfrom.lowlevel import RunVar,checkpoint_if_cancelled\nfrom.streams.buffered import BufferedByteReceiveStream\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nWORKER_MAX_IDLE_TIME=300\n\nT_Retval=TypeVar(\"T_Retval\")\nPosArgsT=TypeVarTuple(\"PosArgsT\")\n\n_process_pool_workers:RunVar[set[Process]]=RunVar(\"_process_pool_workers\")\n_process_pool_idle_workers:RunVar[deque[tuple[Process,float]]]=RunVar(\n\"_process_pool_idle_workers\"\n)\n_default_process_limiter:RunVar[CapacityLimiter]=RunVar(\"_default_process_limiter\")\n\n\nasync def run_sync(\nfunc:Callable[[Unpack[PosArgsT]],T_Retval],\n*args:Unpack[PosArgsT],\ncancellable:bool=False,\nlimiter:CapacityLimiter |None=None,\n)->T_Retval:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n async def send_raw_command(pickled_cmd:bytes)->object:\n  try:\n   await stdin.send(pickled_cmd)\n   response=await buffered.receive_until(b\"\\n\",50)\n   status,length=response.split(b\" \")\n   if status not in(b\"RETURN\",b\"EXCEPTION\"):\n    raise RuntimeError(\n    f\"Worker process returned unexpected response: {response !r}\"\n    )\n    \n   pickled_response=await buffered.receive_exactly(int(length))\n  except BaseException as exc:\n   workers.discard(process)\n   try:\n    process.kill()\n    with CancelScope(shield=True):\n     await process.aclose()\n   except ProcessLookupError:\n    pass\n    \n   if isinstance(exc,get_cancelled_exc_class()):\n    raise\n   else:\n    raise BrokenWorkerProcess from exc\n    \n  retval=pickle.loads(pickled_response)\n  if status ==b\"EXCEPTION\":\n   assert isinstance(retval,BaseException)\n   raise retval\n  else:\n   return retval\n   \n   \n await checkpoint_if_cancelled()\n request=pickle.dumps((\"run\",func,args),protocol=pickle.HIGHEST_PROTOCOL)\n \n \n try:\n  workers=_process_pool_workers.get()\n  idle_workers=_process_pool_idle_workers.get()\n except LookupError:\n  workers=set()\n  idle_workers=deque()\n  _process_pool_workers.set(workers)\n  _process_pool_idle_workers.set(idle_workers)\n  get_async_backend().setup_process_pool_exit_at_shutdown(workers)\n  \n async with limiter or current_default_process_limiter():\n \n \n  process:Process\n  while idle_workers:\n   process,idle_since=idle_workers.pop()\n   if process.returncode is None:\n    stdin=cast(ByteSendStream,process.stdin)\n    buffered=BufferedByteReceiveStream(\n    cast(ByteReceiveStream,process.stdout)\n    )\n    \n    \n    \n    now=current_time()\n    killed_processes:list[Process]=[]\n    while idle_workers:\n     if now -idle_workers[0][1]<WORKER_MAX_IDLE_TIME:\n      break\n      \n     process_to_kill,idle_since=idle_workers.popleft()\n     process_to_kill.kill()\n     workers.remove(process_to_kill)\n     killed_processes.append(process_to_kill)\n     \n    with CancelScope(shield=True):\n     for killed_process in killed_processes:\n      await killed_process.aclose()\n      \n    break\n    \n   workers.remove(process)\n  else:\n   command=[sys.executable,\"-u\",\"-m\",__name__]\n   process=await open_process(\n   command,stdin=subprocess.PIPE,stdout=subprocess.PIPE\n   )\n   try:\n    stdin=cast(ByteSendStream,process.stdin)\n    buffered=BufferedByteReceiveStream(\n    cast(ByteReceiveStream,process.stdout)\n    )\n    with fail_after(20):\n     message=await buffered.receive(6)\n     \n    if message !=b\"READY\\n\":\n     raise BrokenWorkerProcess(\n     f\"Worker process returned unexpected response: {message !r}\"\n     )\n     \n    main_module_path=getattr(sys.modules[\"__main__\"],\"__file__\",None)\n    pickled=pickle.dumps(\n    (\"init\",sys.path,main_module_path),\n    protocol=pickle.HIGHEST_PROTOCOL,\n    )\n    await send_raw_command(pickled)\n   except(BrokenWorkerProcess,get_cancelled_exc_class()):\n    raise\n   except BaseException as exc:\n    process.kill()\n    raise BrokenWorkerProcess(\n    \"Error during worker process initialization\"\n    )from exc\n    \n   workers.add(process)\n   \n  with CancelScope(shield=not cancellable):\n   try:\n    return cast(T_Retval,await send_raw_command(request))\n   finally:\n    if process in workers:\n     idle_workers.append((process,current_time()))\n     \n     \ndef current_default_process_limiter()->CapacityLimiter:\n ''\n\n\n\n\n\n \n try:\n  return _default_process_limiter.get()\n except LookupError:\n  limiter=CapacityLimiter(os.cpu_count()or 2)\n  _default_process_limiter.set(limiter)\n  return limiter\n  \n  \ndef process_worker()->None:\n\n\n stdin=sys.stdin\n stdout=sys.stdout\n sys.stdin=open(os.devnull)\n sys.stdout=open(os.devnull,\"w\")\n \n stdout.buffer.write(b\"READY\\n\")\n while True:\n  retval=exception=None\n  try:\n   command,*args=pickle.load(stdin.buffer)\n  except EOFError:\n   return\n  except BaseException as exc:\n   exception=exc\n  else:\n   if command ==\"run\":\n    func,args=args\n    try:\n     retval=func(*args)\n    except BaseException as exc:\n     exception=exc\n   elif command ==\"init\":\n    main_module_path:str |None\n    sys.path,main_module_path=args\n    del sys.modules[\"__main__\"]\n    if main_module_path and os.path.isfile(main_module_path):\n    \n    \n     try:\n      spec=spec_from_file_location(\"__mp_main__\",main_module_path)\n      if spec and spec.loader:\n       main=module_from_spec(spec)\n       spec.loader.exec_module(main)\n       sys.modules[\"__main__\"]=main\n     except BaseException as exc:\n      exception=exc\n  try:\n   if exception is not None:\n    status=b\"EXCEPTION\"\n    pickled=pickle.dumps(exception,pickle.HIGHEST_PROTOCOL)\n   else:\n    status=b\"RETURN\"\n    pickled=pickle.dumps(retval,pickle.HIGHEST_PROTOCOL)\n  except BaseException as exc:\n   exception=exc\n   status=b\"EXCEPTION\"\n   pickled=pickle.dumps(exc,pickle.HIGHEST_PROTOCOL)\n   \n  stdout.buffer.write(b\"%s %d\\n\"%(status,len(pickled)))\n  stdout.buffer.write(pickled)\n  \n  \n  if isinstance(exception,SystemExit):\n   raise exception\n   \n   \nif __name__ ==\"__main__\":\n process_worker()\n", ["__future__", "anyio._core._eventloop", "anyio._core._exceptions", "anyio._core._subprocesses", "anyio._core._synchronization", "anyio._core._tasks", "anyio.abc", "anyio.lowlevel", "anyio.streams.buffered", "collections", "collections.abc", "importlib.util", "os", "pickle", "subprocess", "sys", "typing", "typing_extensions"]], "anyio.pytest_plugin": [".py", "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Generator,Iterator\nfrom contextlib import ExitStack,contextmanager\nfrom inspect import isasyncgenfunction,iscoroutinefunction,ismethod\nfrom typing import Any,cast\n\nimport pytest\nimport sniffio\nfrom _pytest.fixtures import SubRequest\nfrom _pytest.outcomes import Exit\n\nfrom._core._eventloop import get_all_backends,get_async_backend\nfrom._core._exceptions import iterate_exceptions\nfrom.abc import TestRunner\n\nif sys.version_info <(3,11):\n from exceptiongroup import ExceptionGroup\n \n_current_runner:TestRunner |None=None\n_runner_stack:ExitStack |None=None\n_runner_leases=0\n\n\ndef extract_backend_and_options(backend:object)->tuple[str,dict[str,Any]]:\n if isinstance(backend,str):\n  return backend,{}\n elif isinstance(backend,tuple)and len(backend)==2:\n  if isinstance(backend[0],str)and isinstance(backend[1],dict):\n   return cast(tuple[str,dict[str,Any]],backend)\n   \n raise TypeError(\"anyio_backend must be either a string or tuple of (string, dict)\")\n \n \n@contextmanager\ndef get_runner(\nbackend_name:str,backend_options:dict[str,Any]\n)->Iterator[TestRunner]:\n global _current_runner,_runner_leases,_runner_stack\n if _current_runner is None:\n  asynclib=get_async_backend(backend_name)\n  _runner_stack=ExitStack()\n  if sniffio.current_async_library_cvar.get(None)is None:\n  \n  \n   token=sniffio.current_async_library_cvar.set(backend_name)\n   _runner_stack.callback(sniffio.current_async_library_cvar.reset,token)\n   \n  backend_options=backend_options or{}\n  _current_runner=_runner_stack.enter_context(\n  asynclib.create_test_runner(backend_options)\n  )\n  \n _runner_leases +=1\n try:\n  yield _current_runner\n finally:\n  _runner_leases -=1\n  if not _runner_leases:\n   assert _runner_stack is not None\n   _runner_stack.close()\n   _runner_stack=_current_runner=None\n   \n   \ndef pytest_configure(config:Any)->None:\n config.addinivalue_line(\n \"markers\",\n \"anyio: mark the (coroutine function) test to be run \"\n \"asynchronously via anyio.\",\n )\n \n \n@pytest.hookimpl(hookwrapper=True)\ndef pytest_fixture_setup(fixturedef:Any,request:Any)->Generator[Any]:\n def wrapper(\n *args:Any,anyio_backend:Any,request:SubRequest,**kwargs:Any\n )->Any:\n \n  if(\n  request.instance\n  and ismethod(func)\n  and type(func.__self__)is type(request.instance)\n  ):\n   local_func=func.__func__.__get__(request.instance)\n  else:\n   local_func=func\n   \n  backend_name,backend_options=extract_backend_and_options(anyio_backend)\n  if has_backend_arg:\n   kwargs[\"anyio_backend\"]=anyio_backend\n   \n  if has_request_arg:\n   kwargs[\"request\"]=request\n   \n  with get_runner(backend_name,backend_options)as runner:\n   if isasyncgenfunction(local_func):\n    yield from runner.run_asyncgen_fixture(local_func,kwargs)\n   else:\n    yield runner.run_fixture(local_func,kwargs)\n    \n    \n    \n func=fixturedef.func\n if isasyncgenfunction(func)or iscoroutinefunction(func):\n  if \"anyio_backend\"in request.fixturenames:\n   fixturedef.func=wrapper\n   original_argname=fixturedef.argnames\n   \n   if not(has_backend_arg :=\"anyio_backend\"in fixturedef.argnames):\n    fixturedef.argnames +=(\"anyio_backend\",)\n    \n   if not(has_request_arg :=\"request\"in fixturedef.argnames):\n    fixturedef.argnames +=(\"request\",)\n    \n   try:\n    return(yield)\n   finally:\n    fixturedef.func=func\n    fixturedef.argnames=original_argname\n    \n return(yield)\n \n \n@pytest.hookimpl(tryfirst=True)\ndef pytest_pycollect_makeitem(collector:Any,name:Any,obj:Any)->None:\n if collector.istestfunction(obj,name):\n  inner_func=obj.hypothesis.inner_test if hasattr(obj,\"hypothesis\")else obj\n  if iscoroutinefunction(inner_func):\n   marker=collector.get_closest_marker(\"anyio\")\n   own_markers=getattr(obj,\"pytestmark\",())\n   if marker or any(marker.name ==\"anyio\"for marker in own_markers):\n    pytest.mark.usefixtures(\"anyio_backend\")(obj)\n    \n    \n@pytest.hookimpl(tryfirst=True)\ndef pytest_pyfunc_call(pyfuncitem:Any)->bool |None:\n def run_with_hypothesis(**kwargs:Any)->None:\n  with get_runner(backend_name,backend_options)as runner:\n   runner.run_test(original_func,kwargs)\n   \n backend=pyfuncitem.funcargs.get(\"anyio_backend\")\n if backend:\n  backend_name,backend_options=extract_backend_and_options(backend)\n  \n  if hasattr(pyfuncitem.obj,\"hypothesis\"):\n  \n   original_func=pyfuncitem.obj.hypothesis.inner_test\n   if original_func.__qualname__ !=run_with_hypothesis.__qualname__:\n    if iscoroutinefunction(original_func):\n     pyfuncitem.obj.hypothesis.inner_test=run_with_hypothesis\n     \n   return None\n   \n  if iscoroutinefunction(pyfuncitem.obj):\n   funcargs=pyfuncitem.funcargs\n   testargs={arg:funcargs[arg]for arg in pyfuncitem._fixtureinfo.argnames}\n   with get_runner(backend_name,backend_options)as runner:\n    try:\n     runner.run_test(pyfuncitem.obj,testargs)\n    except ExceptionGroup as excgrp:\n     for exc in iterate_exceptions(excgrp):\n      if isinstance(exc,(Exit,KeyboardInterrupt,SystemExit)):\n       raise exc from excgrp\n       \n     raise\n     \n   return True\n   \n return None\n \n \n@pytest.fixture(scope=\"module\",params=get_all_backends())\ndef anyio_backend(request:Any)->Any:\n return request.param\n \n \n@pytest.fixture\ndef anyio_backend_name(anyio_backend:Any)->str:\n if isinstance(anyio_backend,str):\n  return anyio_backend\n else:\n  return anyio_backend[0]\n  \n  \n@pytest.fixture\ndef anyio_backend_options(anyio_backend:Any)->dict[str,Any]:\n if isinstance(anyio_backend,str):\n  return{}\n else:\n  return anyio_backend[1]\n", ["__future__", "_pytest.fixtures", "_pytest.outcomes", "anyio._core._eventloop", "anyio._core._exceptions", "anyio.abc", "collections.abc", "contextlib", "exceptiongroup", "inspect", "pytest", "sniffio", "sys", "typing"]], "anyio.abc": [".py", "from __future__ import annotations\n\nfrom._eventloop import AsyncBackend as AsyncBackend\nfrom._resources import AsyncResource as AsyncResource\nfrom._sockets import ConnectedUDPSocket as ConnectedUDPSocket\nfrom._sockets import ConnectedUNIXDatagramSocket as ConnectedUNIXDatagramSocket\nfrom._sockets import IPAddressType as IPAddressType\nfrom._sockets import IPSockAddrType as IPSockAddrType\nfrom._sockets import SocketAttribute as SocketAttribute\nfrom._sockets import SocketListener as SocketListener\nfrom._sockets import SocketStream as SocketStream\nfrom._sockets import UDPPacketType as UDPPacketType\nfrom._sockets import UDPSocket as UDPSocket\nfrom._sockets import UNIXDatagramPacketType as UNIXDatagramPacketType\nfrom._sockets import UNIXDatagramSocket as UNIXDatagramSocket\nfrom._sockets import UNIXSocketStream as UNIXSocketStream\nfrom._streams import AnyByteReceiveStream as AnyByteReceiveStream\nfrom._streams import AnyByteSendStream as AnyByteSendStream\nfrom._streams import AnyByteStream as AnyByteStream\nfrom._streams import AnyUnreliableByteReceiveStream as AnyUnreliableByteReceiveStream\nfrom._streams import AnyUnreliableByteSendStream as AnyUnreliableByteSendStream\nfrom._streams import AnyUnreliableByteStream as AnyUnreliableByteStream\nfrom._streams import ByteReceiveStream as ByteReceiveStream\nfrom._streams import ByteSendStream as ByteSendStream\nfrom._streams import ByteStream as ByteStream\nfrom._streams import Listener as Listener\nfrom._streams import ObjectReceiveStream as ObjectReceiveStream\nfrom._streams import ObjectSendStream as ObjectSendStream\nfrom._streams import ObjectStream as ObjectStream\nfrom._streams import UnreliableObjectReceiveStream as UnreliableObjectReceiveStream\nfrom._streams import UnreliableObjectSendStream as UnreliableObjectSendStream\nfrom._streams import UnreliableObjectStream as UnreliableObjectStream\nfrom._subprocesses import Process as Process\nfrom._tasks import TaskGroup as TaskGroup\nfrom._tasks import TaskStatus as TaskStatus\nfrom._testing import TestRunner as TestRunner\n\n\n\nfrom.._core._synchronization import(\nCapacityLimiter as CapacityLimiter,\nCondition as Condition,\nEvent as Event,\nLock as Lock,\nSemaphore as Semaphore,\n)\nfrom.._core._tasks import CancelScope as CancelScope\nfrom..from_thread import BlockingPortal as BlockingPortal\n\n\nfor __value in list(locals().values()):\n if getattr(__value,\"__module__\",\"\").startswith(\"anyio.abc.\"):\n  __value.__module__=__name__\n  \ndel __value\n", ["__future__", "anyio._core._synchronization", "anyio._core._tasks", "anyio.abc._eventloop", "anyio.abc._resources", "anyio.abc._sockets", "anyio.abc._streams", "anyio.abc._subprocesses", "anyio.abc._tasks", "anyio.abc._testing", "anyio.from_thread"], 1], "anyio.abc._tasks": [".py", "from __future__ import annotations\n\nimport sys\nfrom abc import ABCMeta,abstractmethod\nfrom collections.abc import Awaitable,Callable\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING,Any,Protocol,TypeVar,overload\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nif TYPE_CHECKING:\n from.._core._tasks import CancelScope\n \nT_Retval=TypeVar(\"T_Retval\")\nT_contra=TypeVar(\"T_contra\",contravariant=True)\nPosArgsT=TypeVarTuple(\"PosArgsT\")\n\n\nclass TaskStatus(Protocol[T_contra]):\n @overload\n def started(self:TaskStatus[None])->None:...\n \n @overload\n def started(self,value:T_contra)->None:...\n \n def started(self,value:T_contra |None=None)->None:\n  ''\n\n\n\n  \n  \n  \nclass TaskGroup(metaclass=ABCMeta):\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n cancel_scope:CancelScope\n \n @abstractmethod\n def start_soon(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[Any]],\n *args:Unpack[PosArgsT],\n name:object=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n  \n  \n @abstractmethod\n async def start(\n self,\n func:Callable[...,Awaitable[Any]],\n *args:object,\n name:object=None,\n )->Any:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  \n @abstractmethod\n async def __aenter__(self)->TaskGroup:\n  ''\n  \n @abstractmethod\n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  ''\n", ["__future__", "abc", "anyio._core._tasks", "collections.abc", "sys", "types", "typing", "typing_extensions"]], "anyio.abc._subprocesses": [".py", "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom signal import Signals\n\nfrom._resources import AsyncResource\nfrom._streams import ByteReceiveStream,ByteSendStream\n\n\nclass Process(AsyncResource):\n ''\n \n @abstractmethod\n async def wait(self)->int:\n  ''\n\n\n\n  \n  \n @abstractmethod\n def terminate(self)->None:\n  ''\n\n\n\n\n\n\n  \n  \n @abstractmethod\n def kill(self)->None:\n  ''\n\n\n\n\n\n\n  \n  \n @abstractmethod\n def send_signal(self,signal:Signals)->None:\n  ''\n\n\n\n\n\n  \n  \n @property\n @abstractmethod\n def pid(self)->int:\n  ''\n  \n @property\n @abstractmethod\n def returncode(self)->int |None:\n  ''\n\n\n  \n  \n @property\n @abstractmethod\n def stdin(self)->ByteSendStream |None:\n  ''\n  \n @property\n @abstractmethod\n def stdout(self)->ByteReceiveStream |None:\n  ''\n  \n @property\n @abstractmethod\n def stderr(self)->ByteReceiveStream |None:\n  ''\n", ["__future__", "abc", "anyio.abc._resources", "anyio.abc._streams", "signal"]], "anyio.abc._eventloop": [".py", "from __future__ import annotations\n\nimport math\nimport sys\nfrom abc import ABCMeta,abstractmethod\nfrom collections.abc import AsyncIterator,Awaitable,Callable,Sequence\nfrom contextlib import AbstractContextManager\nfrom os import PathLike\nfrom signal import Signals\nfrom socket import AddressFamily,SocketKind,socket\nfrom typing import(\nIO,\nTYPE_CHECKING,\nAny,\nTypeVar,\nUnion,\noverload,\n)\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nif sys.version_info >=(3,10):\n from typing import TypeAlias\nelse:\n from typing_extensions import TypeAlias\n \nif TYPE_CHECKING:\n from _typeshed import HasFileno\n \n from.._core._synchronization import CapacityLimiter,Event,Lock,Semaphore\n from.._core._tasks import CancelScope\n from.._core._testing import TaskInfo\n from..from_thread import BlockingPortal\n from._sockets import(\n ConnectedUDPSocket,\n ConnectedUNIXDatagramSocket,\n IPSockAddrType,\n SocketListener,\n SocketStream,\n UDPSocket,\n UNIXDatagramSocket,\n UNIXSocketStream,\n )\n from._subprocesses import Process\n from._tasks import TaskGroup\n from._testing import TestRunner\n \nT_Retval=TypeVar(\"T_Retval\")\nPosArgsT=TypeVarTuple(\"PosArgsT\")\nStrOrBytesPath:TypeAlias=Union[str,bytes,\"PathLike[str]\",\"PathLike[bytes]\"]\n\n\nclass AsyncBackend(metaclass=ABCMeta):\n @classmethod\n @abstractmethod\n def run(\n cls,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n options:dict[str,Any],\n )->T_Retval:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  \n @classmethod\n @abstractmethod\n def current_token(cls)->object:\n  ''\n\n\n  \n  \n @classmethod\n @abstractmethod\n def current_time(cls)->float:\n  ''\n\n\n\n  \n  \n @classmethod\n @abstractmethod\n def cancelled_exception_class(cls)->type[BaseException]:\n  ''\n  \n @classmethod\n @abstractmethod\n async def checkpoint(cls)->None:\n  ''\n\n\n\n\n  \n  \n @classmethod\n async def checkpoint_if_cancelled(cls)->None:\n  ''\n\n\n\n\n\n  \n  if cls.current_effective_deadline()==-math.inf:\n   await cls.checkpoint()\n   \n @classmethod\n async def cancel_shielded_checkpoint(cls)->None:\n  ''\n\n\n\n\n\n  \n  with cls.create_cancel_scope(shield=True):\n   await cls.sleep(0)\n   \n @classmethod\n @abstractmethod\n async def sleep(cls,delay:float)->None:\n  ''\n\n\n\n  \n  \n @classmethod\n @abstractmethod\n def create_cancel_scope(\n cls,*,deadline:float=math.inf,shield:bool=False\n )->CancelScope:\n  pass\n  \n @classmethod\n @abstractmethod\n def current_effective_deadline(cls)->float:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n @classmethod\n @abstractmethod\n def create_task_group(cls)->TaskGroup:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_event(cls)->Event:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_lock(cls,*,fast_acquire:bool)->Lock:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_semaphore(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->Semaphore:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_capacity_limiter(cls,total_tokens:float)->CapacityLimiter:\n  pass\n  \n @classmethod\n @abstractmethod\n async def run_sync_in_worker_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n args:tuple[Unpack[PosArgsT]],\n abandon_on_cancel:bool=False,\n limiter:CapacityLimiter |None=None,\n )->T_Retval:\n  pass\n  \n @classmethod\n @abstractmethod\n def check_cancelled(cls)->None:\n  pass\n  \n @classmethod\n @abstractmethod\n def run_async_from_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n args:tuple[Unpack[PosArgsT]],\n token:object,\n )->T_Retval:\n  pass\n  \n @classmethod\n @abstractmethod\n def run_sync_from_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n args:tuple[Unpack[PosArgsT]],\n token:object,\n )->T_Retval:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_blocking_portal(cls)->BlockingPortal:\n  pass\n  \n @classmethod\n @abstractmethod\n async def open_process(\n cls,\n command:StrOrBytesPath |Sequence[StrOrBytesPath],\n *,\n stdin:int |IO[Any]|None,\n stdout:int |IO[Any]|None,\n stderr:int |IO[Any]|None,\n **kwargs:Any,\n )->Process:\n  pass\n  \n @classmethod\n @abstractmethod\n def setup_process_pool_exit_at_shutdown(cls,workers:set[Process])->None:\n  pass\n  \n @classmethod\n @abstractmethod\n async def connect_tcp(\n cls,host:str,port:int,local_address:IPSockAddrType |None=None\n )->SocketStream:\n  pass\n  \n @classmethod\n @abstractmethod\n async def connect_unix(cls,path:str |bytes)->UNIXSocketStream:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_tcp_listener(cls,sock:socket)->SocketListener:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_unix_listener(cls,sock:socket)->SocketListener:\n  pass\n  \n @classmethod\n @abstractmethod\n async def create_udp_socket(\n cls,\n family:AddressFamily,\n local_address:IPSockAddrType |None,\n remote_address:IPSockAddrType |None,\n reuse_port:bool,\n )->UDPSocket |ConnectedUDPSocket:\n  pass\n  \n @classmethod\n @overload\n async def create_unix_datagram_socket(\n cls,raw_socket:socket,remote_path:None\n )->UNIXDatagramSocket:...\n \n @classmethod\n @overload\n async def create_unix_datagram_socket(\n cls,raw_socket:socket,remote_path:str |bytes\n )->ConnectedUNIXDatagramSocket:...\n \n @classmethod\n @abstractmethod\n async def create_unix_datagram_socket(\n cls,raw_socket:socket,remote_path:str |bytes |None\n )->UNIXDatagramSocket |ConnectedUNIXDatagramSocket:\n  pass\n  \n @classmethod\n @abstractmethod\n async def getaddrinfo(\n cls,\n host:bytes |str |None,\n port:str |int |None,\n *,\n family:int |AddressFamily=0,\n type:int |SocketKind=0,\n proto:int=0,\n flags:int=0,\n )->list[\n tuple[\n AddressFamily,\n SocketKind,\n int,\n str,\n tuple[str,int]|tuple[str,int,int,int],\n ]\n ]:\n  pass\n  \n @classmethod\n @abstractmethod\n async def getnameinfo(\n cls,sockaddr:IPSockAddrType,flags:int=0\n )->tuple[str,str]:\n  pass\n  \n @classmethod\n @abstractmethod\n async def wait_readable(cls,obj:HasFileno |int)->None:\n  pass\n  \n @classmethod\n @abstractmethod\n async def wait_writable(cls,obj:HasFileno |int)->None:\n  pass\n  \n @classmethod\n @abstractmethod\n def current_default_thread_limiter(cls)->CapacityLimiter:\n  pass\n  \n @classmethod\n @abstractmethod\n def open_signal_receiver(\n cls,*signals:Signals\n )->AbstractContextManager[AsyncIterator[Signals]]:\n  pass\n  \n @classmethod\n @abstractmethod\n def get_current_task(cls)->TaskInfo:\n  pass\n  \n @classmethod\n @abstractmethod\n def get_running_tasks(cls)->Sequence[TaskInfo]:\n  pass\n  \n @classmethod\n @abstractmethod\n async def wait_all_tasks_blocked(cls)->None:\n  pass\n  \n @classmethod\n @abstractmethod\n def create_test_runner(cls,options:dict[str,Any])->TestRunner:\n  pass\n", ["__future__", "_typeshed", "abc", "anyio._core._synchronization", "anyio._core._tasks", "anyio._core._testing", "anyio.abc._sockets", "anyio.abc._subprocesses", "anyio.abc._tasks", "anyio.abc._testing", "anyio.from_thread", "collections.abc", "contextlib", "math", "os", "signal", "socket", "sys", "typing", "typing_extensions"]], "anyio.abc._streams": [".py", "from __future__ import annotations\n\nfrom abc import abstractmethod\nfrom collections.abc import Callable\nfrom typing import Any,Generic,TypeVar,Union\n\nfrom.._core._exceptions import EndOfStream\nfrom.._core._typedattr import TypedAttributeProvider\nfrom._resources import AsyncResource\nfrom._tasks import TaskGroup\n\nT_Item=TypeVar(\"T_Item\")\nT_co=TypeVar(\"T_co\",covariant=True)\nT_contra=TypeVar(\"T_contra\",contravariant=True)\n\n\nclass UnreliableObjectReceiveStream(\nGeneric[T_co],AsyncResource,TypedAttributeProvider\n):\n ''\n\n\n\n\n\n\n\n \n \n def __aiter__(self)->UnreliableObjectReceiveStream[T_co]:\n  return self\n  \n async def __anext__(self)->T_co:\n  try:\n   return await self.receive()\n  except EndOfStream:\n   raise StopAsyncIteration\n   \n @abstractmethod\n async def receive(self)->T_co:\n  ''\n\n\n\n\n\n\n\n  \n  \n  \nclass UnreliableObjectSendStream(\nGeneric[T_contra],AsyncResource,TypedAttributeProvider\n):\n ''\n\n\n\n\n \n \n @abstractmethod\n async def send(self,item:T_contra)->None:\n  ''\n\n\n\n\n\n\n\n  \n  \n  \nclass UnreliableObjectStream(\nUnreliableObjectReceiveStream[T_Item],UnreliableObjectSendStream[T_Item]\n):\n ''\n\n\n \n \n \nclass ObjectReceiveStream(UnreliableObjectReceiveStream[T_co]):\n ''\n\n\n \n \n \nclass ObjectSendStream(UnreliableObjectSendStream[T_contra]):\n ''\n\n\n \n \n \nclass ObjectStream(\nObjectReceiveStream[T_Item],\nObjectSendStream[T_Item],\nUnreliableObjectStream[T_Item],\n):\n ''\n\n\n \n \n @abstractmethod\n async def send_eof(self)->None:\n  ''\n\n\n\n\n  \n  \n  \nclass ByteReceiveStream(AsyncResource,TypedAttributeProvider):\n ''\n\n\n\n\n \n \n def __aiter__(self)->ByteReceiveStream:\n  return self\n  \n async def __anext__(self)->bytes:\n  try:\n   return await self.receive()\n  except EndOfStream:\n   raise StopAsyncIteration\n   \n @abstractmethod\n async def receive(self,max_bytes:int=65536)->bytes:\n  ''\n\n\n\n\n\n\n\n\n  \n  \n  \nclass ByteSendStream(AsyncResource,TypedAttributeProvider):\n ''\n \n @abstractmethod\n async def send(self,item:bytes)->None:\n  ''\n\n\n\n  \n  \n  \nclass ByteStream(ByteReceiveStream,ByteSendStream):\n ''\n \n @abstractmethod\n async def send_eof(self)->None:\n  ''\n\n\n\n\n  \n  \n  \n  \nAnyUnreliableByteReceiveStream=Union[\nUnreliableObjectReceiveStream[bytes],ByteReceiveStream\n]\n\nAnyUnreliableByteSendStream=Union[UnreliableObjectSendStream[bytes],ByteSendStream]\n\nAnyUnreliableByteStream=Union[UnreliableObjectStream[bytes],ByteStream]\n\nAnyByteReceiveStream=Union[ObjectReceiveStream[bytes],ByteReceiveStream]\n\nAnyByteSendStream=Union[ObjectSendStream[bytes],ByteSendStream]\n\nAnyByteStream=Union[ObjectStream[bytes],ByteStream]\n\n\nclass Listener(Generic[T_co],AsyncResource,TypedAttributeProvider):\n ''\n \n @abstractmethod\n async def serve(\n self,handler:Callable[[T_co],Any],task_group:TaskGroup |None=None\n )->None:\n  ''\n\n\n\n\n\n  \n", ["__future__", "abc", "anyio._core._exceptions", "anyio._core._typedattr", "anyio.abc._resources", "anyio.abc._tasks", "collections.abc", "typing"]], "anyio.abc._sockets": [".py", "from __future__ import annotations\n\nimport socket\nfrom abc import abstractmethod\nfrom collections.abc import Callable,Collection,Mapping\nfrom contextlib import AsyncExitStack\nfrom io import IOBase\nfrom ipaddress import IPv4Address,IPv6Address\nfrom socket import AddressFamily\nfrom types import TracebackType\nfrom typing import Any,TypeVar,Union\n\nfrom.._core._typedattr import(\nTypedAttributeProvider,\nTypedAttributeSet,\ntyped_attribute,\n)\nfrom._streams import ByteStream,Listener,UnreliableObjectStream\nfrom._tasks import TaskGroup\n\nIPAddressType=Union[str,IPv4Address,IPv6Address]\nIPSockAddrType=tuple[str,int]\nSockAddrType=Union[IPSockAddrType,str]\nUDPPacketType=tuple[bytes,IPSockAddrType]\nUNIXDatagramPacketType=tuple[bytes,str]\nT_Retval=TypeVar(\"T_Retval\")\n\n\nclass _NullAsyncContextManager:\n async def __aenter__(self)->None:\n  pass\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  return None\n  \n  \nclass SocketAttribute(TypedAttributeSet):\n\n family:AddressFamily=typed_attribute()\n \n local_address:SockAddrType=typed_attribute()\n \n local_port:int=typed_attribute()\n \n raw_socket:socket.socket=typed_attribute()\n \n remote_address:SockAddrType=typed_attribute()\n \n remote_port:int=typed_attribute()\n \n \nclass _SocketProvider(TypedAttributeProvider):\n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  from.._core._sockets import convert_ipv6_sockaddr as convert\n  \n  attributes:dict[Any,Callable[[],Any]]={\n  SocketAttribute.family:lambda:self._raw_socket.family,\n  SocketAttribute.local_address:lambda:convert(\n  self._raw_socket.getsockname()\n  ),\n  SocketAttribute.raw_socket:lambda:self._raw_socket,\n  }\n  try:\n   peername:tuple[str,int]|None=convert(self._raw_socket.getpeername())\n  except OSError:\n   peername=None\n   \n   \n  if peername is not None:\n   attributes[SocketAttribute.remote_address]=lambda:peername\n   \n   \n  if self._raw_socket.family in(AddressFamily.AF_INET,AddressFamily.AF_INET6):\n   attributes[SocketAttribute.local_port]=(\n   lambda:self._raw_socket.getsockname()[1]\n   )\n   if peername is not None:\n    remote_port=peername[1]\n    attributes[SocketAttribute.remote_port]=lambda:remote_port\n    \n  return attributes\n  \n @property\n @abstractmethod\n def _raw_socket(self)->socket.socket:\n  pass\n  \n  \nclass SocketStream(ByteStream,_SocketProvider):\n ''\n\n\n\n \n \n \nclass UNIXSocketStream(SocketStream):\n @abstractmethod\n async def send_fds(self,message:bytes,fds:Collection[int |IOBase])->None:\n  ''\n\n\n\n\n\n  \n  \n @abstractmethod\n async def receive_fds(self,msglen:int,maxfds:int)->tuple[bytes,list[int]]:\n  ''\n\n\n\n\n\n  \n  \n  \nclass SocketListener(Listener[SocketStream],_SocketProvider):\n ''\n\n\n\n \n \n @abstractmethod\n async def accept(self)->SocketStream:\n  ''\n  \n async def serve(\n self,\n handler:Callable[[SocketStream],Any],\n task_group:TaskGroup |None=None,\n )->None:\n  from.. import create_task_group\n  \n  async with AsyncExitStack()as stack:\n   if task_group is None:\n    task_group=await stack.enter_async_context(create_task_group())\n    \n   while True:\n    stream=await self.accept()\n    task_group.start_soon(handler,stream)\n    \n    \nclass UDPSocket(UnreliableObjectStream[UDPPacketType],_SocketProvider):\n ''\n\n\n\n \n \n async def sendto(self,data:bytes,host:str,port:int)->None:\n  ''\n\n\n  \n  return await self.send((data,(host,port)))\n  \n  \nclass ConnectedUDPSocket(UnreliableObjectStream[bytes],_SocketProvider):\n ''\n\n\n\n \n \n \nclass UNIXDatagramSocket(\nUnreliableObjectStream[UNIXDatagramPacketType],_SocketProvider\n):\n ''\n\n\n\n \n \n async def sendto(self,data:bytes,path:str)->None:\n  ''\n  return await self.send((data,path))\n  \n  \nclass ConnectedUNIXDatagramSocket(UnreliableObjectStream[bytes],_SocketProvider):\n ''\n\n\n\n \n", ["__future__", "abc", "anyio", "anyio._core._sockets", "anyio._core._typedattr", "anyio.abc._streams", "anyio.abc._tasks", "collections.abc", "contextlib", "io", "ipaddress", "socket", "types", "typing"]], "anyio.abc._testing": [".py", "from __future__ import annotations\n\nimport types\nfrom abc import ABCMeta,abstractmethod\nfrom collections.abc import AsyncGenerator,Callable,Coroutine,Iterable\nfrom typing import Any,TypeVar\n\n_T=TypeVar(\"_T\")\n\n\nclass TestRunner(metaclass=ABCMeta):\n ''\n\n\n \n \n def __enter__(self)->TestRunner:\n  return self\n  \n @abstractmethod\n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:types.TracebackType |None,\n )->bool |None:...\n \n @abstractmethod\n def run_asyncgen_fixture(\n self,\n fixture_func:Callable[...,AsyncGenerator[_T,Any]],\n kwargs:dict[str,Any],\n )->Iterable[_T]:\n  ''\n\n\n\n\n\n  \n  \n @abstractmethod\n def run_fixture(\n self,\n fixture_func:Callable[...,Coroutine[Any,Any,_T]],\n kwargs:dict[str,Any],\n )->_T:\n  ''\n\n\n\n\n\n  \n  \n @abstractmethod\n def run_test(\n self,test_func:Callable[...,Coroutine[Any,Any,Any]],kwargs:dict[str,Any]\n )->None:\n  ''\n\n\n\n\n  \n", ["__future__", "abc", "collections.abc", "types", "typing"]], "anyio.abc._resources": [".py", "from __future__ import annotations\n\nfrom abc import ABCMeta,abstractmethod\nfrom types import TracebackType\nfrom typing import TypeVar\n\nT=TypeVar(\"T\")\n\n\nclass AsyncResource(metaclass=ABCMeta):\n ''\n\n\n\n\n \n \n __slots__=()\n \n async def __aenter__(self:T)->T:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  await self.aclose()\n  \n @abstractmethod\n async def aclose(self)->None:\n  ''\n", ["__future__", "abc", "types", "typing"]], "anyio.streams": [".py", "", [], 1], "anyio.streams.buffered": [".py", "from __future__ import annotations\n\nfrom collections.abc import Callable,Mapping\nfrom dataclasses import dataclass,field\nfrom typing import Any\n\nfrom.. import ClosedResourceError,DelimiterNotFound,EndOfStream,IncompleteRead\nfrom..abc import AnyByteReceiveStream,ByteReceiveStream\n\n\n@dataclass(eq=False)\nclass BufferedByteReceiveStream(ByteReceiveStream):\n ''\n\n\n \n \n receive_stream:AnyByteReceiveStream\n _buffer:bytearray=field(init=False,default_factory=bytearray)\n _closed:bool=field(init=False,default=False)\n \n async def aclose(self)->None:\n  await self.receive_stream.aclose()\n  self._closed=True\n  \n @property\n def buffer(self)->bytes:\n  ''\n  return bytes(self._buffer)\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return self.receive_stream.extra_attributes\n  \n async def receive(self,max_bytes:int=65536)->bytes:\n  if self._closed:\n   raise ClosedResourceError\n   \n  if self._buffer:\n   chunk=bytes(self._buffer[:max_bytes])\n   del self._buffer[:max_bytes]\n   return chunk\n  elif isinstance(self.receive_stream,ByteReceiveStream):\n   return await self.receive_stream.receive(max_bytes)\n  else:\n  \n  \n   chunk=await self.receive_stream.receive()\n   if len(chunk)>max_bytes:\n   \n    self._buffer.extend(chunk[max_bytes:])\n    return chunk[:max_bytes]\n   else:\n    return chunk\n    \n async def receive_exactly(self,nbytes:int)->bytes:\n  ''\n\n\n\n\n\n\n\n  \n  while True:\n   remaining=nbytes -len(self._buffer)\n   if remaining <=0:\n    retval=self._buffer[:nbytes]\n    del self._buffer[:nbytes]\n    return bytes(retval)\n    \n   try:\n    if isinstance(self.receive_stream,ByteReceiveStream):\n     chunk=await self.receive_stream.receive(remaining)\n    else:\n     chunk=await self.receive_stream.receive()\n   except EndOfStream as exc:\n    raise IncompleteRead from exc\n    \n   self._buffer.extend(chunk)\n   \n async def receive_until(self,delimiter:bytes,max_bytes:int)->bytes:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  delimiter_size=len(delimiter)\n  offset=0\n  while True:\n  \n   index=self._buffer.find(delimiter,offset)\n   if index >=0:\n    found=self._buffer[:index]\n    del self._buffer[:index+len(delimiter):]\n    return bytes(found)\n    \n    \n   if len(self._buffer)>=max_bytes:\n    raise DelimiterNotFound(max_bytes)\n    \n    \n   try:\n    data=await self.receive_stream.receive()\n   except EndOfStream as exc:\n    raise IncompleteRead from exc\n    \n    \n   offset=max(len(self._buffer)-delimiter_size+1,0)\n   self._buffer.extend(data)\n", ["__future__", "anyio", "anyio.abc", "collections.abc", "dataclasses", "typing"]], "anyio.streams.file": [".py", "from __future__ import annotations\n\nfrom collections.abc import Callable,Mapping\nfrom io import SEEK_SET,UnsupportedOperation\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Any,BinaryIO,cast\n\nfrom.. import(\nBrokenResourceError,\nClosedResourceError,\nEndOfStream,\nTypedAttributeSet,\nto_thread,\ntyped_attribute,\n)\nfrom..abc import ByteReceiveStream,ByteSendStream\n\n\nclass FileStreamAttribute(TypedAttributeSet):\n\n file:BinaryIO=typed_attribute()\n \n path:Path=typed_attribute()\n \n fileno:int=typed_attribute()\n \n \nclass _BaseFileStream:\n def __init__(self,file:BinaryIO):\n  self._file=file\n  \n async def aclose(self)->None:\n  await to_thread.run_sync(self._file.close)\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  attributes:dict[Any,Callable[[],Any]]={\n  FileStreamAttribute.file:lambda:self._file,\n  }\n  \n  if hasattr(self._file,\"name\"):\n   attributes[FileStreamAttribute.path]=lambda:Path(self._file.name)\n   \n  try:\n   self._file.fileno()\n  except UnsupportedOperation:\n   pass\n  else:\n   attributes[FileStreamAttribute.fileno]=lambda:self._file.fileno()\n   \n  return attributes\n  \n  \nclass FileReadStream(_BaseFileStream,ByteReceiveStream):\n ''\n\n\n\n\n\n \n \n @classmethod\n async def from_path(cls,path:str |PathLike[str])->FileReadStream:\n  ''\n\n\n\n\n  \n  file=await to_thread.run_sync(Path(path).open,\"rb\")\n  return cls(cast(BinaryIO,file))\n  \n async def receive(self,max_bytes:int=65536)->bytes:\n  try:\n   data=await to_thread.run_sync(self._file.read,max_bytes)\n  except ValueError:\n   raise ClosedResourceError from None\n  except OSError as exc:\n   raise BrokenResourceError from exc\n   \n  if data:\n   return data\n  else:\n   raise EndOfStream\n   \n async def seek(self,position:int,whence:int=SEEK_SET)->int:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  return await to_thread.run_sync(self._file.seek,position,whence)\n  \n async def tell(self)->int:\n  ''\n\n\n\n\n\n\n\n  \n  return await to_thread.run_sync(self._file.tell)\n  \n  \nclass FileWriteStream(_BaseFileStream,ByteSendStream):\n ''\n\n\n\n\n\n \n \n @classmethod\n async def from_path(\n cls,path:str |PathLike[str],append:bool=False\n )->FileWriteStream:\n  ''\n\n\n\n\n\n\n  \n  mode=\"ab\"if append else \"wb\"\n  file=await to_thread.run_sync(Path(path).open,mode)\n  return cls(cast(BinaryIO,file))\n  \n async def send(self,item:bytes)->None:\n  try:\n   await to_thread.run_sync(self._file.write,item)\n  except ValueError:\n   raise ClosedResourceError from None\n  except OSError as exc:\n   raise BrokenResourceError from exc\n", ["__future__", "anyio", "anyio.abc", "anyio.to_thread", "collections.abc", "io", "os", "pathlib", "typing"]], "anyio.streams.stapled": [".py", "from __future__ import annotations\n\nfrom collections.abc import Callable,Mapping,Sequence\nfrom dataclasses import dataclass\nfrom typing import Any,Generic,TypeVar\n\nfrom..abc import(\nByteReceiveStream,\nByteSendStream,\nByteStream,\nListener,\nObjectReceiveStream,\nObjectSendStream,\nObjectStream,\nTaskGroup,\n)\n\nT_Item=TypeVar(\"T_Item\")\nT_Stream=TypeVar(\"T_Stream\")\n\n\n@dataclass(eq=False)\nclass StapledByteStream(ByteStream):\n ''\n\n\n\n\n\n\n\n \n \n send_stream:ByteSendStream\n receive_stream:ByteReceiveStream\n \n async def receive(self,max_bytes:int=65536)->bytes:\n  return await self.receive_stream.receive(max_bytes)\n  \n async def send(self,item:bytes)->None:\n  await self.send_stream.send(item)\n  \n async def send_eof(self)->None:\n  await self.send_stream.aclose()\n  \n async def aclose(self)->None:\n  await self.send_stream.aclose()\n  await self.receive_stream.aclose()\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return{\n  **self.send_stream.extra_attributes,\n  **self.receive_stream.extra_attributes,\n  }\n  \n  \n@dataclass(eq=False)\nclass StapledObjectStream(Generic[T_Item],ObjectStream[T_Item]):\n ''\n\n\n\n\n\n\n\n \n \n send_stream:ObjectSendStream[T_Item]\n receive_stream:ObjectReceiveStream[T_Item]\n \n async def receive(self)->T_Item:\n  return await self.receive_stream.receive()\n  \n async def send(self,item:T_Item)->None:\n  await self.send_stream.send(item)\n  \n async def send_eof(self)->None:\n  await self.send_stream.aclose()\n  \n async def aclose(self)->None:\n  await self.send_stream.aclose()\n  await self.receive_stream.aclose()\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return{\n  **self.send_stream.extra_attributes,\n  **self.receive_stream.extra_attributes,\n  }\n  \n  \n@dataclass(eq=False)\nclass MultiListener(Generic[T_Stream],Listener[T_Stream]):\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n listeners:Sequence[Listener[T_Stream]]\n \n def __post_init__(self)->None:\n  listeners:list[Listener[T_Stream]]=[]\n  for listener in self.listeners:\n   if isinstance(listener,MultiListener):\n    listeners.extend(listener.listeners)\n    del listener.listeners[:]\n   else:\n    listeners.append(listener)\n    \n  self.listeners=listeners\n  \n async def serve(\n self,handler:Callable[[T_Stream],Any],task_group:TaskGroup |None=None\n )->None:\n  from.. import create_task_group\n  \n  async with create_task_group()as tg:\n   for listener in self.listeners:\n    tg.start_soon(listener.serve,handler,task_group)\n    \n async def aclose(self)->None:\n  for listener in self.listeners:\n   await listener.aclose()\n   \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  attributes:dict={}\n  for listener in self.listeners:\n   attributes.update(listener.extra_attributes)\n   \n  return attributes\n", ["__future__", "anyio", "anyio.abc", "collections.abc", "dataclasses", "typing"]], "anyio.streams.memory": [".py", "from __future__ import annotations\n\nimport warnings\nfrom collections import OrderedDict,deque\nfrom dataclasses import dataclass,field\nfrom types import TracebackType\nfrom typing import Generic,NamedTuple,TypeVar\n\nfrom.. import(\nBrokenResourceError,\nClosedResourceError,\nEndOfStream,\nWouldBlock,\n)\nfrom.._core._testing import TaskInfo,get_current_task\nfrom..abc import Event,ObjectReceiveStream,ObjectSendStream\nfrom..lowlevel import checkpoint\n\nT_Item=TypeVar(\"T_Item\")\nT_co=TypeVar(\"T_co\",covariant=True)\nT_contra=TypeVar(\"T_contra\",contravariant=True)\n\n\nclass MemoryObjectStreamStatistics(NamedTuple):\n current_buffer_used:int\n \n max_buffer_size:float\n open_send_streams:int\n open_receive_streams:int\n \n tasks_waiting_send:int\n \n tasks_waiting_receive:int\n \n \n@dataclass(eq=False)\nclass MemoryObjectItemReceiver(Generic[T_Item]):\n task_info:TaskInfo=field(init=False,default_factory=get_current_task)\n item:T_Item=field(init=False)\n \n def __repr__(self)->str:\n \n \n  item=getattr(self,\"item\",None)\n  return f\"{self.__class__.__name__}(task_info={self.task_info}, item={item !r})\"\n  \n  \n@dataclass(eq=False)\nclass MemoryObjectStreamState(Generic[T_Item]):\n max_buffer_size:float=field()\n buffer:deque[T_Item]=field(init=False,default_factory=deque)\n open_send_channels:int=field(init=False,default=0)\n open_receive_channels:int=field(init=False,default=0)\n waiting_receivers:OrderedDict[Event,MemoryObjectItemReceiver[T_Item]]=field(\n init=False,default_factory=OrderedDict\n )\n waiting_senders:OrderedDict[Event,T_Item]=field(\n init=False,default_factory=OrderedDict\n )\n \n def statistics(self)->MemoryObjectStreamStatistics:\n  return MemoryObjectStreamStatistics(\n  len(self.buffer),\n  self.max_buffer_size,\n  self.open_send_channels,\n  self.open_receive_channels,\n  len(self.waiting_senders),\n  len(self.waiting_receivers),\n  )\n  \n  \n@dataclass(eq=False)\nclass MemoryObjectReceiveStream(Generic[T_co],ObjectReceiveStream[T_co]):\n _state:MemoryObjectStreamState[T_co]\n _closed:bool=field(init=False,default=False)\n \n def __post_init__(self)->None:\n  self._state.open_receive_channels +=1\n  \n def receive_nowait(self)->T_co:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  if self._closed:\n   raise ClosedResourceError\n   \n  if self._state.waiting_senders:\n  \n   send_event,item=self._state.waiting_senders.popitem(last=False)\n   self._state.buffer.append(item)\n   send_event.set()\n   \n  if self._state.buffer:\n   return self._state.buffer.popleft()\n  elif not self._state.open_send_channels:\n   raise EndOfStream\n   \n  raise WouldBlock\n  \n async def receive(self)->T_co:\n  await checkpoint()\n  try:\n   return self.receive_nowait()\n  except WouldBlock:\n  \n   receive_event=Event()\n   receiver=MemoryObjectItemReceiver[T_co]()\n   self._state.waiting_receivers[receive_event]=receiver\n   \n   try:\n    await receive_event.wait()\n   finally:\n    self._state.waiting_receivers.pop(receive_event,None)\n    \n   try:\n    return receiver.item\n   except AttributeError:\n    raise EndOfStream\n    \n def clone(self)->MemoryObjectReceiveStream[T_co]:\n  ''\n\n\n\n\n\n\n\n  \n  if self._closed:\n   raise ClosedResourceError\n   \n  return MemoryObjectReceiveStream(_state=self._state)\n  \n def close(self)->None:\n  ''\n\n\n\n\n\n  \n  if not self._closed:\n   self._closed=True\n   self._state.open_receive_channels -=1\n   if self._state.open_receive_channels ==0:\n    send_events=list(self._state.waiting_senders.keys())\n    for event in send_events:\n     event.set()\n     \n async def aclose(self)->None:\n  self.close()\n  \n def statistics(self)->MemoryObjectStreamStatistics:\n  ''\n\n\n\n  \n  return self._state.statistics()\n  \n def __enter__(self)->MemoryObjectReceiveStream[T_co]:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.close()\n  \n def __del__(self)->None:\n  if not self._closed:\n   warnings.warn(\n   f\"Unclosed <{self.__class__.__name__} at {id(self):x}>\",\n   ResourceWarning,\n   source=self,\n   )\n   \n   \n@dataclass(eq=False)\nclass MemoryObjectSendStream(Generic[T_contra],ObjectSendStream[T_contra]):\n _state:MemoryObjectStreamState[T_contra]\n _closed:bool=field(init=False,default=False)\n \n def __post_init__(self)->None:\n  self._state.open_send_channels +=1\n  \n def send_nowait(self,item:T_contra)->None:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  if self._closed:\n   raise ClosedResourceError\n  if not self._state.open_receive_channels:\n   raise BrokenResourceError\n   \n  while self._state.waiting_receivers:\n   receive_event,receiver=self._state.waiting_receivers.popitem(last=False)\n   if not receiver.task_info.has_pending_cancellation():\n    receiver.item=item\n    receive_event.set()\n    return\n    \n  if len(self._state.buffer)<self._state.max_buffer_size:\n   self._state.buffer.append(item)\n  else:\n   raise WouldBlock\n   \n async def send(self,item:T_contra)->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  await checkpoint()\n  try:\n   self.send_nowait(item)\n  except WouldBlock:\n  \n   send_event=Event()\n   self._state.waiting_senders[send_event]=item\n   try:\n    await send_event.wait()\n   except BaseException:\n    self._state.waiting_senders.pop(send_event,None)\n    raise\n    \n   if send_event in self._state.waiting_senders:\n    del self._state.waiting_senders[send_event]\n    raise BrokenResourceError from None\n    \n def clone(self)->MemoryObjectSendStream[T_contra]:\n  ''\n\n\n\n\n\n\n\n  \n  if self._closed:\n   raise ClosedResourceError\n   \n  return MemoryObjectSendStream(_state=self._state)\n  \n def close(self)->None:\n  ''\n\n\n\n\n\n  \n  if not self._closed:\n   self._closed=True\n   self._state.open_send_channels -=1\n   if self._state.open_send_channels ==0:\n    receive_events=list(self._state.waiting_receivers.keys())\n    self._state.waiting_receivers.clear()\n    for event in receive_events:\n     event.set()\n     \n async def aclose(self)->None:\n  self.close()\n  \n def statistics(self)->MemoryObjectStreamStatistics:\n  ''\n\n\n\n  \n  return self._state.statistics()\n  \n def __enter__(self)->MemoryObjectSendStream[T_contra]:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.close()\n  \n def __del__(self)->None:\n  if not self._closed:\n   warnings.warn(\n   f\"Unclosed <{self.__class__.__name__} at {id(self):x}>\",\n   ResourceWarning,\n   source=self,\n   )\n", ["__future__", "anyio", "anyio._core._testing", "anyio.abc", "anyio.lowlevel", "collections", "dataclasses", "types", "typing", "warnings"]], "anyio.streams.tls": [".py", "from __future__ import annotations\n\nimport logging\nimport re\nimport ssl\nimport sys\nfrom collections.abc import Callable,Mapping\nfrom dataclasses import dataclass\nfrom functools import wraps\nfrom typing import Any,TypeVar\n\nfrom.. import(\nBrokenResourceError,\nEndOfStream,\naclose_forcefully,\nget_cancelled_exc_class,\n)\nfrom.._core._typedattr import TypedAttributeSet,typed_attribute\nfrom..abc import AnyByteStream,ByteStream,Listener,TaskGroup\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nT_Retval=TypeVar(\"T_Retval\")\nPosArgsT=TypeVarTuple(\"PosArgsT\")\n_PCTRTT=tuple[tuple[str,str],...]\n_PCTRTTT=tuple[_PCTRTT,...]\n\n\nclass TLSAttribute(TypedAttributeSet):\n ''\n \n \n alpn_protocol:str |None=typed_attribute()\n \n channel_binding_tls_unique:bytes=typed_attribute()\n \n cipher:tuple[str,str,int]=typed_attribute()\n \n \n peer_certificate:None |(dict[str,str |_PCTRTTT |_PCTRTT])=typed_attribute()\n \n peer_certificate_binary:bytes |None=typed_attribute()\n \n server_side:bool=typed_attribute()\n \n \n shared_ciphers:list[tuple[str,str,int]]|None=typed_attribute()\n \n ssl_object:ssl.SSLObject=typed_attribute()\n \n \n standard_compatible:bool=typed_attribute()\n \n tls_version:str=typed_attribute()\n \n \n@dataclass(eq=False)\nclass TLSStream(ByteStream):\n ''\n\n\n\n\n\n\n\n \n \n transport_stream:AnyByteStream\n standard_compatible:bool\n _ssl_object:ssl.SSLObject\n _read_bio:ssl.MemoryBIO\n _write_bio:ssl.MemoryBIO\n \n @classmethod\n async def wrap(\n cls,\n transport_stream:AnyByteStream,\n *,\n server_side:bool |None=None,\n hostname:str |None=None,\n ssl_context:ssl.SSLContext |None=None,\n standard_compatible:bool=True,\n )->TLSStream:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if server_side is None:\n   server_side=not hostname\n   \n  if not ssl_context:\n   purpose=(\n   ssl.Purpose.CLIENT_AUTH if server_side else ssl.Purpose.SERVER_AUTH\n   )\n   ssl_context=ssl.create_default_context(purpose)\n   \n   \n   if hasattr(ssl,\"OP_IGNORE_UNEXPECTED_EOF\"):\n    ssl_context.options &=~ssl.OP_IGNORE_UNEXPECTED_EOF\n    \n  bio_in=ssl.MemoryBIO()\n  bio_out=ssl.MemoryBIO()\n  ssl_object=ssl_context.wrap_bio(\n  bio_in,bio_out,server_side=server_side,server_hostname=hostname\n  )\n  wrapper=cls(\n  transport_stream=transport_stream,\n  standard_compatible=standard_compatible,\n  _ssl_object=ssl_object,\n  _read_bio=bio_in,\n  _write_bio=bio_out,\n  )\n  await wrapper._call_sslobject_method(ssl_object.do_handshake)\n  return wrapper\n  \n async def _call_sslobject_method(\n self,func:Callable[[Unpack[PosArgsT]],T_Retval],*args:Unpack[PosArgsT]\n )->T_Retval:\n  while True:\n   try:\n    result=func(*args)\n   except ssl.SSLWantReadError:\n    try:\n    \n     if self._write_bio.pending:\n      await self.transport_stream.send(self._write_bio.read())\n      \n     data=await self.transport_stream.receive()\n    except EndOfStream:\n     self._read_bio.write_eof()\n    except OSError as exc:\n     self._read_bio.write_eof()\n     self._write_bio.write_eof()\n     raise BrokenResourceError from exc\n    else:\n     self._read_bio.write(data)\n   except ssl.SSLWantWriteError:\n    await self.transport_stream.send(self._write_bio.read())\n   except ssl.SSLSyscallError as exc:\n    self._read_bio.write_eof()\n    self._write_bio.write_eof()\n    raise BrokenResourceError from exc\n   except ssl.SSLError as exc:\n    self._read_bio.write_eof()\n    self._write_bio.write_eof()\n    if isinstance(exc,ssl.SSLEOFError)or(\n    exc.strerror and \"UNEXPECTED_EOF_WHILE_READING\"in exc.strerror\n    ):\n     if self.standard_compatible:\n      raise BrokenResourceError from exc\n     else:\n      raise EndOfStream from None\n      \n    raise\n   else:\n   \n    if self._write_bio.pending:\n     await self.transport_stream.send(self._write_bio.read())\n     \n    return result\n    \n async def unwrap(self)->tuple[AnyByteStream,bytes]:\n  ''\n\n\n\n\n  \n  await self._call_sslobject_method(self._ssl_object.unwrap)\n  self._read_bio.write_eof()\n  self._write_bio.write_eof()\n  return self.transport_stream,self._read_bio.read()\n  \n async def aclose(self)->None:\n  if self.standard_compatible:\n   try:\n    await self.unwrap()\n   except BaseException:\n    await aclose_forcefully(self.transport_stream)\n    raise\n    \n  await self.transport_stream.aclose()\n  \n async def receive(self,max_bytes:int=65536)->bytes:\n  data=await self._call_sslobject_method(self._ssl_object.read,max_bytes)\n  if not data:\n   raise EndOfStream\n   \n  return data\n  \n async def send(self,item:bytes)->None:\n  await self._call_sslobject_method(self._ssl_object.write,item)\n  \n async def send_eof(self)->None:\n  tls_version=self.extra(TLSAttribute.tls_version)\n  match=re.match(r\"TLSv(\\d+)(?:\\.(\\d+))?\",tls_version)\n  if match:\n   major,minor=int(match.group(1)),int(match.group(2)or 0)\n   if(major,minor)<(1,3):\n    raise NotImplementedError(\n    f\"send_eof() requires at least TLSv1.3; current \"\n    f\"session uses {tls_version}\"\n    )\n    \n  raise NotImplementedError(\n  \"send_eof() has not yet been implemented for TLS streams\"\n  )\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return{\n  **self.transport_stream.extra_attributes,\n  TLSAttribute.alpn_protocol:self._ssl_object.selected_alpn_protocol,\n  TLSAttribute.channel_binding_tls_unique:(\n  self._ssl_object.get_channel_binding\n  ),\n  TLSAttribute.cipher:self._ssl_object.cipher,\n  TLSAttribute.peer_certificate:lambda:self._ssl_object.getpeercert(False),\n  TLSAttribute.peer_certificate_binary:lambda:self._ssl_object.getpeercert(\n  True\n  ),\n  TLSAttribute.server_side:lambda:self._ssl_object.server_side,\n  TLSAttribute.shared_ciphers:lambda:self._ssl_object.shared_ciphers()\n  if self._ssl_object.server_side\n  else None,\n  TLSAttribute.standard_compatible:lambda:self.standard_compatible,\n  TLSAttribute.ssl_object:lambda:self._ssl_object,\n  TLSAttribute.tls_version:self._ssl_object.version,\n  }\n  \n  \n@dataclass(eq=False)\nclass TLSListener(Listener[TLSStream]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n listener:Listener[Any]\n ssl_context:ssl.SSLContext\n standard_compatible:bool=True\n handshake_timeout:float=30\n \n @staticmethod\n async def handle_handshake_error(exc:BaseException,stream:AnyByteStream)->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  await aclose_forcefully(stream)\n  \n  \n  if not isinstance(exc,get_cancelled_exc_class()):\n  \n  \n  \n  \n   logging.getLogger(__name__).exception(\n   \"Error during TLS handshake\",exc_info=exc\n   )\n   \n   \n  if not isinstance(exc,Exception)or isinstance(exc,get_cancelled_exc_class()):\n   raise\n   \n async def serve(\n self,\n handler:Callable[[TLSStream],Any],\n task_group:TaskGroup |None=None,\n )->None:\n  @wraps(handler)\n  async def handler_wrapper(stream:AnyByteStream)->None:\n   from.. import fail_after\n   \n   try:\n    with fail_after(self.handshake_timeout):\n     wrapped_stream=await TLSStream.wrap(\n     stream,\n     ssl_context=self.ssl_context,\n     standard_compatible=self.standard_compatible,\n     )\n   except BaseException as exc:\n    await self.handle_handshake_error(exc,stream)\n   else:\n    await handler(wrapped_stream)\n    \n  await self.listener.serve(handler_wrapper,task_group)\n  \n async def aclose(self)->None:\n  await self.listener.aclose()\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return{\n  TLSAttribute.standard_compatible:lambda:self.standard_compatible,\n  }\n", ["__future__", "anyio", "anyio._core._typedattr", "anyio.abc", "collections.abc", "dataclasses", "functools", "logging", "re", "ssl", "sys", "typing", "typing_extensions"]], "anyio.streams.text": [".py", "from __future__ import annotations\n\nimport codecs\nfrom collections.abc import Callable,Mapping\nfrom dataclasses import InitVar,dataclass,field\nfrom typing import Any\n\nfrom..abc import(\nAnyByteReceiveStream,\nAnyByteSendStream,\nAnyByteStream,\nObjectReceiveStream,\nObjectSendStream,\nObjectStream,\n)\n\n\n@dataclass(eq=False)\nclass TextReceiveStream(ObjectReceiveStream[str]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n transport_stream:AnyByteReceiveStream\n encoding:InitVar[str]=\"utf-8\"\n errors:InitVar[str]=\"strict\"\n _decoder:codecs.IncrementalDecoder=field(init=False)\n \n def __post_init__(self,encoding:str,errors:str)->None:\n  decoder_class=codecs.getincrementaldecoder(encoding)\n  self._decoder=decoder_class(errors=errors)\n  \n async def receive(self)->str:\n  while True:\n   chunk=await self.transport_stream.receive()\n   decoded=self._decoder.decode(chunk)\n   if decoded:\n    return decoded\n    \n async def aclose(self)->None:\n  await self.transport_stream.aclose()\n  self._decoder.reset()\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return self.transport_stream.extra_attributes\n  \n  \n@dataclass(eq=False)\nclass TextSendStream(ObjectSendStream[str]):\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n transport_stream:AnyByteSendStream\n encoding:InitVar[str]=\"utf-8\"\n errors:str=\"strict\"\n _encoder:Callable[...,tuple[bytes,int]]=field(init=False)\n \n def __post_init__(self,encoding:str)->None:\n  self._encoder=codecs.getencoder(encoding)\n  \n async def send(self,item:str)->None:\n  encoded=self._encoder(item,self.errors)[0]\n  await self.transport_stream.send(encoded)\n  \n async def aclose(self)->None:\n  await self.transport_stream.aclose()\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return self.transport_stream.extra_attributes\n  \n  \n@dataclass(eq=False)\nclass TextStream(ObjectStream[str]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n transport_stream:AnyByteStream\n encoding:InitVar[str]=\"utf-8\"\n errors:InitVar[str]=\"strict\"\n _receive_stream:TextReceiveStream=field(init=False)\n _send_stream:TextSendStream=field(init=False)\n \n def __post_init__(self,encoding:str,errors:str)->None:\n  self._receive_stream=TextReceiveStream(\n  self.transport_stream,encoding=encoding,errors=errors\n  )\n  self._send_stream=TextSendStream(\n  self.transport_stream,encoding=encoding,errors=errors\n  )\n  \n async def receive(self)->str:\n  return await self._receive_stream.receive()\n  \n async def send(self,item:str)->None:\n  await self._send_stream.send(item)\n  \n async def send_eof(self)->None:\n  await self.transport_stream.send_eof()\n  \n async def aclose(self)->None:\n  await self._send_stream.aclose()\n  await self._receive_stream.aclose()\n  \n @property\n def extra_attributes(self)->Mapping[Any,Callable[[],Any]]:\n  return{\n  **self._send_stream.extra_attributes,\n  **self._receive_stream.extra_attributes,\n  }\n", ["__future__", "anyio.abc", "codecs", "collections.abc", "dataclasses", "typing"]], "anyio._core._fileio": [".py", "from __future__ import annotations\n\nimport os\nimport pathlib\nimport sys\nfrom collections.abc import AsyncIterator,Callable,Iterable,Iterator,Sequence\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom os import PathLike\nfrom typing import(\nIO,\nTYPE_CHECKING,\nAny,\nAnyStr,\nFinal,\nGeneric,\noverload,\n)\n\nfrom.. import to_thread\nfrom..abc import AsyncResource\n\nif TYPE_CHECKING:\n from _typeshed import OpenBinaryMode,OpenTextMode,ReadableBuffer,WriteableBuffer\nelse:\n ReadableBuffer=OpenBinaryMode=OpenTextMode=WriteableBuffer=object\n \n \nclass AsyncFile(AsyncResource,Generic[AnyStr]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,fp:IO[AnyStr])->None:\n  self._fp:Any=fp\n  \n def __getattr__(self,name:str)->object:\n  return getattr(self._fp,name)\n  \n @property\n def wrapped(self)->IO[AnyStr]:\n  ''\n  return self._fp\n  \n async def __aiter__(self)->AsyncIterator[AnyStr]:\n  while True:\n   line=await self.readline()\n   if line:\n    yield line\n   else:\n    break\n    \n async def aclose(self)->None:\n  return await to_thread.run_sync(self._fp.close)\n  \n async def read(self,size:int=-1)->AnyStr:\n  return await to_thread.run_sync(self._fp.read,size)\n  \n async def read1(self:AsyncFile[bytes],size:int=-1)->bytes:\n  return await to_thread.run_sync(self._fp.read1,size)\n  \n async def readline(self)->AnyStr:\n  return await to_thread.run_sync(self._fp.readline)\n  \n async def readlines(self)->list[AnyStr]:\n  return await to_thread.run_sync(self._fp.readlines)\n  \n async def readinto(self:AsyncFile[bytes],b:WriteableBuffer)->int:\n  return await to_thread.run_sync(self._fp.readinto,b)\n  \n async def readinto1(self:AsyncFile[bytes],b:WriteableBuffer)->int:\n  return await to_thread.run_sync(self._fp.readinto1,b)\n  \n @overload\n async def write(self:AsyncFile[bytes],b:ReadableBuffer)->int:...\n \n @overload\n async def write(self:AsyncFile[str],b:str)->int:...\n \n async def write(self,b:ReadableBuffer |str)->int:\n  return await to_thread.run_sync(self._fp.write,b)\n  \n @overload\n async def writelines(\n self:AsyncFile[bytes],lines:Iterable[ReadableBuffer]\n )->None:...\n \n @overload\n async def writelines(self:AsyncFile[str],lines:Iterable[str])->None:...\n \n async def writelines(self,lines:Iterable[ReadableBuffer]|Iterable[str])->None:\n  return await to_thread.run_sync(self._fp.writelines,lines)\n  \n async def truncate(self,size:int |None=None)->int:\n  return await to_thread.run_sync(self._fp.truncate,size)\n  \n async def seek(self,offset:int,whence:int |None=os.SEEK_SET)->int:\n  return await to_thread.run_sync(self._fp.seek,offset,whence)\n  \n async def tell(self)->int:\n  return await to_thread.run_sync(self._fp.tell)\n  \n async def flush(self)->None:\n  return await to_thread.run_sync(self._fp.flush)\n  \n  \n@overload\nasync def open_file(\nfile:str |PathLike[str]|int,\nmode:OpenBinaryMode,\nbuffering:int=...,\nencoding:str |None=...,\nerrors:str |None=...,\nnewline:str |None=...,\nclosefd:bool=...,\nopener:Callable[[str,int],int]|None=...,\n)->AsyncFile[bytes]:...\n\n\n@overload\nasync def open_file(\nfile:str |PathLike[str]|int,\nmode:OpenTextMode=...,\nbuffering:int=...,\nencoding:str |None=...,\nerrors:str |None=...,\nnewline:str |None=...,\nclosefd:bool=...,\nopener:Callable[[str,int],int]|None=...,\n)->AsyncFile[str]:...\n\n\nasync def open_file(\nfile:str |PathLike[str]|int,\nmode:str=\"r\",\nbuffering:int=-1,\nencoding:str |None=None,\nerrors:str |None=None,\nnewline:str |None=None,\nclosefd:bool=True,\nopener:Callable[[str,int],int]|None=None,\n)->AsyncFile[Any]:\n ''\n\n\n\n\n\n\n \n fp=await to_thread.run_sync(\n open,file,mode,buffering,encoding,errors,newline,closefd,opener\n )\n return AsyncFile(fp)\n \n \ndef wrap_file(file:IO[AnyStr])->AsyncFile[AnyStr]:\n ''\n\n\n\n\n\n \n return AsyncFile(file)\n \n \n@dataclass(eq=False)\nclass _PathIterator(AsyncIterator[\"Path\"]):\n iterator:Iterator[PathLike[str]]\n \n async def __anext__(self)->Path:\n  nextval=await to_thread.run_sync(\n  next,self.iterator,None,abandon_on_cancel=True\n  )\n  if nextval is None:\n   raise StopAsyncIteration from None\n   \n  return Path(nextval)\n  \n  \nclass Path:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__=\"_path\",\"__weakref__\"\n \n __weakref__:Any\n \n def __init__(self,*args:str |PathLike[str])->None:\n  self._path:Final[pathlib.Path]=pathlib.Path(*args)\n  \n def __fspath__(self)->str:\n  return self._path.__fspath__()\n  \n def __str__(self)->str:\n  return self._path.__str__()\n  \n def __repr__(self)->str:\n  return f\"{self.__class__.__name__}({self.as_posix()!r})\"\n  \n def __bytes__(self)->bytes:\n  return self._path.__bytes__()\n  \n def __hash__(self)->int:\n  return self._path.__hash__()\n  \n def __eq__(self,other:object)->bool:\n  target=other._path if isinstance(other,Path)else other\n  return self._path.__eq__(target)\n  \n def __lt__(self,other:pathlib.PurePath |Path)->bool:\n  target=other._path if isinstance(other,Path)else other\n  return self._path.__lt__(target)\n  \n def __le__(self,other:pathlib.PurePath |Path)->bool:\n  target=other._path if isinstance(other,Path)else other\n  return self._path.__le__(target)\n  \n def __gt__(self,other:pathlib.PurePath |Path)->bool:\n  target=other._path if isinstance(other,Path)else other\n  return self._path.__gt__(target)\n  \n def __ge__(self,other:pathlib.PurePath |Path)->bool:\n  target=other._path if isinstance(other,Path)else other\n  return self._path.__ge__(target)\n  \n def __truediv__(self,other:str |PathLike[str])->Path:\n  return Path(self._path /other)\n  \n def __rtruediv__(self,other:str |PathLike[str])->Path:\n  return Path(other)/self\n  \n @property\n def parts(self)->tuple[str,...]:\n  return self._path.parts\n  \n @property\n def drive(self)->str:\n  return self._path.drive\n  \n @property\n def root(self)->str:\n  return self._path.root\n  \n @property\n def anchor(self)->str:\n  return self._path.anchor\n  \n @property\n def parents(self)->Sequence[Path]:\n  return tuple(Path(p)for p in self._path.parents)\n  \n @property\n def parent(self)->Path:\n  return Path(self._path.parent)\n  \n @property\n def name(self)->str:\n  return self._path.name\n  \n @property\n def suffix(self)->str:\n  return self._path.suffix\n  \n @property\n def suffixes(self)->list[str]:\n  return self._path.suffixes\n  \n @property\n def stem(self)->str:\n  return self._path.stem\n  \n async def absolute(self)->Path:\n  path=await to_thread.run_sync(self._path.absolute)\n  return Path(path)\n  \n def as_posix(self)->str:\n  return self._path.as_posix()\n  \n def as_uri(self)->str:\n  return self._path.as_uri()\n  \n if sys.version_info >=(3,13):\n  parser=pathlib.Path.parser\n  \n  @classmethod\n  def from_uri(cls,uri:str)->Path:\n   return Path(pathlib.Path.from_uri(uri))\n   \n  def full_match(\n  self,path_pattern:str,*,case_sensitive:bool |None=None\n  )->bool:\n   return self._path.full_match(path_pattern,case_sensitive=case_sensitive)\n   \n  def match(\n  self,path_pattern:str,*,case_sensitive:bool |None=None\n  )->bool:\n   return self._path.match(path_pattern,case_sensitive=case_sensitive)\n else:\n \n  def match(self,path_pattern:str)->bool:\n   return self._path.match(path_pattern)\n   \n def is_relative_to(self,other:str |PathLike[str])->bool:\n  try:\n   self.relative_to(other)\n   return True\n  except ValueError:\n   return False\n   \n async def chmod(self,mode:int,*,follow_symlinks:bool=True)->None:\n  func=partial(os.chmod,follow_symlinks=follow_symlinks)\n  return await to_thread.run_sync(func,self._path,mode)\n  \n @classmethod\n async def cwd(cls)->Path:\n  path=await to_thread.run_sync(pathlib.Path.cwd)\n  return cls(path)\n  \n async def exists(self)->bool:\n  return await to_thread.run_sync(self._path.exists,abandon_on_cancel=True)\n  \n async def expanduser(self)->Path:\n  return Path(\n  await to_thread.run_sync(self._path.expanduser,abandon_on_cancel=True)\n  )\n  \n def glob(self,pattern:str)->AsyncIterator[Path]:\n  gen=self._path.glob(pattern)\n  return _PathIterator(gen)\n  \n async def group(self)->str:\n  return await to_thread.run_sync(self._path.group,abandon_on_cancel=True)\n  \n async def hardlink_to(\n self,target:str |bytes |PathLike[str]|PathLike[bytes]\n )->None:\n  if isinstance(target,Path):\n   target=target._path\n   \n  await to_thread.run_sync(os.link,target,self)\n  \n @classmethod\n async def home(cls)->Path:\n  home_path=await to_thread.run_sync(pathlib.Path.home)\n  return cls(home_path)\n  \n def is_absolute(self)->bool:\n  return self._path.is_absolute()\n  \n async def is_block_device(self)->bool:\n  return await to_thread.run_sync(\n  self._path.is_block_device,abandon_on_cancel=True\n  )\n  \n async def is_char_device(self)->bool:\n  return await to_thread.run_sync(\n  self._path.is_char_device,abandon_on_cancel=True\n  )\n  \n async def is_dir(self)->bool:\n  return await to_thread.run_sync(self._path.is_dir,abandon_on_cancel=True)\n  \n async def is_fifo(self)->bool:\n  return await to_thread.run_sync(self._path.is_fifo,abandon_on_cancel=True)\n  \n async def is_file(self)->bool:\n  return await to_thread.run_sync(self._path.is_file,abandon_on_cancel=True)\n  \n if sys.version_info >=(3,12):\n \n  async def is_junction(self)->bool:\n   return await to_thread.run_sync(self._path.is_junction)\n   \n async def is_mount(self)->bool:\n  return await to_thread.run_sync(\n  os.path.ismount,self._path,abandon_on_cancel=True\n  )\n  \n def is_reserved(self)->bool:\n  return self._path.is_reserved()\n  \n async def is_socket(self)->bool:\n  return await to_thread.run_sync(self._path.is_socket,abandon_on_cancel=True)\n  \n async def is_symlink(self)->bool:\n  return await to_thread.run_sync(self._path.is_symlink,abandon_on_cancel=True)\n  \n def iterdir(self)->AsyncIterator[Path]:\n  gen=self._path.iterdir()\n  return _PathIterator(gen)\n  \n def joinpath(self,*args:str |PathLike[str])->Path:\n  return Path(self._path.joinpath(*args))\n  \n async def lchmod(self,mode:int)->None:\n  await to_thread.run_sync(self._path.lchmod,mode)\n  \n async def lstat(self)->os.stat_result:\n  return await to_thread.run_sync(self._path.lstat,abandon_on_cancel=True)\n  \n async def mkdir(\n self,mode:int=0o777,parents:bool=False,exist_ok:bool=False\n )->None:\n  await to_thread.run_sync(self._path.mkdir,mode,parents,exist_ok)\n  \n @overload\n async def open(\n self,\n mode:OpenBinaryMode,\n buffering:int=...,\n encoding:str |None=...,\n errors:str |None=...,\n newline:str |None=...,\n )->AsyncFile[bytes]:...\n \n @overload\n async def open(\n self,\n mode:OpenTextMode=...,\n buffering:int=...,\n encoding:str |None=...,\n errors:str |None=...,\n newline:str |None=...,\n )->AsyncFile[str]:...\n \n async def open(\n self,\n mode:str=\"r\",\n buffering:int=-1,\n encoding:str |None=None,\n errors:str |None=None,\n newline:str |None=None,\n )->AsyncFile[Any]:\n  fp=await to_thread.run_sync(\n  self._path.open,mode,buffering,encoding,errors,newline\n  )\n  return AsyncFile(fp)\n  \n async def owner(self)->str:\n  return await to_thread.run_sync(self._path.owner,abandon_on_cancel=True)\n  \n async def read_bytes(self)->bytes:\n  return await to_thread.run_sync(self._path.read_bytes)\n  \n async def read_text(\n self,encoding:str |None=None,errors:str |None=None\n )->str:\n  return await to_thread.run_sync(self._path.read_text,encoding,errors)\n  \n if sys.version_info >=(3,12):\n \n  def relative_to(\n  self,*other:str |PathLike[str],walk_up:bool=False\n  )->Path:\n   return Path(self._path.relative_to(*other,walk_up=walk_up))\n   \n else:\n \n  def relative_to(self,*other:str |PathLike[str])->Path:\n   return Path(self._path.relative_to(*other))\n   \n async def readlink(self)->Path:\n  target=await to_thread.run_sync(os.readlink,self._path)\n  return Path(target)\n  \n async def rename(self,target:str |pathlib.PurePath |Path)->Path:\n  if isinstance(target,Path):\n   target=target._path\n   \n  await to_thread.run_sync(self._path.rename,target)\n  return Path(target)\n  \n async def replace(self,target:str |pathlib.PurePath |Path)->Path:\n  if isinstance(target,Path):\n   target=target._path\n   \n  await to_thread.run_sync(self._path.replace,target)\n  return Path(target)\n  \n async def resolve(self,strict:bool=False)->Path:\n  func=partial(self._path.resolve,strict=strict)\n  return Path(await to_thread.run_sync(func,abandon_on_cancel=True))\n  \n def rglob(self,pattern:str)->AsyncIterator[Path]:\n  gen=self._path.rglob(pattern)\n  return _PathIterator(gen)\n  \n async def rmdir(self)->None:\n  await to_thread.run_sync(self._path.rmdir)\n  \n async def samefile(self,other_path:str |PathLike[str])->bool:\n  if isinstance(other_path,Path):\n   other_path=other_path._path\n   \n  return await to_thread.run_sync(\n  self._path.samefile,other_path,abandon_on_cancel=True\n  )\n  \n async def stat(self,*,follow_symlinks:bool=True)->os.stat_result:\n  func=partial(os.stat,follow_symlinks=follow_symlinks)\n  return await to_thread.run_sync(func,self._path,abandon_on_cancel=True)\n  \n async def symlink_to(\n self,\n target:str |bytes |PathLike[str]|PathLike[bytes],\n target_is_directory:bool=False,\n )->None:\n  if isinstance(target,Path):\n   target=target._path\n   \n  await to_thread.run_sync(self._path.symlink_to,target,target_is_directory)\n  \n async def touch(self,mode:int=0o666,exist_ok:bool=True)->None:\n  await to_thread.run_sync(self._path.touch,mode,exist_ok)\n  \n async def unlink(self,missing_ok:bool=False)->None:\n  try:\n   await to_thread.run_sync(self._path.unlink)\n  except FileNotFoundError:\n   if not missing_ok:\n    raise\n    \n if sys.version_info >=(3,12):\n \n  async def walk(\n  self,\n  top_down:bool=True,\n  on_error:Callable[[OSError],object]|None=None,\n  follow_symlinks:bool=False,\n  )->AsyncIterator[tuple[Path,list[str],list[str]]]:\n   def get_next_value()->tuple[pathlib.Path,list[str],list[str]]|None:\n    try:\n     return next(gen)\n    except StopIteration:\n     return None\n     \n   gen=self._path.walk(top_down,on_error,follow_symlinks)\n   while True:\n    value=await to_thread.run_sync(get_next_value)\n    if value is None:\n     return\n     \n    root,dirs,paths=value\n    yield Path(root),dirs,paths\n    \n def with_name(self,name:str)->Path:\n  return Path(self._path.with_name(name))\n  \n def with_stem(self,stem:str)->Path:\n  return Path(self._path.with_name(stem+self._path.suffix))\n  \n def with_suffix(self,suffix:str)->Path:\n  return Path(self._path.with_suffix(suffix))\n  \n def with_segments(self,*pathsegments:str |PathLike[str])->Path:\n  return Path(*pathsegments)\n  \n async def write_bytes(self,data:bytes)->int:\n  return await to_thread.run_sync(self._path.write_bytes,data)\n  \n async def write_text(\n self,\n data:str,\n encoding:str |None=None,\n errors:str |None=None,\n newline:str |None=None,\n )->int:\n \n  def sync_write_text()->int:\n   with self._path.open(\n   \"w\",encoding=encoding,errors=errors,newline=newline\n   )as fp:\n    return fp.write(data)\n    \n  return await to_thread.run_sync(sync_write_text)\n  \n  \nPathLike.register(Path)\n", ["__future__", "_typeshed", "anyio", "anyio.abc", "anyio.to_thread", "collections.abc", "dataclasses", "functools", "os", "pathlib", "sys", "typing"]], "anyio._core": [".py", "", [], 1], "anyio._core._tasks": [".py", "from __future__ import annotations\n\nimport math\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom types import TracebackType\n\nfrom..abc._tasks import TaskGroup,TaskStatus\nfrom._eventloop import get_async_backend\n\n\nclass _IgnoredTaskStatus(TaskStatus[object]):\n def started(self,value:object=None)->None:\n  pass\n  \n  \nTASK_STATUS_IGNORED=_IgnoredTaskStatus()\n\n\nclass CancelScope:\n ''\n\n\n\n\n \n \n def __new__(\n cls,*,deadline:float=math.inf,shield:bool=False\n )->CancelScope:\n  return get_async_backend().create_cancel_scope(shield=shield,deadline=deadline)\n  \n def cancel(self)->None:\n  ''\n  raise NotImplementedError\n  \n @property\n def deadline(self)->float:\n  ''\n\n\n\n\n  \n  raise NotImplementedError\n  \n @deadline.setter\n def deadline(self,value:float)->None:\n  raise NotImplementedError\n  \n @property\n def cancel_called(self)->bool:\n  ''\n  raise NotImplementedError\n  \n @property\n def cancelled_caught(self)->bool:\n  ''\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n @property\n def shield(self)->bool:\n  ''\n\n\n\n\n  \n  raise NotImplementedError\n  \n @shield.setter\n def shield(self,value:bool)->None:\n  raise NotImplementedError\n  \n def __enter__(self)->CancelScope:\n  raise NotImplementedError\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  raise NotImplementedError\n  \n  \n@contextmanager\ndef fail_after(\ndelay:float |None,shield:bool=False\n)->Generator[CancelScope,None,None]:\n ''\n\n\n\n\n\n\n\n\n\n \n current_time=get_async_backend().current_time\n deadline=(current_time()+delay)if delay is not None else math.inf\n with get_async_backend().create_cancel_scope(\n deadline=deadline,shield=shield\n )as cancel_scope:\n  yield cancel_scope\n  \n if cancel_scope.cancelled_caught and current_time()>=cancel_scope.deadline:\n  raise TimeoutError\n  \n  \ndef move_on_after(delay:float |None,shield:bool=False)->CancelScope:\n ''\n\n\n\n\n\n\n\n \n deadline=(\n (get_async_backend().current_time()+delay)if delay is not None else math.inf\n )\n return get_async_backend().create_cancel_scope(deadline=deadline,shield=shield)\n \n \ndef current_effective_deadline()->float:\n ''\n\n\n\n\n\n\n\n\n \n return get_async_backend().current_effective_deadline()\n \n \ndef create_task_group()->TaskGroup:\n ''\n\n\n\n\n \n return get_async_backend().create_task_group()\n", ["__future__", "anyio._core._eventloop", "anyio.abc._tasks", "collections.abc", "contextlib", "math", "types"]], "anyio._core._subprocesses": [".py", "from __future__ import annotations\n\nimport sys\nfrom collections.abc import AsyncIterable,Iterable,Mapping,Sequence\nfrom io import BytesIO\nfrom os import PathLike\nfrom subprocess import DEVNULL,PIPE,CalledProcessError,CompletedProcess\nfrom typing import IO,Any,Union,cast\n\nfrom..abc import Process\nfrom._eventloop import get_async_backend\nfrom._tasks import create_task_group\n\nif sys.version_info >=(3,10):\n from typing import TypeAlias\nelse:\n from typing_extensions import TypeAlias\n \nStrOrBytesPath:TypeAlias=Union[str,bytes,\"PathLike[str]\",\"PathLike[bytes]\"]\n\n\nasync def run_process(\ncommand:StrOrBytesPath |Sequence[StrOrBytesPath],\n*,\ninput:bytes |None=None,\nstdout:int |IO[Any]|None=PIPE,\nstderr:int |IO[Any]|None=PIPE,\ncheck:bool=True,\ncwd:StrOrBytesPath |None=None,\nenv:Mapping[str,str]|None=None,\nstartupinfo:Any=None,\ncreationflags:int=0,\nstart_new_session:bool=False,\npass_fds:Sequence[int]=(),\nuser:str |int |None=None,\ngroup:str |int |None=None,\nextra_groups:Iterable[str |int]|None=None,\numask:int=-1,\n)->CompletedProcess[bytes]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n async def drain_stream(stream:AsyncIterable[bytes],index:int)->None:\n  buffer=BytesIO()\n  async for chunk in stream:\n   buffer.write(chunk)\n   \n  stream_contents[index]=buffer.getvalue()\n  \n async with await open_process(\n command,\n stdin=PIPE if input else DEVNULL,\n stdout=stdout,\n stderr=stderr,\n cwd=cwd,\n env=env,\n startupinfo=startupinfo,\n creationflags=creationflags,\n start_new_session=start_new_session,\n pass_fds=pass_fds,\n user=user,\n group=group,\n extra_groups=extra_groups,\n umask=umask,\n )as process:\n  stream_contents:list[bytes |None]=[None,None]\n  async with create_task_group()as tg:\n   if process.stdout:\n    tg.start_soon(drain_stream,process.stdout,0)\n    \n   if process.stderr:\n    tg.start_soon(drain_stream,process.stderr,1)\n    \n   if process.stdin and input:\n    await process.stdin.send(input)\n    await process.stdin.aclose()\n    \n   await process.wait()\n   \n output,errors=stream_contents\n if check and process.returncode !=0:\n  raise CalledProcessError(cast(int,process.returncode),command,output,errors)\n  \n return CompletedProcess(command,cast(int,process.returncode),output,errors)\n \n \nasync def open_process(\ncommand:StrOrBytesPath |Sequence[StrOrBytesPath],\n*,\nstdin:int |IO[Any]|None=PIPE,\nstdout:int |IO[Any]|None=PIPE,\nstderr:int |IO[Any]|None=PIPE,\ncwd:StrOrBytesPath |None=None,\nenv:Mapping[str,str]|None=None,\nstartupinfo:Any=None,\ncreationflags:int=0,\nstart_new_session:bool=False,\npass_fds:Sequence[int]=(),\nuser:str |int |None=None,\ngroup:str |int |None=None,\nextra_groups:Iterable[str |int]|None=None,\numask:int=-1,\n)->Process:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n kwargs:dict[str,Any]={}\n if user is not None:\n  kwargs[\"user\"]=user\n  \n if group is not None:\n  kwargs[\"group\"]=group\n  \n if extra_groups is not None:\n  kwargs[\"extra_groups\"]=group\n  \n if umask >=0:\n  kwargs[\"umask\"]=umask\n  \n return await get_async_backend().open_process(\n command,\n stdin=stdin,\n stdout=stdout,\n stderr=stderr,\n cwd=cwd,\n env=env,\n startupinfo=startupinfo,\n creationflags=creationflags,\n start_new_session=start_new_session,\n pass_fds=pass_fds,\n **kwargs,\n )\n", ["__future__", "anyio._core._eventloop", "anyio._core._tasks", "anyio.abc", "collections.abc", "io", "os", "subprocess", "sys", "typing", "typing_extensions"]], "anyio._core._eventloop": [".py", "from __future__ import annotations\n\nimport math\nimport sys\nimport threading\nfrom collections.abc import Awaitable,Callable,Generator\nfrom contextlib import contextmanager\nfrom importlib import import_module\nfrom typing import TYPE_CHECKING,Any,TypeVar\n\nimport sniffio\n\nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from typing_extensions import TypeVarTuple,Unpack\n \nif TYPE_CHECKING:\n from..abc import AsyncBackend\n \n \nBACKENDS=\"asyncio\",\"trio\"\n\nT_Retval=TypeVar(\"T_Retval\")\nPosArgsT=TypeVarTuple(\"PosArgsT\")\n\nthreadlocals=threading.local()\nloaded_backends:dict[str,type[AsyncBackend]]={}\n\n\ndef run(\nfunc:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n*args:Unpack[PosArgsT],\nbackend:str=\"asyncio\",\nbackend_options:dict[str,Any]|None=None,\n)->T_Retval:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n try:\n  asynclib_name=sniffio.current_async_library()\n except sniffio.AsyncLibraryNotFoundError:\n  pass\n else:\n  raise RuntimeError(f\"Already running {asynclib_name} in this thread\")\n  \n try:\n  async_backend=get_async_backend(backend)\n except ImportError as exc:\n  raise LookupError(f\"No such backend: {backend}\")from exc\n  \n token=None\n if sniffio.current_async_library_cvar.get(None)is None:\n \n \n  token=sniffio.current_async_library_cvar.set(backend)\n  \n try:\n  backend_options=backend_options or{}\n  return async_backend.run(func,args,{},backend_options)\n finally:\n  if token:\n   sniffio.current_async_library_cvar.reset(token)\n   \n   \nasync def sleep(delay:float)->None:\n ''\n\n\n\n\n \n return await get_async_backend().sleep(delay)\n \n \nasync def sleep_forever()->None:\n ''\n\n\n\n\n\n\n \n await sleep(math.inf)\n \n \nasync def sleep_until(deadline:float)->None:\n ''\n\n\n\n\n\n\n\n \n now=current_time()\n await sleep(max(deadline -now,0))\n \n \ndef current_time()->float:\n ''\n\n\n\n\n \n return get_async_backend().current_time()\n \n \ndef get_all_backends()->tuple[str,...]:\n ''\n return BACKENDS\n \n \ndef get_cancelled_exc_class()->type[BaseException]:\n ''\n return get_async_backend().cancelled_exception_class()\n \n \n \n \n \n \n \n@contextmanager\ndef claim_worker_thread(\nbackend_class:type[AsyncBackend],token:object\n)->Generator[Any,None,None]:\n threadlocals.current_async_backend=backend_class\n threadlocals.current_token=token\n try:\n  yield\n finally:\n  del threadlocals.current_async_backend\n  del threadlocals.current_token\n  \n  \ndef get_async_backend(asynclib_name:str |None=None)->type[AsyncBackend]:\n if asynclib_name is None:\n  asynclib_name=sniffio.current_async_library()\n  \n  \n  \n  \n try:\n  return loaded_backends[asynclib_name]\n except KeyError:\n  module=import_module(f\"anyio._backends._{asynclib_name}\")\n  loaded_backends[asynclib_name]=module.backend_class\n  return module.backend_class\n", ["__future__", "anyio.abc", "collections.abc", "contextlib", "importlib", "math", "sniffio", "sys", "threading", "typing", "typing_extensions"]], "anyio._core._streams": [".py", "from __future__ import annotations\n\nimport math\nfrom typing import TypeVar\nfrom warnings import warn\n\nfrom..streams.memory import(\nMemoryObjectReceiveStream,\nMemoryObjectSendStream,\nMemoryObjectStreamState,\n)\n\nT_Item=TypeVar(\"T_Item\")\n\n\nclass create_memory_object_stream(\ntuple[MemoryObjectSendStream[T_Item],MemoryObjectReceiveStream[T_Item]],\n):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __new__(\n cls,max_buffer_size:float=0,item_type:object=None\n )->tuple[MemoryObjectSendStream[T_Item],MemoryObjectReceiveStream[T_Item]]:\n  if max_buffer_size !=math.inf and not isinstance(max_buffer_size,int):\n   raise ValueError(\"max_buffer_size must be either an integer or math.inf\")\n  if max_buffer_size <0:\n   raise ValueError(\"max_buffer_size cannot be negative\")\n  if item_type is not None:\n   warn(\n   \"The item_type argument has been deprecated in AnyIO 4.0. \"\n   \"Use create_memory_object_stream[YourItemType](...) instead.\",\n   DeprecationWarning,\n   stacklevel=2,\n   )\n   \n  state=MemoryObjectStreamState[T_Item](max_buffer_size)\n  return(MemoryObjectSendStream(state),MemoryObjectReceiveStream(state))\n", ["__future__", "anyio.streams.memory", "math", "typing", "warnings"]], "anyio._core._sockets": [".py", "from __future__ import annotations\n\nimport errno\nimport os\nimport socket\nimport ssl\nimport stat\nimport sys\nfrom collections.abc import Awaitable\nfrom ipaddress import IPv6Address,ip_address\nfrom os import PathLike,chmod\nfrom socket import AddressFamily,SocketKind\nfrom typing import TYPE_CHECKING,Any,Literal,cast,overload\n\nfrom.. import to_thread\nfrom..abc import(\nConnectedUDPSocket,\nConnectedUNIXDatagramSocket,\nIPAddressType,\nIPSockAddrType,\nSocketListener,\nSocketStream,\nUDPSocket,\nUNIXDatagramSocket,\nUNIXSocketStream,\n)\nfrom..streams.stapled import MultiListener\nfrom..streams.tls import TLSStream\nfrom._eventloop import get_async_backend\nfrom._resources import aclose_forcefully\nfrom._synchronization import Event\nfrom._tasks import create_task_group,move_on_after\n\nif TYPE_CHECKING:\n from _typeshed import FileDescriptorLike\nelse:\n FileDescriptorLike=object\n \nif sys.version_info <(3,11):\n from exceptiongroup import ExceptionGroup\n \nif sys.version_info <(3,13):\n from typing_extensions import deprecated\nelse:\n from warnings import deprecated\n \nIPPROTO_IPV6=getattr(socket,\"IPPROTO_IPV6\",41)\n\nAnyIPAddressFamily=Literal[\nAddressFamily.AF_UNSPEC,AddressFamily.AF_INET,AddressFamily.AF_INET6\n]\nIPAddressFamily=Literal[AddressFamily.AF_INET,AddressFamily.AF_INET6]\n\n\n\n@overload\nasync def connect_tcp(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nlocal_host:IPAddressType |None=...,\nssl_context:ssl.SSLContext |None=...,\ntls_standard_compatible:bool=...,\ntls_hostname:str,\nhappy_eyeballs_delay:float=...,\n)->TLSStream:...\n\n\n\n@overload\nasync def connect_tcp(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nlocal_host:IPAddressType |None=...,\nssl_context:ssl.SSLContext,\ntls_standard_compatible:bool=...,\ntls_hostname:str |None=...,\nhappy_eyeballs_delay:float=...,\n)->TLSStream:...\n\n\n\n@overload\nasync def connect_tcp(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nlocal_host:IPAddressType |None=...,\ntls:Literal[True],\nssl_context:ssl.SSLContext |None=...,\ntls_standard_compatible:bool=...,\ntls_hostname:str |None=...,\nhappy_eyeballs_delay:float=...,\n)->TLSStream:...\n\n\n\n@overload\nasync def connect_tcp(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nlocal_host:IPAddressType |None=...,\ntls:Literal[False],\nssl_context:ssl.SSLContext |None=...,\ntls_standard_compatible:bool=...,\ntls_hostname:str |None=...,\nhappy_eyeballs_delay:float=...,\n)->SocketStream:...\n\n\n\n@overload\nasync def connect_tcp(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nlocal_host:IPAddressType |None=...,\nhappy_eyeballs_delay:float=...,\n)->SocketStream:...\n\n\nasync def connect_tcp(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nlocal_host:IPAddressType |None=None,\ntls:bool=False,\nssl_context:ssl.SSLContext |None=None,\ntls_standard_compatible:bool=True,\ntls_hostname:str |None=None,\nhappy_eyeballs_delay:float=0.25,\n)->SocketStream |TLSStream:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n connected_stream:SocketStream |None=None\n \n async def try_connect(remote_host:str,event:Event)->None:\n  nonlocal connected_stream\n  try:\n   stream=await asynclib.connect_tcp(remote_host,remote_port,local_address)\n  except OSError as exc:\n   oserrors.append(exc)\n   return\n  else:\n   if connected_stream is None:\n    connected_stream=stream\n    tg.cancel_scope.cancel()\n   else:\n    await stream.aclose()\n  finally:\n   event.set()\n   \n asynclib=get_async_backend()\n local_address:IPSockAddrType |None=None\n family=socket.AF_UNSPEC\n if local_host:\n  gai_res=await getaddrinfo(str(local_host),None)\n  family,*_,local_address=gai_res[0]\n  \n target_host=str(remote_host)\n try:\n  addr_obj=ip_address(remote_host)\n except ValueError:\n  addr_obj=None\n  \n if addr_obj is not None:\n  if isinstance(addr_obj,IPv6Address):\n   target_addrs=[(socket.AF_INET6,addr_obj.compressed)]\n  else:\n   target_addrs=[(socket.AF_INET,addr_obj.compressed)]\n else:\n \n  gai_res=await getaddrinfo(\n  target_host,remote_port,family=family,type=socket.SOCK_STREAM\n  )\n  \n  \n  \n  v6_found=v4_found=False\n  target_addrs=[]\n  for af,*rest,sa in gai_res:\n   if af ==socket.AF_INET6 and not v6_found:\n    v6_found=True\n    target_addrs.insert(0,(af,sa[0]))\n   elif af ==socket.AF_INET and not v4_found and v6_found:\n    v4_found=True\n    target_addrs.insert(1,(af,sa[0]))\n   else:\n    target_addrs.append((af,sa[0]))\n    \n oserrors:list[OSError]=[]\n async with create_task_group()as tg:\n  for i,(af,addr)in enumerate(target_addrs):\n   event=Event()\n   tg.start_soon(try_connect,addr,event)\n   with move_on_after(happy_eyeballs_delay):\n    await event.wait()\n    \n if connected_stream is None:\n  cause=(\n  oserrors[0]\n  if len(oserrors)==1\n  else ExceptionGroup(\"multiple connection attempts failed\",oserrors)\n  )\n  raise OSError(\"All connection attempts failed\")from cause\n  \n if tls or tls_hostname or ssl_context:\n  try:\n   return await TLSStream.wrap(\n   connected_stream,\n   server_side=False,\n   hostname=tls_hostname or str(remote_host),\n   ssl_context=ssl_context,\n   standard_compatible=tls_standard_compatible,\n   )\n  except BaseException:\n   await aclose_forcefully(connected_stream)\n   raise\n   \n return connected_stream\n \n \nasync def connect_unix(path:str |bytes |PathLike[Any])->UNIXSocketStream:\n ''\n\n\n\n\n\n\n\n \n path=os.fspath(path)\n return await get_async_backend().connect_unix(path)\n \n \nasync def create_tcp_listener(\n*,\nlocal_host:IPAddressType |None=None,\nlocal_port:int=0,\nfamily:AnyIPAddressFamily=socket.AddressFamily.AF_UNSPEC,\nbacklog:int=65536,\nreuse_port:bool=False,\n)->MultiListener[SocketStream]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n asynclib=get_async_backend()\n backlog=min(backlog,65536)\n local_host=str(local_host)if local_host is not None else None\n gai_res=await getaddrinfo(\n local_host,\n local_port,\n family=family,\n type=socket.SocketKind.SOCK_STREAM if sys.platform ==\"win32\"else 0,\n flags=socket.AI_PASSIVE |socket.AI_ADDRCONFIG,\n )\n listeners:list[SocketListener]=[]\n try:\n \n \n  sockaddr:tuple[str,int]|tuple[str,int,int,int]\n  for fam,kind,*_,sockaddr in sorted(set(gai_res)):\n  \n  \n  \n   if sys.platform !=\"win32\"and kind is not SocketKind.SOCK_STREAM:\n    continue\n    \n   raw_socket=socket.socket(fam)\n   raw_socket.setblocking(False)\n   \n   \n   \n   if sys.platform ==\"win32\":\n    raw_socket.setsockopt(socket.SOL_SOCKET,socket.SO_EXCLUSIVEADDRUSE,1)\n   else:\n    raw_socket.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEADDR,1)\n    \n   if reuse_port:\n    raw_socket.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEPORT,1)\n    \n    \n   if fam ==socket.AF_INET6:\n    raw_socket.setsockopt(IPPROTO_IPV6,socket.IPV6_V6ONLY,1)\n    \n    \n    if \"%\"in sockaddr[0]:\n     addr,scope_id=sockaddr[0].split(\"%\",1)\n     sockaddr=(addr,sockaddr[1],0,int(scope_id))\n     \n   raw_socket.bind(sockaddr)\n   raw_socket.listen(backlog)\n   listener=asynclib.create_tcp_listener(raw_socket)\n   listeners.append(listener)\n except BaseException:\n  for listener in listeners:\n   await listener.aclose()\n   \n  raise\n  \n return MultiListener(listeners)\n \n \nasync def create_unix_listener(\npath:str |bytes |PathLike[Any],\n*,\nmode:int |None=None,\nbacklog:int=65536,\n)->SocketListener:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n backlog=min(backlog,65536)\n raw_socket=await setup_unix_local_socket(path,mode,socket.SOCK_STREAM)\n try:\n  raw_socket.listen(backlog)\n  return get_async_backend().create_unix_listener(raw_socket)\n except BaseException:\n  raw_socket.close()\n  raise\n  \n  \nasync def create_udp_socket(\nfamily:AnyIPAddressFamily=AddressFamily.AF_UNSPEC,\n*,\nlocal_host:IPAddressType |None=None,\nlocal_port:int=0,\nreuse_port:bool=False,\n)->UDPSocket:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if family is AddressFamily.AF_UNSPEC and not local_host:\n  raise ValueError('Either \"family\" or \"local_host\" must be given')\n  \n if local_host:\n  gai_res=await getaddrinfo(\n  str(local_host),\n  local_port,\n  family=family,\n  type=socket.SOCK_DGRAM,\n  flags=socket.AI_PASSIVE |socket.AI_ADDRCONFIG,\n  )\n  family=cast(AnyIPAddressFamily,gai_res[0][0])\n  local_address=gai_res[0][-1]\n elif family is AddressFamily.AF_INET6:\n  local_address=(\"::\",0)\n else:\n  local_address=(\"0.0.0.0\",0)\n  \n sock=await get_async_backend().create_udp_socket(\n family,local_address,None,reuse_port\n )\n return cast(UDPSocket,sock)\n \n \nasync def create_connected_udp_socket(\nremote_host:IPAddressType,\nremote_port:int,\n*,\nfamily:AnyIPAddressFamily=AddressFamily.AF_UNSPEC,\nlocal_host:IPAddressType |None=None,\nlocal_port:int=0,\nreuse_port:bool=False,\n)->ConnectedUDPSocket:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n local_address=None\n if local_host:\n  gai_res=await getaddrinfo(\n  str(local_host),\n  local_port,\n  family=family,\n  type=socket.SOCK_DGRAM,\n  flags=socket.AI_PASSIVE |socket.AI_ADDRCONFIG,\n  )\n  family=cast(AnyIPAddressFamily,gai_res[0][0])\n  local_address=gai_res[0][-1]\n  \n gai_res=await getaddrinfo(\n str(remote_host),remote_port,family=family,type=socket.SOCK_DGRAM\n )\n family=cast(AnyIPAddressFamily,gai_res[0][0])\n remote_address=gai_res[0][-1]\n \n sock=await get_async_backend().create_udp_socket(\n family,local_address,remote_address,reuse_port\n )\n return cast(ConnectedUDPSocket,sock)\n \n \nasync def create_unix_datagram_socket(\n*,\nlocal_path:None |str |bytes |PathLike[Any]=None,\nlocal_mode:int |None=None,\n)->UNIXDatagramSocket:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n raw_socket=await setup_unix_local_socket(\n local_path,local_mode,socket.SOCK_DGRAM\n )\n return await get_async_backend().create_unix_datagram_socket(raw_socket,None)\n \n \nasync def create_connected_unix_datagram_socket(\nremote_path:str |bytes |PathLike[Any],\n*,\nlocal_path:None |str |bytes |PathLike[Any]=None,\nlocal_mode:int |None=None,\n)->ConnectedUNIXDatagramSocket:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n remote_path=os.fspath(remote_path)\n raw_socket=await setup_unix_local_socket(\n local_path,local_mode,socket.SOCK_DGRAM\n )\n return await get_async_backend().create_unix_datagram_socket(\n raw_socket,remote_path\n )\n \n \nasync def getaddrinfo(\nhost:bytes |str |None,\nport:str |int |None,\n*,\nfamily:int |AddressFamily=0,\ntype:int |SocketKind=0,\nproto:int=0,\nflags:int=0,\n)->list[tuple[AddressFamily,SocketKind,int,str,tuple[str,int]]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n if isinstance(host,str):\n  try:\n   encoded_host:bytes |None=host.encode(\"ascii\")\n  except UnicodeEncodeError:\n   import idna\n   \n   encoded_host=idna.encode(host,uts46=True)\n else:\n  encoded_host=host\n  \n gai_res=await get_async_backend().getaddrinfo(\n encoded_host,port,family=family,type=type,proto=proto,flags=flags\n )\n return[\n (family,type,proto,canonname,convert_ipv6_sockaddr(sockaddr))\n for family,type,proto,canonname,sockaddr in gai_res\n ]\n \n \ndef getnameinfo(sockaddr:IPSockAddrType,flags:int=0)->Awaitable[tuple[str,str]]:\n ''\n\n\n\n\n\n\n\n\n \n return get_async_backend().getnameinfo(sockaddr,flags)\n \n \n@deprecated(\"This function is deprecated; use `wait_readable` instead\")\ndef wait_socket_readable(sock:socket.socket)->Awaitable[None]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return get_async_backend().wait_readable(sock.fileno())\n \n \n@deprecated(\"This function is deprecated; use `wait_writable` instead\")\ndef wait_socket_writable(sock:socket.socket)->Awaitable[None]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return get_async_backend().wait_writable(sock.fileno())\n \n \ndef wait_readable(obj:FileDescriptorLike)->Awaitable[None]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return get_async_backend().wait_readable(obj)\n \n \ndef wait_writable(obj:FileDescriptorLike)->Awaitable[None]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return get_async_backend().wait_writable(obj)\n \n \n \n \n \n \n \ndef convert_ipv6_sockaddr(\nsockaddr:tuple[str,int,int,int]|tuple[str,int],\n)->tuple[str,int]:\n ''\n\n\n\n\n\n\n\n\n\n \n \n if isinstance(sockaddr,tuple)and len(sockaddr)==4:\n  host,port,flowinfo,scope_id=sockaddr\n  if scope_id:\n  \n  \n  \n   host=host.split(\"%\")[0]\n   \n   \n   return f\"{host}%{scope_id}\",port\n  else:\n   return host,port\n else:\n  return sockaddr\n  \n  \nasync def setup_unix_local_socket(\npath:None |str |bytes |PathLike[Any],\nmode:int |None,\nsocktype:int,\n)->socket.socket:\n ''\n\n\n\n\n\n\n\n\n\n \n path_str:str |None\n if path is not None:\n  path_str=os.fsdecode(path)\n  \n  \n  if not path_str.startswith(\"\\0\"):\n  \n   try:\n    stat_result=os.stat(path)\n   except OSError as e:\n    if e.errno not in(\n    errno.ENOENT,\n    errno.ENOTDIR,\n    errno.EBADF,\n    errno.ELOOP,\n    ):\n     raise\n   else:\n    if stat.S_ISSOCK(stat_result.st_mode):\n     os.unlink(path)\n else:\n  path_str=None\n  \n raw_socket=socket.socket(socket.AF_UNIX,socktype)\n raw_socket.setblocking(False)\n \n if path_str is not None:\n  try:\n   await to_thread.run_sync(raw_socket.bind,path_str,abandon_on_cancel=True)\n   if mode is not None:\n    await to_thread.run_sync(chmod,path_str,mode,abandon_on_cancel=True)\n  except BaseException:\n   raw_socket.close()\n   raise\n   \n return raw_socket\n", ["__future__", "_typeshed", "anyio", "anyio._core._eventloop", "anyio._core._resources", "anyio._core._synchronization", "anyio._core._tasks", "anyio.abc", "anyio.streams.stapled", "anyio.streams.tls", "anyio.to_thread", "collections.abc", "errno", "exceptiongroup", "idna", "ipaddress", "os", "socket", "ssl", "stat", "sys", "typing", "typing_extensions", "warnings"]], "anyio._core._exceptions": [".py", "from __future__ import annotations\n\nimport sys\nfrom collections.abc import Generator\n\nif sys.version_info <(3,11):\n from exceptiongroup import BaseExceptionGroup\n \n \nclass BrokenResourceError(Exception):\n ''\n\n\n \n \n \nclass BrokenWorkerProcess(Exception):\n ''\n\n\n \n \n \nclass BusyResourceError(Exception):\n ''\n\n\n \n \n def __init__(self,action:str):\n  super().__init__(f\"Another task is already {action} this resource\")\n  \n  \nclass ClosedResourceError(Exception):\n ''\n \n \nclass DelimiterNotFound(Exception):\n ''\n\n\n\n \n \n def __init__(self,max_bytes:int)->None:\n  super().__init__(\n  f\"The delimiter was not found among the first {max_bytes} bytes\"\n  )\n  \n  \nclass EndOfStream(Exception):\n ''\n\n \n \n \nclass IncompleteRead(Exception):\n ''\n\n\n\n\n \n \n def __init__(self)->None:\n  super().__init__(\n  \"The stream was closed before the read operation could be completed\"\n  )\n  \n  \nclass TypedAttributeLookupError(LookupError):\n ''\n\n\n \n \n \nclass WouldBlock(Exception):\n ''\n \n \ndef iterate_exceptions(\nexception:BaseException,\n)->Generator[BaseException,None,None]:\n if isinstance(exception,BaseExceptionGroup):\n  for exc in exception.exceptions:\n   yield from iterate_exceptions(exc)\n else:\n  yield exception\n", ["__future__", "collections.abc", "exceptiongroup", "sys"]], "anyio._core._typedattr": [".py", "from __future__ import annotations\n\nfrom collections.abc import Callable,Mapping\nfrom typing import Any,TypeVar,final,overload\n\nfrom._exceptions import TypedAttributeLookupError\n\nT_Attr=TypeVar(\"T_Attr\")\nT_Default=TypeVar(\"T_Default\")\nundefined=object()\n\n\ndef typed_attribute()->Any:\n ''\n return object()\n \n \nclass TypedAttributeSet:\n ''\n\n\n\n \n \n def __init_subclass__(cls)->None:\n  annotations:dict[str,Any]=getattr(cls,\"__annotations__\",{})\n  for attrname in dir(cls):\n   if not attrname.startswith(\"_\")and attrname not in annotations:\n    raise TypeError(\n    f\"Attribute {attrname !r} is missing its type annotation\"\n    )\n    \n  super().__init_subclass__()\n  \n  \nclass TypedAttributeProvider:\n ''\n \n @property\n def extra_attributes(self)->Mapping[T_Attr,Callable[[],T_Attr]]:\n  ''\n\n\n\n\n\n\n\n  \n  return{}\n  \n @overload\n def extra(self,attribute:T_Attr)->T_Attr:...\n \n @overload\n def extra(self,attribute:T_Attr,default:T_Default)->T_Attr |T_Default:...\n \n @final\n def extra(self,attribute:Any,default:object=undefined)->object:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  try:\n   getter=self.extra_attributes[attribute]\n  except KeyError:\n   if default is undefined:\n    raise TypedAttributeLookupError(\"Attribute not found\")from None\n   else:\n    return default\n    \n  return getter()\n", ["__future__", "anyio._core._exceptions", "collections.abc", "typing"]], "anyio._core._synchronization": [".py", "from __future__ import annotations\n\nimport math\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom types import TracebackType\n\nfrom sniffio import AsyncLibraryNotFoundError\n\nfrom..lowlevel import checkpoint\nfrom._eventloop import get_async_backend\nfrom._exceptions import BusyResourceError\nfrom._tasks import CancelScope\nfrom._testing import TaskInfo,get_current_task\n\n\n@dataclass(frozen=True)\nclass EventStatistics:\n ''\n\n \n \n tasks_waiting:int\n \n \n@dataclass(frozen=True)\nclass CapacityLimiterStatistics:\n ''\n\n\n\n\n\n\n\n \n \n borrowed_tokens:int\n total_tokens:float\n borrowers:tuple[object,...]\n tasks_waiting:int\n \n \n@dataclass(frozen=True)\nclass LockStatistics:\n ''\n\n\n\n\n \n \n locked:bool\n owner:TaskInfo |None\n tasks_waiting:int\n \n \n@dataclass(frozen=True)\nclass ConditionStatistics:\n ''\n\n\n\n \n \n tasks_waiting:int\n lock_statistics:LockStatistics\n \n \n@dataclass(frozen=True)\nclass SemaphoreStatistics:\n ''\n\n\n \n \n tasks_waiting:int\n \n \nclass Event:\n def __new__(cls)->Event:\n  try:\n   return get_async_backend().create_event()\n  except AsyncLibraryNotFoundError:\n   return EventAdapter()\n   \n def set(self)->None:\n  ''\n  raise NotImplementedError\n  \n def is_set(self)->bool:\n  ''\n  raise NotImplementedError\n  \n async def wait(self)->None:\n  ''\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def statistics(self)->EventStatistics:\n  ''\n  raise NotImplementedError\n  \n  \nclass EventAdapter(Event):\n _internal_event:Event |None=None\n _is_set:bool=False\n \n def __new__(cls)->EventAdapter:\n  return object.__new__(cls)\n  \n @property\n def _event(self)->Event:\n  if self._internal_event is None:\n   self._internal_event=get_async_backend().create_event()\n   if self._is_set:\n    self._internal_event.set()\n    \n  return self._internal_event\n  \n def set(self)->None:\n  if self._internal_event is None:\n   self._is_set=True\n  else:\n   self._event.set()\n   \n def is_set(self)->bool:\n  if self._internal_event is None:\n   return self._is_set\n   \n  return self._internal_event.is_set()\n  \n async def wait(self)->None:\n  await self._event.wait()\n  \n def statistics(self)->EventStatistics:\n  if self._internal_event is None:\n   return EventStatistics(tasks_waiting=0)\n   \n  return self._internal_event.statistics()\n  \n  \nclass Lock:\n def __new__(cls,*,fast_acquire:bool=False)->Lock:\n  try:\n   return get_async_backend().create_lock(fast_acquire=fast_acquire)\n  except AsyncLibraryNotFoundError:\n   return LockAdapter(fast_acquire=fast_acquire)\n   \n async def __aenter__(self)->None:\n  await self.acquire()\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.release()\n  \n async def acquire(self)->None:\n  ''\n  raise NotImplementedError\n  \n def acquire_nowait(self)->None:\n  ''\n\n\n\n\n  \n  raise NotImplementedError\n  \n def release(self)->None:\n  ''\n  raise NotImplementedError\n  \n def locked(self)->bool:\n  ''\n  raise NotImplementedError\n  \n def statistics(self)->LockStatistics:\n  ''\n\n\n\n  \n  raise NotImplementedError\n  \n  \nclass LockAdapter(Lock):\n _internal_lock:Lock |None=None\n \n def __new__(cls,*,fast_acquire:bool=False)->LockAdapter:\n  return object.__new__(cls)\n  \n def __init__(self,*,fast_acquire:bool=False):\n  self._fast_acquire=fast_acquire\n  \n @property\n def _lock(self)->Lock:\n  if self._internal_lock is None:\n   self._internal_lock=get_async_backend().create_lock(\n   fast_acquire=self._fast_acquire\n   )\n   \n  return self._internal_lock\n  \n async def __aenter__(self)->None:\n  await self._lock.acquire()\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  if self._internal_lock is not None:\n   self._internal_lock.release()\n   \n async def acquire(self)->None:\n  ''\n  await self._lock.acquire()\n  \n def acquire_nowait(self)->None:\n  ''\n\n\n\n\n  \n  self._lock.acquire_nowait()\n  \n def release(self)->None:\n  ''\n  self._lock.release()\n  \n def locked(self)->bool:\n  ''\n  return self._lock.locked()\n  \n def statistics(self)->LockStatistics:\n  ''\n\n\n\n\n  \n  if self._internal_lock is None:\n   return LockStatistics(False,None,0)\n   \n  return self._internal_lock.statistics()\n  \n  \nclass Condition:\n _owner_task:TaskInfo |None=None\n \n def __init__(self,lock:Lock |None=None):\n  self._lock=lock or Lock()\n  self._waiters:deque[Event]=deque()\n  \n async def __aenter__(self)->None:\n  await self.acquire()\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.release()\n  \n def _check_acquired(self)->None:\n  if self._owner_task !=get_current_task():\n   raise RuntimeError(\"The current task is not holding the underlying lock\")\n   \n async def acquire(self)->None:\n  ''\n  await self._lock.acquire()\n  self._owner_task=get_current_task()\n  \n def acquire_nowait(self)->None:\n  ''\n\n\n\n\n  \n  self._lock.acquire_nowait()\n  self._owner_task=get_current_task()\n  \n def release(self)->None:\n  ''\n  self._lock.release()\n  \n def locked(self)->bool:\n  ''\n  return self._lock.locked()\n  \n def notify(self,n:int=1)->None:\n  ''\n  self._check_acquired()\n  for _ in range(n):\n   try:\n    event=self._waiters.popleft()\n   except IndexError:\n    break\n    \n   event.set()\n   \n def notify_all(self)->None:\n  ''\n  self._check_acquired()\n  for event in self._waiters:\n   event.set()\n   \n  self._waiters.clear()\n  \n async def wait(self)->None:\n  ''\n  await checkpoint()\n  event=Event()\n  self._waiters.append(event)\n  self.release()\n  try:\n   await event.wait()\n  except BaseException:\n   if not event.is_set():\n    self._waiters.remove(event)\n    \n   raise\n  finally:\n   with CancelScope(shield=True):\n    await self.acquire()\n    \n def statistics(self)->ConditionStatistics:\n  ''\n\n\n\n  \n  return ConditionStatistics(len(self._waiters),self._lock.statistics())\n  \n  \nclass Semaphore:\n def __new__(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->Semaphore:\n  try:\n   return get_async_backend().create_semaphore(\n   initial_value,max_value=max_value,fast_acquire=fast_acquire\n   )\n  except AsyncLibraryNotFoundError:\n   return SemaphoreAdapter(initial_value,max_value=max_value)\n   \n def __init__(\n self,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n ):\n  if not isinstance(initial_value,int):\n   raise TypeError(\"initial_value must be an integer\")\n  if initial_value <0:\n   raise ValueError(\"initial_value must be >= 0\")\n  if max_value is not None:\n   if not isinstance(max_value,int):\n    raise TypeError(\"max_value must be an integer or None\")\n   if max_value <initial_value:\n    raise ValueError(\n    \"max_value must be equal to or higher than initial_value\"\n    )\n    \n  self._fast_acquire=fast_acquire\n  \n async def __aenter__(self)->Semaphore:\n  await self.acquire()\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.release()\n  \n async def acquire(self)->None:\n  ''\n  raise NotImplementedError\n  \n def acquire_nowait(self)->None:\n  ''\n\n\n\n\n  \n  raise NotImplementedError\n  \n def release(self)->None:\n  ''\n  raise NotImplementedError\n  \n @property\n def value(self)->int:\n  ''\n  raise NotImplementedError\n  \n @property\n def max_value(self)->int |None:\n  ''\n  raise NotImplementedError\n  \n def statistics(self)->SemaphoreStatistics:\n  ''\n\n\n\n  \n  raise NotImplementedError\n  \n  \nclass SemaphoreAdapter(Semaphore):\n _internal_semaphore:Semaphore |None=None\n \n def __new__(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->SemaphoreAdapter:\n  return object.__new__(cls)\n  \n def __init__(\n self,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->None:\n  super().__init__(initial_value,max_value=max_value,fast_acquire=fast_acquire)\n  self._initial_value=initial_value\n  self._max_value=max_value\n  \n @property\n def _semaphore(self)->Semaphore:\n  if self._internal_semaphore is None:\n   self._internal_semaphore=get_async_backend().create_semaphore(\n   self._initial_value,max_value=self._max_value\n   )\n   \n  return self._internal_semaphore\n  \n async def acquire(self)->None:\n  await self._semaphore.acquire()\n  \n def acquire_nowait(self)->None:\n  self._semaphore.acquire_nowait()\n  \n def release(self)->None:\n  self._semaphore.release()\n  \n @property\n def value(self)->int:\n  if self._internal_semaphore is None:\n   return self._initial_value\n   \n  return self._semaphore.value\n  \n @property\n def max_value(self)->int |None:\n  return self._max_value\n  \n def statistics(self)->SemaphoreStatistics:\n  if self._internal_semaphore is None:\n   return SemaphoreStatistics(tasks_waiting=0)\n   \n  return self._semaphore.statistics()\n  \n  \nclass CapacityLimiter:\n def __new__(cls,total_tokens:float)->CapacityLimiter:\n  try:\n   return get_async_backend().create_capacity_limiter(total_tokens)\n  except AsyncLibraryNotFoundError:\n   return CapacityLimiterAdapter(total_tokens)\n   \n async def __aenter__(self)->None:\n  raise NotImplementedError\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  raise NotImplementedError\n  \n @property\n def total_tokens(self)->float:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n @total_tokens.setter\n def total_tokens(self,value:float)->None:\n  raise NotImplementedError\n  \n @property\n def borrowed_tokens(self)->int:\n  ''\n  raise NotImplementedError\n  \n @property\n def available_tokens(self)->float:\n  ''\n  raise NotImplementedError\n  \n def acquire_nowait(self)->None:\n  ''\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def acquire_on_behalf_of_nowait(self,borrower:object)->None:\n  ''\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n async def acquire(self)->None:\n  ''\n\n\n\n  \n  raise NotImplementedError\n  \n async def acquire_on_behalf_of(self,borrower:object)->None:\n  ''\n\n\n\n\n  \n  raise NotImplementedError\n  \n def release(self)->None:\n  ''\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def release_on_behalf_of(self,borrower:object)->None:\n  ''\n\n\n\n\n\n  \n  raise NotImplementedError\n  \n def statistics(self)->CapacityLimiterStatistics:\n  ''\n\n\n\n\n  \n  raise NotImplementedError\n  \n  \nclass CapacityLimiterAdapter(CapacityLimiter):\n _internal_limiter:CapacityLimiter |None=None\n \n def __new__(cls,total_tokens:float)->CapacityLimiterAdapter:\n  return object.__new__(cls)\n  \n def __init__(self,total_tokens:float)->None:\n  self.total_tokens=total_tokens\n  \n @property\n def _limiter(self)->CapacityLimiter:\n  if self._internal_limiter is None:\n   self._internal_limiter=get_async_backend().create_capacity_limiter(\n   self._total_tokens\n   )\n   \n  return self._internal_limiter\n  \n async def __aenter__(self)->None:\n  await self._limiter.__aenter__()\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  return await self._limiter.__aexit__(exc_type,exc_val,exc_tb)\n  \n @property\n def total_tokens(self)->float:\n  if self._internal_limiter is None:\n   return self._total_tokens\n   \n  return self._internal_limiter.total_tokens\n  \n @total_tokens.setter\n def total_tokens(self,value:float)->None:\n  if not isinstance(value,int)and value is not math.inf:\n   raise TypeError(\"total_tokens must be an int or math.inf\")\n  elif value <1:\n   raise ValueError(\"total_tokens must be >= 1\")\n   \n  if self._internal_limiter is None:\n   self._total_tokens=value\n   return\n   \n  self._limiter.total_tokens=value\n  \n @property\n def borrowed_tokens(self)->int:\n  if self._internal_limiter is None:\n   return 0\n   \n  return self._internal_limiter.borrowed_tokens\n  \n @property\n def available_tokens(self)->float:\n  if self._internal_limiter is None:\n   return self._total_tokens\n   \n  return self._internal_limiter.available_tokens\n  \n def acquire_nowait(self)->None:\n  self._limiter.acquire_nowait()\n  \n def acquire_on_behalf_of_nowait(self,borrower:object)->None:\n  self._limiter.acquire_on_behalf_of_nowait(borrower)\n  \n async def acquire(self)->None:\n  await self._limiter.acquire()\n  \n async def acquire_on_behalf_of(self,borrower:object)->None:\n  await self._limiter.acquire_on_behalf_of(borrower)\n  \n def release(self)->None:\n  self._limiter.release()\n  \n def release_on_behalf_of(self,borrower:object)->None:\n  self._limiter.release_on_behalf_of(borrower)\n  \n def statistics(self)->CapacityLimiterStatistics:\n  if self._internal_limiter is None:\n   return CapacityLimiterStatistics(\n   borrowed_tokens=0,\n   total_tokens=self.total_tokens,\n   borrowers=(),\n   tasks_waiting=0,\n   )\n   \n  return self._internal_limiter.statistics()\n  \n  \nclass ResourceGuard:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__=\"action\",\"_guarded\"\n \n def __init__(self,action:str=\"using\"):\n  self.action:str=action\n  self._guarded=False\n  \n def __enter__(self)->None:\n  if self._guarded:\n   raise BusyResourceError(self.action)\n   \n  self._guarded=True\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  self._guarded=False\n  return None\n", ["__future__", "anyio._core._eventloop", "anyio._core._exceptions", "anyio._core._tasks", "anyio._core._testing", "anyio.lowlevel", "collections", "dataclasses", "math", "sniffio", "types"]], "anyio._core._asyncio_selector_thread": [".py", "from __future__ import annotations\n\nimport asyncio\nimport socket\nimport threading\nfrom collections.abc import Callable\nfrom selectors import EVENT_READ,EVENT_WRITE,DefaultSelector\nfrom typing import TYPE_CHECKING,Any\n\nif TYPE_CHECKING:\n from _typeshed import FileDescriptorLike\n \n_selector_lock=threading.Lock()\n_selector:Selector |None=None\n\n\nclass Selector:\n def __init__(self)->None:\n  self._thread=threading.Thread(target=self.run,name=\"AnyIO socket selector\")\n  self._selector=DefaultSelector()\n  self._send,self._receive=socket.socketpair()\n  self._send.setblocking(False)\n  self._receive.setblocking(False)\n  self._selector.register(self._receive,EVENT_READ)\n  self._closed=False\n  \n def start(self)->None:\n  self._thread.start()\n  threading._register_atexit(self._stop)\n  \n def _stop(self)->None:\n  global _selector\n  self._closed=True\n  self._notify_self()\n  self._send.close()\n  self._thread.join()\n  self._selector.unregister(self._receive)\n  self._receive.close()\n  self._selector.close()\n  _selector=None\n  assert(\n  not self._selector.get_map()\n  ),\"selector still has registered file descriptors after shutdown\"\n  \n def _notify_self(self)->None:\n  try:\n   self._send.send(b\"\\x00\")\n  except BlockingIOError:\n   pass\n   \n def add_reader(self,fd:FileDescriptorLike,callback:Callable[[],Any])->None:\n  loop=asyncio.get_running_loop()\n  try:\n   key=self._selector.get_key(fd)\n  except KeyError:\n   self._selector.register(fd,EVENT_READ,{EVENT_READ:(loop,callback)})\n  else:\n   if EVENT_READ in key.data:\n    raise ValueError(\n    \"this file descriptor is already registered for reading\"\n    )\n    \n   key.data[EVENT_READ]=loop,callback\n   self._selector.modify(fd,key.events |EVENT_READ,key.data)\n   \n  self._notify_self()\n  \n def add_writer(self,fd:FileDescriptorLike,callback:Callable[[],Any])->None:\n  loop=asyncio.get_running_loop()\n  try:\n   key=self._selector.get_key(fd)\n  except KeyError:\n   self._selector.register(fd,EVENT_WRITE,{EVENT_WRITE:(loop,callback)})\n  else:\n   if EVENT_WRITE in key.data:\n    raise ValueError(\n    \"this file descriptor is already registered for writing\"\n    )\n    \n   key.data[EVENT_WRITE]=loop,callback\n   self._selector.modify(fd,key.events |EVENT_WRITE,key.data)\n   \n  self._notify_self()\n  \n def remove_reader(self,fd:FileDescriptorLike)->bool:\n  try:\n   key=self._selector.get_key(fd)\n  except KeyError:\n   return False\n   \n  if new_events :=key.events ^EVENT_READ:\n   del key.data[EVENT_READ]\n   self._selector.modify(fd,new_events,key.data)\n  else:\n   self._selector.unregister(fd)\n   \n  return True\n  \n def remove_writer(self,fd:FileDescriptorLike)->bool:\n  try:\n   key=self._selector.get_key(fd)\n  except KeyError:\n   return False\n   \n  if new_events :=key.events ^EVENT_WRITE:\n   del key.data[EVENT_WRITE]\n   self._selector.modify(fd,new_events,key.data)\n  else:\n   self._selector.unregister(fd)\n   \n  return True\n  \n def run(self)->None:\n  while not self._closed:\n   for key,events in self._selector.select():\n    if key.fileobj is self._receive:\n     try:\n      while self._receive.recv(4096):\n       pass\n     except BlockingIOError:\n      pass\n      \n     continue\n     \n    if events&EVENT_READ:\n     loop,callback=key.data[EVENT_READ]\n     self.remove_reader(key.fd)\n     try:\n      loop.call_soon_threadsafe(callback)\n     except RuntimeError:\n      pass\n      \n    if events&EVENT_WRITE:\n     loop,callback=key.data[EVENT_WRITE]\n     self.remove_writer(key.fd)\n     try:\n      loop.call_soon_threadsafe(callback)\n     except RuntimeError:\n      pass\n      \n      \ndef get_selector()->Selector:\n global _selector\n \n with _selector_lock:\n  if _selector is None:\n   _selector=Selector()\n   _selector.start()\n   \n  return _selector\n", ["__future__", "_typeshed", "asyncio", "collections.abc", "selectors", "socket", "threading", "typing"]], "anyio._core._signals": [".py", "from __future__ import annotations\n\nfrom collections.abc import AsyncIterator\nfrom contextlib import AbstractContextManager\nfrom signal import Signals\n\nfrom._eventloop import get_async_backend\n\n\ndef open_signal_receiver(\n*signals:Signals,\n)->AbstractContextManager[AsyncIterator[Signals]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n return get_async_backend().open_signal_receiver(*signals)\n", ["__future__", "anyio._core._eventloop", "collections.abc", "contextlib", "signal"]], "anyio._core._testing": [".py", "from __future__ import annotations\n\nfrom collections.abc import Awaitable,Generator\nfrom typing import Any,cast\n\nfrom._eventloop import get_async_backend\n\n\nclass TaskInfo:\n ''\n\n\n\n\n\n\n\n \n \n __slots__=\"_name\",\"id\",\"parent_id\",\"name\",\"coro\"\n \n def __init__(\n self,\n id:int,\n parent_id:int |None,\n name:str |None,\n coro:Generator[Any,Any,Any]|Awaitable[Any],\n ):\n  func=get_current_task\n  self._name=f\"{func.__module__}.{func.__qualname__}\"\n  self.id:int=id\n  self.parent_id:int |None=parent_id\n  self.name:str |None=name\n  self.coro:Generator[Any,Any,Any]|Awaitable[Any]=coro\n  \n def __eq__(self,other:object)->bool:\n  if isinstance(other,TaskInfo):\n   return self.id ==other.id\n   \n  return NotImplemented\n  \n def __hash__(self)->int:\n  return hash(self.id)\n  \n def __repr__(self)->str:\n  return f\"{self.__class__.__name__}(id={self.id !r}, name={self.name !r})\"\n  \n def has_pending_cancellation(self)->bool:\n  ''\n\n\n  \n  return False\n  \n  \ndef get_current_task()->TaskInfo:\n ''\n\n\n\n\n \n return get_async_backend().get_current_task()\n \n \ndef get_running_tasks()->list[TaskInfo]:\n ''\n\n\n\n\n \n return cast(\"list[TaskInfo]\",get_async_backend().get_running_tasks())\n \n \nasync def wait_all_tasks_blocked()->None:\n ''\n await get_async_backend().wait_all_tasks_blocked()\n", ["__future__", "anyio._core._eventloop", "collections.abc", "typing"]], "anyio._core._resources": [".py", "from __future__ import annotations\n\nfrom..abc import AsyncResource\nfrom._tasks import CancelScope\n\n\nasync def aclose_forcefully(resource:AsyncResource)->None:\n ''\n\n\n\n\n\n\n \n with CancelScope()as scope:\n  scope.cancel()\n  await resource.aclose()\n", ["__future__", "anyio._core._tasks", "anyio.abc"]], "anyio._backends": [".py", "", [], 1], "anyio._backends._asyncio": [".py", "from __future__ import annotations\n\nimport array\nimport asyncio\nimport concurrent.futures\nimport math\nimport os\nimport socket\nimport sys\nimport threading\nimport weakref\nfrom asyncio import(\nAbstractEventLoop,\nCancelledError,\nall_tasks,\ncreate_task,\ncurrent_task,\nget_running_loop,\nsleep,\n)\nfrom asyncio.base_events import _run_until_complete_cb\nfrom collections import OrderedDict,deque\nfrom collections.abc import(\nAsyncGenerator,\nAsyncIterator,\nAwaitable,\nCallable,\nCollection,\nCoroutine,\nIterable,\nIterator,\nMutableMapping,\nSequence,\n)\nfrom concurrent.futures import Future\nfrom contextlib import AbstractContextManager,suppress\nfrom contextvars import Context,copy_context\nfrom dataclasses import dataclass\nfrom functools import partial,wraps\nfrom inspect import(\nCORO_RUNNING,\nCORO_SUSPENDED,\ngetcoroutinestate,\niscoroutine,\n)\nfrom io import IOBase\nfrom os import PathLike\nfrom queue import Queue\nfrom signal import Signals\nfrom socket import AddressFamily,SocketKind\nfrom threading import Thread\nfrom types import TracebackType\nfrom typing import(\nIO,\nTYPE_CHECKING,\nAny,\nOptional,\nTypeVar,\ncast,\n)\nfrom weakref import WeakKeyDictionary\n\nimport sniffio\n\nfrom.. import(\nCapacityLimiterStatistics,\nEventStatistics,\nLockStatistics,\nTaskInfo,\nabc,\n)\nfrom.._core._eventloop import claim_worker_thread,threadlocals\nfrom.._core._exceptions import(\nBrokenResourceError,\nBusyResourceError,\nClosedResourceError,\nEndOfStream,\nWouldBlock,\niterate_exceptions,\n)\nfrom.._core._sockets import convert_ipv6_sockaddr\nfrom.._core._streams import create_memory_object_stream\nfrom.._core._synchronization import(\nCapacityLimiter as BaseCapacityLimiter,\n)\nfrom.._core._synchronization import Event as BaseEvent\nfrom.._core._synchronization import Lock as BaseLock\nfrom.._core._synchronization import(\nResourceGuard,\nSemaphoreStatistics,\n)\nfrom.._core._synchronization import Semaphore as BaseSemaphore\nfrom.._core._tasks import CancelScope as BaseCancelScope\nfrom..abc import(\nAsyncBackend,\nIPSockAddrType,\nSocketListener,\nUDPPacketType,\nUNIXDatagramPacketType,\n)\nfrom..abc._eventloop import StrOrBytesPath\nfrom..lowlevel import RunVar\nfrom..streams.memory import MemoryObjectReceiveStream,MemoryObjectSendStream\n\nif TYPE_CHECKING:\n from _typeshed import FileDescriptorLike\nelse:\n FileDescriptorLike=object\n \nif sys.version_info >=(3,10):\n from typing import ParamSpec\nelse:\n from typing_extensions import ParamSpec\n \nif sys.version_info >=(3,11):\n from asyncio import Runner\n from typing import TypeVarTuple,Unpack\nelse:\n import contextvars\n import enum\n import signal\n from asyncio import coroutines,events,exceptions,tasks\n \n from exceptiongroup import BaseExceptionGroup\n from typing_extensions import TypeVarTuple,Unpack\n \n class _State(enum.Enum):\n  CREATED=\"created\"\n  INITIALIZED=\"initialized\"\n  CLOSED=\"closed\"\n  \n class Runner:\n \n  def __init__(\n  self,\n  *,\n  debug:bool |None=None,\n  loop_factory:Callable[[],AbstractEventLoop]|None=None,\n  ):\n   self._state=_State.CREATED\n   self._debug=debug\n   self._loop_factory=loop_factory\n   self._loop:AbstractEventLoop |None=None\n   self._context=None\n   self._interrupt_count=0\n   self._set_event_loop=False\n   \n  def __enter__(self)->Runner:\n   self._lazy_init()\n   return self\n   \n  def __exit__(\n  self,\n  exc_type:type[BaseException],\n  exc_val:BaseException,\n  exc_tb:TracebackType,\n  )->None:\n   self.close()\n   \n  def close(self)->None:\n   ''\n   if self._state is not _State.INITIALIZED:\n    return\n   try:\n    loop=self._loop\n    _cancel_all_tasks(loop)\n    loop.run_until_complete(loop.shutdown_asyncgens())\n    if hasattr(loop,\"shutdown_default_executor\"):\n     loop.run_until_complete(loop.shutdown_default_executor())\n    else:\n     loop.run_until_complete(_shutdown_default_executor(loop))\n   finally:\n    if self._set_event_loop:\n     events.set_event_loop(None)\n    loop.close()\n    self._loop=None\n    self._state=_State.CLOSED\n    \n  def get_loop(self)->AbstractEventLoop:\n   ''\n   self._lazy_init()\n   return self._loop\n   \n  def run(self,coro:Coroutine[T_Retval],*,context=None)->T_Retval:\n   ''\n   if not coroutines.iscoroutine(coro):\n    raise ValueError(f\"a coroutine was expected, got {coro !r}\")\n    \n   if events._get_running_loop()is not None:\n   \n    raise RuntimeError(\n    \"Runner.run() cannot be called from a running event loop\"\n    )\n    \n   self._lazy_init()\n   \n   if context is None:\n    context=self._context\n   task=context.run(self._loop.create_task,coro)\n   \n   if(\n   threading.current_thread()is threading.main_thread()\n   and signal.getsignal(signal.SIGINT)is signal.default_int_handler\n   ):\n    sigint_handler=partial(self._on_sigint,main_task=task)\n    try:\n     signal.signal(signal.SIGINT,sigint_handler)\n    except ValueError:\n    \n    \n    \n     sigint_handler=None\n   else:\n    sigint_handler=None\n    \n   self._interrupt_count=0\n   try:\n    return self._loop.run_until_complete(task)\n   except exceptions.CancelledError:\n    if self._interrupt_count >0:\n     uncancel=getattr(task,\"uncancel\",None)\n     if uncancel is not None and uncancel()==0:\n      raise KeyboardInterrupt()\n    raise\n   finally:\n    if(\n    sigint_handler is not None\n    and signal.getsignal(signal.SIGINT)is sigint_handler\n    ):\n     signal.signal(signal.SIGINT,signal.default_int_handler)\n     \n  def _lazy_init(self)->None:\n   if self._state is _State.CLOSED:\n    raise RuntimeError(\"Runner is closed\")\n   if self._state is _State.INITIALIZED:\n    return\n   if self._loop_factory is None:\n    self._loop=events.new_event_loop()\n    if not self._set_event_loop:\n    \n    \n     events.set_event_loop(self._loop)\n     self._set_event_loop=True\n   else:\n    self._loop=self._loop_factory()\n   if self._debug is not None:\n    self._loop.set_debug(self._debug)\n   self._context=contextvars.copy_context()\n   self._state=_State.INITIALIZED\n   \n  def _on_sigint(self,signum,frame,main_task:asyncio.Task)->None:\n   self._interrupt_count +=1\n   if self._interrupt_count ==1 and not main_task.done():\n    main_task.cancel()\n    \n    self._loop.call_soon_threadsafe(lambda:None)\n    return\n   raise KeyboardInterrupt()\n   \n def _cancel_all_tasks(loop:AbstractEventLoop)->None:\n  to_cancel=tasks.all_tasks(loop)\n  if not to_cancel:\n   return\n   \n  for task in to_cancel:\n   task.cancel()\n   \n  loop.run_until_complete(tasks.gather(*to_cancel,return_exceptions=True))\n  \n  for task in to_cancel:\n   if task.cancelled():\n    continue\n   if task.exception()is not None:\n    loop.call_exception_handler(\n    {\n    \"message\":\"unhandled exception during asyncio.run() shutdown\",\n    \"exception\":task.exception(),\n    \"task\":task,\n    }\n    )\n    \n async def _shutdown_default_executor(loop:AbstractEventLoop)->None:\n  ''\n  \n  def _do_shutdown(future:asyncio.futures.Future)->None:\n   try:\n    loop._default_executor.shutdown(wait=True)\n    loop.call_soon_threadsafe(future.set_result,None)\n   except Exception as ex:\n    loop.call_soon_threadsafe(future.set_exception,ex)\n    \n  loop._executor_shutdown_called=True\n  if loop._default_executor is None:\n   return\n  future=loop.create_future()\n  thread=threading.Thread(target=_do_shutdown,args=(future,))\n  thread.start()\n  try:\n   await future\n  finally:\n   thread.join()\n   \n   \nT_Retval=TypeVar(\"T_Retval\")\nT_contra=TypeVar(\"T_contra\",contravariant=True)\nPosArgsT=TypeVarTuple(\"PosArgsT\")\nP=ParamSpec(\"P\")\n\n_root_task:RunVar[asyncio.Task |None]=RunVar(\"_root_task\")\n\n\ndef find_root_task()->asyncio.Task:\n root_task=_root_task.get(None)\n if root_task is not None and not root_task.done():\n  return root_task\n  \n  \n for task in all_tasks():\n  if task._callbacks and not task.done():\n   callbacks=[cb for cb,context in task._callbacks]\n   for cb in callbacks:\n    if(\n    cb is _run_until_complete_cb\n    or getattr(cb,\"__module__\",None)==\"uvloop.loop\"\n    ):\n     _root_task.set(task)\n     return task\n     \n     \n task=cast(asyncio.Task,current_task())\n state=_task_states.get(task)\n if state:\n  cancel_scope=state.cancel_scope\n  while cancel_scope and cancel_scope._parent_scope is not None:\n   cancel_scope=cancel_scope._parent_scope\n   \n  if cancel_scope is not None:\n   return cast(asyncio.Task,cancel_scope._host_task)\n   \n return task\n \n \ndef get_callable_name(func:Callable)->str:\n module=getattr(func,\"__module__\",None)\n qualname=getattr(func,\"__qualname__\",None)\n return \".\".join([x for x in(module,qualname)if x])\n \n \n \n \n \n \n_run_vars:WeakKeyDictionary[asyncio.AbstractEventLoop,Any]=WeakKeyDictionary()\n\n\ndef _task_started(task:asyncio.Task)->bool:\n ''\n \n \n coro=task.get_coro()\n assert coro is not None\n try:\n  return getcoroutinestate(coro)in(CORO_RUNNING,CORO_SUSPENDED)\n except AttributeError:\n \n  raise Exception(f\"Cannot determine if task {task} has started or not\")from None\n  \n  \n  \n  \n  \n  \n  \ndef is_anyio_cancellation(exc:CancelledError)->bool:\n\n\n\n while True:\n  if(\n  exc.args\n  and isinstance(exc.args[0],str)\n  and exc.args[0].startswith(\"Cancelled by cancel scope \")\n  ):\n   return True\n   \n  if isinstance(exc.__context__,CancelledError):\n   exc=exc.__context__\n   continue\n   \n  return False\n  \n  \nclass CancelScope(BaseCancelScope):\n def __new__(\n cls,*,deadline:float=math.inf,shield:bool=False\n )->CancelScope:\n  return object.__new__(cls)\n  \n def __init__(self,deadline:float=math.inf,shield:bool=False):\n  self._deadline=deadline\n  self._shield=shield\n  self._parent_scope:CancelScope |None=None\n  self._child_scopes:set[CancelScope]=set()\n  self._cancel_called=False\n  self._cancelled_caught=False\n  self._active=False\n  self._timeout_handle:asyncio.TimerHandle |None=None\n  self._cancel_handle:asyncio.Handle |None=None\n  self._tasks:set[asyncio.Task]=set()\n  self._host_task:asyncio.Task |None=None\n  if sys.version_info >=(3,11):\n   self._pending_uncancellations:int |None=0\n  else:\n   self._pending_uncancellations=None\n   \n def __enter__(self)->CancelScope:\n  if self._active:\n   raise RuntimeError(\n   \"Each CancelScope may only be used for a single 'with' block\"\n   )\n   \n  self._host_task=host_task=cast(asyncio.Task,current_task())\n  self._tasks.add(host_task)\n  try:\n   task_state=_task_states[host_task]\n  except KeyError:\n   task_state=TaskState(None,self)\n   _task_states[host_task]=task_state\n  else:\n   self._parent_scope=task_state.cancel_scope\n   task_state.cancel_scope=self\n   if self._parent_scope is not None:\n   \n   \n    self._parent_scope._child_scopes.add(self)\n    self._parent_scope._tasks.discard(host_task)\n    \n  self._timeout()\n  self._active=True\n  \n  \n  if self._cancel_called:\n   self._deliver_cancellation(self)\n   \n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  del exc_tb\n  \n  if not self._active:\n   raise RuntimeError(\"This cancel scope is not active\")\n  if current_task()is not self._host_task:\n   raise RuntimeError(\n   \"Attempted to exit cancel scope in a different task than it was \"\n   \"entered in\"\n   )\n   \n  assert self._host_task is not None\n  host_task_state=_task_states.get(self._host_task)\n  if host_task_state is None or host_task_state.cancel_scope is not self:\n   raise RuntimeError(\n   \"Attempted to exit a cancel scope that isn't the current tasks's \"\n   \"current cancel scope\"\n   )\n   \n  try:\n   self._active=False\n   if self._timeout_handle:\n    self._timeout_handle.cancel()\n    self._timeout_handle=None\n    \n   self._tasks.remove(self._host_task)\n   if self._parent_scope is not None:\n    self._parent_scope._child_scopes.remove(self)\n    self._parent_scope._tasks.add(self._host_task)\n    \n   host_task_state.cancel_scope=self._parent_scope\n   \n   \n   \n   self._restart_cancellation_in_parent()\n   \n   \n   \n   \n   if self._cancel_called and not self._parent_cancellation_is_visible_to_us:\n   \n    while self._pending_uncancellations:\n     self._host_task.uncancel()\n     self._pending_uncancellations -=1\n     \n     \n    cannot_swallow_exc_val=False\n    if exc_val is not None:\n     for exc in iterate_exceptions(exc_val):\n      if isinstance(exc,CancelledError)and is_anyio_cancellation(\n      exc\n      ):\n       self._cancelled_caught=True\n      else:\n       cannot_swallow_exc_val=True\n       \n    return self._cancelled_caught and not cannot_swallow_exc_val\n   else:\n    if self._pending_uncancellations:\n     assert self._parent_scope is not None\n     assert self._parent_scope._pending_uncancellations is not None\n     self._parent_scope._pending_uncancellations +=(\n     self._pending_uncancellations\n     )\n     self._pending_uncancellations=0\n     \n    return False\n  finally:\n   self._host_task=None\n   del exc_val\n   \n @property\n def _effectively_cancelled(self)->bool:\n  cancel_scope:CancelScope |None=self\n  while cancel_scope is not None:\n   if cancel_scope._cancel_called:\n    return True\n    \n   if cancel_scope.shield:\n    return False\n    \n   cancel_scope=cancel_scope._parent_scope\n   \n  return False\n  \n @property\n def _parent_cancellation_is_visible_to_us(self)->bool:\n  return(\n  self._parent_scope is not None\n  and not self.shield\n  and self._parent_scope._effectively_cancelled\n  )\n  \n def _timeout(self)->None:\n  if self._deadline !=math.inf:\n   loop=get_running_loop()\n   if loop.time()>=self._deadline:\n    self.cancel()\n   else:\n    self._timeout_handle=loop.call_at(self._deadline,self._timeout)\n    \n def _deliver_cancellation(self,origin:CancelScope)->bool:\n  ''\n\n\n\n\n\n\n\n\n  \n  should_retry=False\n  current=current_task()\n  for task in self._tasks:\n   should_retry=True\n   if task._must_cancel:\n    continue\n    \n    \n   if task is not current and(task is self._host_task or _task_started(task)):\n    waiter=task._fut_waiter\n    if not isinstance(waiter,asyncio.Future)or not waiter.done():\n     task.cancel(f\"Cancelled by cancel scope {id(origin):x}\")\n     if(\n     task is origin._host_task\n     and origin._pending_uncancellations is not None\n     ):\n      origin._pending_uncancellations +=1\n      \n      \n      \n  for scope in self._child_scopes:\n   if not scope._shield and not scope.cancel_called:\n    should_retry=scope._deliver_cancellation(origin)or should_retry\n    \n    \n  if origin is self:\n   if should_retry:\n    self._cancel_handle=get_running_loop().call_soon(\n    self._deliver_cancellation,origin\n    )\n   else:\n    self._cancel_handle=None\n    \n  return should_retry\n  \n def _restart_cancellation_in_parent(self)->None:\n  ''\n\n\n  \n  scope=self._parent_scope\n  while scope is not None:\n   if scope._cancel_called:\n    if scope._cancel_handle is None:\n     scope._deliver_cancellation(scope)\n     \n    break\n    \n    \n   if scope._shield:\n    break\n    \n   scope=scope._parent_scope\n   \n def cancel(self)->None:\n  if not self._cancel_called:\n   if self._timeout_handle:\n    self._timeout_handle.cancel()\n    self._timeout_handle=None\n    \n   self._cancel_called=True\n   if self._host_task is not None:\n    self._deliver_cancellation(self)\n    \n @property\n def deadline(self)->float:\n  return self._deadline\n  \n @deadline.setter\n def deadline(self,value:float)->None:\n  self._deadline=float(value)\n  if self._timeout_handle is not None:\n   self._timeout_handle.cancel()\n   self._timeout_handle=None\n   \n  if self._active and not self._cancel_called:\n   self._timeout()\n   \n @property\n def cancel_called(self)->bool:\n  return self._cancel_called\n  \n @property\n def cancelled_caught(self)->bool:\n  return self._cancelled_caught\n  \n @property\n def shield(self)->bool:\n  return self._shield\n  \n @shield.setter\n def shield(self,value:bool)->None:\n  if self._shield !=value:\n   self._shield=value\n   if not value:\n    self._restart_cancellation_in_parent()\n    \n    \n    \n    \n    \n    \n    \nclass TaskState:\n ''\n\n\n \n \n __slots__=\"parent_id\",\"cancel_scope\",\"__weakref__\"\n \n def __init__(self,parent_id:int |None,cancel_scope:CancelScope |None):\n  self.parent_id=parent_id\n  self.cancel_scope=cancel_scope\n  \n  \nclass TaskStateStore(MutableMapping[\"Awaitable[Any] | asyncio.Task\",TaskState]):\n def __init__(self)->None:\n  self._task_states=WeakKeyDictionary[asyncio.Task,TaskState]()\n  self._preliminary_task_states:dict[Awaitable[Any],TaskState]={}\n  \n def __getitem__(self,key:Awaitable[Any]|asyncio.Task,/)->TaskState:\n  assert isinstance(key,asyncio.Task)\n  try:\n   return self._task_states[key]\n  except KeyError:\n   if coro :=key.get_coro():\n    if state :=self._preliminary_task_states.get(coro):\n     return state\n     \n  raise KeyError(key)\n  \n def __setitem__(\n self,key:asyncio.Task |Awaitable[Any],value:TaskState,/\n )->None:\n  if isinstance(key,asyncio.Task):\n   self._task_states[key]=value\n  else:\n   self._preliminary_task_states[key]=value\n   \n def __delitem__(self,key:asyncio.Task |Awaitable[Any],/)->None:\n  if isinstance(key,asyncio.Task):\n   del self._task_states[key]\n  else:\n   del self._preliminary_task_states[key]\n   \n def __len__(self)->int:\n  return len(self._task_states)+len(self._preliminary_task_states)\n  \n def __iter__(self)->Iterator[Awaitable[Any]|asyncio.Task]:\n  yield from self._task_states\n  yield from self._preliminary_task_states\n  \n  \n_task_states=TaskStateStore()\n\n\n\n\n\n\n\nclass _AsyncioTaskStatus(abc.TaskStatus):\n def __init__(self,future:asyncio.Future,parent_id:int):\n  self._future=future\n  self._parent_id=parent_id\n  \n def started(self,value:T_contra |None=None)->None:\n  try:\n   self._future.set_result(value)\n  except asyncio.InvalidStateError:\n   if not self._future.cancelled():\n    raise RuntimeError(\n    \"called 'started' twice on the same task status\"\n    )from None\n    \n  task=cast(asyncio.Task,current_task())\n  _task_states[task].parent_id=self._parent_id\n  \n  \nasync def _wait(tasks:Iterable[asyncio.Task[object]])->None:\n tasks=set(tasks)\n waiter=get_running_loop().create_future()\n \n def on_completion(task:asyncio.Task[object])->None:\n  tasks.discard(task)\n  if not tasks and not waiter.done():\n   waiter.set_result(None)\n   \n for task in tasks:\n  task.add_done_callback(on_completion)\n  del task\n  \n try:\n  await waiter\n finally:\n  while tasks:\n   tasks.pop().remove_done_callback(on_completion)\n   \n   \nclass TaskGroup(abc.TaskGroup):\n def __init__(self)->None:\n  self.cancel_scope:CancelScope=CancelScope()\n  self._active=False\n  self._exceptions:list[BaseException]=[]\n  self._tasks:set[asyncio.Task]=set()\n  \n async def __aenter__(self)->TaskGroup:\n  self.cancel_scope.__enter__()\n  self._active=True\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  try:\n   if exc_val is not None:\n    self.cancel_scope.cancel()\n    if not isinstance(exc_val,CancelledError):\n     self._exceptions.append(exc_val)\n     \n   try:\n    if self._tasks:\n     with CancelScope()as wait_scope:\n      while self._tasks:\n       try:\n        await _wait(self._tasks)\n       except CancelledError as exc:\n       \n       \n        wait_scope.shield=True\n        self.cancel_scope.cancel()\n        \n        \n        \n        \n        if exc_val is None or(\n        isinstance(exc_val,CancelledError)\n        and not is_anyio_cancellation(exc)\n        ):\n         exc_val=exc\n    else:\n    \n    \n     await AsyncIOBackend.cancel_shielded_checkpoint()\n     \n    self._active=False\n    if self._exceptions:\n     raise BaseExceptionGroup(\n     \"unhandled errors in a TaskGroup\",self._exceptions\n     )\n    elif exc_val:\n     raise exc_val\n   except BaseException as exc:\n    if self.cancel_scope.__exit__(type(exc),exc,exc.__traceback__):\n     return True\n     \n    raise\n    \n   return self.cancel_scope.__exit__(exc_type,exc_val,exc_tb)\n  finally:\n   del exc_val,exc_tb,self._exceptions\n   \n def _spawn(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[Any]],\n args:tuple[Unpack[PosArgsT]],\n name:object,\n task_status_future:asyncio.Future |None=None,\n )->asyncio.Task:\n  def task_done(_task:asyncio.Task)->None:\n  \n   assert task_state.cancel_scope is not None\n   assert _task in task_state.cancel_scope._tasks\n   task_state.cancel_scope._tasks.remove(_task)\n   self._tasks.remove(task)\n   del _task_states[_task]\n   \n   try:\n    exc=_task.exception()\n   except CancelledError as e:\n    while isinstance(e.__context__,CancelledError):\n     e=e.__context__\n     \n    exc=e\n    \n   if exc is not None:\n   \n   \n   \n    if task_status_future is not None and task_status_future.cancelled():\n     return\n     \n    if task_status_future is None or task_status_future.done():\n     if not isinstance(exc,CancelledError):\n      self._exceptions.append(exc)\n      \n     if not self.cancel_scope._effectively_cancelled:\n      self.cancel_scope.cancel()\n    else:\n     task_status_future.set_exception(exc)\n   elif task_status_future is not None and not task_status_future.done():\n    task_status_future.set_exception(\n    RuntimeError(\"Child exited without calling task_status.started()\")\n    )\n    \n  if not self._active:\n   raise RuntimeError(\n   \"This task group is not active; no new tasks can be started.\"\n   )\n   \n  kwargs={}\n  if task_status_future:\n   parent_id=id(current_task())\n   kwargs[\"task_status\"]=_AsyncioTaskStatus(\n   task_status_future,id(self.cancel_scope._host_task)\n   )\n  else:\n   parent_id=id(self.cancel_scope._host_task)\n   \n  coro=func(*args,**kwargs)\n  if not iscoroutine(coro):\n   prefix=f\"{func.__module__}.\"if hasattr(func,\"__module__\")else \"\"\n   raise TypeError(\n   f\"Expected {prefix}{func.__qualname__}() to return a coroutine, but \"\n   f\"the return value ({coro !r}) is not a coroutine object\"\n   )\n   \n   \n  _task_states[coro]=task_state=TaskState(\n  parent_id=parent_id,cancel_scope=self.cancel_scope\n  )\n  name=get_callable_name(func)if name is None else str(name)\n  try:\n   task=create_task(coro,name=name)\n  finally:\n   del _task_states[coro]\n   \n  _task_states[task]=task_state\n  self.cancel_scope._tasks.add(task)\n  self._tasks.add(task)\n  \n  if task.done():\n  \n   task_done(task)\n  else:\n   task.add_done_callback(task_done)\n   \n  return task\n  \n def start_soon(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[Any]],\n *args:Unpack[PosArgsT],\n name:object=None,\n )->None:\n  self._spawn(func,args,name)\n  \n async def start(\n self,func:Callable[...,Awaitable[Any]],*args:object,name:object=None\n )->Any:\n  future:asyncio.Future=asyncio.Future()\n  task=self._spawn(func,args,name,future)\n  \n  \n  \n  \n  \n  try:\n   return await future\n  except CancelledError:\n  \n   task.cancel()\n   with CancelScope(shield=True),suppress(CancelledError):\n    await task\n    \n   raise\n   \n   \n   \n   \n   \n   \n_Retval_Queue_Type=tuple[Optional[T_Retval],Optional[BaseException]]\n\n\nclass WorkerThread(Thread):\n MAX_IDLE_TIME=10\n \n def __init__(\n self,\n root_task:asyncio.Task,\n workers:set[WorkerThread],\n idle_workers:deque[WorkerThread],\n ):\n  super().__init__(name=\"AnyIO worker thread\")\n  self.root_task=root_task\n  self.workers=workers\n  self.idle_workers=idle_workers\n  self.loop=root_task._loop\n  self.queue:Queue[\n  tuple[Context,Callable,tuple,asyncio.Future,CancelScope]|None\n  ]=Queue(2)\n  self.idle_since=AsyncIOBackend.current_time()\n  self.stopping=False\n  \n def _report_result(\n self,future:asyncio.Future,result:Any,exc:BaseException |None\n )->None:\n  self.idle_since=AsyncIOBackend.current_time()\n  if not self.stopping:\n   self.idle_workers.append(self)\n   \n  if not future.cancelled():\n   if exc is not None:\n    if isinstance(exc,StopIteration):\n     new_exc=RuntimeError(\"coroutine raised StopIteration\")\n     new_exc.__cause__=exc\n     exc=new_exc\n     \n    future.set_exception(exc)\n   else:\n    future.set_result(result)\n    \n def run(self)->None:\n  with claim_worker_thread(AsyncIOBackend,self.loop):\n   while True:\n    item=self.queue.get()\n    if item is None:\n    \n     return\n     \n    context,func,args,future,cancel_scope=item\n    if not future.cancelled():\n     result=None\n     exception:BaseException |None=None\n     threadlocals.current_cancel_scope=cancel_scope\n     try:\n      result=context.run(func,*args)\n     except BaseException as exc:\n      exception=exc\n     finally:\n      del threadlocals.current_cancel_scope\n      \n     if not self.loop.is_closed():\n      self.loop.call_soon_threadsafe(\n      self._report_result,future,result,exception\n      )\n      \n    self.queue.task_done()\n    \n def stop(self,f:asyncio.Task |None=None)->None:\n  self.stopping=True\n  self.queue.put_nowait(None)\n  self.workers.discard(self)\n  try:\n   self.idle_workers.remove(self)\n  except ValueError:\n   pass\n   \n   \n_threadpool_idle_workers:RunVar[deque[WorkerThread]]=RunVar(\n\"_threadpool_idle_workers\"\n)\n_threadpool_workers:RunVar[set[WorkerThread]]=RunVar(\"_threadpool_workers\")\n\n\nclass BlockingPortal(abc.BlockingPortal):\n def __new__(cls)->BlockingPortal:\n  return object.__new__(cls)\n  \n def __init__(self)->None:\n  super().__init__()\n  self._loop=get_running_loop()\n  \n def _spawn_task_from_thread(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]|T_Retval],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n name:object,\n future:Future[T_Retval],\n )->None:\n  AsyncIOBackend.run_sync_from_thread(\n  partial(self._task_group.start_soon,name=name),\n  (self._call_func,func,args,kwargs,future),\n  self._loop,\n  )\n  \n  \n  \n  \n  \n  \n  \n@dataclass(eq=False)\nclass StreamReaderWrapper(abc.ByteReceiveStream):\n _stream:asyncio.StreamReader\n \n async def receive(self,max_bytes:int=65536)->bytes:\n  data=await self._stream.read(max_bytes)\n  if data:\n   return data\n  else:\n   raise EndOfStream\n   \n async def aclose(self)->None:\n  self._stream.set_exception(ClosedResourceError())\n  await AsyncIOBackend.checkpoint()\n  \n  \n@dataclass(eq=False)\nclass StreamWriterWrapper(abc.ByteSendStream):\n _stream:asyncio.StreamWriter\n \n async def send(self,item:bytes)->None:\n  self._stream.write(item)\n  await self._stream.drain()\n  \n async def aclose(self)->None:\n  self._stream.close()\n  await AsyncIOBackend.checkpoint()\n  \n  \n@dataclass(eq=False)\nclass Process(abc.Process):\n _process:asyncio.subprocess.Process\n _stdin:StreamWriterWrapper |None\n _stdout:StreamReaderWrapper |None\n _stderr:StreamReaderWrapper |None\n \n async def aclose(self)->None:\n  with CancelScope(shield=True)as scope:\n   if self._stdin:\n    await self._stdin.aclose()\n   if self._stdout:\n    await self._stdout.aclose()\n   if self._stderr:\n    await self._stderr.aclose()\n    \n   scope.shield=False\n   try:\n    await self.wait()\n   except BaseException:\n    scope.shield=True\n    self.kill()\n    await self.wait()\n    raise\n    \n async def wait(self)->int:\n  return await self._process.wait()\n  \n def terminate(self)->None:\n  self._process.terminate()\n  \n def kill(self)->None:\n  self._process.kill()\n  \n def send_signal(self,signal:int)->None:\n  self._process.send_signal(signal)\n  \n @property\n def pid(self)->int:\n  return self._process.pid\n  \n @property\n def returncode(self)->int |None:\n  return self._process.returncode\n  \n @property\n def stdin(self)->abc.ByteSendStream |None:\n  return self._stdin\n  \n @property\n def stdout(self)->abc.ByteReceiveStream |None:\n  return self._stdout\n  \n @property\n def stderr(self)->abc.ByteReceiveStream |None:\n  return self._stderr\n  \n  \ndef _forcibly_shutdown_process_pool_on_exit(\nworkers:set[Process],_task:object\n)->None:\n ''\n \n child_watcher:asyncio.AbstractChildWatcher |None=None\n if sys.version_info <(3,12):\n  try:\n   child_watcher=asyncio.get_event_loop_policy().get_child_watcher()\n  except NotImplementedError:\n   pass\n   \n   \n for process in workers:\n  if process.returncode is None:\n   continue\n   \n  process._stdin._stream._transport.close()\n  process._stdout._stream._transport.close()\n  process._stderr._stream._transport.close()\n  process.kill()\n  if child_watcher:\n   child_watcher.remove_child_handler(process.pid)\n   \n   \nasync def _shutdown_process_pool_on_exit(workers:set[abc.Process])->None:\n ''\n\n\n\n\n\n \n process:abc.Process\n try:\n  await sleep(math.inf)\n except asyncio.CancelledError:\n  for process in workers:\n   if process.returncode is None:\n    process.kill()\n    \n  for process in workers:\n   await process.aclose()\n   \n   \n   \n   \n   \n   \n   \nclass StreamProtocol(asyncio.Protocol):\n read_queue:deque[bytes]\n read_event:asyncio.Event\n write_event:asyncio.Event\n exception:Exception |None=None\n is_at_eof:bool=False\n \n def connection_made(self,transport:asyncio.BaseTransport)->None:\n  self.read_queue=deque()\n  self.read_event=asyncio.Event()\n  self.write_event=asyncio.Event()\n  self.write_event.set()\n  cast(asyncio.Transport,transport).set_write_buffer_limits(0)\n  \n def connection_lost(self,exc:Exception |None)->None:\n  if exc:\n   self.exception=BrokenResourceError()\n   self.exception.__cause__=exc\n   \n  self.read_event.set()\n  self.write_event.set()\n  \n def data_received(self,data:bytes)->None:\n \n  self.read_queue.append(bytes(data))\n  self.read_event.set()\n  \n def eof_received(self)->bool |None:\n  self.is_at_eof=True\n  self.read_event.set()\n  return True\n  \n def pause_writing(self)->None:\n  self.write_event=asyncio.Event()\n  \n def resume_writing(self)->None:\n  self.write_event.set()\n  \n  \nclass DatagramProtocol(asyncio.DatagramProtocol):\n read_queue:deque[tuple[bytes,IPSockAddrType]]\n read_event:asyncio.Event\n write_event:asyncio.Event\n exception:Exception |None=None\n \n def connection_made(self,transport:asyncio.BaseTransport)->None:\n  self.read_queue=deque(maxlen=100)\n  self.read_event=asyncio.Event()\n  self.write_event=asyncio.Event()\n  self.write_event.set()\n  \n def connection_lost(self,exc:Exception |None)->None:\n  self.read_event.set()\n  self.write_event.set()\n  \n def datagram_received(self,data:bytes,addr:IPSockAddrType)->None:\n  addr=convert_ipv6_sockaddr(addr)\n  self.read_queue.append((data,addr))\n  self.read_event.set()\n  \n def error_received(self,exc:Exception)->None:\n  self.exception=exc\n  \n def pause_writing(self)->None:\n  self.write_event.clear()\n  \n def resume_writing(self)->None:\n  self.write_event.set()\n  \n  \nclass SocketStream(abc.SocketStream):\n def __init__(self,transport:asyncio.Transport,protocol:StreamProtocol):\n  self._transport=transport\n  self._protocol=protocol\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  self._closed=False\n  \n @property\n def _raw_socket(self)->socket.socket:\n  return self._transport.get_extra_info(\"socket\")\n  \n async def receive(self,max_bytes:int=65536)->bytes:\n  with self._receive_guard:\n   if(\n   not self._protocol.read_event.is_set()\n   and not self._transport.is_closing()\n   and not self._protocol.is_at_eof\n   ):\n    self._transport.resume_reading()\n    await self._protocol.read_event.wait()\n    self._transport.pause_reading()\n   else:\n    await AsyncIOBackend.checkpoint()\n    \n   try:\n    chunk=self._protocol.read_queue.popleft()\n   except IndexError:\n    if self._closed:\n     raise ClosedResourceError from None\n    elif self._protocol.exception:\n     raise self._protocol.exception from None\n    else:\n     raise EndOfStream from None\n     \n   if len(chunk)>max_bytes:\n   \n    chunk,leftover=chunk[:max_bytes],chunk[max_bytes:]\n    self._protocol.read_queue.appendleft(leftover)\n    \n    \n    \n   if not self._protocol.read_queue:\n    self._protocol.read_event.clear()\n    \n  return chunk\n  \n async def send(self,item:bytes)->None:\n  with self._send_guard:\n   await AsyncIOBackend.checkpoint()\n   \n   if self._closed:\n    raise ClosedResourceError\n   elif self._protocol.exception is not None:\n    raise self._protocol.exception\n    \n   try:\n    self._transport.write(item)\n   except RuntimeError as exc:\n    if self._transport.is_closing():\n     raise BrokenResourceError from exc\n    else:\n     raise\n     \n   await self._protocol.write_event.wait()\n   \n async def send_eof(self)->None:\n  try:\n   self._transport.write_eof()\n  except OSError:\n   pass\n   \n async def aclose(self)->None:\n  if not self._transport.is_closing():\n   self._closed=True\n   try:\n    self._transport.write_eof()\n   except OSError:\n    pass\n    \n   self._transport.close()\n   await sleep(0)\n   self._transport.abort()\n   \n   \nclass _RawSocketMixin:\n _receive_future:asyncio.Future |None=None\n _send_future:asyncio.Future |None=None\n _closing=False\n \n def __init__(self,raw_socket:socket.socket):\n  self.__raw_socket=raw_socket\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  \n @property\n def _raw_socket(self)->socket.socket:\n  return self.__raw_socket\n  \n def _wait_until_readable(self,loop:asyncio.AbstractEventLoop)->asyncio.Future:\n  def callback(f:object)->None:\n   del self._receive_future\n   loop.remove_reader(self.__raw_socket)\n   \n  f=self._receive_future=asyncio.Future()\n  loop.add_reader(self.__raw_socket,f.set_result,None)\n  f.add_done_callback(callback)\n  return f\n  \n def _wait_until_writable(self,loop:asyncio.AbstractEventLoop)->asyncio.Future:\n  def callback(f:object)->None:\n   del self._send_future\n   loop.remove_writer(self.__raw_socket)\n   \n  f=self._send_future=asyncio.Future()\n  loop.add_writer(self.__raw_socket,f.set_result,None)\n  f.add_done_callback(callback)\n  return f\n  \n async def aclose(self)->None:\n  if not self._closing:\n   self._closing=True\n   if self.__raw_socket.fileno()!=-1:\n    self.__raw_socket.close()\n    \n   if self._receive_future:\n    self._receive_future.set_result(None)\n   if self._send_future:\n    self._send_future.set_result(None)\n    \n    \nclass UNIXSocketStream(_RawSocketMixin,abc.UNIXSocketStream):\n async def send_eof(self)->None:\n  with self._send_guard:\n   self._raw_socket.shutdown(socket.SHUT_WR)\n   \n async def receive(self,max_bytes:int=65536)->bytes:\n  loop=get_running_loop()\n  await AsyncIOBackend.checkpoint()\n  with self._receive_guard:\n   while True:\n    try:\n     data=self._raw_socket.recv(max_bytes)\n    except BlockingIOError:\n     await self._wait_until_readable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     if not data:\n      raise EndOfStream\n      \n     return data\n     \n async def send(self,item:bytes)->None:\n  loop=get_running_loop()\n  await AsyncIOBackend.checkpoint()\n  with self._send_guard:\n   view=memoryview(item)\n   while view:\n    try:\n     bytes_sent=self._raw_socket.send(view)\n    except BlockingIOError:\n     await self._wait_until_writable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     view=view[bytes_sent:]\n     \n async def receive_fds(self,msglen:int,maxfds:int)->tuple[bytes,list[int]]:\n  if not isinstance(msglen,int)or msglen <0:\n   raise ValueError(\"msglen must be a non-negative integer\")\n  if not isinstance(maxfds,int)or maxfds <1:\n   raise ValueError(\"maxfds must be a positive integer\")\n   \n  loop=get_running_loop()\n  fds=array.array(\"i\")\n  await AsyncIOBackend.checkpoint()\n  with self._receive_guard:\n   while True:\n    try:\n     message,ancdata,flags,addr=self._raw_socket.recvmsg(\n     msglen,socket.CMSG_LEN(maxfds *fds.itemsize)\n     )\n    except BlockingIOError:\n     await self._wait_until_readable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     if not message and not ancdata:\n      raise EndOfStream\n      \n     break\n     \n  for cmsg_level,cmsg_type,cmsg_data in ancdata:\n   if cmsg_level !=socket.SOL_SOCKET or cmsg_type !=socket.SCM_RIGHTS:\n    raise RuntimeError(\n    f\"Received unexpected ancillary data; message = {message !r}, \"\n    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n    )\n    \n   fds.frombytes(cmsg_data[:len(cmsg_data)-(len(cmsg_data)%fds.itemsize)])\n   \n  return message,list(fds)\n  \n async def send_fds(self,message:bytes,fds:Collection[int |IOBase])->None:\n  if not message:\n   raise ValueError(\"message must not be empty\")\n  if not fds:\n   raise ValueError(\"fds must not be empty\")\n   \n  loop=get_running_loop()\n  filenos:list[int]=[]\n  for fd in fds:\n   if isinstance(fd,int):\n    filenos.append(fd)\n   elif isinstance(fd,IOBase):\n    filenos.append(fd.fileno())\n    \n  fdarray=array.array(\"i\",filenos)\n  await AsyncIOBackend.checkpoint()\n  with self._send_guard:\n   while True:\n    try:\n    \n    \n     self._raw_socket.sendmsg(\n     [message],[(socket.SOL_SOCKET,socket.SCM_RIGHTS,fdarray)]\n     )\n     break\n    except BlockingIOError:\n     await self._wait_until_writable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n      \n      \nclass TCPSocketListener(abc.SocketListener):\n _accept_scope:CancelScope |None=None\n _closed=False\n \n def __init__(self,raw_socket:socket.socket):\n  self.__raw_socket=raw_socket\n  self._loop=cast(asyncio.BaseEventLoop,get_running_loop())\n  self._accept_guard=ResourceGuard(\"accepting connections from\")\n  \n @property\n def _raw_socket(self)->socket.socket:\n  return self.__raw_socket\n  \n async def accept(self)->abc.SocketStream:\n  if self._closed:\n   raise ClosedResourceError\n   \n  with self._accept_guard:\n   await AsyncIOBackend.checkpoint()\n   with CancelScope()as self._accept_scope:\n    try:\n     client_sock,_addr=await self._loop.sock_accept(self._raw_socket)\n    except asyncio.CancelledError:\n    \n     try:\n      self._loop.remove_reader(self._raw_socket)\n     except(ValueError,NotImplementedError):\n      pass\n      \n     if self._closed:\n      raise ClosedResourceError from None\n      \n     raise\n    finally:\n     self._accept_scope=None\n     \n  client_sock.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)\n  transport,protocol=await self._loop.connect_accepted_socket(\n  StreamProtocol,client_sock\n  )\n  return SocketStream(transport,protocol)\n  \n async def aclose(self)->None:\n  if self._closed:\n   return\n   \n  self._closed=True\n  if self._accept_scope:\n  \n   try:\n    self._loop.remove_reader(self._raw_socket)\n   except(ValueError,NotImplementedError):\n    pass\n    \n   self._accept_scope.cancel()\n   await sleep(0)\n   \n  self._raw_socket.close()\n  \n  \nclass UNIXSocketListener(abc.SocketListener):\n def __init__(self,raw_socket:socket.socket):\n  self.__raw_socket=raw_socket\n  self._loop=get_running_loop()\n  self._accept_guard=ResourceGuard(\"accepting connections from\")\n  self._closed=False\n  \n async def accept(self)->abc.SocketStream:\n  await AsyncIOBackend.checkpoint()\n  with self._accept_guard:\n   while True:\n    try:\n     client_sock,_=self.__raw_socket.accept()\n     client_sock.setblocking(False)\n     return UNIXSocketStream(client_sock)\n    except BlockingIOError:\n     f:asyncio.Future=asyncio.Future()\n     self._loop.add_reader(self.__raw_socket,f.set_result,None)\n     f.add_done_callback(\n     lambda _:self._loop.remove_reader(self.__raw_socket)\n     )\n     await f\n    except OSError as exc:\n     if self._closed:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n      \n async def aclose(self)->None:\n  self._closed=True\n  self.__raw_socket.close()\n  \n @property\n def _raw_socket(self)->socket.socket:\n  return self.__raw_socket\n  \n  \nclass UDPSocket(abc.UDPSocket):\n def __init__(\n self,transport:asyncio.DatagramTransport,protocol:DatagramProtocol\n ):\n  self._transport=transport\n  self._protocol=protocol\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  self._closed=False\n  \n @property\n def _raw_socket(self)->socket.socket:\n  return self._transport.get_extra_info(\"socket\")\n  \n async def aclose(self)->None:\n  if not self._transport.is_closing():\n   self._closed=True\n   self._transport.close()\n   \n async def receive(self)->tuple[bytes,IPSockAddrType]:\n  with self._receive_guard:\n   await AsyncIOBackend.checkpoint()\n   \n   \n   if not self._protocol.read_queue and not self._transport.is_closing():\n    self._protocol.read_event.clear()\n    await self._protocol.read_event.wait()\n    \n   try:\n    return self._protocol.read_queue.popleft()\n   except IndexError:\n    if self._closed:\n     raise ClosedResourceError from None\n    else:\n     raise BrokenResourceError from None\n     \n async def send(self,item:UDPPacketType)->None:\n  with self._send_guard:\n   await AsyncIOBackend.checkpoint()\n   await self._protocol.write_event.wait()\n   if self._closed:\n    raise ClosedResourceError\n   elif self._transport.is_closing():\n    raise BrokenResourceError\n   else:\n    self._transport.sendto(*item)\n    \n    \nclass ConnectedUDPSocket(abc.ConnectedUDPSocket):\n def __init__(\n self,transport:asyncio.DatagramTransport,protocol:DatagramProtocol\n ):\n  self._transport=transport\n  self._protocol=protocol\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  self._closed=False\n  \n @property\n def _raw_socket(self)->socket.socket:\n  return self._transport.get_extra_info(\"socket\")\n  \n async def aclose(self)->None:\n  if not self._transport.is_closing():\n   self._closed=True\n   self._transport.close()\n   \n async def receive(self)->bytes:\n  with self._receive_guard:\n   await AsyncIOBackend.checkpoint()\n   \n   \n   if not self._protocol.read_queue and not self._transport.is_closing():\n    self._protocol.read_event.clear()\n    await self._protocol.read_event.wait()\n    \n   try:\n    packet=self._protocol.read_queue.popleft()\n   except IndexError:\n    if self._closed:\n     raise ClosedResourceError from None\n    else:\n     raise BrokenResourceError from None\n     \n   return packet[0]\n   \n async def send(self,item:bytes)->None:\n  with self._send_guard:\n   await AsyncIOBackend.checkpoint()\n   await self._protocol.write_event.wait()\n   if self._closed:\n    raise ClosedResourceError\n   elif self._transport.is_closing():\n    raise BrokenResourceError\n   else:\n    self._transport.sendto(item)\n    \n    \nclass UNIXDatagramSocket(_RawSocketMixin,abc.UNIXDatagramSocket):\n async def receive(self)->UNIXDatagramPacketType:\n  loop=get_running_loop()\n  await AsyncIOBackend.checkpoint()\n  with self._receive_guard:\n   while True:\n    try:\n     data=self._raw_socket.recvfrom(65536)\n    except BlockingIOError:\n     await self._wait_until_readable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     return data\n     \n async def send(self,item:UNIXDatagramPacketType)->None:\n  loop=get_running_loop()\n  await AsyncIOBackend.checkpoint()\n  with self._send_guard:\n   while True:\n    try:\n     self._raw_socket.sendto(*item)\n    except BlockingIOError:\n     await self._wait_until_writable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     return\n     \n     \nclass ConnectedUNIXDatagramSocket(_RawSocketMixin,abc.ConnectedUNIXDatagramSocket):\n async def receive(self)->bytes:\n  loop=get_running_loop()\n  await AsyncIOBackend.checkpoint()\n  with self._receive_guard:\n   while True:\n    try:\n     data=self._raw_socket.recv(65536)\n    except BlockingIOError:\n     await self._wait_until_readable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     return data\n     \n async def send(self,item:bytes)->None:\n  loop=get_running_loop()\n  await AsyncIOBackend.checkpoint()\n  with self._send_guard:\n   while True:\n    try:\n     self._raw_socket.send(item)\n    except BlockingIOError:\n     await self._wait_until_writable(loop)\n    except OSError as exc:\n     if self._closing:\n      raise ClosedResourceError from None\n     else:\n      raise BrokenResourceError from exc\n    else:\n     return\n     \n     \n_read_events:RunVar[dict[int,asyncio.Event]]=RunVar(\"read_events\")\n_write_events:RunVar[dict[int,asyncio.Event]]=RunVar(\"write_events\")\n\n\n\n\n\n\n\nclass Event(BaseEvent):\n def __new__(cls)->Event:\n  return object.__new__(cls)\n  \n def __init__(self)->None:\n  self._event=asyncio.Event()\n  \n def set(self)->None:\n  self._event.set()\n  \n def is_set(self)->bool:\n  return self._event.is_set()\n  \n async def wait(self)->None:\n  if self.is_set():\n   await AsyncIOBackend.checkpoint()\n  else:\n   await self._event.wait()\n   \n def statistics(self)->EventStatistics:\n  return EventStatistics(len(self._event._waiters))\n  \n  \nclass Lock(BaseLock):\n def __new__(cls,*,fast_acquire:bool=False)->Lock:\n  return object.__new__(cls)\n  \n def __init__(self,*,fast_acquire:bool=False)->None:\n  self._fast_acquire=fast_acquire\n  self._owner_task:asyncio.Task |None=None\n  self._waiters:deque[tuple[asyncio.Task,asyncio.Future]]=deque()\n  \n async def acquire(self)->None:\n  task=cast(asyncio.Task,current_task())\n  if self._owner_task is None and not self._waiters:\n   await AsyncIOBackend.checkpoint_if_cancelled()\n   self._owner_task=task\n   \n   \n   \n   if not self._fast_acquire:\n    try:\n     await AsyncIOBackend.cancel_shielded_checkpoint()\n    except CancelledError:\n     self.release()\n     raise\n     \n   return\n   \n  if self._owner_task ==task:\n   raise RuntimeError(\"Attempted to acquire an already held Lock\")\n   \n  fut:asyncio.Future[None]=asyncio.Future()\n  item=task,fut\n  self._waiters.append(item)\n  try:\n   await fut\n  except CancelledError:\n   self._waiters.remove(item)\n   if self._owner_task is task:\n    self.release()\n    \n   raise\n   \n  self._waiters.remove(item)\n  \n def acquire_nowait(self)->None:\n  task=cast(asyncio.Task,current_task())\n  if self._owner_task is None and not self._waiters:\n   self._owner_task=task\n   return\n   \n  if self._owner_task is task:\n   raise RuntimeError(\"Attempted to acquire an already held Lock\")\n   \n  raise WouldBlock\n  \n def locked(self)->bool:\n  return self._owner_task is not None\n  \n def release(self)->None:\n  if self._owner_task !=current_task():\n   raise RuntimeError(\"The current task is not holding this lock\")\n   \n  for task,fut in self._waiters:\n   if not fut.cancelled():\n    self._owner_task=task\n    fut.set_result(None)\n    return\n    \n  self._owner_task=None\n  \n def statistics(self)->LockStatistics:\n  task_info=AsyncIOTaskInfo(self._owner_task)if self._owner_task else None\n  return LockStatistics(self.locked(),task_info,len(self._waiters))\n  \n  \nclass Semaphore(BaseSemaphore):\n def __new__(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->Semaphore:\n  return object.__new__(cls)\n  \n def __init__(\n self,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n ):\n  super().__init__(initial_value,max_value=max_value)\n  self._value=initial_value\n  self._max_value=max_value\n  self._fast_acquire=fast_acquire\n  self._waiters:deque[asyncio.Future[None]]=deque()\n  \n async def acquire(self)->None:\n  if self._value >0 and not self._waiters:\n   await AsyncIOBackend.checkpoint_if_cancelled()\n   self._value -=1\n   \n   \n   \n   if not self._fast_acquire:\n    try:\n     await AsyncIOBackend.cancel_shielded_checkpoint()\n    except CancelledError:\n     self.release()\n     raise\n     \n   return\n   \n  fut:asyncio.Future[None]=asyncio.Future()\n  self._waiters.append(fut)\n  try:\n   await fut\n  except CancelledError:\n   try:\n    self._waiters.remove(fut)\n   except ValueError:\n    self.release()\n    \n   raise\n   \n def acquire_nowait(self)->None:\n  if self._value ==0:\n   raise WouldBlock\n   \n  self._value -=1\n  \n def release(self)->None:\n  if self._max_value is not None and self._value ==self._max_value:\n   raise ValueError(\"semaphore released too many times\")\n   \n  for fut in self._waiters:\n   if not fut.cancelled():\n    fut.set_result(None)\n    self._waiters.remove(fut)\n    return\n    \n  self._value +=1\n  \n @property\n def value(self)->int:\n  return self._value\n  \n @property\n def max_value(self)->int |None:\n  return self._max_value\n  \n def statistics(self)->SemaphoreStatistics:\n  return SemaphoreStatistics(len(self._waiters))\n  \n  \nclass CapacityLimiter(BaseCapacityLimiter):\n _total_tokens:float=0\n \n def __new__(cls,total_tokens:float)->CapacityLimiter:\n  return object.__new__(cls)\n  \n def __init__(self,total_tokens:float):\n  self._borrowers:set[Any]=set()\n  self._wait_queue:OrderedDict[Any,asyncio.Event]=OrderedDict()\n  self.total_tokens=total_tokens\n  \n async def __aenter__(self)->None:\n  await self.acquire()\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.release()\n  \n @property\n def total_tokens(self)->float:\n  return self._total_tokens\n  \n @total_tokens.setter\n def total_tokens(self,value:float)->None:\n  if not isinstance(value,int)and not math.isinf(value):\n   raise TypeError(\"total_tokens must be an int or math.inf\")\n  if value <1:\n   raise ValueError(\"total_tokens must be >= 1\")\n   \n  waiters_to_notify=max(value -self._total_tokens,0)\n  self._total_tokens=value\n  \n  \n  while self._wait_queue and waiters_to_notify:\n   event=self._wait_queue.popitem(last=False)[1]\n   event.set()\n   waiters_to_notify -=1\n   \n @property\n def borrowed_tokens(self)->int:\n  return len(self._borrowers)\n  \n @property\n def available_tokens(self)->float:\n  return self._total_tokens -len(self._borrowers)\n  \n def acquire_nowait(self)->None:\n  self.acquire_on_behalf_of_nowait(current_task())\n  \n def acquire_on_behalf_of_nowait(self,borrower:object)->None:\n  if borrower in self._borrowers:\n   raise RuntimeError(\n   \"this borrower is already holding one of this CapacityLimiter's \"\n   \"tokens\"\n   )\n   \n  if self._wait_queue or len(self._borrowers)>=self._total_tokens:\n   raise WouldBlock\n   \n  self._borrowers.add(borrower)\n  \n async def acquire(self)->None:\n  return await self.acquire_on_behalf_of(current_task())\n  \n async def acquire_on_behalf_of(self,borrower:object)->None:\n  await AsyncIOBackend.checkpoint_if_cancelled()\n  try:\n   self.acquire_on_behalf_of_nowait(borrower)\n  except WouldBlock:\n   event=asyncio.Event()\n   self._wait_queue[borrower]=event\n   try:\n    await event.wait()\n   except BaseException:\n    self._wait_queue.pop(borrower,None)\n    raise\n    \n   self._borrowers.add(borrower)\n  else:\n   try:\n    await AsyncIOBackend.cancel_shielded_checkpoint()\n   except BaseException:\n    self.release()\n    raise\n    \n def release(self)->None:\n  self.release_on_behalf_of(current_task())\n  \n def release_on_behalf_of(self,borrower:object)->None:\n  try:\n   self._borrowers.remove(borrower)\n  except KeyError:\n   raise RuntimeError(\n   \"this borrower isn't holding any of this CapacityLimiter's tokens\"\n   )from None\n   \n   \n  if self._wait_queue and len(self._borrowers)<self._total_tokens:\n   event=self._wait_queue.popitem(last=False)[1]\n   event.set()\n   \n def statistics(self)->CapacityLimiterStatistics:\n  return CapacityLimiterStatistics(\n  self.borrowed_tokens,\n  self.total_tokens,\n  tuple(self._borrowers),\n  len(self._wait_queue),\n  )\n  \n  \n_default_thread_limiter:RunVar[CapacityLimiter]=RunVar(\"_default_thread_limiter\")\n\n\n\n\n\n\n\nclass _SignalReceiver:\n def __init__(self,signals:tuple[Signals,...]):\n  self._signals=signals\n  self._loop=get_running_loop()\n  self._signal_queue:deque[Signals]=deque()\n  self._future:asyncio.Future=asyncio.Future()\n  self._handled_signals:set[Signals]=set()\n  \n def _deliver(self,signum:Signals)->None:\n  self._signal_queue.append(signum)\n  if not self._future.done():\n   self._future.set_result(None)\n   \n def __enter__(self)->_SignalReceiver:\n  for sig in set(self._signals):\n   self._loop.add_signal_handler(sig,self._deliver,sig)\n   self._handled_signals.add(sig)\n   \n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  for sig in self._handled_signals:\n   self._loop.remove_signal_handler(sig)\n  return None\n  \n def __aiter__(self)->_SignalReceiver:\n  return self\n  \n async def __anext__(self)->Signals:\n  await AsyncIOBackend.checkpoint()\n  if not self._signal_queue:\n   self._future=asyncio.Future()\n   await self._future\n   \n  return self._signal_queue.popleft()\n  \n  \n  \n  \n  \n  \n  \nclass AsyncIOTaskInfo(TaskInfo):\n def __init__(self,task:asyncio.Task):\n  task_state=_task_states.get(task)\n  if task_state is None:\n   parent_id=None\n  else:\n   parent_id=task_state.parent_id\n   \n  coro=task.get_coro()\n  assert coro is not None,\"created TaskInfo from a completed Task\"\n  super().__init__(id(task),parent_id,task.get_name(),coro)\n  self._task=weakref.ref(task)\n  \n def has_pending_cancellation(self)->bool:\n  if not(task :=self._task()):\n  \n   return False\n   \n  if task._must_cancel:\n   return True\n  elif(\n  isinstance(task._fut_waiter,asyncio.Future)\n  and task._fut_waiter.cancelled()\n  ):\n   return True\n   \n  if task_state :=_task_states.get(task):\n   if cancel_scope :=task_state.cancel_scope:\n    return cancel_scope._effectively_cancelled\n    \n  return False\n  \n  \nclass TestRunner(abc.TestRunner):\n _send_stream:MemoryObjectSendStream[tuple[Awaitable[Any],asyncio.Future[Any]]]\n \n def __init__(\n self,\n *,\n debug:bool |None=None,\n use_uvloop:bool=False,\n loop_factory:Callable[[],AbstractEventLoop]|None=None,\n )->None:\n  if use_uvloop and loop_factory is None:\n   import uvloop\n   \n   loop_factory=uvloop.new_event_loop\n   \n  self._runner=Runner(debug=debug,loop_factory=loop_factory)\n  self._exceptions:list[BaseException]=[]\n  self._runner_task:asyncio.Task |None=None\n  \n def __enter__(self)->TestRunner:\n  self._runner.__enter__()\n  self.get_loop().set_exception_handler(self._exception_handler)\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self._runner.__exit__(exc_type,exc_val,exc_tb)\n  \n def get_loop(self)->AbstractEventLoop:\n  return self._runner.get_loop()\n  \n def _exception_handler(\n self,loop:asyncio.AbstractEventLoop,context:dict[str,Any]\n )->None:\n  if isinstance(context.get(\"exception\"),Exception):\n   self._exceptions.append(context[\"exception\"])\n  else:\n   loop.default_exception_handler(context)\n   \n def _raise_async_exceptions(self)->None:\n \n  if self._exceptions:\n   exceptions,self._exceptions=self._exceptions,[]\n   if len(exceptions)==1:\n    raise exceptions[0]\n   elif exceptions:\n    raise BaseExceptionGroup(\n    \"Multiple exceptions occurred in asynchronous callbacks\",exceptions\n    )\n    \n async def _run_tests_and_fixtures(\n self,\n receive_stream:MemoryObjectReceiveStream[\n tuple[Awaitable[T_Retval],asyncio.Future[T_Retval]]\n ],\n )->None:\n  from _pytest.outcomes import OutcomeException\n  \n  with receive_stream,self._send_stream:\n   async for coro,future in receive_stream:\n    try:\n     retval=await coro\n    except CancelledError as exc:\n     if not future.cancelled():\n      future.cancel(*exc.args)\n      \n     raise\n    except BaseException as exc:\n     if not future.cancelled():\n      future.set_exception(exc)\n      \n     if not isinstance(exc,(Exception,OutcomeException)):\n      raise\n    else:\n     if not future.cancelled():\n      future.set_result(retval)\n      \n async def _call_in_runner_task(\n self,\n func:Callable[P,Awaitable[T_Retval]],\n *args:P.args,\n **kwargs:P.kwargs,\n )->T_Retval:\n  if not self._runner_task:\n   self._send_stream,receive_stream=create_memory_object_stream[\n   tuple[Awaitable[Any],asyncio.Future]\n   ](1)\n   self._runner_task=self.get_loop().create_task(\n   self._run_tests_and_fixtures(receive_stream)\n   )\n   \n  coro=func(*args,**kwargs)\n  future:asyncio.Future[T_Retval]=self.get_loop().create_future()\n  self._send_stream.send_nowait((coro,future))\n  return await future\n  \n def run_asyncgen_fixture(\n self,\n fixture_func:Callable[...,AsyncGenerator[T_Retval,Any]],\n kwargs:dict[str,Any],\n )->Iterable[T_Retval]:\n  asyncgen=fixture_func(**kwargs)\n  fixturevalue:T_Retval=self.get_loop().run_until_complete(\n  self._call_in_runner_task(asyncgen.asend,None)\n  )\n  self._raise_async_exceptions()\n  \n  yield fixturevalue\n  \n  try:\n   self.get_loop().run_until_complete(\n   self._call_in_runner_task(asyncgen.asend,None)\n   )\n  except StopAsyncIteration:\n   self._raise_async_exceptions()\n  else:\n   self.get_loop().run_until_complete(asyncgen.aclose())\n   raise RuntimeError(\"Async generator fixture did not stop\")\n   \n def run_fixture(\n self,\n fixture_func:Callable[...,Coroutine[Any,Any,T_Retval]],\n kwargs:dict[str,Any],\n )->T_Retval:\n  retval=self.get_loop().run_until_complete(\n  self._call_in_runner_task(fixture_func,**kwargs)\n  )\n  self._raise_async_exceptions()\n  return retval\n  \n def run_test(\n self,test_func:Callable[...,Coroutine[Any,Any,Any]],kwargs:dict[str,Any]\n )->None:\n  try:\n   self.get_loop().run_until_complete(\n   self._call_in_runner_task(test_func,**kwargs)\n   )\n  except Exception as exc:\n   self._exceptions.append(exc)\n   \n  self._raise_async_exceptions()\n  \n  \nclass AsyncIOBackend(AsyncBackend):\n @classmethod\n def run(\n cls,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n options:dict[str,Any],\n )->T_Retval:\n  @wraps(func)\n  async def wrapper()->T_Retval:\n   task=cast(asyncio.Task,current_task())\n   task.set_name(get_callable_name(func))\n   _task_states[task]=TaskState(None,None)\n   \n   try:\n    return await func(*args)\n   finally:\n    del _task_states[task]\n    \n  debug=options.get(\"debug\",None)\n  loop_factory=options.get(\"loop_factory\",None)\n  if loop_factory is None and options.get(\"use_uvloop\",False):\n   import uvloop\n   \n   loop_factory=uvloop.new_event_loop\n   \n  with Runner(debug=debug,loop_factory=loop_factory)as runner:\n   return runner.run(wrapper())\n   \n @classmethod\n def current_token(cls)->object:\n  return get_running_loop()\n  \n @classmethod\n def current_time(cls)->float:\n  return get_running_loop().time()\n  \n @classmethod\n def cancelled_exception_class(cls)->type[BaseException]:\n  return CancelledError\n  \n @classmethod\n async def checkpoint(cls)->None:\n  await sleep(0)\n  \n @classmethod\n async def checkpoint_if_cancelled(cls)->None:\n  task=current_task()\n  if task is None:\n   return\n   \n  try:\n   cancel_scope=_task_states[task].cancel_scope\n  except KeyError:\n   return\n   \n  while cancel_scope:\n   if cancel_scope.cancel_called:\n    await sleep(0)\n   elif cancel_scope.shield:\n    break\n   else:\n    cancel_scope=cancel_scope._parent_scope\n    \n @classmethod\n async def cancel_shielded_checkpoint(cls)->None:\n  with CancelScope(shield=True):\n   await sleep(0)\n   \n @classmethod\n async def sleep(cls,delay:float)->None:\n  await sleep(delay)\n  \n @classmethod\n def create_cancel_scope(\n cls,*,deadline:float=math.inf,shield:bool=False\n )->CancelScope:\n  return CancelScope(deadline=deadline,shield=shield)\n  \n @classmethod\n def current_effective_deadline(cls)->float:\n  if(task :=current_task())is None:\n   return math.inf\n   \n  try:\n   cancel_scope=_task_states[task].cancel_scope\n  except KeyError:\n   return math.inf\n   \n  deadline=math.inf\n  while cancel_scope:\n   deadline=min(deadline,cancel_scope.deadline)\n   if cancel_scope._cancel_called:\n    deadline=-math.inf\n    break\n   elif cancel_scope.shield:\n    break\n   else:\n    cancel_scope=cancel_scope._parent_scope\n    \n  return deadline\n  \n @classmethod\n def create_task_group(cls)->abc.TaskGroup:\n  return TaskGroup()\n  \n @classmethod\n def create_event(cls)->abc.Event:\n  return Event()\n  \n @classmethod\n def create_lock(cls,*,fast_acquire:bool)->abc.Lock:\n  return Lock(fast_acquire=fast_acquire)\n  \n @classmethod\n def create_semaphore(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->abc.Semaphore:\n  return Semaphore(initial_value,max_value=max_value,fast_acquire=fast_acquire)\n  \n @classmethod\n def create_capacity_limiter(cls,total_tokens:float)->abc.CapacityLimiter:\n  return CapacityLimiter(total_tokens)\n  \n @classmethod\n async def run_sync_in_worker_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n args:tuple[Unpack[PosArgsT]],\n abandon_on_cancel:bool=False,\n limiter:abc.CapacityLimiter |None=None,\n )->T_Retval:\n  await cls.checkpoint()\n  \n  \n  \n  try:\n   idle_workers=_threadpool_idle_workers.get()\n   workers=_threadpool_workers.get()\n  except LookupError:\n   idle_workers=deque()\n   workers=set()\n   _threadpool_idle_workers.set(idle_workers)\n   _threadpool_workers.set(workers)\n   \n  async with limiter or cls.current_default_thread_limiter():\n   with CancelScope(shield=not abandon_on_cancel)as scope:\n    future:asyncio.Future=asyncio.Future()\n    root_task=find_root_task()\n    if not idle_workers:\n     worker=WorkerThread(root_task,workers,idle_workers)\n     worker.start()\n     workers.add(worker)\n     root_task.add_done_callback(worker.stop)\n    else:\n     worker=idle_workers.pop()\n     \n     \n     \n     now=cls.current_time()\n     while idle_workers:\n      if(\n      now -idle_workers[0].idle_since\n      <WorkerThread.MAX_IDLE_TIME\n      ):\n       break\n       \n      expired_worker=idle_workers.popleft()\n      expired_worker.root_task.remove_done_callback(\n      expired_worker.stop\n      )\n      expired_worker.stop()\n      \n    context=copy_context()\n    context.run(sniffio.current_async_library_cvar.set,None)\n    if abandon_on_cancel or scope._parent_scope is None:\n     worker_scope=scope\n    else:\n     worker_scope=scope._parent_scope\n     \n    worker.queue.put_nowait((context,func,args,future,worker_scope))\n    return await future\n    \n @classmethod\n def check_cancelled(cls)->None:\n  scope:CancelScope |None=threadlocals.current_cancel_scope\n  while scope is not None:\n   if scope.cancel_called:\n    raise CancelledError(f\"Cancelled by cancel scope {id(scope):x}\")\n    \n   if scope.shield:\n    return\n    \n   scope=scope._parent_scope\n   \n @classmethod\n def run_async_from_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n args:tuple[Unpack[PosArgsT]],\n token:object,\n )->T_Retval:\n  async def task_wrapper(scope:CancelScope)->T_Retval:\n   __tracebackhide__=True\n   task=cast(asyncio.Task,current_task())\n   _task_states[task]=TaskState(None,scope)\n   scope._tasks.add(task)\n   try:\n    return await func(*args)\n   except CancelledError as exc:\n    raise concurrent.futures.CancelledError(str(exc))from None\n   finally:\n    scope._tasks.discard(task)\n    \n  loop=cast(AbstractEventLoop,token)\n  context=copy_context()\n  context.run(sniffio.current_async_library_cvar.set,\"asyncio\")\n  wrapper=task_wrapper(threadlocals.current_cancel_scope)\n  f:concurrent.futures.Future[T_Retval]=context.run(\n  asyncio.run_coroutine_threadsafe,wrapper,loop\n  )\n  return f.result()\n  \n @classmethod\n def run_sync_from_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n args:tuple[Unpack[PosArgsT]],\n token:object,\n )->T_Retval:\n  @wraps(func)\n  def wrapper()->None:\n   try:\n    sniffio.current_async_library_cvar.set(\"asyncio\")\n    f.set_result(func(*args))\n   except BaseException as exc:\n    f.set_exception(exc)\n    if not isinstance(exc,Exception):\n     raise\n     \n  f:concurrent.futures.Future[T_Retval]=Future()\n  loop=cast(AbstractEventLoop,token)\n  loop.call_soon_threadsafe(wrapper)\n  return f.result()\n  \n @classmethod\n def create_blocking_portal(cls)->abc.BlockingPortal:\n  return BlockingPortal()\n  \n @classmethod\n async def open_process(\n cls,\n command:StrOrBytesPath |Sequence[StrOrBytesPath],\n *,\n stdin:int |IO[Any]|None,\n stdout:int |IO[Any]|None,\n stderr:int |IO[Any]|None,\n **kwargs:Any,\n )->Process:\n  await cls.checkpoint()\n  if isinstance(command,PathLike):\n   command=os.fspath(command)\n   \n  if isinstance(command,(str,bytes)):\n   process=await asyncio.create_subprocess_shell(\n   command,\n   stdin=stdin,\n   stdout=stdout,\n   stderr=stderr,\n   **kwargs,\n   )\n  else:\n   process=await asyncio.create_subprocess_exec(\n   *command,\n   stdin=stdin,\n   stdout=stdout,\n   stderr=stderr,\n   **kwargs,\n   )\n   \n  stdin_stream=StreamWriterWrapper(process.stdin)if process.stdin else None\n  stdout_stream=StreamReaderWrapper(process.stdout)if process.stdout else None\n  stderr_stream=StreamReaderWrapper(process.stderr)if process.stderr else None\n  return Process(process,stdin_stream,stdout_stream,stderr_stream)\n  \n @classmethod\n def setup_process_pool_exit_at_shutdown(cls,workers:set[abc.Process])->None:\n  create_task(\n  _shutdown_process_pool_on_exit(workers),\n  name=\"AnyIO process pool shutdown task\",\n  )\n  find_root_task().add_done_callback(\n  partial(_forcibly_shutdown_process_pool_on_exit,workers)\n  )\n  \n @classmethod\n async def connect_tcp(\n cls,host:str,port:int,local_address:IPSockAddrType |None=None\n )->abc.SocketStream:\n  transport,protocol=cast(\n  tuple[asyncio.Transport,StreamProtocol],\n  await get_running_loop().create_connection(\n  StreamProtocol,host,port,local_addr=local_address\n  ),\n  )\n  transport.pause_reading()\n  return SocketStream(transport,protocol)\n  \n @classmethod\n async def connect_unix(cls,path:str |bytes)->abc.UNIXSocketStream:\n  await cls.checkpoint()\n  loop=get_running_loop()\n  raw_socket=socket.socket(socket.AF_UNIX)\n  raw_socket.setblocking(False)\n  while True:\n   try:\n    raw_socket.connect(path)\n   except BlockingIOError:\n    f:asyncio.Future=asyncio.Future()\n    loop.add_writer(raw_socket,f.set_result,None)\n    f.add_done_callback(lambda _:loop.remove_writer(raw_socket))\n    await f\n   except BaseException:\n    raw_socket.close()\n    raise\n   else:\n    return UNIXSocketStream(raw_socket)\n    \n @classmethod\n def create_tcp_listener(cls,sock:socket.socket)->SocketListener:\n  return TCPSocketListener(sock)\n  \n @classmethod\n def create_unix_listener(cls,sock:socket.socket)->SocketListener:\n  return UNIXSocketListener(sock)\n  \n @classmethod\n async def create_udp_socket(\n cls,\n family:AddressFamily,\n local_address:IPSockAddrType |None,\n remote_address:IPSockAddrType |None,\n reuse_port:bool,\n )->UDPSocket |ConnectedUDPSocket:\n  transport,protocol=await get_running_loop().create_datagram_endpoint(\n  DatagramProtocol,\n  local_addr=local_address,\n  remote_addr=remote_address,\n  family=family,\n  reuse_port=reuse_port,\n  )\n  if protocol.exception:\n   transport.close()\n   raise protocol.exception\n   \n  if not remote_address:\n   return UDPSocket(transport,protocol)\n  else:\n   return ConnectedUDPSocket(transport,protocol)\n   \n @classmethod\n async def create_unix_datagram_socket(\n cls,raw_socket:socket.socket,remote_path:str |bytes |None\n )->abc.UNIXDatagramSocket |abc.ConnectedUNIXDatagramSocket:\n  await cls.checkpoint()\n  loop=get_running_loop()\n  \n  if remote_path:\n   while True:\n    try:\n     raw_socket.connect(remote_path)\n    except BlockingIOError:\n     f:asyncio.Future=asyncio.Future()\n     loop.add_writer(raw_socket,f.set_result,None)\n     f.add_done_callback(lambda _:loop.remove_writer(raw_socket))\n     await f\n    except BaseException:\n     raw_socket.close()\n     raise\n    else:\n     return ConnectedUNIXDatagramSocket(raw_socket)\n  else:\n   return UNIXDatagramSocket(raw_socket)\n   \n @classmethod\n async def getaddrinfo(\n cls,\n host:bytes |str |None,\n port:str |int |None,\n *,\n family:int |AddressFamily=0,\n type:int |SocketKind=0,\n proto:int=0,\n flags:int=0,\n )->list[\n tuple[\n AddressFamily,\n SocketKind,\n int,\n str,\n tuple[str,int]|tuple[str,int,int,int],\n ]\n ]:\n  return await get_running_loop().getaddrinfo(\n  host,port,family=family,type=type,proto=proto,flags=flags\n  )\n  \n @classmethod\n async def getnameinfo(\n cls,sockaddr:IPSockAddrType,flags:int=0\n )->tuple[str,str]:\n  return await get_running_loop().getnameinfo(sockaddr,flags)\n  \n @classmethod\n async def wait_readable(cls,obj:FileDescriptorLike)->None:\n  await cls.checkpoint()\n  try:\n   read_events=_read_events.get()\n  except LookupError:\n   read_events={}\n   _read_events.set(read_events)\n   \n  if not isinstance(obj,int):\n   obj=obj.fileno()\n   \n  if read_events.get(obj):\n   raise BusyResourceError(\"reading from\")\n   \n  loop=get_running_loop()\n  event=asyncio.Event()\n  try:\n   loop.add_reader(obj,event.set)\n  except NotImplementedError:\n   from anyio._core._asyncio_selector_thread import get_selector\n   \n   selector=get_selector()\n   selector.add_reader(obj,event.set)\n   remove_reader=selector.remove_reader\n  else:\n   remove_reader=loop.remove_reader\n   \n  read_events[obj]=event\n  try:\n   await event.wait()\n  finally:\n   remove_reader(obj)\n   del read_events[obj]\n   \n @classmethod\n async def wait_writable(cls,obj:FileDescriptorLike)->None:\n  await cls.checkpoint()\n  try:\n   write_events=_write_events.get()\n  except LookupError:\n   write_events={}\n   _write_events.set(write_events)\n   \n  if not isinstance(obj,int):\n   obj=obj.fileno()\n   \n  if write_events.get(obj):\n   raise BusyResourceError(\"writing to\")\n   \n  loop=get_running_loop()\n  event=asyncio.Event()\n  try:\n   loop.add_writer(obj,event.set)\n  except NotImplementedError:\n   from anyio._core._asyncio_selector_thread import get_selector\n   \n   selector=get_selector()\n   selector.add_writer(obj,event.set)\n   remove_writer=selector.remove_writer\n  else:\n   remove_writer=loop.remove_writer\n   \n  write_events[obj]=event\n  try:\n   await event.wait()\n  finally:\n   del write_events[obj]\n   remove_writer(obj)\n   \n @classmethod\n def current_default_thread_limiter(cls)->CapacityLimiter:\n  try:\n   return _default_thread_limiter.get()\n  except LookupError:\n   limiter=CapacityLimiter(40)\n   _default_thread_limiter.set(limiter)\n   return limiter\n   \n @classmethod\n def open_signal_receiver(\n cls,*signals:Signals\n )->AbstractContextManager[AsyncIterator[Signals]]:\n  return _SignalReceiver(signals)\n  \n @classmethod\n def get_current_task(cls)->TaskInfo:\n  return AsyncIOTaskInfo(current_task())\n  \n @classmethod\n def get_running_tasks(cls)->Sequence[TaskInfo]:\n  return[AsyncIOTaskInfo(task)for task in all_tasks()if not task.done()]\n  \n @classmethod\n async def wait_all_tasks_blocked(cls)->None:\n  await cls.checkpoint()\n  this_task=current_task()\n  while True:\n   for task in all_tasks():\n    if task is this_task:\n     continue\n     \n    waiter=task._fut_waiter\n    if waiter is None or waiter.done():\n     await sleep(0.1)\n     break\n   else:\n    return\n    \n @classmethod\n def create_test_runner(cls,options:dict[str,Any])->TestRunner:\n  return TestRunner(**options)\n  \n  \nbackend_class=AsyncIOBackend\n", ["__future__", "_pytest.outcomes", "_typeshed", "anyio", "anyio._core._asyncio_selector_thread", "anyio._core._eventloop", "anyio._core._exceptions", "anyio._core._sockets", "anyio._core._streams", "anyio._core._synchronization", "anyio._core._tasks", "anyio.abc", "anyio.abc._eventloop", "anyio.lowlevel", "anyio.streams.memory", "array", "asyncio", "asyncio.base_events", "collections", "collections.abc", "concurrent.futures", "contextlib", "contextvars", "dataclasses", "enum", "exceptiongroup", "functools", "inspect", "io", "math", "os", "queue", "signal", "sniffio", "socket", "sys", "threading", "types", "typing", "typing_extensions", "uvloop", "weakref"]], "anyio._backends._trio": [".py", "from __future__ import annotations\n\nimport array\nimport math\nimport os\nimport socket\nimport sys\nimport types\nimport weakref\nfrom collections.abc import(\nAsyncGenerator,\nAsyncIterator,\nAwaitable,\nCallable,\nCollection,\nCoroutine,\nIterable,\nSequence,\n)\nfrom concurrent.futures import Future\nfrom contextlib import AbstractContextManager\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom io import IOBase\nfrom os import PathLike\nfrom signal import Signals\nfrom socket import AddressFamily,SocketKind\nfrom types import TracebackType\nfrom typing import(\nIO,\nTYPE_CHECKING,\nAny,\nGeneric,\nNoReturn,\nTypeVar,\ncast,\noverload,\n)\n\nimport trio.from_thread\nimport trio.lowlevel\nfrom outcome import Error,Outcome,Value\nfrom trio.lowlevel import(\ncurrent_root_task,\ncurrent_task,\nwait_readable,\nwait_writable,\n)\nfrom trio.socket import SocketType as TrioSocketType\nfrom trio.to_thread import run_sync\n\nfrom.. import(\nCapacityLimiterStatistics,\nEventStatistics,\nLockStatistics,\nTaskInfo,\nWouldBlock,\nabc,\n)\nfrom.._core._eventloop import claim_worker_thread\nfrom.._core._exceptions import(\nBrokenResourceError,\nBusyResourceError,\nClosedResourceError,\nEndOfStream,\n)\nfrom.._core._sockets import convert_ipv6_sockaddr\nfrom.._core._streams import create_memory_object_stream\nfrom.._core._synchronization import(\nCapacityLimiter as BaseCapacityLimiter,\n)\nfrom.._core._synchronization import Event as BaseEvent\nfrom.._core._synchronization import Lock as BaseLock\nfrom.._core._synchronization import(\nResourceGuard,\nSemaphoreStatistics,\n)\nfrom.._core._synchronization import Semaphore as BaseSemaphore\nfrom.._core._tasks import CancelScope as BaseCancelScope\nfrom..abc import IPSockAddrType,UDPPacketType,UNIXDatagramPacketType\nfrom..abc._eventloop import AsyncBackend,StrOrBytesPath\nfrom..streams.memory import MemoryObjectSendStream\n\nif TYPE_CHECKING:\n from _typeshed import HasFileno\n \nif sys.version_info >=(3,10):\n from typing import ParamSpec\nelse:\n from typing_extensions import ParamSpec\n \nif sys.version_info >=(3,11):\n from typing import TypeVarTuple,Unpack\nelse:\n from exceptiongroup import BaseExceptionGroup\n from typing_extensions import TypeVarTuple,Unpack\n \nT=TypeVar(\"T\")\nT_Retval=TypeVar(\"T_Retval\")\nT_SockAddr=TypeVar(\"T_SockAddr\",str,IPSockAddrType)\nPosArgsT=TypeVarTuple(\"PosArgsT\")\nP=ParamSpec(\"P\")\n\n\n\n\n\n\nRunVar=trio.lowlevel.RunVar\n\n\n\n\n\n\n\nclass CancelScope(BaseCancelScope):\n def __new__(\n cls,original:trio.CancelScope |None=None,**kwargs:object\n )->CancelScope:\n  return object.__new__(cls)\n  \n def __init__(self,original:trio.CancelScope |None=None,**kwargs:Any)->None:\n  self.__original=original or trio.CancelScope(**kwargs)\n  \n def __enter__(self)->CancelScope:\n  self.__original.__enter__()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n \n  return self.__original.__exit__(exc_type,exc_val,exc_tb)\n  \n def cancel(self)->None:\n  self.__original.cancel()\n  \n @property\n def deadline(self)->float:\n  return self.__original.deadline\n  \n @deadline.setter\n def deadline(self,value:float)->None:\n  self.__original.deadline=value\n  \n @property\n def cancel_called(self)->bool:\n  return self.__original.cancel_called\n  \n @property\n def cancelled_caught(self)->bool:\n  return self.__original.cancelled_caught\n  \n @property\n def shield(self)->bool:\n  return self.__original.shield\n  \n @shield.setter\n def shield(self,value:bool)->None:\n  self.__original.shield=value\n  \n  \n  \n  \n  \n  \n  \nclass TaskGroup(abc.TaskGroup):\n def __init__(self)->None:\n  self._active=False\n  self._nursery_manager=trio.open_nursery(strict_exception_groups=True)\n  self.cancel_scope=None\n  \n async def __aenter__(self)->TaskGroup:\n  self._active=True\n  self._nursery=await self._nursery_manager.__aenter__()\n  self.cancel_scope=CancelScope(self._nursery.cancel_scope)\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  try:\n   return await self._nursery_manager.__aexit__(exc_type,exc_val,exc_tb)\n  except BaseExceptionGroup as exc:\n   if not exc.split(trio.Cancelled)[1]:\n    raise trio.Cancelled._create()from exc\n    \n   raise\n  finally:\n   del exc_val,exc_tb\n   self._active=False\n   \n def start_soon(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[Any]],\n *args:Unpack[PosArgsT],\n name:object=None,\n )->None:\n  if not self._active:\n   raise RuntimeError(\n   \"This task group is not active; no new tasks can be started.\"\n   )\n   \n  self._nursery.start_soon(func,*args,name=name)\n  \n async def start(\n self,func:Callable[...,Awaitable[Any]],*args:object,name:object=None\n )->Any:\n  if not self._active:\n   raise RuntimeError(\n   \"This task group is not active; no new tasks can be started.\"\n   )\n   \n  return await self._nursery.start(func,*args,name=name)\n  \n  \n  \n  \n  \n  \n  \nclass BlockingPortal(abc.BlockingPortal):\n def __new__(cls)->BlockingPortal:\n  return object.__new__(cls)\n  \n def __init__(self)->None:\n  super().__init__()\n  self._token=trio.lowlevel.current_trio_token()\n  \n def _spawn_task_from_thread(\n self,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]|T_Retval],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n name:object,\n future:Future[T_Retval],\n )->None:\n  trio.from_thread.run_sync(\n  partial(self._task_group.start_soon,name=name),\n  self._call_func,\n  func,\n  args,\n  kwargs,\n  future,\n  trio_token=self._token,\n  )\n  \n  \n  \n  \n  \n  \n  \n@dataclass(eq=False)\nclass ReceiveStreamWrapper(abc.ByteReceiveStream):\n _stream:trio.abc.ReceiveStream\n \n async def receive(self,max_bytes:int |None=None)->bytes:\n  try:\n   data=await self._stream.receive_some(max_bytes)\n  except trio.ClosedResourceError as exc:\n   raise ClosedResourceError from exc.__cause__\n  except trio.BrokenResourceError as exc:\n   raise BrokenResourceError from exc.__cause__\n   \n  if data:\n   return data\n  else:\n   raise EndOfStream\n   \n async def aclose(self)->None:\n  await self._stream.aclose()\n  \n  \n@dataclass(eq=False)\nclass SendStreamWrapper(abc.ByteSendStream):\n _stream:trio.abc.SendStream\n \n async def send(self,item:bytes)->None:\n  try:\n   await self._stream.send_all(item)\n  except trio.ClosedResourceError as exc:\n   raise ClosedResourceError from exc.__cause__\n  except trio.BrokenResourceError as exc:\n   raise BrokenResourceError from exc.__cause__\n   \n async def aclose(self)->None:\n  await self._stream.aclose()\n  \n  \n@dataclass(eq=False)\nclass Process(abc.Process):\n _process:trio.Process\n _stdin:abc.ByteSendStream |None\n _stdout:abc.ByteReceiveStream |None\n _stderr:abc.ByteReceiveStream |None\n \n async def aclose(self)->None:\n  with CancelScope(shield=True):\n   if self._stdin:\n    await self._stdin.aclose()\n   if self._stdout:\n    await self._stdout.aclose()\n   if self._stderr:\n    await self._stderr.aclose()\n    \n  try:\n   await self.wait()\n  except BaseException:\n   self.kill()\n   with CancelScope(shield=True):\n    await self.wait()\n   raise\n   \n async def wait(self)->int:\n  return await self._process.wait()\n  \n def terminate(self)->None:\n  self._process.terminate()\n  \n def kill(self)->None:\n  self._process.kill()\n  \n def send_signal(self,signal:Signals)->None:\n  self._process.send_signal(signal)\n  \n @property\n def pid(self)->int:\n  return self._process.pid\n  \n @property\n def returncode(self)->int |None:\n  return self._process.returncode\n  \n @property\n def stdin(self)->abc.ByteSendStream |None:\n  return self._stdin\n  \n @property\n def stdout(self)->abc.ByteReceiveStream |None:\n  return self._stdout\n  \n @property\n def stderr(self)->abc.ByteReceiveStream |None:\n  return self._stderr\n  \n  \nclass _ProcessPoolShutdownInstrument(trio.abc.Instrument):\n def after_run(self)->None:\n  super().after_run()\n  \n  \ncurrent_default_worker_process_limiter:trio.lowlevel.RunVar=RunVar(\n\"current_default_worker_process_limiter\"\n)\n\n\nasync def _shutdown_process_pool(workers:set[abc.Process])->None:\n try:\n  await trio.sleep(math.inf)\n except trio.Cancelled:\n  for process in workers:\n   if process.returncode is None:\n    process.kill()\n    \n  with CancelScope(shield=True):\n   for process in workers:\n    await process.aclose()\n    \n    \n    \n    \n    \n    \n    \nclass _TrioSocketMixin(Generic[T_SockAddr]):\n def __init__(self,trio_socket:TrioSocketType)->None:\n  self._trio_socket=trio_socket\n  self._closed=False\n  \n def _check_closed(self)->None:\n  if self._closed:\n   raise ClosedResourceError\n  if self._trio_socket.fileno()<0:\n   raise BrokenResourceError\n   \n @property\n def _raw_socket(self)->socket.socket:\n  return self._trio_socket._sock\n  \n async def aclose(self)->None:\n  if self._trio_socket.fileno()>=0:\n   self._closed=True\n   self._trio_socket.close()\n   \n def _convert_socket_error(self,exc:BaseException)->NoReturn:\n  if isinstance(exc,trio.ClosedResourceError):\n   raise ClosedResourceError from exc\n  elif self._trio_socket.fileno()<0 and self._closed:\n   raise ClosedResourceError from None\n  elif isinstance(exc,OSError):\n   raise BrokenResourceError from exc\n  else:\n   raise exc\n   \n   \nclass SocketStream(_TrioSocketMixin,abc.SocketStream):\n def __init__(self,trio_socket:TrioSocketType)->None:\n  super().__init__(trio_socket)\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  \n async def receive(self,max_bytes:int=65536)->bytes:\n  with self._receive_guard:\n   try:\n    data=await self._trio_socket.recv(max_bytes)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n   if data:\n    return data\n   else:\n    raise EndOfStream\n    \n async def send(self,item:bytes)->None:\n  with self._send_guard:\n   view=memoryview(item)\n   while view:\n    try:\n     bytes_sent=await self._trio_socket.send(view)\n    except BaseException as exc:\n     self._convert_socket_error(exc)\n     \n    view=view[bytes_sent:]\n    \n async def send_eof(self)->None:\n  self._trio_socket.shutdown(socket.SHUT_WR)\n  \n  \nclass UNIXSocketStream(SocketStream,abc.UNIXSocketStream):\n async def receive_fds(self,msglen:int,maxfds:int)->tuple[bytes,list[int]]:\n  if not isinstance(msglen,int)or msglen <0:\n   raise ValueError(\"msglen must be a non-negative integer\")\n  if not isinstance(maxfds,int)or maxfds <1:\n   raise ValueError(\"maxfds must be a positive integer\")\n   \n  fds=array.array(\"i\")\n  await trio.lowlevel.checkpoint()\n  with self._receive_guard:\n   while True:\n    try:\n     message,ancdata,flags,addr=await self._trio_socket.recvmsg(\n     msglen,socket.CMSG_LEN(maxfds *fds.itemsize)\n     )\n    except BaseException as exc:\n     self._convert_socket_error(exc)\n    else:\n     if not message and not ancdata:\n      raise EndOfStream\n      \n     break\n     \n  for cmsg_level,cmsg_type,cmsg_data in ancdata:\n   if cmsg_level !=socket.SOL_SOCKET or cmsg_type !=socket.SCM_RIGHTS:\n    raise RuntimeError(\n    f\"Received unexpected ancillary data; message = {message !r}, \"\n    f\"cmsg_level = {cmsg_level}, cmsg_type = {cmsg_type}\"\n    )\n    \n   fds.frombytes(cmsg_data[:len(cmsg_data)-(len(cmsg_data)%fds.itemsize)])\n   \n  return message,list(fds)\n  \n async def send_fds(self,message:bytes,fds:Collection[int |IOBase])->None:\n  if not message:\n   raise ValueError(\"message must not be empty\")\n  if not fds:\n   raise ValueError(\"fds must not be empty\")\n   \n  filenos:list[int]=[]\n  for fd in fds:\n   if isinstance(fd,int):\n    filenos.append(fd)\n   elif isinstance(fd,IOBase):\n    filenos.append(fd.fileno())\n    \n  fdarray=array.array(\"i\",filenos)\n  await trio.lowlevel.checkpoint()\n  with self._send_guard:\n   while True:\n    try:\n     await self._trio_socket.sendmsg(\n     [message],\n     [\n     (\n     socket.SOL_SOCKET,\n     socket.SCM_RIGHTS,\n     fdarray,\n     )\n     ],\n     )\n     break\n    except BaseException as exc:\n     self._convert_socket_error(exc)\n     \n     \nclass TCPSocketListener(_TrioSocketMixin,abc.SocketListener):\n def __init__(self,raw_socket:socket.socket):\n  super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n  self._accept_guard=ResourceGuard(\"accepting connections from\")\n  \n async def accept(self)->SocketStream:\n  with self._accept_guard:\n   try:\n    trio_socket,_addr=await self._trio_socket.accept()\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n  trio_socket.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)\n  return SocketStream(trio_socket)\n  \n  \nclass UNIXSocketListener(_TrioSocketMixin,abc.SocketListener):\n def __init__(self,raw_socket:socket.socket):\n  super().__init__(trio.socket.from_stdlib_socket(raw_socket))\n  self._accept_guard=ResourceGuard(\"accepting connections from\")\n  \n async def accept(self)->UNIXSocketStream:\n  with self._accept_guard:\n   try:\n    trio_socket,_addr=await self._trio_socket.accept()\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n  return UNIXSocketStream(trio_socket)\n  \n  \nclass UDPSocket(_TrioSocketMixin[IPSockAddrType],abc.UDPSocket):\n def __init__(self,trio_socket:TrioSocketType)->None:\n  super().__init__(trio_socket)\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  \n async def receive(self)->tuple[bytes,IPSockAddrType]:\n  with self._receive_guard:\n   try:\n    data,addr=await self._trio_socket.recvfrom(65536)\n    return data,convert_ipv6_sockaddr(addr)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n async def send(self,item:UDPPacketType)->None:\n  with self._send_guard:\n   try:\n    await self._trio_socket.sendto(*item)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n    \nclass ConnectedUDPSocket(_TrioSocketMixin[IPSockAddrType],abc.ConnectedUDPSocket):\n def __init__(self,trio_socket:TrioSocketType)->None:\n  super().__init__(trio_socket)\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  \n async def receive(self)->bytes:\n  with self._receive_guard:\n   try:\n    return await self._trio_socket.recv(65536)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n async def send(self,item:bytes)->None:\n  with self._send_guard:\n   try:\n    await self._trio_socket.send(item)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n    \nclass UNIXDatagramSocket(_TrioSocketMixin[str],abc.UNIXDatagramSocket):\n def __init__(self,trio_socket:TrioSocketType)->None:\n  super().__init__(trio_socket)\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  \n async def receive(self)->UNIXDatagramPacketType:\n  with self._receive_guard:\n   try:\n    data,addr=await self._trio_socket.recvfrom(65536)\n    return data,addr\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n async def send(self,item:UNIXDatagramPacketType)->None:\n  with self._send_guard:\n   try:\n    await self._trio_socket.sendto(*item)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n    \nclass ConnectedUNIXDatagramSocket(\n_TrioSocketMixin[str],abc.ConnectedUNIXDatagramSocket\n):\n def __init__(self,trio_socket:TrioSocketType)->None:\n  super().__init__(trio_socket)\n  self._receive_guard=ResourceGuard(\"reading from\")\n  self._send_guard=ResourceGuard(\"writing to\")\n  \n async def receive(self)->bytes:\n  with self._receive_guard:\n   try:\n    return await self._trio_socket.recv(65536)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n async def send(self,item:bytes)->None:\n  with self._send_guard:\n   try:\n    await self._trio_socket.send(item)\n   except BaseException as exc:\n    self._convert_socket_error(exc)\n    \n    \n    \n    \n    \n    \n    \nclass Event(BaseEvent):\n def __new__(cls)->Event:\n  return object.__new__(cls)\n  \n def __init__(self)->None:\n  self.__original=trio.Event()\n  \n def is_set(self)->bool:\n  return self.__original.is_set()\n  \n async def wait(self)->None:\n  return await self.__original.wait()\n  \n def statistics(self)->EventStatistics:\n  orig_statistics=self.__original.statistics()\n  return EventStatistics(tasks_waiting=orig_statistics.tasks_waiting)\n  \n def set(self)->None:\n  self.__original.set()\n  \n  \nclass Lock(BaseLock):\n def __new__(cls,*,fast_acquire:bool=False)->Lock:\n  return object.__new__(cls)\n  \n def __init__(self,*,fast_acquire:bool=False)->None:\n  self._fast_acquire=fast_acquire\n  self.__original=trio.Lock()\n  \n @staticmethod\n def _convert_runtime_error_msg(exc:RuntimeError)->None:\n  if exc.args ==(\"attempt to re-acquire an already held Lock\",):\n   exc.args=(\"Attempted to acquire an already held Lock\",)\n   \n async def acquire(self)->None:\n  if not self._fast_acquire:\n   try:\n    await self.__original.acquire()\n   except RuntimeError as exc:\n    self._convert_runtime_error_msg(exc)\n    raise\n    \n   return\n   \n   \n  await trio.lowlevel.checkpoint_if_cancelled()\n  try:\n   self.__original.acquire_nowait()\n  except trio.WouldBlock:\n   await self.__original._lot.park()\n  except RuntimeError as exc:\n   self._convert_runtime_error_msg(exc)\n   raise\n   \n def acquire_nowait(self)->None:\n  try:\n   self.__original.acquire_nowait()\n  except trio.WouldBlock:\n   raise WouldBlock from None\n  except RuntimeError as exc:\n   self._convert_runtime_error_msg(exc)\n   raise\n   \n def locked(self)->bool:\n  return self.__original.locked()\n  \n def release(self)->None:\n  self.__original.release()\n  \n def statistics(self)->LockStatistics:\n  orig_statistics=self.__original.statistics()\n  owner=TrioTaskInfo(orig_statistics.owner)if orig_statistics.owner else None\n  return LockStatistics(\n  orig_statistics.locked,owner,orig_statistics.tasks_waiting\n  )\n  \n  \nclass Semaphore(BaseSemaphore):\n def __new__(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->Semaphore:\n  return object.__new__(cls)\n  \n def __init__(\n self,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->None:\n  super().__init__(initial_value,max_value=max_value,fast_acquire=fast_acquire)\n  self.__original=trio.Semaphore(initial_value,max_value=max_value)\n  \n async def acquire(self)->None:\n  if not self._fast_acquire:\n   await self.__original.acquire()\n   return\n   \n   \n  await trio.lowlevel.checkpoint_if_cancelled()\n  try:\n   self.__original.acquire_nowait()\n  except trio.WouldBlock:\n   await self.__original._lot.park()\n   \n def acquire_nowait(self)->None:\n  try:\n   self.__original.acquire_nowait()\n  except trio.WouldBlock:\n   raise WouldBlock from None\n   \n @property\n def max_value(self)->int |None:\n  return self.__original.max_value\n  \n @property\n def value(self)->int:\n  return self.__original.value\n  \n def release(self)->None:\n  self.__original.release()\n  \n def statistics(self)->SemaphoreStatistics:\n  orig_statistics=self.__original.statistics()\n  return SemaphoreStatistics(orig_statistics.tasks_waiting)\n  \n  \nclass CapacityLimiter(BaseCapacityLimiter):\n def __new__(\n cls,\n total_tokens:float |None=None,\n *,\n original:trio.CapacityLimiter |None=None,\n )->CapacityLimiter:\n  return object.__new__(cls)\n  \n def __init__(\n self,\n total_tokens:float |None=None,\n *,\n original:trio.CapacityLimiter |None=None,\n )->None:\n  if original is not None:\n   self.__original=original\n  else:\n   assert total_tokens is not None\n   self.__original=trio.CapacityLimiter(total_tokens)\n   \n async def __aenter__(self)->None:\n  return await self.__original.__aenter__()\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  await self.__original.__aexit__(exc_type,exc_val,exc_tb)\n  \n @property\n def total_tokens(self)->float:\n  return self.__original.total_tokens\n  \n @total_tokens.setter\n def total_tokens(self,value:float)->None:\n  self.__original.total_tokens=value\n  \n @property\n def borrowed_tokens(self)->int:\n  return self.__original.borrowed_tokens\n  \n @property\n def available_tokens(self)->float:\n  return self.__original.available_tokens\n  \n def acquire_nowait(self)->None:\n  self.__original.acquire_nowait()\n  \n def acquire_on_behalf_of_nowait(self,borrower:object)->None:\n  self.__original.acquire_on_behalf_of_nowait(borrower)\n  \n async def acquire(self)->None:\n  await self.__original.acquire()\n  \n async def acquire_on_behalf_of(self,borrower:object)->None:\n  await self.__original.acquire_on_behalf_of(borrower)\n  \n def release(self)->None:\n  return self.__original.release()\n  \n def release_on_behalf_of(self,borrower:object)->None:\n  return self.__original.release_on_behalf_of(borrower)\n  \n def statistics(self)->CapacityLimiterStatistics:\n  orig=self.__original.statistics()\n  return CapacityLimiterStatistics(\n  borrowed_tokens=orig.borrowed_tokens,\n  total_tokens=orig.total_tokens,\n  borrowers=tuple(orig.borrowers),\n  tasks_waiting=orig.tasks_waiting,\n  )\n  \n  \n_capacity_limiter_wrapper:trio.lowlevel.RunVar=RunVar(\"_capacity_limiter_wrapper\")\n\n\n\n\n\n\n\nclass _SignalReceiver:\n _iterator:AsyncIterator[int]\n \n def __init__(self,signals:tuple[Signals,...]):\n  self._signals=signals\n  \n def __enter__(self)->_SignalReceiver:\n  self._cm=trio.open_signal_receiver(*self._signals)\n  self._iterator=self._cm.__enter__()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:TracebackType |None,\n )->bool |None:\n  return self._cm.__exit__(exc_type,exc_val,exc_tb)\n  \n def __aiter__(self)->_SignalReceiver:\n  return self\n  \n async def __anext__(self)->Signals:\n  signum=await self._iterator.__anext__()\n  return Signals(signum)\n  \n  \n  \n  \n  \n  \n  \nclass TestRunner(abc.TestRunner):\n def __init__(self,**options:Any)->None:\n  from queue import Queue\n  \n  self._call_queue:Queue[Callable[[],object]]=Queue()\n  self._send_stream:MemoryObjectSendStream |None=None\n  self._options=options\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc_val:BaseException |None,\n exc_tb:types.TracebackType |None,\n )->None:\n  if self._send_stream:\n   self._send_stream.close()\n   while self._send_stream is not None:\n    self._call_queue.get()()\n    \n async def _run_tests_and_fixtures(self)->None:\n  self._send_stream,receive_stream=create_memory_object_stream(1)\n  with receive_stream:\n   async for coro,outcome_holder in receive_stream:\n    try:\n     retval=await coro\n    except BaseException as exc:\n     outcome_holder.append(Error(exc))\n    else:\n     outcome_holder.append(Value(retval))\n     \n def _main_task_finished(self,outcome:object)->None:\n  self._send_stream=None\n  \n def _call_in_runner_task(\n self,\n func:Callable[P,Awaitable[T_Retval]],\n *args:P.args,\n **kwargs:P.kwargs,\n )->T_Retval:\n  if self._send_stream is None:\n   trio.lowlevel.start_guest_run(\n   self._run_tests_and_fixtures,\n   run_sync_soon_threadsafe=self._call_queue.put,\n   done_callback=self._main_task_finished,\n   **self._options,\n   )\n   while self._send_stream is None:\n    self._call_queue.get()()\n    \n  outcome_holder:list[Outcome]=[]\n  self._send_stream.send_nowait((func(*args,**kwargs),outcome_holder))\n  while not outcome_holder:\n   self._call_queue.get()()\n   \n  return outcome_holder[0].unwrap()\n  \n def run_asyncgen_fixture(\n self,\n fixture_func:Callable[...,AsyncGenerator[T_Retval,Any]],\n kwargs:dict[str,Any],\n )->Iterable[T_Retval]:\n  asyncgen=fixture_func(**kwargs)\n  fixturevalue:T_Retval=self._call_in_runner_task(asyncgen.asend,None)\n  \n  yield fixturevalue\n  \n  try:\n   self._call_in_runner_task(asyncgen.asend,None)\n  except StopAsyncIteration:\n   pass\n  else:\n   self._call_in_runner_task(asyncgen.aclose)\n   raise RuntimeError(\"Async generator fixture did not stop\")\n   \n def run_fixture(\n self,\n fixture_func:Callable[...,Coroutine[Any,Any,T_Retval]],\n kwargs:dict[str,Any],\n )->T_Retval:\n  return self._call_in_runner_task(fixture_func,**kwargs)\n  \n def run_test(\n self,test_func:Callable[...,Coroutine[Any,Any,Any]],kwargs:dict[str,Any]\n )->None:\n  self._call_in_runner_task(test_func,**kwargs)\n  \n  \nclass TrioTaskInfo(TaskInfo):\n def __init__(self,task:trio.lowlevel.Task):\n  parent_id=None\n  if task.parent_nursery and task.parent_nursery.parent_task:\n   parent_id=id(task.parent_nursery.parent_task)\n   \n  super().__init__(id(task),parent_id,task.name,task.coro)\n  self._task=weakref.proxy(task)\n  \n def has_pending_cancellation(self)->bool:\n  try:\n   return self._task._cancel_status.effectively_cancelled\n  except ReferenceError:\n  \n  \n   return False\n   \n   \nclass TrioBackend(AsyncBackend):\n @classmethod\n def run(\n cls,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n args:tuple[Unpack[PosArgsT]],\n kwargs:dict[str,Any],\n options:dict[str,Any],\n )->T_Retval:\n  return trio.run(func,*args)\n  \n @classmethod\n def current_token(cls)->object:\n  return trio.lowlevel.current_trio_token()\n  \n @classmethod\n def current_time(cls)->float:\n  return trio.current_time()\n  \n @classmethod\n def cancelled_exception_class(cls)->type[BaseException]:\n  return trio.Cancelled\n  \n @classmethod\n async def checkpoint(cls)->None:\n  await trio.lowlevel.checkpoint()\n  \n @classmethod\n async def checkpoint_if_cancelled(cls)->None:\n  await trio.lowlevel.checkpoint_if_cancelled()\n  \n @classmethod\n async def cancel_shielded_checkpoint(cls)->None:\n  await trio.lowlevel.cancel_shielded_checkpoint()\n  \n @classmethod\n async def sleep(cls,delay:float)->None:\n  await trio.sleep(delay)\n  \n @classmethod\n def create_cancel_scope(\n cls,*,deadline:float=math.inf,shield:bool=False\n )->abc.CancelScope:\n  return CancelScope(deadline=deadline,shield=shield)\n  \n @classmethod\n def current_effective_deadline(cls)->float:\n  return trio.current_effective_deadline()\n  \n @classmethod\n def create_task_group(cls)->abc.TaskGroup:\n  return TaskGroup()\n  \n @classmethod\n def create_event(cls)->abc.Event:\n  return Event()\n  \n @classmethod\n def create_lock(cls,*,fast_acquire:bool)->Lock:\n  return Lock(fast_acquire=fast_acquire)\n  \n @classmethod\n def create_semaphore(\n cls,\n initial_value:int,\n *,\n max_value:int |None=None,\n fast_acquire:bool=False,\n )->abc.Semaphore:\n  return Semaphore(initial_value,max_value=max_value,fast_acquire=fast_acquire)\n  \n @classmethod\n def create_capacity_limiter(cls,total_tokens:float)->CapacityLimiter:\n  return CapacityLimiter(total_tokens)\n  \n @classmethod\n async def run_sync_in_worker_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n args:tuple[Unpack[PosArgsT]],\n abandon_on_cancel:bool=False,\n limiter:abc.CapacityLimiter |None=None,\n )->T_Retval:\n  def wrapper()->T_Retval:\n   with claim_worker_thread(TrioBackend,token):\n    return func(*args)\n    \n  token=TrioBackend.current_token()\n  return await run_sync(\n  wrapper,\n  abandon_on_cancel=abandon_on_cancel,\n  limiter=cast(trio.CapacityLimiter,limiter),\n  )\n  \n @classmethod\n def check_cancelled(cls)->None:\n  trio.from_thread.check_cancelled()\n  \n @classmethod\n def run_async_from_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],Awaitable[T_Retval]],\n args:tuple[Unpack[PosArgsT]],\n token:object,\n )->T_Retval:\n  return trio.from_thread.run(func,*args)\n  \n @classmethod\n def run_sync_from_thread(\n cls,\n func:Callable[[Unpack[PosArgsT]],T_Retval],\n args:tuple[Unpack[PosArgsT]],\n token:object,\n )->T_Retval:\n  return trio.from_thread.run_sync(func,*args)\n  \n @classmethod\n def create_blocking_portal(cls)->abc.BlockingPortal:\n  return BlockingPortal()\n  \n @classmethod\n async def open_process(\n cls,\n command:StrOrBytesPath |Sequence[StrOrBytesPath],\n *,\n stdin:int |IO[Any]|None,\n stdout:int |IO[Any]|None,\n stderr:int |IO[Any]|None,\n **kwargs:Any,\n )->Process:\n  def convert_item(item:StrOrBytesPath)->str:\n   str_or_bytes=os.fspath(item)\n   if isinstance(str_or_bytes,str):\n    return str_or_bytes\n   else:\n    return os.fsdecode(str_or_bytes)\n    \n  if isinstance(command,(str,bytes,PathLike)):\n   process=await trio.lowlevel.open_process(\n   convert_item(command),\n   stdin=stdin,\n   stdout=stdout,\n   stderr=stderr,\n   shell=True,\n   **kwargs,\n   )\n  else:\n   process=await trio.lowlevel.open_process(\n   [convert_item(item)for item in command],\n   stdin=stdin,\n   stdout=stdout,\n   stderr=stderr,\n   shell=False,\n   **kwargs,\n   )\n   \n  stdin_stream=SendStreamWrapper(process.stdin)if process.stdin else None\n  stdout_stream=ReceiveStreamWrapper(process.stdout)if process.stdout else None\n  stderr_stream=ReceiveStreamWrapper(process.stderr)if process.stderr else None\n  return Process(process,stdin_stream,stdout_stream,stderr_stream)\n  \n @classmethod\n def setup_process_pool_exit_at_shutdown(cls,workers:set[abc.Process])->None:\n  trio.lowlevel.spawn_system_task(_shutdown_process_pool,workers)\n  \n @classmethod\n async def connect_tcp(\n cls,host:str,port:int,local_address:IPSockAddrType |None=None\n )->SocketStream:\n  family=socket.AF_INET6 if \":\"in host else socket.AF_INET\n  trio_socket=trio.socket.socket(family)\n  trio_socket.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)\n  if local_address:\n   await trio_socket.bind(local_address)\n   \n  try:\n   await trio_socket.connect((host,port))\n  except BaseException:\n   trio_socket.close()\n   raise\n   \n  return SocketStream(trio_socket)\n  \n @classmethod\n async def connect_unix(cls,path:str |bytes)->abc.UNIXSocketStream:\n  trio_socket=trio.socket.socket(socket.AF_UNIX)\n  try:\n   await trio_socket.connect(path)\n  except BaseException:\n   trio_socket.close()\n   raise\n   \n  return UNIXSocketStream(trio_socket)\n  \n @classmethod\n def create_tcp_listener(cls,sock:socket.socket)->abc.SocketListener:\n  return TCPSocketListener(sock)\n  \n @classmethod\n def create_unix_listener(cls,sock:socket.socket)->abc.SocketListener:\n  return UNIXSocketListener(sock)\n  \n @classmethod\n async def create_udp_socket(\n cls,\n family:socket.AddressFamily,\n local_address:IPSockAddrType |None,\n remote_address:IPSockAddrType |None,\n reuse_port:bool,\n )->UDPSocket |ConnectedUDPSocket:\n  trio_socket=trio.socket.socket(family=family,type=socket.SOCK_DGRAM)\n  \n  if reuse_port:\n   trio_socket.setsockopt(socket.SOL_SOCKET,socket.SO_REUSEPORT,1)\n   \n  if local_address:\n   await trio_socket.bind(local_address)\n   \n  if remote_address:\n   await trio_socket.connect(remote_address)\n   return ConnectedUDPSocket(trio_socket)\n  else:\n   return UDPSocket(trio_socket)\n   \n @classmethod\n @overload\n async def create_unix_datagram_socket(\n cls,raw_socket:socket.socket,remote_path:None\n )->abc.UNIXDatagramSocket:...\n \n @classmethod\n @overload\n async def create_unix_datagram_socket(\n cls,raw_socket:socket.socket,remote_path:str |bytes\n )->abc.ConnectedUNIXDatagramSocket:...\n \n @classmethod\n async def create_unix_datagram_socket(\n cls,raw_socket:socket.socket,remote_path:str |bytes |None\n )->abc.UNIXDatagramSocket |abc.ConnectedUNIXDatagramSocket:\n  trio_socket=trio.socket.from_stdlib_socket(raw_socket)\n  \n  if remote_path:\n   await trio_socket.connect(remote_path)\n   return ConnectedUNIXDatagramSocket(trio_socket)\n  else:\n   return UNIXDatagramSocket(trio_socket)\n   \n @classmethod\n async def getaddrinfo(\n cls,\n host:bytes |str |None,\n port:str |int |None,\n *,\n family:int |AddressFamily=0,\n type:int |SocketKind=0,\n proto:int=0,\n flags:int=0,\n )->list[\n tuple[\n AddressFamily,\n SocketKind,\n int,\n str,\n tuple[str,int]|tuple[str,int,int,int],\n ]\n ]:\n  return await trio.socket.getaddrinfo(host,port,family,type,proto,flags)\n  \n @classmethod\n async def getnameinfo(\n cls,sockaddr:IPSockAddrType,flags:int=0\n )->tuple[str,str]:\n  return await trio.socket.getnameinfo(sockaddr,flags)\n  \n @classmethod\n async def wait_readable(cls,obj:HasFileno |int)->None:\n  try:\n   await wait_readable(obj)\n  except trio.ClosedResourceError as exc:\n   raise ClosedResourceError().with_traceback(exc.__traceback__)from None\n  except trio.BusyResourceError:\n   raise BusyResourceError(\"reading from\")from None\n   \n @classmethod\n async def wait_writable(cls,obj:HasFileno |int)->None:\n  try:\n   await wait_writable(obj)\n  except trio.ClosedResourceError as exc:\n   raise ClosedResourceError().with_traceback(exc.__traceback__)from None\n  except trio.BusyResourceError:\n   raise BusyResourceError(\"writing to\")from None\n   \n @classmethod\n def current_default_thread_limiter(cls)->CapacityLimiter:\n  try:\n   return _capacity_limiter_wrapper.get()\n  except LookupError:\n   limiter=CapacityLimiter(\n   original=trio.to_thread.current_default_thread_limiter()\n   )\n   _capacity_limiter_wrapper.set(limiter)\n   return limiter\n   \n @classmethod\n def open_signal_receiver(\n cls,*signals:Signals\n )->AbstractContextManager[AsyncIterator[Signals]]:\n  return _SignalReceiver(signals)\n  \n @classmethod\n def get_current_task(cls)->TaskInfo:\n  task=current_task()\n  return TrioTaskInfo(task)\n  \n @classmethod\n def get_running_tasks(cls)->Sequence[TaskInfo]:\n  root_task=current_root_task()\n  assert root_task\n  task_infos=[TrioTaskInfo(root_task)]\n  nurseries=root_task.child_nurseries\n  while nurseries:\n   new_nurseries:list[trio.Nursery]=[]\n   for nursery in nurseries:\n    for task in nursery.child_tasks:\n     task_infos.append(TrioTaskInfo(task))\n     new_nurseries.extend(task.child_nurseries)\n     \n   nurseries=new_nurseries\n   \n  return task_infos\n  \n @classmethod\n async def wait_all_tasks_blocked(cls)->None:\n  from trio.testing import wait_all_tasks_blocked\n  \n  await wait_all_tasks_blocked()\n  \n @classmethod\n def create_test_runner(cls,options:dict[str,Any])->TestRunner:\n  return TestRunner(**options)\n  \n  \nbackend_class=TrioBackend\n", ["__future__", "_typeshed", "anyio", "anyio._core._eventloop", "anyio._core._exceptions", "anyio._core._sockets", "anyio._core._streams", "anyio._core._synchronization", "anyio._core._tasks", "anyio.abc", "anyio.abc._eventloop", "anyio.streams.memory", "array", "collections.abc", "concurrent.futures", "contextlib", "dataclasses", "exceptiongroup", "functools", "io", "math", "os", "outcome", "queue", "signal", "socket", "sys", "trio.from_thread", "trio.lowlevel", "trio.socket", "trio.testing", "trio.to_thread", "types", "typing", "typing_extensions", "weakref"]], "idna": [".py", "from.core import(\nIDNABidiError,\nIDNAError,\nInvalidCodepoint,\nInvalidCodepointContext,\nalabel,\ncheck_bidi,\ncheck_hyphen_ok,\ncheck_initial_combiner,\ncheck_label,\ncheck_nfc,\ndecode,\nencode,\nulabel,\nuts46_remap,\nvalid_contextj,\nvalid_contexto,\nvalid_label_length,\nvalid_string_length,\n)\nfrom.intranges import intranges_contain\nfrom.package_data import __version__\n\n__all__=[\n\"__version__\",\n\"IDNABidiError\",\n\"IDNAError\",\n\"InvalidCodepoint\",\n\"InvalidCodepointContext\",\n\"alabel\",\n\"check_bidi\",\n\"check_hyphen_ok\",\n\"check_initial_combiner\",\n\"check_label\",\n\"check_nfc\",\n\"decode\",\n\"encode\",\n\"intranges_contain\",\n\"ulabel\",\n\"uts46_remap\",\n\"valid_contextj\",\n\"valid_contexto\",\n\"valid_label_length\",\n\"valid_string_length\",\n]\n", ["idna.core", "idna.intranges", "idna.package_data"], 1], "idna.codec": [".py", "import codecs\nimport re\nfrom typing import Any,Optional,Tuple\n\nfrom.core import IDNAError,alabel,decode,encode,ulabel\n\n_unicode_dots_re=re.compile(\"[\\u002e\\u3002\\uff0e\\uff61]\")\n\n\nclass Codec(codecs.Codec):\n def encode(self,data:str,errors:str=\"strict\")->Tuple[bytes,int]:\n  if errors !=\"strict\":\n   raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n   \n  if not data:\n   return b\"\",0\n   \n  return encode(data),len(data)\n  \n def decode(self,data:bytes,errors:str=\"strict\")->Tuple[str,int]:\n  if errors !=\"strict\":\n   raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n   \n  if not data:\n   return \"\",0\n   \n  return decode(data),len(data)\n  \n  \nclass IncrementalEncoder(codecs.BufferedIncrementalEncoder):\n def _buffer_encode(self,data:str,errors:str,final:bool)->Tuple[bytes,int]:\n  if errors !=\"strict\":\n   raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n   \n  if not data:\n   return b\"\",0\n   \n  labels=_unicode_dots_re.split(data)\n  trailing_dot=b\"\"\n  if labels:\n   if not labels[-1]:\n    trailing_dot=b\".\"\n    del labels[-1]\n   elif not final:\n   \n    del labels[-1]\n    if labels:\n     trailing_dot=b\".\"\n     \n  result=[]\n  size=0\n  for label in labels:\n   result.append(alabel(label))\n   if size:\n    size +=1\n   size +=len(label)\n   \n   \n  result_bytes=b\".\".join(result)+trailing_dot\n  size +=len(trailing_dot)\n  return result_bytes,size\n  \n  \nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n def _buffer_decode(self,data:Any,errors:str,final:bool)->Tuple[str,int]:\n  if errors !=\"strict\":\n   raise IDNAError('Unsupported error handling \"{}\"'.format(errors))\n   \n  if not data:\n   return(\"\",0)\n   \n  if not isinstance(data,str):\n   data=str(data,\"ascii\")\n   \n  labels=_unicode_dots_re.split(data)\n  trailing_dot=\"\"\n  if labels:\n   if not labels[-1]:\n    trailing_dot=\".\"\n    del labels[-1]\n   elif not final:\n   \n    del labels[-1]\n    if labels:\n     trailing_dot=\".\"\n     \n  result=[]\n  size=0\n  for label in labels:\n   result.append(ulabel(label))\n   if size:\n    size +=1\n   size +=len(label)\n   \n  result_str=\".\".join(result)+trailing_dot\n  size +=len(trailing_dot)\n  return(result_str,size)\n  \n  \nclass StreamWriter(Codec,codecs.StreamWriter):\n pass\n \n \nclass StreamReader(Codec,codecs.StreamReader):\n pass\n \n \ndef search_function(name:str)->Optional[codecs.CodecInfo]:\n if name !=\"idna2008\":\n  return None\n return codecs.CodecInfo(\n name=name,\n encode=Codec().encode,\n decode=Codec().decode,\n incrementalencoder=IncrementalEncoder,\n incrementaldecoder=IncrementalDecoder,\n streamwriter=StreamWriter,\n streamreader=StreamReader,\n )\n \n \ncodecs.register(search_function)\n", ["codecs", "idna.core", "re", "typing"]], "idna.idnadata": [".py", "\n\n__version__=\"15.1.0\"\nscripts={\n\"Greek\":(\n0x37000000374,\n0x37500000378,\n0x37A0000037E,\n0x37F00000380,\n0x38400000385,\n0x38600000387,\n0x3880000038B,\n0x38C0000038D,\n0x38E000003A2,\n0x3A3000003E2,\n0x3F000000400,\n0x1D2600001D2B,\n0x1D5D00001D62,\n0x1D6600001D6B,\n0x1DBF00001DC0,\n0x1F0000001F16,\n0x1F1800001F1E,\n0x1F2000001F46,\n0x1F4800001F4E,\n0x1F5000001F58,\n0x1F5900001F5A,\n0x1F5B00001F5C,\n0x1F5D00001F5E,\n0x1F5F00001F7E,\n0x1F8000001FB5,\n0x1FB600001FC5,\n0x1FC600001FD4,\n0x1FD600001FDC,\n0x1FDD00001FF0,\n0x1FF200001FF5,\n0x1FF600001FFF,\n0x212600002127,\n0xAB650000AB66,\n0x101400001018F,\n0x101A0000101A1,\n0x1D2000001D246,\n),\n\"Han\":(\n0x2E8000002E9A,\n0x2E9B00002EF4,\n0x2F0000002FD6,\n0x300500003006,\n0x300700003008,\n0x30210000302A,\n0x30380000303C,\n0x340000004DC0,\n0x4E000000A000,\n0xF9000000FA6E,\n0xFA700000FADA,\n0x16FE200016FE4,\n0x16FF000016FF2,\n0x200000002A6E0,\n0x2A7000002B73A,\n0x2B7400002B81E,\n0x2B8200002CEA2,\n0x2CEB00002EBE1,\n0x2EBF00002EE5E,\n0x2F8000002FA1E,\n0x300000003134B,\n0x31350000323B0,\n),\n\"Hebrew\":(\n0x591000005C8,\n0x5D0000005EB,\n0x5EF000005F5,\n0xFB1D0000FB37,\n0xFB380000FB3D,\n0xFB3E0000FB3F,\n0xFB400000FB42,\n0xFB430000FB45,\n0xFB460000FB50,\n),\n\"Hiragana\":(\n0x304100003097,\n0x309D000030A0,\n0x1B0010001B120,\n0x1B1320001B133,\n0x1B1500001B153,\n0x1F2000001F201,\n),\n\"Katakana\":(\n0x30A1000030FB,\n0x30FD00003100,\n0x31F000003200,\n0x32D0000032FF,\n0x330000003358,\n0xFF660000FF70,\n0xFF710000FF9E,\n0x1AFF00001AFF4,\n0x1AFF50001AFFC,\n0x1AFFD0001AFFF,\n0x1B0000001B001,\n0x1B1200001B123,\n0x1B1550001B156,\n0x1B1640001B168,\n),\n}\njoining_types={\n0xAD:84,\n0x300:84,\n0x301:84,\n0x302:84,\n0x303:84,\n0x304:84,\n0x305:84,\n0x306:84,\n0x307:84,\n0x308:84,\n0x309:84,\n0x30A:84,\n0x30B:84,\n0x30C:84,\n0x30D:84,\n0x30E:84,\n0x30F:84,\n0x310:84,\n0x311:84,\n0x312:84,\n0x313:84,\n0x314:84,\n0x315:84,\n0x316:84,\n0x317:84,\n0x318:84,\n0x319:84,\n0x31A:84,\n0x31B:84,\n0x31C:84,\n0x31D:84,\n0x31E:84,\n0x31F:84,\n0x320:84,\n0x321:84,\n0x322:84,\n0x323:84,\n0x324:84,\n0x325:84,\n0x326:84,\n0x327:84,\n0x328:84,\n0x329:84,\n0x32A:84,\n0x32B:84,\n0x32C:84,\n0x32D:84,\n0x32E:84,\n0x32F:84,\n0x330:84,\n0x331:84,\n0x332:84,\n0x333:84,\n0x334:84,\n0x335:84,\n0x336:84,\n0x337:84,\n0x338:84,\n0x339:84,\n0x33A:84,\n0x33B:84,\n0x33C:84,\n0x33D:84,\n0x33E:84,\n0x33F:84,\n0x340:84,\n0x341:84,\n0x342:84,\n0x343:84,\n0x344:84,\n0x345:84,\n0x346:84,\n0x347:84,\n0x348:84,\n0x349:84,\n0x34A:84,\n0x34B:84,\n0x34C:84,\n0x34D:84,\n0x34E:84,\n0x34F:84,\n0x350:84,\n0x351:84,\n0x352:84,\n0x353:84,\n0x354:84,\n0x355:84,\n0x356:84,\n0x357:84,\n0x358:84,\n0x359:84,\n0x35A:84,\n0x35B:84,\n0x35C:84,\n0x35D:84,\n0x35E:84,\n0x35F:84,\n0x360:84,\n0x361:84,\n0x362:84,\n0x363:84,\n0x364:84,\n0x365:84,\n0x366:84,\n0x367:84,\n0x368:84,\n0x369:84,\n0x36A:84,\n0x36B:84,\n0x36C:84,\n0x36D:84,\n0x36E:84,\n0x36F:84,\n0x483:84,\n0x484:84,\n0x485:84,\n0x486:84,\n0x487:84,\n0x488:84,\n0x489:84,\n0x591:84,\n0x592:84,\n0x593:84,\n0x594:84,\n0x595:84,\n0x596:84,\n0x597:84,\n0x598:84,\n0x599:84,\n0x59A:84,\n0x59B:84,\n0x59C:84,\n0x59D:84,\n0x59E:84,\n0x59F:84,\n0x5A0:84,\n0x5A1:84,\n0x5A2:84,\n0x5A3:84,\n0x5A4:84,\n0x5A5:84,\n0x5A6:84,\n0x5A7:84,\n0x5A8:84,\n0x5A9:84,\n0x5AA:84,\n0x5AB:84,\n0x5AC:84,\n0x5AD:84,\n0x5AE:84,\n0x5AF:84,\n0x5B0:84,\n0x5B1:84,\n0x5B2:84,\n0x5B3:84,\n0x5B4:84,\n0x5B5:84,\n0x5B6:84,\n0x5B7:84,\n0x5B8:84,\n0x5B9:84,\n0x5BA:84,\n0x5BB:84,\n0x5BC:84,\n0x5BD:84,\n0x5BF:84,\n0x5C1:84,\n0x5C2:84,\n0x5C4:84,\n0x5C5:84,\n0x5C7:84,\n0x610:84,\n0x611:84,\n0x612:84,\n0x613:84,\n0x614:84,\n0x615:84,\n0x616:84,\n0x617:84,\n0x618:84,\n0x619:84,\n0x61A:84,\n0x61C:84,\n0x620:68,\n0x622:82,\n0x623:82,\n0x624:82,\n0x625:82,\n0x626:68,\n0x627:82,\n0x628:68,\n0x629:82,\n0x62A:68,\n0x62B:68,\n0x62C:68,\n0x62D:68,\n0x62E:68,\n0x62F:82,\n0x630:82,\n0x631:82,\n0x632:82,\n0x633:68,\n0x634:68,\n0x635:68,\n0x636:68,\n0x637:68,\n0x638:68,\n0x639:68,\n0x63A:68,\n0x63B:68,\n0x63C:68,\n0x63D:68,\n0x63E:68,\n0x63F:68,\n0x640:67,\n0x641:68,\n0x642:68,\n0x643:68,\n0x644:68,\n0x645:68,\n0x646:68,\n0x647:68,\n0x648:82,\n0x649:68,\n0x64A:68,\n0x64B:84,\n0x64C:84,\n0x64D:84,\n0x64E:84,\n0x64F:84,\n0x650:84,\n0x651:84,\n0x652:84,\n0x653:84,\n0x654:84,\n0x655:84,\n0x656:84,\n0x657:84,\n0x658:84,\n0x659:84,\n0x65A:84,\n0x65B:84,\n0x65C:84,\n0x65D:84,\n0x65E:84,\n0x65F:84,\n0x66E:68,\n0x66F:68,\n0x670:84,\n0x671:82,\n0x672:82,\n0x673:82,\n0x675:82,\n0x676:82,\n0x677:82,\n0x678:68,\n0x679:68,\n0x67A:68,\n0x67B:68,\n0x67C:68,\n0x67D:68,\n0x67E:68,\n0x67F:68,\n0x680:68,\n0x681:68,\n0x682:68,\n0x683:68,\n0x684:68,\n0x685:68,\n0x686:68,\n0x687:68,\n0x688:82,\n0x689:82,\n0x68A:82,\n0x68B:82,\n0x68C:82,\n0x68D:82,\n0x68E:82,\n0x68F:82,\n0x690:82,\n0x691:82,\n0x692:82,\n0x693:82,\n0x694:82,\n0x695:82,\n0x696:82,\n0x697:82,\n0x698:82,\n0x699:82,\n0x69A:68,\n0x69B:68,\n0x69C:68,\n0x69D:68,\n0x69E:68,\n0x69F:68,\n0x6A0:68,\n0x6A1:68,\n0x6A2:68,\n0x6A3:68,\n0x6A4:68,\n0x6A5:68,\n0x6A6:68,\n0x6A7:68,\n0x6A8:68,\n0x6A9:68,\n0x6AA:68,\n0x6AB:68,\n0x6AC:68,\n0x6AD:68,\n0x6AE:68,\n0x6AF:68,\n0x6B0:68,\n0x6B1:68,\n0x6B2:68,\n0x6B3:68,\n0x6B4:68,\n0x6B5:68,\n0x6B6:68,\n0x6B7:68,\n0x6B8:68,\n0x6B9:68,\n0x6BA:68,\n0x6BB:68,\n0x6BC:68,\n0x6BD:68,\n0x6BE:68,\n0x6BF:68,\n0x6C0:82,\n0x6C1:68,\n0x6C2:68,\n0x6C3:82,\n0x6C4:82,\n0x6C5:82,\n0x6C6:82,\n0x6C7:82,\n0x6C8:82,\n0x6C9:82,\n0x6CA:82,\n0x6CB:82,\n0x6CC:68,\n0x6CD:82,\n0x6CE:68,\n0x6CF:82,\n0x6D0:68,\n0x6D1:68,\n0x6D2:82,\n0x6D3:82,\n0x6D5:82,\n0x6D6:84,\n0x6D7:84,\n0x6D8:84,\n0x6D9:84,\n0x6DA:84,\n0x6DB:84,\n0x6DC:84,\n0x6DF:84,\n0x6E0:84,\n0x6E1:84,\n0x6E2:84,\n0x6E3:84,\n0x6E4:84,\n0x6E7:84,\n0x6E8:84,\n0x6EA:84,\n0x6EB:84,\n0x6EC:84,\n0x6ED:84,\n0x6EE:82,\n0x6EF:82,\n0x6FA:68,\n0x6FB:68,\n0x6FC:68,\n0x6FF:68,\n0x70F:84,\n0x710:82,\n0x711:84,\n0x712:68,\n0x713:68,\n0x714:68,\n0x715:82,\n0x716:82,\n0x717:82,\n0x718:82,\n0x719:82,\n0x71A:68,\n0x71B:68,\n0x71C:68,\n0x71D:68,\n0x71E:82,\n0x71F:68,\n0x720:68,\n0x721:68,\n0x722:68,\n0x723:68,\n0x724:68,\n0x725:68,\n0x726:68,\n0x727:68,\n0x728:82,\n0x729:68,\n0x72A:82,\n0x72B:68,\n0x72C:82,\n0x72D:68,\n0x72E:68,\n0x72F:82,\n0x730:84,\n0x731:84,\n0x732:84,\n0x733:84,\n0x734:84,\n0x735:84,\n0x736:84,\n0x737:84,\n0x738:84,\n0x739:84,\n0x73A:84,\n0x73B:84,\n0x73C:84,\n0x73D:84,\n0x73E:84,\n0x73F:84,\n0x740:84,\n0x741:84,\n0x742:84,\n0x743:84,\n0x744:84,\n0x745:84,\n0x746:84,\n0x747:84,\n0x748:84,\n0x749:84,\n0x74A:84,\n0x74D:82,\n0x74E:68,\n0x74F:68,\n0x750:68,\n0x751:68,\n0x752:68,\n0x753:68,\n0x754:68,\n0x755:68,\n0x756:68,\n0x757:68,\n0x758:68,\n0x759:82,\n0x75A:82,\n0x75B:82,\n0x75C:68,\n0x75D:68,\n0x75E:68,\n0x75F:68,\n0x760:68,\n0x761:68,\n0x762:68,\n0x763:68,\n0x764:68,\n0x765:68,\n0x766:68,\n0x767:68,\n0x768:68,\n0x769:68,\n0x76A:68,\n0x76B:82,\n0x76C:82,\n0x76D:68,\n0x76E:68,\n0x76F:68,\n0x770:68,\n0x771:82,\n0x772:68,\n0x773:82,\n0x774:82,\n0x775:68,\n0x776:68,\n0x777:68,\n0x778:82,\n0x779:82,\n0x77A:68,\n0x77B:68,\n0x77C:68,\n0x77D:68,\n0x77E:68,\n0x77F:68,\n0x7A6:84,\n0x7A7:84,\n0x7A8:84,\n0x7A9:84,\n0x7AA:84,\n0x7AB:84,\n0x7AC:84,\n0x7AD:84,\n0x7AE:84,\n0x7AF:84,\n0x7B0:84,\n0x7CA:68,\n0x7CB:68,\n0x7CC:68,\n0x7CD:68,\n0x7CE:68,\n0x7CF:68,\n0x7D0:68,\n0x7D1:68,\n0x7D2:68,\n0x7D3:68,\n0x7D4:68,\n0x7D5:68,\n0x7D6:68,\n0x7D7:68,\n0x7D8:68,\n0x7D9:68,\n0x7DA:68,\n0x7DB:68,\n0x7DC:68,\n0x7DD:68,\n0x7DE:68,\n0x7DF:68,\n0x7E0:68,\n0x7E1:68,\n0x7E2:68,\n0x7E3:68,\n0x7E4:68,\n0x7E5:68,\n0x7E6:68,\n0x7E7:68,\n0x7E8:68,\n0x7E9:68,\n0x7EA:68,\n0x7EB:84,\n0x7EC:84,\n0x7ED:84,\n0x7EE:84,\n0x7EF:84,\n0x7F0:84,\n0x7F1:84,\n0x7F2:84,\n0x7F3:84,\n0x7FA:67,\n0x7FD:84,\n0x816:84,\n0x817:84,\n0x818:84,\n0x819:84,\n0x81B:84,\n0x81C:84,\n0x81D:84,\n0x81E:84,\n0x81F:84,\n0x820:84,\n0x821:84,\n0x822:84,\n0x823:84,\n0x825:84,\n0x826:84,\n0x827:84,\n0x829:84,\n0x82A:84,\n0x82B:84,\n0x82C:84,\n0x82D:84,\n0x840:82,\n0x841:68,\n0x842:68,\n0x843:68,\n0x844:68,\n0x845:68,\n0x846:82,\n0x847:82,\n0x848:68,\n0x849:82,\n0x84A:68,\n0x84B:68,\n0x84C:68,\n0x84D:68,\n0x84E:68,\n0x84F:68,\n0x850:68,\n0x851:68,\n0x852:68,\n0x853:68,\n0x854:82,\n0x855:68,\n0x856:82,\n0x857:82,\n0x858:82,\n0x859:84,\n0x85A:84,\n0x85B:84,\n0x860:68,\n0x862:68,\n0x863:68,\n0x864:68,\n0x865:68,\n0x867:82,\n0x868:68,\n0x869:82,\n0x86A:82,\n0x870:82,\n0x871:82,\n0x872:82,\n0x873:82,\n0x874:82,\n0x875:82,\n0x876:82,\n0x877:82,\n0x878:82,\n0x879:82,\n0x87A:82,\n0x87B:82,\n0x87C:82,\n0x87D:82,\n0x87E:82,\n0x87F:82,\n0x880:82,\n0x881:82,\n0x882:82,\n0x883:67,\n0x884:67,\n0x885:67,\n0x886:68,\n0x889:68,\n0x88A:68,\n0x88B:68,\n0x88C:68,\n0x88D:68,\n0x88E:82,\n0x898:84,\n0x899:84,\n0x89A:84,\n0x89B:84,\n0x89C:84,\n0x89D:84,\n0x89E:84,\n0x89F:84,\n0x8A0:68,\n0x8A1:68,\n0x8A2:68,\n0x8A3:68,\n0x8A4:68,\n0x8A5:68,\n0x8A6:68,\n0x8A7:68,\n0x8A8:68,\n0x8A9:68,\n0x8AA:82,\n0x8AB:82,\n0x8AC:82,\n0x8AE:82,\n0x8AF:68,\n0x8B0:68,\n0x8B1:82,\n0x8B2:82,\n0x8B3:68,\n0x8B4:68,\n0x8B5:68,\n0x8B6:68,\n0x8B7:68,\n0x8B8:68,\n0x8B9:82,\n0x8BA:68,\n0x8BB:68,\n0x8BC:68,\n0x8BD:68,\n0x8BE:68,\n0x8BF:68,\n0x8C0:68,\n0x8C1:68,\n0x8C2:68,\n0x8C3:68,\n0x8C4:68,\n0x8C5:68,\n0x8C6:68,\n0x8C7:68,\n0x8C8:68,\n0x8CA:84,\n0x8CB:84,\n0x8CC:84,\n0x8CD:84,\n0x8CE:84,\n0x8CF:84,\n0x8D0:84,\n0x8D1:84,\n0x8D2:84,\n0x8D3:84,\n0x8D4:84,\n0x8D5:84,\n0x8D6:84,\n0x8D7:84,\n0x8D8:84,\n0x8D9:84,\n0x8DA:84,\n0x8DB:84,\n0x8DC:84,\n0x8DD:84,\n0x8DE:84,\n0x8DF:84,\n0x8E0:84,\n0x8E1:84,\n0x8E3:84,\n0x8E4:84,\n0x8E5:84,\n0x8E6:84,\n0x8E7:84,\n0x8E8:84,\n0x8E9:84,\n0x8EA:84,\n0x8EB:84,\n0x8EC:84,\n0x8ED:84,\n0x8EE:84,\n0x8EF:84,\n0x8F0:84,\n0x8F1:84,\n0x8F2:84,\n0x8F3:84,\n0x8F4:84,\n0x8F5:84,\n0x8F6:84,\n0x8F7:84,\n0x8F8:84,\n0x8F9:84,\n0x8FA:84,\n0x8FB:84,\n0x8FC:84,\n0x8FD:84,\n0x8FE:84,\n0x8FF:84,\n0x900:84,\n0x901:84,\n0x902:84,\n0x93A:84,\n0x93C:84,\n0x941:84,\n0x942:84,\n0x943:84,\n0x944:84,\n0x945:84,\n0x946:84,\n0x947:84,\n0x948:84,\n0x94D:84,\n0x951:84,\n0x952:84,\n0x953:84,\n0x954:84,\n0x955:84,\n0x956:84,\n0x957:84,\n0x962:84,\n0x963:84,\n0x981:84,\n0x9BC:84,\n0x9C1:84,\n0x9C2:84,\n0x9C3:84,\n0x9C4:84,\n0x9CD:84,\n0x9E2:84,\n0x9E3:84,\n0x9FE:84,\n0xA01:84,\n0xA02:84,\n0xA3C:84,\n0xA41:84,\n0xA42:84,\n0xA47:84,\n0xA48:84,\n0xA4B:84,\n0xA4C:84,\n0xA4D:84,\n0xA51:84,\n0xA70:84,\n0xA71:84,\n0xA75:84,\n0xA81:84,\n0xA82:84,\n0xABC:84,\n0xAC1:84,\n0xAC2:84,\n0xAC3:84,\n0xAC4:84,\n0xAC5:84,\n0xAC7:84,\n0xAC8:84,\n0xACD:84,\n0xAE2:84,\n0xAE3:84,\n0xAFA:84,\n0xAFB:84,\n0xAFC:84,\n0xAFD:84,\n0xAFE:84,\n0xAFF:84,\n0xB01:84,\n0xB3C:84,\n0xB3F:84,\n0xB41:84,\n0xB42:84,\n0xB43:84,\n0xB44:84,\n0xB4D:84,\n0xB55:84,\n0xB56:84,\n0xB62:84,\n0xB63:84,\n0xB82:84,\n0xBC0:84,\n0xBCD:84,\n0xC00:84,\n0xC04:84,\n0xC3C:84,\n0xC3E:84,\n0xC3F:84,\n0xC40:84,\n0xC46:84,\n0xC47:84,\n0xC48:84,\n0xC4A:84,\n0xC4B:84,\n0xC4C:84,\n0xC4D:84,\n0xC55:84,\n0xC56:84,\n0xC62:84,\n0xC63:84,\n0xC81:84,\n0xCBC:84,\n0xCBF:84,\n0xCC6:84,\n0xCCC:84,\n0xCCD:84,\n0xCE2:84,\n0xCE3:84,\n0xD00:84,\n0xD01:84,\n0xD3B:84,\n0xD3C:84,\n0xD41:84,\n0xD42:84,\n0xD43:84,\n0xD44:84,\n0xD4D:84,\n0xD62:84,\n0xD63:84,\n0xD81:84,\n0xDCA:84,\n0xDD2:84,\n0xDD3:84,\n0xDD4:84,\n0xDD6:84,\n0xE31:84,\n0xE34:84,\n0xE35:84,\n0xE36:84,\n0xE37:84,\n0xE38:84,\n0xE39:84,\n0xE3A:84,\n0xE47:84,\n0xE48:84,\n0xE49:84,\n0xE4A:84,\n0xE4B:84,\n0xE4C:84,\n0xE4D:84,\n0xE4E:84,\n0xEB1:84,\n0xEB4:84,\n0xEB5:84,\n0xEB6:84,\n0xEB7:84,\n0xEB8:84,\n0xEB9:84,\n0xEBA:84,\n0xEBB:84,\n0xEBC:84,\n0xEC8:84,\n0xEC9:84,\n0xECA:84,\n0xECB:84,\n0xECC:84,\n0xECD:84,\n0xECE:84,\n0xF18:84,\n0xF19:84,\n0xF35:84,\n0xF37:84,\n0xF39:84,\n0xF71:84,\n0xF72:84,\n0xF73:84,\n0xF74:84,\n0xF75:84,\n0xF76:84,\n0xF77:84,\n0xF78:84,\n0xF79:84,\n0xF7A:84,\n0xF7B:84,\n0xF7C:84,\n0xF7D:84,\n0xF7E:84,\n0xF80:84,\n0xF81:84,\n0xF82:84,\n0xF83:84,\n0xF84:84,\n0xF86:84,\n0xF87:84,\n0xF8D:84,\n0xF8E:84,\n0xF8F:84,\n0xF90:84,\n0xF91:84,\n0xF92:84,\n0xF93:84,\n0xF94:84,\n0xF95:84,\n0xF96:84,\n0xF97:84,\n0xF99:84,\n0xF9A:84,\n0xF9B:84,\n0xF9C:84,\n0xF9D:84,\n0xF9E:84,\n0xF9F:84,\n0xFA0:84,\n0xFA1:84,\n0xFA2:84,\n0xFA3:84,\n0xFA4:84,\n0xFA5:84,\n0xFA6:84,\n0xFA7:84,\n0xFA8:84,\n0xFA9:84,\n0xFAA:84,\n0xFAB:84,\n0xFAC:84,\n0xFAD:84,\n0xFAE:84,\n0xFAF:84,\n0xFB0:84,\n0xFB1:84,\n0xFB2:84,\n0xFB3:84,\n0xFB4:84,\n0xFB5:84,\n0xFB6:84,\n0xFB7:84,\n0xFB8:84,\n0xFB9:84,\n0xFBA:84,\n0xFBB:84,\n0xFBC:84,\n0xFC6:84,\n0x102D:84,\n0x102E:84,\n0x102F:84,\n0x1030:84,\n0x1032:84,\n0x1033:84,\n0x1034:84,\n0x1035:84,\n0x1036:84,\n0x1037:84,\n0x1039:84,\n0x103A:84,\n0x103D:84,\n0x103E:84,\n0x1058:84,\n0x1059:84,\n0x105E:84,\n0x105F:84,\n0x1060:84,\n0x1071:84,\n0x1072:84,\n0x1073:84,\n0x1074:84,\n0x1082:84,\n0x1085:84,\n0x1086:84,\n0x108D:84,\n0x109D:84,\n0x135D:84,\n0x135E:84,\n0x135F:84,\n0x1712:84,\n0x1713:84,\n0x1714:84,\n0x1732:84,\n0x1733:84,\n0x1752:84,\n0x1753:84,\n0x1772:84,\n0x1773:84,\n0x17B4:84,\n0x17B5:84,\n0x17B7:84,\n0x17B8:84,\n0x17B9:84,\n0x17BA:84,\n0x17BB:84,\n0x17BC:84,\n0x17BD:84,\n0x17C6:84,\n0x17C9:84,\n0x17CA:84,\n0x17CB:84,\n0x17CC:84,\n0x17CD:84,\n0x17CE:84,\n0x17CF:84,\n0x17D0:84,\n0x17D1:84,\n0x17D2:84,\n0x17D3:84,\n0x17DD:84,\n0x1807:68,\n0x180A:67,\n0x180B:84,\n0x180C:84,\n0x180D:84,\n0x180F:84,\n0x1820:68,\n0x1821:68,\n0x1822:68,\n0x1823:68,\n0x1824:68,\n0x1825:68,\n0x1826:68,\n0x1827:68,\n0x1828:68,\n0x1829:68,\n0x182A:68,\n0x182B:68,\n0x182C:68,\n0x182D:68,\n0x182E:68,\n0x182F:68,\n0x1830:68,\n0x1831:68,\n0x1832:68,\n0x1833:68,\n0x1834:68,\n0x1835:68,\n0x1836:68,\n0x1837:68,\n0x1838:68,\n0x1839:68,\n0x183A:68,\n0x183B:68,\n0x183C:68,\n0x183D:68,\n0x183E:68,\n0x183F:68,\n0x1840:68,\n0x1841:68,\n0x1842:68,\n0x1843:68,\n0x1844:68,\n0x1845:68,\n0x1846:68,\n0x1847:68,\n0x1848:68,\n0x1849:68,\n0x184A:68,\n0x184B:68,\n0x184C:68,\n0x184D:68,\n0x184E:68,\n0x184F:68,\n0x1850:68,\n0x1851:68,\n0x1852:68,\n0x1853:68,\n0x1854:68,\n0x1855:68,\n0x1856:68,\n0x1857:68,\n0x1858:68,\n0x1859:68,\n0x185A:68,\n0x185B:68,\n0x185C:68,\n0x185D:68,\n0x185E:68,\n0x185F:68,\n0x1860:68,\n0x1861:68,\n0x1862:68,\n0x1863:68,\n0x1864:68,\n0x1865:68,\n0x1866:68,\n0x1867:68,\n0x1868:68,\n0x1869:68,\n0x186A:68,\n0x186B:68,\n0x186C:68,\n0x186D:68,\n0x186E:68,\n0x186F:68,\n0x1870:68,\n0x1871:68,\n0x1872:68,\n0x1873:68,\n0x1874:68,\n0x1875:68,\n0x1876:68,\n0x1877:68,\n0x1878:68,\n0x1885:84,\n0x1886:84,\n0x1887:68,\n0x1888:68,\n0x1889:68,\n0x188A:68,\n0x188B:68,\n0x188C:68,\n0x188D:68,\n0x188E:68,\n0x188F:68,\n0x1890:68,\n0x1891:68,\n0x1892:68,\n0x1893:68,\n0x1894:68,\n0x1895:68,\n0x1896:68,\n0x1897:68,\n0x1898:68,\n0x1899:68,\n0x189A:68,\n0x189B:68,\n0x189C:68,\n0x189D:68,\n0x189E:68,\n0x189F:68,\n0x18A0:68,\n0x18A1:68,\n0x18A2:68,\n0x18A3:68,\n0x18A4:68,\n0x18A5:68,\n0x18A6:68,\n0x18A7:68,\n0x18A8:68,\n0x18A9:84,\n0x18AA:68,\n0x1920:84,\n0x1921:84,\n0x1922:84,\n0x1927:84,\n0x1928:84,\n0x1932:84,\n0x1939:84,\n0x193A:84,\n0x193B:84,\n0x1A17:84,\n0x1A18:84,\n0x1A1B:84,\n0x1A56:84,\n0x1A58:84,\n0x1A59:84,\n0x1A5A:84,\n0x1A5B:84,\n0x1A5C:84,\n0x1A5D:84,\n0x1A5E:84,\n0x1A60:84,\n0x1A62:84,\n0x1A65:84,\n0x1A66:84,\n0x1A67:84,\n0x1A68:84,\n0x1A69:84,\n0x1A6A:84,\n0x1A6B:84,\n0x1A6C:84,\n0x1A73:84,\n0x1A74:84,\n0x1A75:84,\n0x1A76:84,\n0x1A77:84,\n0x1A78:84,\n0x1A79:84,\n0x1A7A:84,\n0x1A7B:84,\n0x1A7C:84,\n0x1A7F:84,\n0x1AB0:84,\n0x1AB1:84,\n0x1AB2:84,\n0x1AB3:84,\n0x1AB4:84,\n0x1AB5:84,\n0x1AB6:84,\n0x1AB7:84,\n0x1AB8:84,\n0x1AB9:84,\n0x1ABA:84,\n0x1ABB:84,\n0x1ABC:84,\n0x1ABD:84,\n0x1ABE:84,\n0x1ABF:84,\n0x1AC0:84,\n0x1AC1:84,\n0x1AC2:84,\n0x1AC3:84,\n0x1AC4:84,\n0x1AC5:84,\n0x1AC6:84,\n0x1AC7:84,\n0x1AC8:84,\n0x1AC9:84,\n0x1ACA:84,\n0x1ACB:84,\n0x1ACC:84,\n0x1ACD:84,\n0x1ACE:84,\n0x1B00:84,\n0x1B01:84,\n0x1B02:84,\n0x1B03:84,\n0x1B34:84,\n0x1B36:84,\n0x1B37:84,\n0x1B38:84,\n0x1B39:84,\n0x1B3A:84,\n0x1B3C:84,\n0x1B42:84,\n0x1B6B:84,\n0x1B6C:84,\n0x1B6D:84,\n0x1B6E:84,\n0x1B6F:84,\n0x1B70:84,\n0x1B71:84,\n0x1B72:84,\n0x1B73:84,\n0x1B80:84,\n0x1B81:84,\n0x1BA2:84,\n0x1BA3:84,\n0x1BA4:84,\n0x1BA5:84,\n0x1BA8:84,\n0x1BA9:84,\n0x1BAB:84,\n0x1BAC:84,\n0x1BAD:84,\n0x1BE6:84,\n0x1BE8:84,\n0x1BE9:84,\n0x1BED:84,\n0x1BEF:84,\n0x1BF0:84,\n0x1BF1:84,\n0x1C2C:84,\n0x1C2D:84,\n0x1C2E:84,\n0x1C2F:84,\n0x1C30:84,\n0x1C31:84,\n0x1C32:84,\n0x1C33:84,\n0x1C36:84,\n0x1C37:84,\n0x1CD0:84,\n0x1CD1:84,\n0x1CD2:84,\n0x1CD4:84,\n0x1CD5:84,\n0x1CD6:84,\n0x1CD7:84,\n0x1CD8:84,\n0x1CD9:84,\n0x1CDA:84,\n0x1CDB:84,\n0x1CDC:84,\n0x1CDD:84,\n0x1CDE:84,\n0x1CDF:84,\n0x1CE0:84,\n0x1CE2:84,\n0x1CE3:84,\n0x1CE4:84,\n0x1CE5:84,\n0x1CE6:84,\n0x1CE7:84,\n0x1CE8:84,\n0x1CED:84,\n0x1CF4:84,\n0x1CF8:84,\n0x1CF9:84,\n0x1DC0:84,\n0x1DC1:84,\n0x1DC2:84,\n0x1DC3:84,\n0x1DC4:84,\n0x1DC5:84,\n0x1DC6:84,\n0x1DC7:84,\n0x1DC8:84,\n0x1DC9:84,\n0x1DCA:84,\n0x1DCB:84,\n0x1DCC:84,\n0x1DCD:84,\n0x1DCE:84,\n0x1DCF:84,\n0x1DD0:84,\n0x1DD1:84,\n0x1DD2:84,\n0x1DD3:84,\n0x1DD4:84,\n0x1DD5:84,\n0x1DD6:84,\n0x1DD7:84,\n0x1DD8:84,\n0x1DD9:84,\n0x1DDA:84,\n0x1DDB:84,\n0x1DDC:84,\n0x1DDD:84,\n0x1DDE:84,\n0x1DDF:84,\n0x1DE0:84,\n0x1DE1:84,\n0x1DE2:84,\n0x1DE3:84,\n0x1DE4:84,\n0x1DE5:84,\n0x1DE6:84,\n0x1DE7:84,\n0x1DE8:84,\n0x1DE9:84,\n0x1DEA:84,\n0x1DEB:84,\n0x1DEC:84,\n0x1DED:84,\n0x1DEE:84,\n0x1DEF:84,\n0x1DF0:84,\n0x1DF1:84,\n0x1DF2:84,\n0x1DF3:84,\n0x1DF4:84,\n0x1DF5:84,\n0x1DF6:84,\n0x1DF7:84,\n0x1DF8:84,\n0x1DF9:84,\n0x1DFA:84,\n0x1DFB:84,\n0x1DFC:84,\n0x1DFD:84,\n0x1DFE:84,\n0x1DFF:84,\n0x200B:84,\n0x200D:67,\n0x200E:84,\n0x200F:84,\n0x202A:84,\n0x202B:84,\n0x202C:84,\n0x202D:84,\n0x202E:84,\n0x2060:84,\n0x2061:84,\n0x2062:84,\n0x2063:84,\n0x2064:84,\n0x206A:84,\n0x206B:84,\n0x206C:84,\n0x206D:84,\n0x206E:84,\n0x206F:84,\n0x20D0:84,\n0x20D1:84,\n0x20D2:84,\n0x20D3:84,\n0x20D4:84,\n0x20D5:84,\n0x20D6:84,\n0x20D7:84,\n0x20D8:84,\n0x20D9:84,\n0x20DA:84,\n0x20DB:84,\n0x20DC:84,\n0x20DD:84,\n0x20DE:84,\n0x20DF:84,\n0x20E0:84,\n0x20E1:84,\n0x20E2:84,\n0x20E3:84,\n0x20E4:84,\n0x20E5:84,\n0x20E6:84,\n0x20E7:84,\n0x20E8:84,\n0x20E9:84,\n0x20EA:84,\n0x20EB:84,\n0x20EC:84,\n0x20ED:84,\n0x20EE:84,\n0x20EF:84,\n0x20F0:84,\n0x2CEF:84,\n0x2CF0:84,\n0x2CF1:84,\n0x2D7F:84,\n0x2DE0:84,\n0x2DE1:84,\n0x2DE2:84,\n0x2DE3:84,\n0x2DE4:84,\n0x2DE5:84,\n0x2DE6:84,\n0x2DE7:84,\n0x2DE8:84,\n0x2DE9:84,\n0x2DEA:84,\n0x2DEB:84,\n0x2DEC:84,\n0x2DED:84,\n0x2DEE:84,\n0x2DEF:84,\n0x2DF0:84,\n0x2DF1:84,\n0x2DF2:84,\n0x2DF3:84,\n0x2DF4:84,\n0x2DF5:84,\n0x2DF6:84,\n0x2DF7:84,\n0x2DF8:84,\n0x2DF9:84,\n0x2DFA:84,\n0x2DFB:84,\n0x2DFC:84,\n0x2DFD:84,\n0x2DFE:84,\n0x2DFF:84,\n0x302A:84,\n0x302B:84,\n0x302C:84,\n0x302D:84,\n0x3099:84,\n0x309A:84,\n0xA66F:84,\n0xA670:84,\n0xA671:84,\n0xA672:84,\n0xA674:84,\n0xA675:84,\n0xA676:84,\n0xA677:84,\n0xA678:84,\n0xA679:84,\n0xA67A:84,\n0xA67B:84,\n0xA67C:84,\n0xA67D:84,\n0xA69E:84,\n0xA69F:84,\n0xA6F0:84,\n0xA6F1:84,\n0xA802:84,\n0xA806:84,\n0xA80B:84,\n0xA825:84,\n0xA826:84,\n0xA82C:84,\n0xA840:68,\n0xA841:68,\n0xA842:68,\n0xA843:68,\n0xA844:68,\n0xA845:68,\n0xA846:68,\n0xA847:68,\n0xA848:68,\n0xA849:68,\n0xA84A:68,\n0xA84B:68,\n0xA84C:68,\n0xA84D:68,\n0xA84E:68,\n0xA84F:68,\n0xA850:68,\n0xA851:68,\n0xA852:68,\n0xA853:68,\n0xA854:68,\n0xA855:68,\n0xA856:68,\n0xA857:68,\n0xA858:68,\n0xA859:68,\n0xA85A:68,\n0xA85B:68,\n0xA85C:68,\n0xA85D:68,\n0xA85E:68,\n0xA85F:68,\n0xA860:68,\n0xA861:68,\n0xA862:68,\n0xA863:68,\n0xA864:68,\n0xA865:68,\n0xA866:68,\n0xA867:68,\n0xA868:68,\n0xA869:68,\n0xA86A:68,\n0xA86B:68,\n0xA86C:68,\n0xA86D:68,\n0xA86E:68,\n0xA86F:68,\n0xA870:68,\n0xA871:68,\n0xA872:76,\n0xA8C4:84,\n0xA8C5:84,\n0xA8E0:84,\n0xA8E1:84,\n0xA8E2:84,\n0xA8E3:84,\n0xA8E4:84,\n0xA8E5:84,\n0xA8E6:84,\n0xA8E7:84,\n0xA8E8:84,\n0xA8E9:84,\n0xA8EA:84,\n0xA8EB:84,\n0xA8EC:84,\n0xA8ED:84,\n0xA8EE:84,\n0xA8EF:84,\n0xA8F0:84,\n0xA8F1:84,\n0xA8FF:84,\n0xA926:84,\n0xA927:84,\n0xA928:84,\n0xA929:84,\n0xA92A:84,\n0xA92B:84,\n0xA92C:84,\n0xA92D:84,\n0xA947:84,\n0xA948:84,\n0xA949:84,\n0xA94A:84,\n0xA94B:84,\n0xA94C:84,\n0xA94D:84,\n0xA94E:84,\n0xA94F:84,\n0xA950:84,\n0xA951:84,\n0xA980:84,\n0xA981:84,\n0xA982:84,\n0xA9B3:84,\n0xA9B6:84,\n0xA9B7:84,\n0xA9B8:84,\n0xA9B9:84,\n0xA9BC:84,\n0xA9BD:84,\n0xA9E5:84,\n0xAA29:84,\n0xAA2A:84,\n0xAA2B:84,\n0xAA2C:84,\n0xAA2D:84,\n0xAA2E:84,\n0xAA31:84,\n0xAA32:84,\n0xAA35:84,\n0xAA36:84,\n0xAA43:84,\n0xAA4C:84,\n0xAA7C:84,\n0xAAB0:84,\n0xAAB2:84,\n0xAAB3:84,\n0xAAB4:84,\n0xAAB7:84,\n0xAAB8:84,\n0xAABE:84,\n0xAABF:84,\n0xAAC1:84,\n0xAAEC:84,\n0xAAED:84,\n0xAAF6:84,\n0xABE5:84,\n0xABE8:84,\n0xABED:84,\n0xFB1E:84,\n0xFE00:84,\n0xFE01:84,\n0xFE02:84,\n0xFE03:84,\n0xFE04:84,\n0xFE05:84,\n0xFE06:84,\n0xFE07:84,\n0xFE08:84,\n0xFE09:84,\n0xFE0A:84,\n0xFE0B:84,\n0xFE0C:84,\n0xFE0D:84,\n0xFE0E:84,\n0xFE0F:84,\n0xFE20:84,\n0xFE21:84,\n0xFE22:84,\n0xFE23:84,\n0xFE24:84,\n0xFE25:84,\n0xFE26:84,\n0xFE27:84,\n0xFE28:84,\n0xFE29:84,\n0xFE2A:84,\n0xFE2B:84,\n0xFE2C:84,\n0xFE2D:84,\n0xFE2E:84,\n0xFE2F:84,\n0xFEFF:84,\n0xFFF9:84,\n0xFFFA:84,\n0xFFFB:84,\n0x101FD:84,\n0x102E0:84,\n0x10376:84,\n0x10377:84,\n0x10378:84,\n0x10379:84,\n0x1037A:84,\n0x10A01:84,\n0x10A02:84,\n0x10A03:84,\n0x10A05:84,\n0x10A06:84,\n0x10A0C:84,\n0x10A0D:84,\n0x10A0E:84,\n0x10A0F:84,\n0x10A38:84,\n0x10A39:84,\n0x10A3A:84,\n0x10A3F:84,\n0x10AC0:68,\n0x10AC1:68,\n0x10AC2:68,\n0x10AC3:68,\n0x10AC4:68,\n0x10AC5:82,\n0x10AC7:82,\n0x10AC9:82,\n0x10ACA:82,\n0x10ACD:76,\n0x10ACE:82,\n0x10ACF:82,\n0x10AD0:82,\n0x10AD1:82,\n0x10AD2:82,\n0x10AD3:68,\n0x10AD4:68,\n0x10AD5:68,\n0x10AD6:68,\n0x10AD7:76,\n0x10AD8:68,\n0x10AD9:68,\n0x10ADA:68,\n0x10ADB:68,\n0x10ADC:68,\n0x10ADD:82,\n0x10ADE:68,\n0x10ADF:68,\n0x10AE0:68,\n0x10AE1:82,\n0x10AE4:82,\n0x10AE5:84,\n0x10AE6:84,\n0x10AEB:68,\n0x10AEC:68,\n0x10AED:68,\n0x10AEE:68,\n0x10AEF:82,\n0x10B80:68,\n0x10B81:82,\n0x10B82:68,\n0x10B83:82,\n0x10B84:82,\n0x10B85:82,\n0x10B86:68,\n0x10B87:68,\n0x10B88:68,\n0x10B89:82,\n0x10B8A:68,\n0x10B8B:68,\n0x10B8C:82,\n0x10B8D:68,\n0x10B8E:82,\n0x10B8F:82,\n0x10B90:68,\n0x10B91:82,\n0x10BA9:82,\n0x10BAA:82,\n0x10BAB:82,\n0x10BAC:82,\n0x10BAD:68,\n0x10BAE:68,\n0x10D00:76,\n0x10D01:68,\n0x10D02:68,\n0x10D03:68,\n0x10D04:68,\n0x10D05:68,\n0x10D06:68,\n0x10D07:68,\n0x10D08:68,\n0x10D09:68,\n0x10D0A:68,\n0x10D0B:68,\n0x10D0C:68,\n0x10D0D:68,\n0x10D0E:68,\n0x10D0F:68,\n0x10D10:68,\n0x10D11:68,\n0x10D12:68,\n0x10D13:68,\n0x10D14:68,\n0x10D15:68,\n0x10D16:68,\n0x10D17:68,\n0x10D18:68,\n0x10D19:68,\n0x10D1A:68,\n0x10D1B:68,\n0x10D1C:68,\n0x10D1D:68,\n0x10D1E:68,\n0x10D1F:68,\n0x10D20:68,\n0x10D21:68,\n0x10D22:82,\n0x10D23:68,\n0x10D24:84,\n0x10D25:84,\n0x10D26:84,\n0x10D27:84,\n0x10EAB:84,\n0x10EAC:84,\n0x10EFD:84,\n0x10EFE:84,\n0x10EFF:84,\n0x10F30:68,\n0x10F31:68,\n0x10F32:68,\n0x10F33:82,\n0x10F34:68,\n0x10F35:68,\n0x10F36:68,\n0x10F37:68,\n0x10F38:68,\n0x10F39:68,\n0x10F3A:68,\n0x10F3B:68,\n0x10F3C:68,\n0x10F3D:68,\n0x10F3E:68,\n0x10F3F:68,\n0x10F40:68,\n0x10F41:68,\n0x10F42:68,\n0x10F43:68,\n0x10F44:68,\n0x10F46:84,\n0x10F47:84,\n0x10F48:84,\n0x10F49:84,\n0x10F4A:84,\n0x10F4B:84,\n0x10F4C:84,\n0x10F4D:84,\n0x10F4E:84,\n0x10F4F:84,\n0x10F50:84,\n0x10F51:68,\n0x10F52:68,\n0x10F53:68,\n0x10F54:82,\n0x10F70:68,\n0x10F71:68,\n0x10F72:68,\n0x10F73:68,\n0x10F74:82,\n0x10F75:82,\n0x10F76:68,\n0x10F77:68,\n0x10F78:68,\n0x10F79:68,\n0x10F7A:68,\n0x10F7B:68,\n0x10F7C:68,\n0x10F7D:68,\n0x10F7E:68,\n0x10F7F:68,\n0x10F80:68,\n0x10F81:68,\n0x10F82:84,\n0x10F83:84,\n0x10F84:84,\n0x10F85:84,\n0x10FB0:68,\n0x10FB2:68,\n0x10FB3:68,\n0x10FB4:82,\n0x10FB5:82,\n0x10FB6:82,\n0x10FB8:68,\n0x10FB9:82,\n0x10FBA:82,\n0x10FBB:68,\n0x10FBC:68,\n0x10FBD:82,\n0x10FBE:68,\n0x10FBF:68,\n0x10FC1:68,\n0x10FC2:82,\n0x10FC3:82,\n0x10FC4:68,\n0x10FC9:82,\n0x10FCA:68,\n0x10FCB:76,\n0x11001:84,\n0x11038:84,\n0x11039:84,\n0x1103A:84,\n0x1103B:84,\n0x1103C:84,\n0x1103D:84,\n0x1103E:84,\n0x1103F:84,\n0x11040:84,\n0x11041:84,\n0x11042:84,\n0x11043:84,\n0x11044:84,\n0x11045:84,\n0x11046:84,\n0x11070:84,\n0x11073:84,\n0x11074:84,\n0x1107F:84,\n0x11080:84,\n0x11081:84,\n0x110B3:84,\n0x110B4:84,\n0x110B5:84,\n0x110B6:84,\n0x110B9:84,\n0x110BA:84,\n0x110C2:84,\n0x11100:84,\n0x11101:84,\n0x11102:84,\n0x11127:84,\n0x11128:84,\n0x11129:84,\n0x1112A:84,\n0x1112B:84,\n0x1112D:84,\n0x1112E:84,\n0x1112F:84,\n0x11130:84,\n0x11131:84,\n0x11132:84,\n0x11133:84,\n0x11134:84,\n0x11173:84,\n0x11180:84,\n0x11181:84,\n0x111B6:84,\n0x111B7:84,\n0x111B8:84,\n0x111B9:84,\n0x111BA:84,\n0x111BB:84,\n0x111BC:84,\n0x111BD:84,\n0x111BE:84,\n0x111C9:84,\n0x111CA:84,\n0x111CB:84,\n0x111CC:84,\n0x111CF:84,\n0x1122F:84,\n0x11230:84,\n0x11231:84,\n0x11234:84,\n0x11236:84,\n0x11237:84,\n0x1123E:84,\n0x11241:84,\n0x112DF:84,\n0x112E3:84,\n0x112E4:84,\n0x112E5:84,\n0x112E6:84,\n0x112E7:84,\n0x112E8:84,\n0x112E9:84,\n0x112EA:84,\n0x11300:84,\n0x11301:84,\n0x1133B:84,\n0x1133C:84,\n0x11340:84,\n0x11366:84,\n0x11367:84,\n0x11368:84,\n0x11369:84,\n0x1136A:84,\n0x1136B:84,\n0x1136C:84,\n0x11370:84,\n0x11371:84,\n0x11372:84,\n0x11373:84,\n0x11374:84,\n0x11438:84,\n0x11439:84,\n0x1143A:84,\n0x1143B:84,\n0x1143C:84,\n0x1143D:84,\n0x1143E:84,\n0x1143F:84,\n0x11442:84,\n0x11443:84,\n0x11444:84,\n0x11446:84,\n0x1145E:84,\n0x114B3:84,\n0x114B4:84,\n0x114B5:84,\n0x114B6:84,\n0x114B7:84,\n0x114B8:84,\n0x114BA:84,\n0x114BF:84,\n0x114C0:84,\n0x114C2:84,\n0x114C3:84,\n0x115B2:84,\n0x115B3:84,\n0x115B4:84,\n0x115B5:84,\n0x115BC:84,\n0x115BD:84,\n0x115BF:84,\n0x115C0:84,\n0x115DC:84,\n0x115DD:84,\n0x11633:84,\n0x11634:84,\n0x11635:84,\n0x11636:84,\n0x11637:84,\n0x11638:84,\n0x11639:84,\n0x1163A:84,\n0x1163D:84,\n0x1163F:84,\n0x11640:84,\n0x116AB:84,\n0x116AD:84,\n0x116B0:84,\n0x116B1:84,\n0x116B2:84,\n0x116B3:84,\n0x116B4:84,\n0x116B5:84,\n0x116B7:84,\n0x1171D:84,\n0x1171E:84,\n0x1171F:84,\n0x11722:84,\n0x11723:84,\n0x11724:84,\n0x11725:84,\n0x11727:84,\n0x11728:84,\n0x11729:84,\n0x1172A:84,\n0x1172B:84,\n0x1182F:84,\n0x11830:84,\n0x11831:84,\n0x11832:84,\n0x11833:84,\n0x11834:84,\n0x11835:84,\n0x11836:84,\n0x11837:84,\n0x11839:84,\n0x1183A:84,\n0x1193B:84,\n0x1193C:84,\n0x1193E:84,\n0x11943:84,\n0x119D4:84,\n0x119D5:84,\n0x119D6:84,\n0x119D7:84,\n0x119DA:84,\n0x119DB:84,\n0x119E0:84,\n0x11A01:84,\n0x11A02:84,\n0x11A03:84,\n0x11A04:84,\n0x11A05:84,\n0x11A06:84,\n0x11A07:84,\n0x11A08:84,\n0x11A09:84,\n0x11A0A:84,\n0x11A33:84,\n0x11A34:84,\n0x11A35:84,\n0x11A36:84,\n0x11A37:84,\n0x11A38:84,\n0x11A3B:84,\n0x11A3C:84,\n0x11A3D:84,\n0x11A3E:84,\n0x11A47:84,\n0x11A51:84,\n0x11A52:84,\n0x11A53:84,\n0x11A54:84,\n0x11A55:84,\n0x11A56:84,\n0x11A59:84,\n0x11A5A:84,\n0x11A5B:84,\n0x11A8A:84,\n0x11A8B:84,\n0x11A8C:84,\n0x11A8D:84,\n0x11A8E:84,\n0x11A8F:84,\n0x11A90:84,\n0x11A91:84,\n0x11A92:84,\n0x11A93:84,\n0x11A94:84,\n0x11A95:84,\n0x11A96:84,\n0x11A98:84,\n0x11A99:84,\n0x11C30:84,\n0x11C31:84,\n0x11C32:84,\n0x11C33:84,\n0x11C34:84,\n0x11C35:84,\n0x11C36:84,\n0x11C38:84,\n0x11C39:84,\n0x11C3A:84,\n0x11C3B:84,\n0x11C3C:84,\n0x11C3D:84,\n0x11C3F:84,\n0x11C92:84,\n0x11C93:84,\n0x11C94:84,\n0x11C95:84,\n0x11C96:84,\n0x11C97:84,\n0x11C98:84,\n0x11C99:84,\n0x11C9A:84,\n0x11C9B:84,\n0x11C9C:84,\n0x11C9D:84,\n0x11C9E:84,\n0x11C9F:84,\n0x11CA0:84,\n0x11CA1:84,\n0x11CA2:84,\n0x11CA3:84,\n0x11CA4:84,\n0x11CA5:84,\n0x11CA6:84,\n0x11CA7:84,\n0x11CAA:84,\n0x11CAB:84,\n0x11CAC:84,\n0x11CAD:84,\n0x11CAE:84,\n0x11CAF:84,\n0x11CB0:84,\n0x11CB2:84,\n0x11CB3:84,\n0x11CB5:84,\n0x11CB6:84,\n0x11D31:84,\n0x11D32:84,\n0x11D33:84,\n0x11D34:84,\n0x11D35:84,\n0x11D36:84,\n0x11D3A:84,\n0x11D3C:84,\n0x11D3D:84,\n0x11D3F:84,\n0x11D40:84,\n0x11D41:84,\n0x11D42:84,\n0x11D43:84,\n0x11D44:84,\n0x11D45:84,\n0x11D47:84,\n0x11D90:84,\n0x11D91:84,\n0x11D95:84,\n0x11D97:84,\n0x11EF3:84,\n0x11EF4:84,\n0x11F00:84,\n0x11F01:84,\n0x11F36:84,\n0x11F37:84,\n0x11F38:84,\n0x11F39:84,\n0x11F3A:84,\n0x11F40:84,\n0x11F42:84,\n0x13430:84,\n0x13431:84,\n0x13432:84,\n0x13433:84,\n0x13434:84,\n0x13435:84,\n0x13436:84,\n0x13437:84,\n0x13438:84,\n0x13439:84,\n0x1343A:84,\n0x1343B:84,\n0x1343C:84,\n0x1343D:84,\n0x1343E:84,\n0x1343F:84,\n0x13440:84,\n0x13447:84,\n0x13448:84,\n0x13449:84,\n0x1344A:84,\n0x1344B:84,\n0x1344C:84,\n0x1344D:84,\n0x1344E:84,\n0x1344F:84,\n0x13450:84,\n0x13451:84,\n0x13452:84,\n0x13453:84,\n0x13454:84,\n0x13455:84,\n0x16AF0:84,\n0x16AF1:84,\n0x16AF2:84,\n0x16AF3:84,\n0x16AF4:84,\n0x16B30:84,\n0x16B31:84,\n0x16B32:84,\n0x16B33:84,\n0x16B34:84,\n0x16B35:84,\n0x16B36:84,\n0x16F4F:84,\n0x16F8F:84,\n0x16F90:84,\n0x16F91:84,\n0x16F92:84,\n0x16FE4:84,\n0x1BC9D:84,\n0x1BC9E:84,\n0x1BCA0:84,\n0x1BCA1:84,\n0x1BCA2:84,\n0x1BCA3:84,\n0x1CF00:84,\n0x1CF01:84,\n0x1CF02:84,\n0x1CF03:84,\n0x1CF04:84,\n0x1CF05:84,\n0x1CF06:84,\n0x1CF07:84,\n0x1CF08:84,\n0x1CF09:84,\n0x1CF0A:84,\n0x1CF0B:84,\n0x1CF0C:84,\n0x1CF0D:84,\n0x1CF0E:84,\n0x1CF0F:84,\n0x1CF10:84,\n0x1CF11:84,\n0x1CF12:84,\n0x1CF13:84,\n0x1CF14:84,\n0x1CF15:84,\n0x1CF16:84,\n0x1CF17:84,\n0x1CF18:84,\n0x1CF19:84,\n0x1CF1A:84,\n0x1CF1B:84,\n0x1CF1C:84,\n0x1CF1D:84,\n0x1CF1E:84,\n0x1CF1F:84,\n0x1CF20:84,\n0x1CF21:84,\n0x1CF22:84,\n0x1CF23:84,\n0x1CF24:84,\n0x1CF25:84,\n0x1CF26:84,\n0x1CF27:84,\n0x1CF28:84,\n0x1CF29:84,\n0x1CF2A:84,\n0x1CF2B:84,\n0x1CF2C:84,\n0x1CF2D:84,\n0x1CF30:84,\n0x1CF31:84,\n0x1CF32:84,\n0x1CF33:84,\n0x1CF34:84,\n0x1CF35:84,\n0x1CF36:84,\n0x1CF37:84,\n0x1CF38:84,\n0x1CF39:84,\n0x1CF3A:84,\n0x1CF3B:84,\n0x1CF3C:84,\n0x1CF3D:84,\n0x1CF3E:84,\n0x1CF3F:84,\n0x1CF40:84,\n0x1CF41:84,\n0x1CF42:84,\n0x1CF43:84,\n0x1CF44:84,\n0x1CF45:84,\n0x1CF46:84,\n0x1D167:84,\n0x1D168:84,\n0x1D169:84,\n0x1D173:84,\n0x1D174:84,\n0x1D175:84,\n0x1D176:84,\n0x1D177:84,\n0x1D178:84,\n0x1D179:84,\n0x1D17A:84,\n0x1D17B:84,\n0x1D17C:84,\n0x1D17D:84,\n0x1D17E:84,\n0x1D17F:84,\n0x1D180:84,\n0x1D181:84,\n0x1D182:84,\n0x1D185:84,\n0x1D186:84,\n0x1D187:84,\n0x1D188:84,\n0x1D189:84,\n0x1D18A:84,\n0x1D18B:84,\n0x1D1AA:84,\n0x1D1AB:84,\n0x1D1AC:84,\n0x1D1AD:84,\n0x1D242:84,\n0x1D243:84,\n0x1D244:84,\n0x1DA00:84,\n0x1DA01:84,\n0x1DA02:84,\n0x1DA03:84,\n0x1DA04:84,\n0x1DA05:84,\n0x1DA06:84,\n0x1DA07:84,\n0x1DA08:84,\n0x1DA09:84,\n0x1DA0A:84,\n0x1DA0B:84,\n0x1DA0C:84,\n0x1DA0D:84,\n0x1DA0E:84,\n0x1DA0F:84,\n0x1DA10:84,\n0x1DA11:84,\n0x1DA12:84,\n0x1DA13:84,\n0x1DA14:84,\n0x1DA15:84,\n0x1DA16:84,\n0x1DA17:84,\n0x1DA18:84,\n0x1DA19:84,\n0x1DA1A:84,\n0x1DA1B:84,\n0x1DA1C:84,\n0x1DA1D:84,\n0x1DA1E:84,\n0x1DA1F:84,\n0x1DA20:84,\n0x1DA21:84,\n0x1DA22:84,\n0x1DA23:84,\n0x1DA24:84,\n0x1DA25:84,\n0x1DA26:84,\n0x1DA27:84,\n0x1DA28:84,\n0x1DA29:84,\n0x1DA2A:84,\n0x1DA2B:84,\n0x1DA2C:84,\n0x1DA2D:84,\n0x1DA2E:84,\n0x1DA2F:84,\n0x1DA30:84,\n0x1DA31:84,\n0x1DA32:84,\n0x1DA33:84,\n0x1DA34:84,\n0x1DA35:84,\n0x1DA36:84,\n0x1DA3B:84,\n0x1DA3C:84,\n0x1DA3D:84,\n0x1DA3E:84,\n0x1DA3F:84,\n0x1DA40:84,\n0x1DA41:84,\n0x1DA42:84,\n0x1DA43:84,\n0x1DA44:84,\n0x1DA45:84,\n0x1DA46:84,\n0x1DA47:84,\n0x1DA48:84,\n0x1DA49:84,\n0x1DA4A:84,\n0x1DA4B:84,\n0x1DA4C:84,\n0x1DA4D:84,\n0x1DA4E:84,\n0x1DA4F:84,\n0x1DA50:84,\n0x1DA51:84,\n0x1DA52:84,\n0x1DA53:84,\n0x1DA54:84,\n0x1DA55:84,\n0x1DA56:84,\n0x1DA57:84,\n0x1DA58:84,\n0x1DA59:84,\n0x1DA5A:84,\n0x1DA5B:84,\n0x1DA5C:84,\n0x1DA5D:84,\n0x1DA5E:84,\n0x1DA5F:84,\n0x1DA60:84,\n0x1DA61:84,\n0x1DA62:84,\n0x1DA63:84,\n0x1DA64:84,\n0x1DA65:84,\n0x1DA66:84,\n0x1DA67:84,\n0x1DA68:84,\n0x1DA69:84,\n0x1DA6A:84,\n0x1DA6B:84,\n0x1DA6C:84,\n0x1DA75:84,\n0x1DA84:84,\n0x1DA9B:84,\n0x1DA9C:84,\n0x1DA9D:84,\n0x1DA9E:84,\n0x1DA9F:84,\n0x1DAA1:84,\n0x1DAA2:84,\n0x1DAA3:84,\n0x1DAA4:84,\n0x1DAA5:84,\n0x1DAA6:84,\n0x1DAA7:84,\n0x1DAA8:84,\n0x1DAA9:84,\n0x1DAAA:84,\n0x1DAAB:84,\n0x1DAAC:84,\n0x1DAAD:84,\n0x1DAAE:84,\n0x1DAAF:84,\n0x1E000:84,\n0x1E001:84,\n0x1E002:84,\n0x1E003:84,\n0x1E004:84,\n0x1E005:84,\n0x1E006:84,\n0x1E008:84,\n0x1E009:84,\n0x1E00A:84,\n0x1E00B:84,\n0x1E00C:84,\n0x1E00D:84,\n0x1E00E:84,\n0x1E00F:84,\n0x1E010:84,\n0x1E011:84,\n0x1E012:84,\n0x1E013:84,\n0x1E014:84,\n0x1E015:84,\n0x1E016:84,\n0x1E017:84,\n0x1E018:84,\n0x1E01B:84,\n0x1E01C:84,\n0x1E01D:84,\n0x1E01E:84,\n0x1E01F:84,\n0x1E020:84,\n0x1E021:84,\n0x1E023:84,\n0x1E024:84,\n0x1E026:84,\n0x1E027:84,\n0x1E028:84,\n0x1E029:84,\n0x1E02A:84,\n0x1E08F:84,\n0x1E130:84,\n0x1E131:84,\n0x1E132:84,\n0x1E133:84,\n0x1E134:84,\n0x1E135:84,\n0x1E136:84,\n0x1E2AE:84,\n0x1E2EC:84,\n0x1E2ED:84,\n0x1E2EE:84,\n0x1E2EF:84,\n0x1E4EC:84,\n0x1E4ED:84,\n0x1E4EE:84,\n0x1E4EF:84,\n0x1E8D0:84,\n0x1E8D1:84,\n0x1E8D2:84,\n0x1E8D3:84,\n0x1E8D4:84,\n0x1E8D5:84,\n0x1E8D6:84,\n0x1E900:68,\n0x1E901:68,\n0x1E902:68,\n0x1E903:68,\n0x1E904:68,\n0x1E905:68,\n0x1E906:68,\n0x1E907:68,\n0x1E908:68,\n0x1E909:68,\n0x1E90A:68,\n0x1E90B:68,\n0x1E90C:68,\n0x1E90D:68,\n0x1E90E:68,\n0x1E90F:68,\n0x1E910:68,\n0x1E911:68,\n0x1E912:68,\n0x1E913:68,\n0x1E914:68,\n0x1E915:68,\n0x1E916:68,\n0x1E917:68,\n0x1E918:68,\n0x1E919:68,\n0x1E91A:68,\n0x1E91B:68,\n0x1E91C:68,\n0x1E91D:68,\n0x1E91E:68,\n0x1E91F:68,\n0x1E920:68,\n0x1E921:68,\n0x1E922:68,\n0x1E923:68,\n0x1E924:68,\n0x1E925:68,\n0x1E926:68,\n0x1E927:68,\n0x1E928:68,\n0x1E929:68,\n0x1E92A:68,\n0x1E92B:68,\n0x1E92C:68,\n0x1E92D:68,\n0x1E92E:68,\n0x1E92F:68,\n0x1E930:68,\n0x1E931:68,\n0x1E932:68,\n0x1E933:68,\n0x1E934:68,\n0x1E935:68,\n0x1E936:68,\n0x1E937:68,\n0x1E938:68,\n0x1E939:68,\n0x1E93A:68,\n0x1E93B:68,\n0x1E93C:68,\n0x1E93D:68,\n0x1E93E:68,\n0x1E93F:68,\n0x1E940:68,\n0x1E941:68,\n0x1E942:68,\n0x1E943:68,\n0x1E944:84,\n0x1E945:84,\n0x1E946:84,\n0x1E947:84,\n0x1E948:84,\n0x1E949:84,\n0x1E94A:84,\n0x1E94B:84,\n0xE0001:84,\n0xE0020:84,\n0xE0021:84,\n0xE0022:84,\n0xE0023:84,\n0xE0024:84,\n0xE0025:84,\n0xE0026:84,\n0xE0027:84,\n0xE0028:84,\n0xE0029:84,\n0xE002A:84,\n0xE002B:84,\n0xE002C:84,\n0xE002D:84,\n0xE002E:84,\n0xE002F:84,\n0xE0030:84,\n0xE0031:84,\n0xE0032:84,\n0xE0033:84,\n0xE0034:84,\n0xE0035:84,\n0xE0036:84,\n0xE0037:84,\n0xE0038:84,\n0xE0039:84,\n0xE003A:84,\n0xE003B:84,\n0xE003C:84,\n0xE003D:84,\n0xE003E:84,\n0xE003F:84,\n0xE0040:84,\n0xE0041:84,\n0xE0042:84,\n0xE0043:84,\n0xE0044:84,\n0xE0045:84,\n0xE0046:84,\n0xE0047:84,\n0xE0048:84,\n0xE0049:84,\n0xE004A:84,\n0xE004B:84,\n0xE004C:84,\n0xE004D:84,\n0xE004E:84,\n0xE004F:84,\n0xE0050:84,\n0xE0051:84,\n0xE0052:84,\n0xE0053:84,\n0xE0054:84,\n0xE0055:84,\n0xE0056:84,\n0xE0057:84,\n0xE0058:84,\n0xE0059:84,\n0xE005A:84,\n0xE005B:84,\n0xE005C:84,\n0xE005D:84,\n0xE005E:84,\n0xE005F:84,\n0xE0060:84,\n0xE0061:84,\n0xE0062:84,\n0xE0063:84,\n0xE0064:84,\n0xE0065:84,\n0xE0066:84,\n0xE0067:84,\n0xE0068:84,\n0xE0069:84,\n0xE006A:84,\n0xE006B:84,\n0xE006C:84,\n0xE006D:84,\n0xE006E:84,\n0xE006F:84,\n0xE0070:84,\n0xE0071:84,\n0xE0072:84,\n0xE0073:84,\n0xE0074:84,\n0xE0075:84,\n0xE0076:84,\n0xE0077:84,\n0xE0078:84,\n0xE0079:84,\n0xE007A:84,\n0xE007B:84,\n0xE007C:84,\n0xE007D:84,\n0xE007E:84,\n0xE007F:84,\n0xE0100:84,\n0xE0101:84,\n0xE0102:84,\n0xE0103:84,\n0xE0104:84,\n0xE0105:84,\n0xE0106:84,\n0xE0107:84,\n0xE0108:84,\n0xE0109:84,\n0xE010A:84,\n0xE010B:84,\n0xE010C:84,\n0xE010D:84,\n0xE010E:84,\n0xE010F:84,\n0xE0110:84,\n0xE0111:84,\n0xE0112:84,\n0xE0113:84,\n0xE0114:84,\n0xE0115:84,\n0xE0116:84,\n0xE0117:84,\n0xE0118:84,\n0xE0119:84,\n0xE011A:84,\n0xE011B:84,\n0xE011C:84,\n0xE011D:84,\n0xE011E:84,\n0xE011F:84,\n0xE0120:84,\n0xE0121:84,\n0xE0122:84,\n0xE0123:84,\n0xE0124:84,\n0xE0125:84,\n0xE0126:84,\n0xE0127:84,\n0xE0128:84,\n0xE0129:84,\n0xE012A:84,\n0xE012B:84,\n0xE012C:84,\n0xE012D:84,\n0xE012E:84,\n0xE012F:84,\n0xE0130:84,\n0xE0131:84,\n0xE0132:84,\n0xE0133:84,\n0xE0134:84,\n0xE0135:84,\n0xE0136:84,\n0xE0137:84,\n0xE0138:84,\n0xE0139:84,\n0xE013A:84,\n0xE013B:84,\n0xE013C:84,\n0xE013D:84,\n0xE013E:84,\n0xE013F:84,\n0xE0140:84,\n0xE0141:84,\n0xE0142:84,\n0xE0143:84,\n0xE0144:84,\n0xE0145:84,\n0xE0146:84,\n0xE0147:84,\n0xE0148:84,\n0xE0149:84,\n0xE014A:84,\n0xE014B:84,\n0xE014C:84,\n0xE014D:84,\n0xE014E:84,\n0xE014F:84,\n0xE0150:84,\n0xE0151:84,\n0xE0152:84,\n0xE0153:84,\n0xE0154:84,\n0xE0155:84,\n0xE0156:84,\n0xE0157:84,\n0xE0158:84,\n0xE0159:84,\n0xE015A:84,\n0xE015B:84,\n0xE015C:84,\n0xE015D:84,\n0xE015E:84,\n0xE015F:84,\n0xE0160:84,\n0xE0161:84,\n0xE0162:84,\n0xE0163:84,\n0xE0164:84,\n0xE0165:84,\n0xE0166:84,\n0xE0167:84,\n0xE0168:84,\n0xE0169:84,\n0xE016A:84,\n0xE016B:84,\n0xE016C:84,\n0xE016D:84,\n0xE016E:84,\n0xE016F:84,\n0xE0170:84,\n0xE0171:84,\n0xE0172:84,\n0xE0173:84,\n0xE0174:84,\n0xE0175:84,\n0xE0176:84,\n0xE0177:84,\n0xE0178:84,\n0xE0179:84,\n0xE017A:84,\n0xE017B:84,\n0xE017C:84,\n0xE017D:84,\n0xE017E:84,\n0xE017F:84,\n0xE0180:84,\n0xE0181:84,\n0xE0182:84,\n0xE0183:84,\n0xE0184:84,\n0xE0185:84,\n0xE0186:84,\n0xE0187:84,\n0xE0188:84,\n0xE0189:84,\n0xE018A:84,\n0xE018B:84,\n0xE018C:84,\n0xE018D:84,\n0xE018E:84,\n0xE018F:84,\n0xE0190:84,\n0xE0191:84,\n0xE0192:84,\n0xE0193:84,\n0xE0194:84,\n0xE0195:84,\n0xE0196:84,\n0xE0197:84,\n0xE0198:84,\n0xE0199:84,\n0xE019A:84,\n0xE019B:84,\n0xE019C:84,\n0xE019D:84,\n0xE019E:84,\n0xE019F:84,\n0xE01A0:84,\n0xE01A1:84,\n0xE01A2:84,\n0xE01A3:84,\n0xE01A4:84,\n0xE01A5:84,\n0xE01A6:84,\n0xE01A7:84,\n0xE01A8:84,\n0xE01A9:84,\n0xE01AA:84,\n0xE01AB:84,\n0xE01AC:84,\n0xE01AD:84,\n0xE01AE:84,\n0xE01AF:84,\n0xE01B0:84,\n0xE01B1:84,\n0xE01B2:84,\n0xE01B3:84,\n0xE01B4:84,\n0xE01B5:84,\n0xE01B6:84,\n0xE01B7:84,\n0xE01B8:84,\n0xE01B9:84,\n0xE01BA:84,\n0xE01BB:84,\n0xE01BC:84,\n0xE01BD:84,\n0xE01BE:84,\n0xE01BF:84,\n0xE01C0:84,\n0xE01C1:84,\n0xE01C2:84,\n0xE01C3:84,\n0xE01C4:84,\n0xE01C5:84,\n0xE01C6:84,\n0xE01C7:84,\n0xE01C8:84,\n0xE01C9:84,\n0xE01CA:84,\n0xE01CB:84,\n0xE01CC:84,\n0xE01CD:84,\n0xE01CE:84,\n0xE01CF:84,\n0xE01D0:84,\n0xE01D1:84,\n0xE01D2:84,\n0xE01D3:84,\n0xE01D4:84,\n0xE01D5:84,\n0xE01D6:84,\n0xE01D7:84,\n0xE01D8:84,\n0xE01D9:84,\n0xE01DA:84,\n0xE01DB:84,\n0xE01DC:84,\n0xE01DD:84,\n0xE01DE:84,\n0xE01DF:84,\n0xE01E0:84,\n0xE01E1:84,\n0xE01E2:84,\n0xE01E3:84,\n0xE01E4:84,\n0xE01E5:84,\n0xE01E6:84,\n0xE01E7:84,\n0xE01E8:84,\n0xE01E9:84,\n0xE01EA:84,\n0xE01EB:84,\n0xE01EC:84,\n0xE01ED:84,\n0xE01EE:84,\n0xE01EF:84,\n}\ncodepoint_classes={\n\"PVALID\":(\n0x2D0000002E,\n0x300000003A,\n0x610000007B,\n0xDF000000F7,\n0xF800000100,\n0x10100000102,\n0x10300000104,\n0x10500000106,\n0x10700000108,\n0x1090000010A,\n0x10B0000010C,\n0x10D0000010E,\n0x10F00000110,\n0x11100000112,\n0x11300000114,\n0x11500000116,\n0x11700000118,\n0x1190000011A,\n0x11B0000011C,\n0x11D0000011E,\n0x11F00000120,\n0x12100000122,\n0x12300000124,\n0x12500000126,\n0x12700000128,\n0x1290000012A,\n0x12B0000012C,\n0x12D0000012E,\n0x12F00000130,\n0x13100000132,\n0x13500000136,\n0x13700000139,\n0x13A0000013B,\n0x13C0000013D,\n0x13E0000013F,\n0x14200000143,\n0x14400000145,\n0x14600000147,\n0x14800000149,\n0x14B0000014C,\n0x14D0000014E,\n0x14F00000150,\n0x15100000152,\n0x15300000154,\n0x15500000156,\n0x15700000158,\n0x1590000015A,\n0x15B0000015C,\n0x15D0000015E,\n0x15F00000160,\n0x16100000162,\n0x16300000164,\n0x16500000166,\n0x16700000168,\n0x1690000016A,\n0x16B0000016C,\n0x16D0000016E,\n0x16F00000170,\n0x17100000172,\n0x17300000174,\n0x17500000176,\n0x17700000178,\n0x17A0000017B,\n0x17C0000017D,\n0x17E0000017F,\n0x18000000181,\n0x18300000184,\n0x18500000186,\n0x18800000189,\n0x18C0000018E,\n0x19200000193,\n0x19500000196,\n0x1990000019C,\n0x19E0000019F,\n0x1A1000001A2,\n0x1A3000001A4,\n0x1A5000001A6,\n0x1A8000001A9,\n0x1AA000001AC,\n0x1AD000001AE,\n0x1B0000001B1,\n0x1B4000001B5,\n0x1B6000001B7,\n0x1B9000001BC,\n0x1BD000001C4,\n0x1CE000001CF,\n0x1D0000001D1,\n0x1D2000001D3,\n0x1D4000001D5,\n0x1D6000001D7,\n0x1D8000001D9,\n0x1DA000001DB,\n0x1DC000001DE,\n0x1DF000001E0,\n0x1E1000001E2,\n0x1E3000001E4,\n0x1E5000001E6,\n0x1E7000001E8,\n0x1E9000001EA,\n0x1EB000001EC,\n0x1ED000001EE,\n0x1EF000001F1,\n0x1F5000001F6,\n0x1F9000001FA,\n0x1FB000001FC,\n0x1FD000001FE,\n0x1FF00000200,\n0x20100000202,\n0x20300000204,\n0x20500000206,\n0x20700000208,\n0x2090000020A,\n0x20B0000020C,\n0x20D0000020E,\n0x20F00000210,\n0x21100000212,\n0x21300000214,\n0x21500000216,\n0x21700000218,\n0x2190000021A,\n0x21B0000021C,\n0x21D0000021E,\n0x21F00000220,\n0x22100000222,\n0x22300000224,\n0x22500000226,\n0x22700000228,\n0x2290000022A,\n0x22B0000022C,\n0x22D0000022E,\n0x22F00000230,\n0x23100000232,\n0x2330000023A,\n0x23C0000023D,\n0x23F00000241,\n0x24200000243,\n0x24700000248,\n0x2490000024A,\n0x24B0000024C,\n0x24D0000024E,\n0x24F000002B0,\n0x2B9000002C2,\n0x2C6000002D2,\n0x2EC000002ED,\n0x2EE000002EF,\n0x30000000340,\n0x34200000343,\n0x3460000034F,\n0x35000000370,\n0x37100000372,\n0x37300000374,\n0x37700000378,\n0x37B0000037E,\n0x39000000391,\n0x3AC000003CF,\n0x3D7000003D8,\n0x3D9000003DA,\n0x3DB000003DC,\n0x3DD000003DE,\n0x3DF000003E0,\n0x3E1000003E2,\n0x3E3000003E4,\n0x3E5000003E6,\n0x3E7000003E8,\n0x3E9000003EA,\n0x3EB000003EC,\n0x3ED000003EE,\n0x3EF000003F0,\n0x3F3000003F4,\n0x3F8000003F9,\n0x3FB000003FD,\n0x43000000460,\n0x46100000462,\n0x46300000464,\n0x46500000466,\n0x46700000468,\n0x4690000046A,\n0x46B0000046C,\n0x46D0000046E,\n0x46F00000470,\n0x47100000472,\n0x47300000474,\n0x47500000476,\n0x47700000478,\n0x4790000047A,\n0x47B0000047C,\n0x47D0000047E,\n0x47F00000480,\n0x48100000482,\n0x48300000488,\n0x48B0000048C,\n0x48D0000048E,\n0x48F00000490,\n0x49100000492,\n0x49300000494,\n0x49500000496,\n0x49700000498,\n0x4990000049A,\n0x49B0000049C,\n0x49D0000049E,\n0x49F000004A0,\n0x4A1000004A2,\n0x4A3000004A4,\n0x4A5000004A6,\n0x4A7000004A8,\n0x4A9000004AA,\n0x4AB000004AC,\n0x4AD000004AE,\n0x4AF000004B0,\n0x4B1000004B2,\n0x4B3000004B4,\n0x4B5000004B6,\n0x4B7000004B8,\n0x4B9000004BA,\n0x4BB000004BC,\n0x4BD000004BE,\n0x4BF000004C0,\n0x4C2000004C3,\n0x4C4000004C5,\n0x4C6000004C7,\n0x4C8000004C9,\n0x4CA000004CB,\n0x4CC000004CD,\n0x4CE000004D0,\n0x4D1000004D2,\n0x4D3000004D4,\n0x4D5000004D6,\n0x4D7000004D8,\n0x4D9000004DA,\n0x4DB000004DC,\n0x4DD000004DE,\n0x4DF000004E0,\n0x4E1000004E2,\n0x4E3000004E4,\n0x4E5000004E6,\n0x4E7000004E8,\n0x4E9000004EA,\n0x4EB000004EC,\n0x4ED000004EE,\n0x4EF000004F0,\n0x4F1000004F2,\n0x4F3000004F4,\n0x4F5000004F6,\n0x4F7000004F8,\n0x4F9000004FA,\n0x4FB000004FC,\n0x4FD000004FE,\n0x4FF00000500,\n0x50100000502,\n0x50300000504,\n0x50500000506,\n0x50700000508,\n0x5090000050A,\n0x50B0000050C,\n0x50D0000050E,\n0x50F00000510,\n0x51100000512,\n0x51300000514,\n0x51500000516,\n0x51700000518,\n0x5190000051A,\n0x51B0000051C,\n0x51D0000051E,\n0x51F00000520,\n0x52100000522,\n0x52300000524,\n0x52500000526,\n0x52700000528,\n0x5290000052A,\n0x52B0000052C,\n0x52D0000052E,\n0x52F00000530,\n0x5590000055A,\n0x56000000587,\n0x58800000589,\n0x591000005BE,\n0x5BF000005C0,\n0x5C1000005C3,\n0x5C4000005C6,\n0x5C7000005C8,\n0x5D0000005EB,\n0x5EF000005F3,\n0x6100000061B,\n0x62000000640,\n0x64100000660,\n0x66E00000675,\n0x679000006D4,\n0x6D5000006DD,\n0x6DF000006E9,\n0x6EA000006F0,\n0x6FA00000700,\n0x7100000074B,\n0x74D000007B2,\n0x7C0000007F6,\n0x7FD000007FE,\n0x8000000082E,\n0x8400000085C,\n0x8600000086B,\n0x87000000888,\n0x8890000088F,\n0x898000008E2,\n0x8E300000958,\n0x96000000964,\n0x96600000970,\n0x97100000984,\n0x9850000098D,\n0x98F00000991,\n0x993000009A9,\n0x9AA000009B1,\n0x9B2000009B3,\n0x9B6000009BA,\n0x9BC000009C5,\n0x9C7000009C9,\n0x9CB000009CF,\n0x9D7000009D8,\n0x9E0000009E4,\n0x9E6000009F2,\n0x9FC000009FD,\n0x9FE000009FF,\n0xA0100000A04,\n0xA0500000A0B,\n0xA0F00000A11,\n0xA1300000A29,\n0xA2A00000A31,\n0xA3200000A33,\n0xA3500000A36,\n0xA3800000A3A,\n0xA3C00000A3D,\n0xA3E00000A43,\n0xA4700000A49,\n0xA4B00000A4E,\n0xA5100000A52,\n0xA5C00000A5D,\n0xA6600000A76,\n0xA8100000A84,\n0xA8500000A8E,\n0xA8F00000A92,\n0xA9300000AA9,\n0xAAA00000AB1,\n0xAB200000AB4,\n0xAB500000ABA,\n0xABC00000AC6,\n0xAC700000ACA,\n0xACB00000ACE,\n0xAD000000AD1,\n0xAE000000AE4,\n0xAE600000AF0,\n0xAF900000B00,\n0xB0100000B04,\n0xB0500000B0D,\n0xB0F00000B11,\n0xB1300000B29,\n0xB2A00000B31,\n0xB3200000B34,\n0xB3500000B3A,\n0xB3C00000B45,\n0xB4700000B49,\n0xB4B00000B4E,\n0xB5500000B58,\n0xB5F00000B64,\n0xB6600000B70,\n0xB7100000B72,\n0xB8200000B84,\n0xB8500000B8B,\n0xB8E00000B91,\n0xB9200000B96,\n0xB9900000B9B,\n0xB9C00000B9D,\n0xB9E00000BA0,\n0xBA300000BA5,\n0xBA800000BAB,\n0xBAE00000BBA,\n0xBBE00000BC3,\n0xBC600000BC9,\n0xBCA00000BCE,\n0xBD000000BD1,\n0xBD700000BD8,\n0xBE600000BF0,\n0xC0000000C0D,\n0xC0E00000C11,\n0xC1200000C29,\n0xC2A00000C3A,\n0xC3C00000C45,\n0xC4600000C49,\n0xC4A00000C4E,\n0xC5500000C57,\n0xC5800000C5B,\n0xC5D00000C5E,\n0xC6000000C64,\n0xC6600000C70,\n0xC8000000C84,\n0xC8500000C8D,\n0xC8E00000C91,\n0xC9200000CA9,\n0xCAA00000CB4,\n0xCB500000CBA,\n0xCBC00000CC5,\n0xCC600000CC9,\n0xCCA00000CCE,\n0xCD500000CD7,\n0xCDD00000CDF,\n0xCE000000CE4,\n0xCE600000CF0,\n0xCF100000CF4,\n0xD0000000D0D,\n0xD0E00000D11,\n0xD1200000D45,\n0xD4600000D49,\n0xD4A00000D4F,\n0xD5400000D58,\n0xD5F00000D64,\n0xD6600000D70,\n0xD7A00000D80,\n0xD8100000D84,\n0xD8500000D97,\n0xD9A00000DB2,\n0xDB300000DBC,\n0xDBD00000DBE,\n0xDC000000DC7,\n0xDCA00000DCB,\n0xDCF00000DD5,\n0xDD600000DD7,\n0xDD800000DE0,\n0xDE600000DF0,\n0xDF200000DF4,\n0xE0100000E33,\n0xE3400000E3B,\n0xE4000000E4F,\n0xE5000000E5A,\n0xE8100000E83,\n0xE8400000E85,\n0xE8600000E8B,\n0xE8C00000EA4,\n0xEA500000EA6,\n0xEA700000EB3,\n0xEB400000EBE,\n0xEC000000EC5,\n0xEC600000EC7,\n0xEC800000ECF,\n0xED000000EDA,\n0xEDE00000EE0,\n0xF0000000F01,\n0xF0B00000F0C,\n0xF1800000F1A,\n0xF2000000F2A,\n0xF3500000F36,\n0xF3700000F38,\n0xF3900000F3A,\n0xF3E00000F43,\n0xF4400000F48,\n0xF4900000F4D,\n0xF4E00000F52,\n0xF5300000F57,\n0xF5800000F5C,\n0xF5D00000F69,\n0xF6A00000F6D,\n0xF7100000F73,\n0xF7400000F75,\n0xF7A00000F81,\n0xF8200000F85,\n0xF8600000F93,\n0xF9400000F98,\n0xF9900000F9D,\n0xF9E00000FA2,\n0xFA300000FA7,\n0xFA800000FAC,\n0xFAD00000FB9,\n0xFBA00000FBD,\n0xFC600000FC7,\n0x10000000104A,\n0x10500000109E,\n0x10D0000010FB,\n0x10FD00001100,\n0x120000001249,\n0x124A0000124E,\n0x125000001257,\n0x125800001259,\n0x125A0000125E,\n0x126000001289,\n0x128A0000128E,\n0x1290000012B1,\n0x12B2000012B6,\n0x12B8000012BF,\n0x12C0000012C1,\n0x12C2000012C6,\n0x12C8000012D7,\n0x12D800001311,\n0x131200001316,\n0x13180000135B,\n0x135D00001360,\n0x138000001390,\n0x13A0000013F6,\n0x14010000166D,\n0x166F00001680,\n0x16810000169B,\n0x16A0000016EB,\n0x16F1000016F9,\n0x170000001716,\n0x171F00001735,\n0x174000001754,\n0x17600000176D,\n0x176E00001771,\n0x177200001774,\n0x1780000017B4,\n0x17B6000017D4,\n0x17D7000017D8,\n0x17DC000017DE,\n0x17E0000017EA,\n0x18100000181A,\n0x182000001879,\n0x1880000018AB,\n0x18B0000018F6,\n0x19000000191F,\n0x19200000192C,\n0x19300000193C,\n0x19460000196E,\n0x197000001975,\n0x1980000019AC,\n0x19B0000019CA,\n0x19D0000019DA,\n0x1A0000001A1C,\n0x1A2000001A5F,\n0x1A6000001A7D,\n0x1A7F00001A8A,\n0x1A9000001A9A,\n0x1AA700001AA8,\n0x1AB000001ABE,\n0x1ABF00001ACF,\n0x1B0000001B4D,\n0x1B5000001B5A,\n0x1B6B00001B74,\n0x1B8000001BF4,\n0x1C0000001C38,\n0x1C4000001C4A,\n0x1C4D00001C7E,\n0x1CD000001CD3,\n0x1CD400001CFB,\n0x1D0000001D2C,\n0x1D2F00001D30,\n0x1D3B00001D3C,\n0x1D4E00001D4F,\n0x1D6B00001D78,\n0x1D7900001D9B,\n0x1DC000001E00,\n0x1E0100001E02,\n0x1E0300001E04,\n0x1E0500001E06,\n0x1E0700001E08,\n0x1E0900001E0A,\n0x1E0B00001E0C,\n0x1E0D00001E0E,\n0x1E0F00001E10,\n0x1E1100001E12,\n0x1E1300001E14,\n0x1E1500001E16,\n0x1E1700001E18,\n0x1E1900001E1A,\n0x1E1B00001E1C,\n0x1E1D00001E1E,\n0x1E1F00001E20,\n0x1E2100001E22,\n0x1E2300001E24,\n0x1E2500001E26,\n0x1E2700001E28,\n0x1E2900001E2A,\n0x1E2B00001E2C,\n0x1E2D00001E2E,\n0x1E2F00001E30,\n0x1E3100001E32,\n0x1E3300001E34,\n0x1E3500001E36,\n0x1E3700001E38,\n0x1E3900001E3A,\n0x1E3B00001E3C,\n0x1E3D00001E3E,\n0x1E3F00001E40,\n0x1E4100001E42,\n0x1E4300001E44,\n0x1E4500001E46,\n0x1E4700001E48,\n0x1E4900001E4A,\n0x1E4B00001E4C,\n0x1E4D00001E4E,\n0x1E4F00001E50,\n0x1E5100001E52,\n0x1E5300001E54,\n0x1E5500001E56,\n0x1E5700001E58,\n0x1E5900001E5A,\n0x1E5B00001E5C,\n0x1E5D00001E5E,\n0x1E5F00001E60,\n0x1E6100001E62,\n0x1E6300001E64,\n0x1E6500001E66,\n0x1E6700001E68,\n0x1E6900001E6A,\n0x1E6B00001E6C,\n0x1E6D00001E6E,\n0x1E6F00001E70,\n0x1E7100001E72,\n0x1E7300001E74,\n0x1E7500001E76,\n0x1E7700001E78,\n0x1E7900001E7A,\n0x1E7B00001E7C,\n0x1E7D00001E7E,\n0x1E7F00001E80,\n0x1E8100001E82,\n0x1E8300001E84,\n0x1E8500001E86,\n0x1E8700001E88,\n0x1E8900001E8A,\n0x1E8B00001E8C,\n0x1E8D00001E8E,\n0x1E8F00001E90,\n0x1E9100001E92,\n0x1E9300001E94,\n0x1E9500001E9A,\n0x1E9C00001E9E,\n0x1E9F00001EA0,\n0x1EA100001EA2,\n0x1EA300001EA4,\n0x1EA500001EA6,\n0x1EA700001EA8,\n0x1EA900001EAA,\n0x1EAB00001EAC,\n0x1EAD00001EAE,\n0x1EAF00001EB0,\n0x1EB100001EB2,\n0x1EB300001EB4,\n0x1EB500001EB6,\n0x1EB700001EB8,\n0x1EB900001EBA,\n0x1EBB00001EBC,\n0x1EBD00001EBE,\n0x1EBF00001EC0,\n0x1EC100001EC2,\n0x1EC300001EC4,\n0x1EC500001EC6,\n0x1EC700001EC8,\n0x1EC900001ECA,\n0x1ECB00001ECC,\n0x1ECD00001ECE,\n0x1ECF00001ED0,\n0x1ED100001ED2,\n0x1ED300001ED4,\n0x1ED500001ED6,\n0x1ED700001ED8,\n0x1ED900001EDA,\n0x1EDB00001EDC,\n0x1EDD00001EDE,\n0x1EDF00001EE0,\n0x1EE100001EE2,\n0x1EE300001EE4,\n0x1EE500001EE6,\n0x1EE700001EE8,\n0x1EE900001EEA,\n0x1EEB00001EEC,\n0x1EED00001EEE,\n0x1EEF00001EF0,\n0x1EF100001EF2,\n0x1EF300001EF4,\n0x1EF500001EF6,\n0x1EF700001EF8,\n0x1EF900001EFA,\n0x1EFB00001EFC,\n0x1EFD00001EFE,\n0x1EFF00001F08,\n0x1F1000001F16,\n0x1F2000001F28,\n0x1F3000001F38,\n0x1F4000001F46,\n0x1F5000001F58,\n0x1F6000001F68,\n0x1F7000001F71,\n0x1F7200001F73,\n0x1F7400001F75,\n0x1F7600001F77,\n0x1F7800001F79,\n0x1F7A00001F7B,\n0x1F7C00001F7D,\n0x1FB000001FB2,\n0x1FB600001FB7,\n0x1FC600001FC7,\n0x1FD000001FD3,\n0x1FD600001FD8,\n0x1FE000001FE3,\n0x1FE400001FE8,\n0x1FF600001FF7,\n0x214E0000214F,\n0x218400002185,\n0x2C3000002C60,\n0x2C6100002C62,\n0x2C6500002C67,\n0x2C6800002C69,\n0x2C6A00002C6B,\n0x2C6C00002C6D,\n0x2C7100002C72,\n0x2C7300002C75,\n0x2C7600002C7C,\n0x2C8100002C82,\n0x2C8300002C84,\n0x2C8500002C86,\n0x2C8700002C88,\n0x2C8900002C8A,\n0x2C8B00002C8C,\n0x2C8D00002C8E,\n0x2C8F00002C90,\n0x2C9100002C92,\n0x2C9300002C94,\n0x2C9500002C96,\n0x2C9700002C98,\n0x2C9900002C9A,\n0x2C9B00002C9C,\n0x2C9D00002C9E,\n0x2C9F00002CA0,\n0x2CA100002CA2,\n0x2CA300002CA4,\n0x2CA500002CA6,\n0x2CA700002CA8,\n0x2CA900002CAA,\n0x2CAB00002CAC,\n0x2CAD00002CAE,\n0x2CAF00002CB0,\n0x2CB100002CB2,\n0x2CB300002CB4,\n0x2CB500002CB6,\n0x2CB700002CB8,\n0x2CB900002CBA,\n0x2CBB00002CBC,\n0x2CBD00002CBE,\n0x2CBF00002CC0,\n0x2CC100002CC2,\n0x2CC300002CC4,\n0x2CC500002CC6,\n0x2CC700002CC8,\n0x2CC900002CCA,\n0x2CCB00002CCC,\n0x2CCD00002CCE,\n0x2CCF00002CD0,\n0x2CD100002CD2,\n0x2CD300002CD4,\n0x2CD500002CD6,\n0x2CD700002CD8,\n0x2CD900002CDA,\n0x2CDB00002CDC,\n0x2CDD00002CDE,\n0x2CDF00002CE0,\n0x2CE100002CE2,\n0x2CE300002CE5,\n0x2CEC00002CED,\n0x2CEE00002CF2,\n0x2CF300002CF4,\n0x2D0000002D26,\n0x2D2700002D28,\n0x2D2D00002D2E,\n0x2D3000002D68,\n0x2D7F00002D97,\n0x2DA000002DA7,\n0x2DA800002DAF,\n0x2DB000002DB7,\n0x2DB800002DBF,\n0x2DC000002DC7,\n0x2DC800002DCF,\n0x2DD000002DD7,\n0x2DD800002DDF,\n0x2DE000002E00,\n0x2E2F00002E30,\n0x300500003008,\n0x302A0000302E,\n0x303C0000303D,\n0x304100003097,\n0x30990000309B,\n0x309D0000309F,\n0x30A1000030FB,\n0x30FC000030FF,\n0x310500003130,\n0x31A0000031C0,\n0x31F000003200,\n0x340000004DC0,\n0x4E000000A48D,\n0xA4D00000A4FE,\n0xA5000000A60D,\n0xA6100000A62C,\n0xA6410000A642,\n0xA6430000A644,\n0xA6450000A646,\n0xA6470000A648,\n0xA6490000A64A,\n0xA64B0000A64C,\n0xA64D0000A64E,\n0xA64F0000A650,\n0xA6510000A652,\n0xA6530000A654,\n0xA6550000A656,\n0xA6570000A658,\n0xA6590000A65A,\n0xA65B0000A65C,\n0xA65D0000A65E,\n0xA65F0000A660,\n0xA6610000A662,\n0xA6630000A664,\n0xA6650000A666,\n0xA6670000A668,\n0xA6690000A66A,\n0xA66B0000A66C,\n0xA66D0000A670,\n0xA6740000A67E,\n0xA67F0000A680,\n0xA6810000A682,\n0xA6830000A684,\n0xA6850000A686,\n0xA6870000A688,\n0xA6890000A68A,\n0xA68B0000A68C,\n0xA68D0000A68E,\n0xA68F0000A690,\n0xA6910000A692,\n0xA6930000A694,\n0xA6950000A696,\n0xA6970000A698,\n0xA6990000A69A,\n0xA69B0000A69C,\n0xA69E0000A6E6,\n0xA6F00000A6F2,\n0xA7170000A720,\n0xA7230000A724,\n0xA7250000A726,\n0xA7270000A728,\n0xA7290000A72A,\n0xA72B0000A72C,\n0xA72D0000A72E,\n0xA72F0000A732,\n0xA7330000A734,\n0xA7350000A736,\n0xA7370000A738,\n0xA7390000A73A,\n0xA73B0000A73C,\n0xA73D0000A73E,\n0xA73F0000A740,\n0xA7410000A742,\n0xA7430000A744,\n0xA7450000A746,\n0xA7470000A748,\n0xA7490000A74A,\n0xA74B0000A74C,\n0xA74D0000A74E,\n0xA74F0000A750,\n0xA7510000A752,\n0xA7530000A754,\n0xA7550000A756,\n0xA7570000A758,\n0xA7590000A75A,\n0xA75B0000A75C,\n0xA75D0000A75E,\n0xA75F0000A760,\n0xA7610000A762,\n0xA7630000A764,\n0xA7650000A766,\n0xA7670000A768,\n0xA7690000A76A,\n0xA76B0000A76C,\n0xA76D0000A76E,\n0xA76F0000A770,\n0xA7710000A779,\n0xA77A0000A77B,\n0xA77C0000A77D,\n0xA77F0000A780,\n0xA7810000A782,\n0xA7830000A784,\n0xA7850000A786,\n0xA7870000A789,\n0xA78C0000A78D,\n0xA78E0000A790,\n0xA7910000A792,\n0xA7930000A796,\n0xA7970000A798,\n0xA7990000A79A,\n0xA79B0000A79C,\n0xA79D0000A79E,\n0xA79F0000A7A0,\n0xA7A10000A7A2,\n0xA7A30000A7A4,\n0xA7A50000A7A6,\n0xA7A70000A7A8,\n0xA7A90000A7AA,\n0xA7AF0000A7B0,\n0xA7B50000A7B6,\n0xA7B70000A7B8,\n0xA7B90000A7BA,\n0xA7BB0000A7BC,\n0xA7BD0000A7BE,\n0xA7BF0000A7C0,\n0xA7C10000A7C2,\n0xA7C30000A7C4,\n0xA7C80000A7C9,\n0xA7CA0000A7CB,\n0xA7D10000A7D2,\n0xA7D30000A7D4,\n0xA7D50000A7D6,\n0xA7D70000A7D8,\n0xA7D90000A7DA,\n0xA7F60000A7F8,\n0xA7FA0000A828,\n0xA82C0000A82D,\n0xA8400000A874,\n0xA8800000A8C6,\n0xA8D00000A8DA,\n0xA8E00000A8F8,\n0xA8FB0000A8FC,\n0xA8FD0000A92E,\n0xA9300000A954,\n0xA9800000A9C1,\n0xA9CF0000A9DA,\n0xA9E00000A9FF,\n0xAA000000AA37,\n0xAA400000AA4E,\n0xAA500000AA5A,\n0xAA600000AA77,\n0xAA7A0000AAC3,\n0xAADB0000AADE,\n0xAAE00000AAF0,\n0xAAF20000AAF7,\n0xAB010000AB07,\n0xAB090000AB0F,\n0xAB110000AB17,\n0xAB200000AB27,\n0xAB280000AB2F,\n0xAB300000AB5B,\n0xAB600000AB69,\n0xABC00000ABEB,\n0xABEC0000ABEE,\n0xABF00000ABFA,\n0xAC000000D7A4,\n0xFA0E0000FA10,\n0xFA110000FA12,\n0xFA130000FA15,\n0xFA1F0000FA20,\n0xFA210000FA22,\n0xFA230000FA25,\n0xFA270000FA2A,\n0xFB1E0000FB1F,\n0xFE200000FE30,\n0xFE730000FE74,\n0x100000001000C,\n0x1000D00010027,\n0x100280001003B,\n0x1003C0001003E,\n0x1003F0001004E,\n0x100500001005E,\n0x10080000100FB,\n0x101FD000101FE,\n0x102800001029D,\n0x102A0000102D1,\n0x102E0000102E1,\n0x1030000010320,\n0x1032D00010341,\n0x103420001034A,\n0x103500001037B,\n0x103800001039E,\n0x103A0000103C4,\n0x103C8000103D0,\n0x104280001049E,\n0x104A0000104AA,\n0x104D8000104FC,\n0x1050000010528,\n0x1053000010564,\n0x10597000105A2,\n0x105A3000105B2,\n0x105B3000105BA,\n0x105BB000105BD,\n0x1060000010737,\n0x1074000010756,\n0x1076000010768,\n0x1078000010781,\n0x1080000010806,\n0x1080800010809,\n0x1080A00010836,\n0x1083700010839,\n0x1083C0001083D,\n0x1083F00010856,\n0x1086000010877,\n0x108800001089F,\n0x108E0000108F3,\n0x108F4000108F6,\n0x1090000010916,\n0x109200001093A,\n0x10980000109B8,\n0x109BE000109C0,\n0x10A0000010A04,\n0x10A0500010A07,\n0x10A0C00010A14,\n0x10A1500010A18,\n0x10A1900010A36,\n0x10A3800010A3B,\n0x10A3F00010A40,\n0x10A6000010A7D,\n0x10A8000010A9D,\n0x10AC000010AC8,\n0x10AC900010AE7,\n0x10B0000010B36,\n0x10B4000010B56,\n0x10B6000010B73,\n0x10B8000010B92,\n0x10C0000010C49,\n0x10CC000010CF3,\n0x10D0000010D28,\n0x10D3000010D3A,\n0x10E8000010EAA,\n0x10EAB00010EAD,\n0x10EB000010EB2,\n0x10EFD00010F1D,\n0x10F2700010F28,\n0x10F3000010F51,\n0x10F7000010F86,\n0x10FB000010FC5,\n0x10FE000010FF7,\n0x1100000011047,\n0x1106600011076,\n0x1107F000110BB,\n0x110C2000110C3,\n0x110D0000110E9,\n0x110F0000110FA,\n0x1110000011135,\n0x1113600011140,\n0x1114400011148,\n0x1115000011174,\n0x1117600011177,\n0x11180000111C5,\n0x111C9000111CD,\n0x111CE000111DB,\n0x111DC000111DD,\n0x1120000011212,\n0x1121300011238,\n0x1123E00011242,\n0x1128000011287,\n0x1128800011289,\n0x1128A0001128E,\n0x1128F0001129E,\n0x1129F000112A9,\n0x112B0000112EB,\n0x112F0000112FA,\n0x1130000011304,\n0x113050001130D,\n0x1130F00011311,\n0x1131300011329,\n0x1132A00011331,\n0x1133200011334,\n0x113350001133A,\n0x1133B00011345,\n0x1134700011349,\n0x1134B0001134E,\n0x1135000011351,\n0x1135700011358,\n0x1135D00011364,\n0x113660001136D,\n0x1137000011375,\n0x114000001144B,\n0x114500001145A,\n0x1145E00011462,\n0x11480000114C6,\n0x114C7000114C8,\n0x114D0000114DA,\n0x11580000115B6,\n0x115B8000115C1,\n0x115D8000115DE,\n0x1160000011641,\n0x1164400011645,\n0x116500001165A,\n0x11680000116B9,\n0x116C0000116CA,\n0x117000001171B,\n0x1171D0001172C,\n0x117300001173A,\n0x1174000011747,\n0x118000001183B,\n0x118C0000118EA,\n0x118FF00011907,\n0x119090001190A,\n0x1190C00011914,\n0x1191500011917,\n0x1191800011936,\n0x1193700011939,\n0x1193B00011944,\n0x119500001195A,\n0x119A0000119A8,\n0x119AA000119D8,\n0x119DA000119E2,\n0x119E3000119E5,\n0x11A0000011A3F,\n0x11A4700011A48,\n0x11A5000011A9A,\n0x11A9D00011A9E,\n0x11AB000011AF9,\n0x11C0000011C09,\n0x11C0A00011C37,\n0x11C3800011C41,\n0x11C5000011C5A,\n0x11C7200011C90,\n0x11C9200011CA8,\n0x11CA900011CB7,\n0x11D0000011D07,\n0x11D0800011D0A,\n0x11D0B00011D37,\n0x11D3A00011D3B,\n0x11D3C00011D3E,\n0x11D3F00011D48,\n0x11D5000011D5A,\n0x11D6000011D66,\n0x11D6700011D69,\n0x11D6A00011D8F,\n0x11D9000011D92,\n0x11D9300011D99,\n0x11DA000011DAA,\n0x11EE000011EF7,\n0x11F0000011F11,\n0x11F1200011F3B,\n0x11F3E00011F43,\n0x11F5000011F5A,\n0x11FB000011FB1,\n0x120000001239A,\n0x1248000012544,\n0x12F9000012FF1,\n0x1300000013430,\n0x1344000013456,\n0x1440000014647,\n0x1680000016A39,\n0x16A4000016A5F,\n0x16A6000016A6A,\n0x16A7000016ABF,\n0x16AC000016ACA,\n0x16AD000016AEE,\n0x16AF000016AF5,\n0x16B0000016B37,\n0x16B4000016B44,\n0x16B5000016B5A,\n0x16B6300016B78,\n0x16B7D00016B90,\n0x16E6000016E80,\n0x16F0000016F4B,\n0x16F4F00016F88,\n0x16F8F00016FA0,\n0x16FE000016FE2,\n0x16FE300016FE5,\n0x16FF000016FF2,\n0x17000000187F8,\n0x1880000018CD6,\n0x18D0000018D09,\n0x1AFF00001AFF4,\n0x1AFF50001AFFC,\n0x1AFFD0001AFFF,\n0x1B0000001B123,\n0x1B1320001B133,\n0x1B1500001B153,\n0x1B1550001B156,\n0x1B1640001B168,\n0x1B1700001B2FC,\n0x1BC000001BC6B,\n0x1BC700001BC7D,\n0x1BC800001BC89,\n0x1BC900001BC9A,\n0x1BC9D0001BC9F,\n0x1CF000001CF2E,\n0x1CF300001CF47,\n0x1DA000001DA37,\n0x1DA3B0001DA6D,\n0x1DA750001DA76,\n0x1DA840001DA85,\n0x1DA9B0001DAA0,\n0x1DAA10001DAB0,\n0x1DF000001DF1F,\n0x1DF250001DF2B,\n0x1E0000001E007,\n0x1E0080001E019,\n0x1E01B0001E022,\n0x1E0230001E025,\n0x1E0260001E02B,\n0x1E08F0001E090,\n0x1E1000001E12D,\n0x1E1300001E13E,\n0x1E1400001E14A,\n0x1E14E0001E14F,\n0x1E2900001E2AF,\n0x1E2C00001E2FA,\n0x1E4D00001E4FA,\n0x1E7E00001E7E7,\n0x1E7E80001E7EC,\n0x1E7ED0001E7EF,\n0x1E7F00001E7FF,\n0x1E8000001E8C5,\n0x1E8D00001E8D7,\n0x1E9220001E94C,\n0x1E9500001E95A,\n0x200000002A6E0,\n0x2A7000002B73A,\n0x2B7400002B81E,\n0x2B8200002CEA2,\n0x2CEB00002EBE1,\n0x2EBF00002EE5E,\n0x300000003134B,\n0x31350000323B0,\n),\n\"CONTEXTJ\":(0x200C0000200E,),\n\"CONTEXTO\":(\n0xB7000000B8,\n0x37500000376,\n0x5F3000005F5,\n0x6600000066A,\n0x6F0000006FA,\n0x30FB000030FC,\n),\n}\n", []], "idna.core": [".py", "import bisect\nimport re\nimport unicodedata\nfrom typing import Optional,Union\n\nfrom. import idnadata\nfrom.intranges import intranges_contain\n\n_virama_combining_class=9\n_alabel_prefix=b\"xn--\"\n_unicode_dots_re=re.compile(\"[\\u002e\\u3002\\uff0e\\uff61]\")\n\n\nclass IDNAError(UnicodeError):\n ''\n \n pass\n \n \nclass IDNABidiError(IDNAError):\n ''\n \n pass\n \n \nclass InvalidCodepoint(IDNAError):\n ''\n \n pass\n \n \nclass InvalidCodepointContext(IDNAError):\n ''\n \n pass\n \n \ndef _combining_class(cp:int)->int:\n v=unicodedata.combining(chr(cp))\n if v ==0:\n  if not unicodedata.name(chr(cp)):\n   raise ValueError(\"Unknown character in unicodedata\")\n return v\n \n \ndef _is_script(cp:str,script:str)->bool:\n return intranges_contain(ord(cp),idnadata.scripts[script])\n \n \ndef _punycode(s:str)->bytes:\n return s.encode(\"punycode\")\n \n \ndef _unot(s:int)->str:\n return \"U+{:04X}\".format(s)\n \n \ndef valid_label_length(label:Union[bytes,str])->bool:\n if len(label)>63:\n  return False\n return True\n \n \ndef valid_string_length(label:Union[bytes,str],trailing_dot:bool)->bool:\n if len(label)>(254 if trailing_dot else 253):\n  return False\n return True\n \n \ndef check_bidi(label:str,check_ltr:bool=False)->bool:\n\n bidi_label=False\n for idx,cp in enumerate(label,1):\n  direction=unicodedata.bidirectional(cp)\n  if direction ==\"\":\n  \n   raise IDNABidiError(\"Unknown directionality in label {} at position {}\".format(repr(label),idx))\n  if direction in[\"R\",\"AL\",\"AN\"]:\n   bidi_label=True\n if not bidi_label and not check_ltr:\n  return True\n  \n  \n direction=unicodedata.bidirectional(label[0])\n if direction in[\"R\",\"AL\"]:\n  rtl=True\n elif direction ==\"L\":\n  rtl=False\n else:\n  raise IDNABidiError(\"First codepoint in label {} must be directionality L, R or AL\".format(repr(label)))\n  \n valid_ending=False\n number_type:Optional[str]=None\n for idx,cp in enumerate(label,1):\n  direction=unicodedata.bidirectional(cp)\n  \n  if rtl:\n  \n   if direction not in[\n   \"R\",\n   \"AL\",\n   \"AN\",\n   \"EN\",\n   \"ES\",\n   \"CS\",\n   \"ET\",\n   \"ON\",\n   \"BN\",\n   \"NSM\",\n   ]:\n    raise IDNABidiError(\"Invalid direction for codepoint at position {} in a right-to-left label\".format(idx))\n    \n   if direction in[\"R\",\"AL\",\"EN\",\"AN\"]:\n    valid_ending=True\n   elif direction !=\"NSM\":\n    valid_ending=False\n    \n   if direction in[\"AN\",\"EN\"]:\n    if not number_type:\n     number_type=direction\n    else:\n     if number_type !=direction:\n      raise IDNABidiError(\"Can not mix numeral types in a right-to-left label\")\n  else:\n  \n   if direction not in[\"L\",\"EN\",\"ES\",\"CS\",\"ET\",\"ON\",\"BN\",\"NSM\"]:\n    raise IDNABidiError(\"Invalid direction for codepoint at position {} in a left-to-right label\".format(idx))\n    \n   if direction in[\"L\",\"EN\"]:\n    valid_ending=True\n   elif direction !=\"NSM\":\n    valid_ending=False\n    \n if not valid_ending:\n  raise IDNABidiError(\"Label ends with illegal codepoint directionality\")\n  \n return True\n \n \ndef check_initial_combiner(label:str)->bool:\n if unicodedata.category(label[0])[0]==\"M\":\n  raise IDNAError(\"Label begins with an illegal combining character\")\n return True\n \n \ndef check_hyphen_ok(label:str)->bool:\n if label[2:4]==\"--\":\n  raise IDNAError(\"Label has disallowed hyphens in 3rd and 4th position\")\n if label[0]==\"-\"or label[-1]==\"-\":\n  raise IDNAError(\"Label must not start or end with a hyphen\")\n return True\n \n \ndef check_nfc(label:str)->None:\n if unicodedata.normalize(\"NFC\",label)!=label:\n  raise IDNAError(\"Label must be in Normalization Form C\")\n  \n  \ndef valid_contextj(label:str,pos:int)->bool:\n cp_value=ord(label[pos])\n \n if cp_value ==0x200C:\n  if pos >0:\n   if _combining_class(ord(label[pos -1]))==_virama_combining_class:\n    return True\n    \n  ok=False\n  for i in range(pos -1,-1,-1):\n   joining_type=idnadata.joining_types.get(ord(label[i]))\n   if joining_type ==ord(\"T\"):\n    continue\n   elif joining_type in[ord(\"L\"),ord(\"D\")]:\n    ok=True\n    break\n   else:\n    break\n    \n  if not ok:\n   return False\n   \n  ok=False\n  for i in range(pos+1,len(label)):\n   joining_type=idnadata.joining_types.get(ord(label[i]))\n   if joining_type ==ord(\"T\"):\n    continue\n   elif joining_type in[ord(\"R\"),ord(\"D\")]:\n    ok=True\n    break\n   else:\n    break\n  return ok\n  \n if cp_value ==0x200D:\n  if pos >0:\n   if _combining_class(ord(label[pos -1]))==_virama_combining_class:\n    return True\n  return False\n  \n else:\n  return False\n  \n  \ndef valid_contexto(label:str,pos:int,exception:bool=False)->bool:\n cp_value=ord(label[pos])\n \n if cp_value ==0x00B7:\n  if 0 <pos <len(label)-1:\n   if ord(label[pos -1])==0x006C and ord(label[pos+1])==0x006C:\n    return True\n  return False\n  \n elif cp_value ==0x0375:\n  if pos <len(label)-1 and len(label)>1:\n   return _is_script(label[pos+1],\"Greek\")\n  return False\n  \n elif cp_value ==0x05F3 or cp_value ==0x05F4:\n  if pos >0:\n   return _is_script(label[pos -1],\"Hebrew\")\n  return False\n  \n elif cp_value ==0x30FB:\n  for cp in label:\n   if cp ==\"\\u30fb\":\n    continue\n   if _is_script(cp,\"Hiragana\")or _is_script(cp,\"Katakana\")or _is_script(cp,\"Han\"):\n    return True\n  return False\n  \n elif 0x660 <=cp_value <=0x669:\n  for cp in label:\n   if 0x6F0 <=ord(cp)<=0x06F9:\n    return False\n  return True\n  \n elif 0x6F0 <=cp_value <=0x6F9:\n  for cp in label:\n   if 0x660 <=ord(cp)<=0x0669:\n    return False\n  return True\n  \n return False\n \n \ndef check_label(label:Union[str,bytes,bytearray])->None:\n if isinstance(label,(bytes,bytearray)):\n  label=label.decode(\"utf-8\")\n if len(label)==0:\n  raise IDNAError(\"Empty Label\")\n  \n check_nfc(label)\n check_hyphen_ok(label)\n check_initial_combiner(label)\n \n for pos,cp in enumerate(label):\n  cp_value=ord(cp)\n  if intranges_contain(cp_value,idnadata.codepoint_classes[\"PVALID\"]):\n   continue\n  elif intranges_contain(cp_value,idnadata.codepoint_classes[\"CONTEXTJ\"]):\n   try:\n    if not valid_contextj(label,pos):\n     raise InvalidCodepointContext(\n     \"Joiner {} not allowed at position {} in {}\".format(_unot(cp_value),pos+1,repr(label))\n     )\n   except ValueError:\n    raise IDNAError(\n    \"Unknown codepoint adjacent to joiner {} at position {} in {}\".format(\n    _unot(cp_value),pos+1,repr(label)\n    )\n    )\n  elif intranges_contain(cp_value,idnadata.codepoint_classes[\"CONTEXTO\"]):\n   if not valid_contexto(label,pos):\n    raise InvalidCodepointContext(\n    \"Codepoint {} not allowed at position {} in {}\".format(_unot(cp_value),pos+1,repr(label))\n    )\n  else:\n   raise InvalidCodepoint(\n   \"Codepoint {} at position {} of {} not allowed\".format(_unot(cp_value),pos+1,repr(label))\n   )\n   \n check_bidi(label)\n \n \ndef alabel(label:str)->bytes:\n try:\n  label_bytes=label.encode(\"ascii\")\n  ulabel(label_bytes)\n  if not valid_label_length(label_bytes):\n   raise IDNAError(\"Label too long\")\n  return label_bytes\n except UnicodeEncodeError:\n  pass\n  \n check_label(label)\n label_bytes=_alabel_prefix+_punycode(label)\n \n if not valid_label_length(label_bytes):\n  raise IDNAError(\"Label too long\")\n  \n return label_bytes\n \n \ndef ulabel(label:Union[str,bytes,bytearray])->str:\n if not isinstance(label,(bytes,bytearray)):\n  try:\n   label_bytes=label.encode(\"ascii\")\n  except UnicodeEncodeError:\n   check_label(label)\n   return label\n else:\n  label_bytes=label\n  \n label_bytes=label_bytes.lower()\n if label_bytes.startswith(_alabel_prefix):\n  label_bytes=label_bytes[len(_alabel_prefix):]\n  if not label_bytes:\n   raise IDNAError(\"Malformed A-label, no Punycode eligible content found\")\n  if label_bytes.decode(\"ascii\")[-1]==\"-\":\n   raise IDNAError(\"A-label must not end with a hyphen\")\n else:\n  check_label(label_bytes)\n  return label_bytes.decode(\"ascii\")\n  \n try:\n  label=label_bytes.decode(\"punycode\")\n except UnicodeError:\n  raise IDNAError(\"Invalid A-label\")\n check_label(label)\n return label\n \n \ndef uts46_remap(domain:str,std3_rules:bool=True,transitional:bool=False)->str:\n ''\n from.uts46data import uts46data\n \n output=\"\"\n \n for pos,char in enumerate(domain):\n  code_point=ord(char)\n  try:\n   uts46row=uts46data[code_point if code_point <256 else bisect.bisect_left(uts46data,(code_point,\"Z\"))-1]\n   status=uts46row[1]\n   replacement:Optional[str]=None\n   if len(uts46row)==3:\n    replacement=uts46row[2]\n   if(\n   status ==\"V\"\n   or(status ==\"D\"and not transitional)\n   or(status ==\"3\"and not std3_rules and replacement is None)\n   ):\n    output +=char\n   elif replacement is not None and(\n   status ==\"M\"or(status ==\"3\"and not std3_rules)or(status ==\"D\"and transitional)\n   ):\n    output +=replacement\n   elif status !=\"I\":\n    raise IndexError()\n  except IndexError:\n   raise InvalidCodepoint(\n   \"Codepoint {} not allowed at position {} in {}\".format(_unot(code_point),pos+1,repr(domain))\n   )\n   \n return unicodedata.normalize(\"NFC\",output)\n \n \ndef encode(\ns:Union[str,bytes,bytearray],\nstrict:bool=False,\nuts46:bool=False,\nstd3_rules:bool=False,\ntransitional:bool=False,\n)->bytes:\n if not isinstance(s,str):\n  try:\n   s=str(s,\"ascii\")\n  except UnicodeDecodeError:\n   raise IDNAError(\"should pass a unicode string to the function rather than a byte string.\")\n if uts46:\n  s=uts46_remap(s,std3_rules,transitional)\n trailing_dot=False\n result=[]\n if strict:\n  labels=s.split(\".\")\n else:\n  labels=_unicode_dots_re.split(s)\n if not labels or labels ==[\"\"]:\n  raise IDNAError(\"Empty domain\")\n if labels[-1]==\"\":\n  del labels[-1]\n  trailing_dot=True\n for label in labels:\n  s=alabel(label)\n  if s:\n   result.append(s)\n  else:\n   raise IDNAError(\"Empty label\")\n if trailing_dot:\n  result.append(b\"\")\n s=b\".\".join(result)\n if not valid_string_length(s,trailing_dot):\n  raise IDNAError(\"Domain too long\")\n return s\n \n \ndef decode(\ns:Union[str,bytes,bytearray],\nstrict:bool=False,\nuts46:bool=False,\nstd3_rules:bool=False,\n)->str:\n try:\n  if not isinstance(s,str):\n   s=str(s,\"ascii\")\n except UnicodeDecodeError:\n  raise IDNAError(\"Invalid ASCII in A-label\")\n if uts46:\n  s=uts46_remap(s,std3_rules,False)\n trailing_dot=False\n result=[]\n if not strict:\n  labels=_unicode_dots_re.split(s)\n else:\n  labels=s.split(\".\")\n if not labels or labels ==[\"\"]:\n  raise IDNAError(\"Empty domain\")\n if not labels[-1]:\n  del labels[-1]\n  trailing_dot=True\n for label in labels:\n  s=ulabel(label)\n  if s:\n   result.append(s)\n  else:\n   raise IDNAError(\"Empty label\")\n if trailing_dot:\n  result.append(\"\")\n return \".\".join(result)\n", ["bisect", "idna", "idna.idnadata", "idna.intranges", "idna.uts46data", "re", "typing", "unicodedata"]], "idna.compat": [".py", "from typing import Any,Union\n\nfrom.core import decode,encode\n\n\ndef ToASCII(label:str)->bytes:\n return encode(label)\n \n \ndef ToUnicode(label:Union[bytes,bytearray])->str:\n return decode(label)\n \n \ndef nameprep(s:Any)->None:\n raise NotImplementedError(\"IDNA 2008 does not utilise nameprep protocol\")\n", ["idna.core", "typing"]], "idna.package_data": [".py", "__version__=\"3.10\"\n", []], "idna.intranges": [".py", "''\n\n\n\n\n\n\nimport bisect\nfrom typing import List,Tuple\n\n\ndef intranges_from_list(list_:List[int])->Tuple[int,...]:\n ''\n\n\n\n\n \n \n sorted_list=sorted(list_)\n ranges=[]\n last_write=-1\n for i in range(len(sorted_list)):\n  if i+1 <len(sorted_list):\n   if sorted_list[i]==sorted_list[i+1]-1:\n    continue\n  current_range=sorted_list[last_write+1:i+1]\n  ranges.append(_encode_range(current_range[0],current_range[-1]+1))\n  last_write=i\n  \n return tuple(ranges)\n \n \ndef _encode_range(start:int,end:int)->int:\n return(start <<32)|end\n \n \ndef _decode_range(r:int)->Tuple[int,int]:\n return(r >>32),(r&((1 <<32)-1))\n \n \ndef intranges_contain(int_:int,ranges:Tuple[int,...])->bool:\n ''\n tuple_=_encode_range(int_,0)\n pos=bisect.bisect_left(ranges,tuple_)\n \n \n if pos >0:\n  left,right=_decode_range(ranges[pos -1])\n  if left <=int_ <right:\n   return True\n   \n if pos <len(ranges):\n  left,_=_decode_range(ranges[pos])\n  if left ==int_:\n   return True\n return False\n", ["bisect", "typing"]], "idna.uts46data": [".py", "\n\n\nfrom typing import List,Tuple,Union\n\n\"\"\"IDNA Mapping Table from UTS46.\"\"\"\n\n\n__version__=\"15.1.0\"\n\n\ndef _seg_0()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x0,\"3\"),\n (0x1,\"3\"),\n (0x2,\"3\"),\n (0x3,\"3\"),\n (0x4,\"3\"),\n (0x5,\"3\"),\n (0x6,\"3\"),\n (0x7,\"3\"),\n (0x8,\"3\"),\n (0x9,\"3\"),\n (0xA,\"3\"),\n (0xB,\"3\"),\n (0xC,\"3\"),\n (0xD,\"3\"),\n (0xE,\"3\"),\n (0xF,\"3\"),\n (0x10,\"3\"),\n (0x11,\"3\"),\n (0x12,\"3\"),\n (0x13,\"3\"),\n (0x14,\"3\"),\n (0x15,\"3\"),\n (0x16,\"3\"),\n (0x17,\"3\"),\n (0x18,\"3\"),\n (0x19,\"3\"),\n (0x1A,\"3\"),\n (0x1B,\"3\"),\n (0x1C,\"3\"),\n (0x1D,\"3\"),\n (0x1E,\"3\"),\n (0x1F,\"3\"),\n (0x20,\"3\"),\n (0x21,\"3\"),\n (0x22,\"3\"),\n (0x23,\"3\"),\n (0x24,\"3\"),\n (0x25,\"3\"),\n (0x26,\"3\"),\n (0x27,\"3\"),\n (0x28,\"3\"),\n (0x29,\"3\"),\n (0x2A,\"3\"),\n (0x2B,\"3\"),\n (0x2C,\"3\"),\n (0x2D,\"V\"),\n (0x2E,\"V\"),\n (0x2F,\"3\"),\n (0x30,\"V\"),\n (0x31,\"V\"),\n (0x32,\"V\"),\n (0x33,\"V\"),\n (0x34,\"V\"),\n (0x35,\"V\"),\n (0x36,\"V\"),\n (0x37,\"V\"),\n (0x38,\"V\"),\n (0x39,\"V\"),\n (0x3A,\"3\"),\n (0x3B,\"3\"),\n (0x3C,\"3\"),\n (0x3D,\"3\"),\n (0x3E,\"3\"),\n (0x3F,\"3\"),\n (0x40,\"3\"),\n (0x41,\"M\",\"a\"),\n (0x42,\"M\",\"b\"),\n (0x43,\"M\",\"c\"),\n (0x44,\"M\",\"d\"),\n (0x45,\"M\",\"e\"),\n (0x46,\"M\",\"f\"),\n (0x47,\"M\",\"g\"),\n (0x48,\"M\",\"h\"),\n (0x49,\"M\",\"i\"),\n (0x4A,\"M\",\"j\"),\n (0x4B,\"M\",\"k\"),\n (0x4C,\"M\",\"l\"),\n (0x4D,\"M\",\"m\"),\n (0x4E,\"M\",\"n\"),\n (0x4F,\"M\",\"o\"),\n (0x50,\"M\",\"p\"),\n (0x51,\"M\",\"q\"),\n (0x52,\"M\",\"r\"),\n (0x53,\"M\",\"s\"),\n (0x54,\"M\",\"t\"),\n (0x55,\"M\",\"u\"),\n (0x56,\"M\",\"v\"),\n (0x57,\"M\",\"w\"),\n (0x58,\"M\",\"x\"),\n (0x59,\"M\",\"y\"),\n (0x5A,\"M\",\"z\"),\n (0x5B,\"3\"),\n (0x5C,\"3\"),\n (0x5D,\"3\"),\n (0x5E,\"3\"),\n (0x5F,\"3\"),\n (0x60,\"3\"),\n (0x61,\"V\"),\n (0x62,\"V\"),\n (0x63,\"V\"),\n ]\n \n \ndef _seg_1()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x64,\"V\"),\n (0x65,\"V\"),\n (0x66,\"V\"),\n (0x67,\"V\"),\n (0x68,\"V\"),\n (0x69,\"V\"),\n (0x6A,\"V\"),\n (0x6B,\"V\"),\n (0x6C,\"V\"),\n (0x6D,\"V\"),\n (0x6E,\"V\"),\n (0x6F,\"V\"),\n (0x70,\"V\"),\n (0x71,\"V\"),\n (0x72,\"V\"),\n (0x73,\"V\"),\n (0x74,\"V\"),\n (0x75,\"V\"),\n (0x76,\"V\"),\n (0x77,\"V\"),\n (0x78,\"V\"),\n (0x79,\"V\"),\n (0x7A,\"V\"),\n (0x7B,\"3\"),\n (0x7C,\"3\"),\n (0x7D,\"3\"),\n (0x7E,\"3\"),\n (0x7F,\"3\"),\n (0x80,\"X\"),\n (0x81,\"X\"),\n (0x82,\"X\"),\n (0x83,\"X\"),\n (0x84,\"X\"),\n (0x85,\"X\"),\n (0x86,\"X\"),\n (0x87,\"X\"),\n (0x88,\"X\"),\n (0x89,\"X\"),\n (0x8A,\"X\"),\n (0x8B,\"X\"),\n (0x8C,\"X\"),\n (0x8D,\"X\"),\n (0x8E,\"X\"),\n (0x8F,\"X\"),\n (0x90,\"X\"),\n (0x91,\"X\"),\n (0x92,\"X\"),\n (0x93,\"X\"),\n (0x94,\"X\"),\n (0x95,\"X\"),\n (0x96,\"X\"),\n (0x97,\"X\"),\n (0x98,\"X\"),\n (0x99,\"X\"),\n (0x9A,\"X\"),\n (0x9B,\"X\"),\n (0x9C,\"X\"),\n (0x9D,\"X\"),\n (0x9E,\"X\"),\n (0x9F,\"X\"),\n (0xA0,\"3\",\" \"),\n (0xA1,\"V\"),\n (0xA2,\"V\"),\n (0xA3,\"V\"),\n (0xA4,\"V\"),\n (0xA5,\"V\"),\n (0xA6,\"V\"),\n (0xA7,\"V\"),\n (0xA8,\"3\",\" \u0308\"),\n (0xA9,\"V\"),\n (0xAA,\"M\",\"a\"),\n (0xAB,\"V\"),\n (0xAC,\"V\"),\n (0xAD,\"I\"),\n (0xAE,\"V\"),\n (0xAF,\"3\",\" \u0304\"),\n (0xB0,\"V\"),\n (0xB1,\"V\"),\n (0xB2,\"M\",\"2\"),\n (0xB3,\"M\",\"3\"),\n (0xB4,\"3\",\" \u0301\"),\n (0xB5,\"M\",\"\u03bc\"),\n (0xB6,\"V\"),\n (0xB7,\"V\"),\n (0xB8,\"3\",\" \u0327\"),\n (0xB9,\"M\",\"1\"),\n (0xBA,\"M\",\"o\"),\n (0xBB,\"V\"),\n (0xBC,\"M\",\"1\u20444\"),\n (0xBD,\"M\",\"1\u20442\"),\n (0xBE,\"M\",\"3\u20444\"),\n (0xBF,\"V\"),\n (0xC0,\"M\",\"\u00e0\"),\n (0xC1,\"M\",\"\u00e1\"),\n (0xC2,\"M\",\"\u00e2\"),\n (0xC3,\"M\",\"\u00e3\"),\n (0xC4,\"M\",\"\u00e4\"),\n (0xC5,\"M\",\"\u00e5\"),\n (0xC6,\"M\",\"\u00e6\"),\n (0xC7,\"M\",\"\u00e7\"),\n ]\n \n \ndef _seg_2()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xC8,\"M\",\"\u00e8\"),\n (0xC9,\"M\",\"\u00e9\"),\n (0xCA,\"M\",\"\u00ea\"),\n (0xCB,\"M\",\"\u00eb\"),\n (0xCC,\"M\",\"\u00ec\"),\n (0xCD,\"M\",\"\u00ed\"),\n (0xCE,\"M\",\"\u00ee\"),\n (0xCF,\"M\",\"\u00ef\"),\n (0xD0,\"M\",\"\u00f0\"),\n (0xD1,\"M\",\"\u00f1\"),\n (0xD2,\"M\",\"\u00f2\"),\n (0xD3,\"M\",\"\u00f3\"),\n (0xD4,\"M\",\"\u00f4\"),\n (0xD5,\"M\",\"\u00f5\"),\n (0xD6,\"M\",\"\u00f6\"),\n (0xD7,\"V\"),\n (0xD8,\"M\",\"\u00f8\"),\n (0xD9,\"M\",\"\u00f9\"),\n (0xDA,\"M\",\"\u00fa\"),\n (0xDB,\"M\",\"\u00fb\"),\n (0xDC,\"M\",\"\u00fc\"),\n (0xDD,\"M\",\"\u00fd\"),\n (0xDE,\"M\",\"\u00fe\"),\n (0xDF,\"D\",\"ss\"),\n (0xE0,\"V\"),\n (0xE1,\"V\"),\n (0xE2,\"V\"),\n (0xE3,\"V\"),\n (0xE4,\"V\"),\n (0xE5,\"V\"),\n (0xE6,\"V\"),\n (0xE7,\"V\"),\n (0xE8,\"V\"),\n (0xE9,\"V\"),\n (0xEA,\"V\"),\n (0xEB,\"V\"),\n (0xEC,\"V\"),\n (0xED,\"V\"),\n (0xEE,\"V\"),\n (0xEF,\"V\"),\n (0xF0,\"V\"),\n (0xF1,\"V\"),\n (0xF2,\"V\"),\n (0xF3,\"V\"),\n (0xF4,\"V\"),\n (0xF5,\"V\"),\n (0xF6,\"V\"),\n (0xF7,\"V\"),\n (0xF8,\"V\"),\n (0xF9,\"V\"),\n (0xFA,\"V\"),\n (0xFB,\"V\"),\n (0xFC,\"V\"),\n (0xFD,\"V\"),\n (0xFE,\"V\"),\n (0xFF,\"V\"),\n (0x100,\"M\",\"\u0101\"),\n (0x101,\"V\"),\n (0x102,\"M\",\"\u0103\"),\n (0x103,\"V\"),\n (0x104,\"M\",\"\u0105\"),\n (0x105,\"V\"),\n (0x106,\"M\",\"\u0107\"),\n (0x107,\"V\"),\n (0x108,\"M\",\"\u0109\"),\n (0x109,\"V\"),\n (0x10A,\"M\",\"\u010b\"),\n (0x10B,\"V\"),\n (0x10C,\"M\",\"\u010d\"),\n (0x10D,\"V\"),\n (0x10E,\"M\",\"\u010f\"),\n (0x10F,\"V\"),\n (0x110,\"M\",\"\u0111\"),\n (0x111,\"V\"),\n (0x112,\"M\",\"\u0113\"),\n (0x113,\"V\"),\n (0x114,\"M\",\"\u0115\"),\n (0x115,\"V\"),\n (0x116,\"M\",\"\u0117\"),\n (0x117,\"V\"),\n (0x118,\"M\",\"\u0119\"),\n (0x119,\"V\"),\n (0x11A,\"M\",\"\u011b\"),\n (0x11B,\"V\"),\n (0x11C,\"M\",\"\u011d\"),\n (0x11D,\"V\"),\n (0x11E,\"M\",\"\u011f\"),\n (0x11F,\"V\"),\n (0x120,\"M\",\"\u0121\"),\n (0x121,\"V\"),\n (0x122,\"M\",\"\u0123\"),\n (0x123,\"V\"),\n (0x124,\"M\",\"\u0125\"),\n (0x125,\"V\"),\n (0x126,\"M\",\"\u0127\"),\n (0x127,\"V\"),\n (0x128,\"M\",\"\u0129\"),\n (0x129,\"V\"),\n (0x12A,\"M\",\"\u012b\"),\n (0x12B,\"V\"),\n ]\n \n \ndef _seg_3()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x12C,\"M\",\"\u012d\"),\n (0x12D,\"V\"),\n (0x12E,\"M\",\"\u012f\"),\n (0x12F,\"V\"),\n (0x130,\"M\",\"i\u0307\"),\n (0x131,\"V\"),\n (0x132,\"M\",\"ij\"),\n (0x134,\"M\",\"\u0135\"),\n (0x135,\"V\"),\n (0x136,\"M\",\"\u0137\"),\n (0x137,\"V\"),\n (0x139,\"M\",\"\u013a\"),\n (0x13A,\"V\"),\n (0x13B,\"M\",\"\u013c\"),\n (0x13C,\"V\"),\n (0x13D,\"M\",\"\u013e\"),\n (0x13E,\"V\"),\n (0x13F,\"M\",\"l\u00b7\"),\n (0x141,\"M\",\"\u0142\"),\n (0x142,\"V\"),\n (0x143,\"M\",\"\u0144\"),\n (0x144,\"V\"),\n (0x145,\"M\",\"\u0146\"),\n (0x146,\"V\"),\n (0x147,\"M\",\"\u0148\"),\n (0x148,\"V\"),\n (0x149,\"M\",\"\u02bcn\"),\n (0x14A,\"M\",\"\u014b\"),\n (0x14B,\"V\"),\n (0x14C,\"M\",\"\u014d\"),\n (0x14D,\"V\"),\n (0x14E,\"M\",\"\u014f\"),\n (0x14F,\"V\"),\n (0x150,\"M\",\"\u0151\"),\n (0x151,\"V\"),\n (0x152,\"M\",\"\u0153\"),\n (0x153,\"V\"),\n (0x154,\"M\",\"\u0155\"),\n (0x155,\"V\"),\n (0x156,\"M\",\"\u0157\"),\n (0x157,\"V\"),\n (0x158,\"M\",\"\u0159\"),\n (0x159,\"V\"),\n (0x15A,\"M\",\"\u015b\"),\n (0x15B,\"V\"),\n (0x15C,\"M\",\"\u015d\"),\n (0x15D,\"V\"),\n (0x15E,\"M\",\"\u015f\"),\n (0x15F,\"V\"),\n (0x160,\"M\",\"\u0161\"),\n (0x161,\"V\"),\n (0x162,\"M\",\"\u0163\"),\n (0x163,\"V\"),\n (0x164,\"M\",\"\u0165\"),\n (0x165,\"V\"),\n (0x166,\"M\",\"\u0167\"),\n (0x167,\"V\"),\n (0x168,\"M\",\"\u0169\"),\n (0x169,\"V\"),\n (0x16A,\"M\",\"\u016b\"),\n (0x16B,\"V\"),\n (0x16C,\"M\",\"\u016d\"),\n (0x16D,\"V\"),\n (0x16E,\"M\",\"\u016f\"),\n (0x16F,\"V\"),\n (0x170,\"M\",\"\u0171\"),\n (0x171,\"V\"),\n (0x172,\"M\",\"\u0173\"),\n (0x173,\"V\"),\n (0x174,\"M\",\"\u0175\"),\n (0x175,\"V\"),\n (0x176,\"M\",\"\u0177\"),\n (0x177,\"V\"),\n (0x178,\"M\",\"\u00ff\"),\n (0x179,\"M\",\"\u017a\"),\n (0x17A,\"V\"),\n (0x17B,\"M\",\"\u017c\"),\n (0x17C,\"V\"),\n (0x17D,\"M\",\"\u017e\"),\n (0x17E,\"V\"),\n (0x17F,\"M\",\"s\"),\n (0x180,\"V\"),\n (0x181,\"M\",\"\u0253\"),\n (0x182,\"M\",\"\u0183\"),\n (0x183,\"V\"),\n (0x184,\"M\",\"\u0185\"),\n (0x185,\"V\"),\n (0x186,\"M\",\"\u0254\"),\n (0x187,\"M\",\"\u0188\"),\n (0x188,\"V\"),\n (0x189,\"M\",\"\u0256\"),\n (0x18A,\"M\",\"\u0257\"),\n (0x18B,\"M\",\"\u018c\"),\n (0x18C,\"V\"),\n (0x18E,\"M\",\"\u01dd\"),\n (0x18F,\"M\",\"\u0259\"),\n (0x190,\"M\",\"\u025b\"),\n (0x191,\"M\",\"\u0192\"),\n (0x192,\"V\"),\n (0x193,\"M\",\"\u0260\"),\n ]\n \n \ndef _seg_4()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x194,\"M\",\"\u0263\"),\n (0x195,\"V\"),\n (0x196,\"M\",\"\u0269\"),\n (0x197,\"M\",\"\u0268\"),\n (0x198,\"M\",\"\u0199\"),\n (0x199,\"V\"),\n (0x19C,\"M\",\"\u026f\"),\n (0x19D,\"M\",\"\u0272\"),\n (0x19E,\"V\"),\n (0x19F,\"M\",\"\u0275\"),\n (0x1A0,\"M\",\"\u01a1\"),\n (0x1A1,\"V\"),\n (0x1A2,\"M\",\"\u01a3\"),\n (0x1A3,\"V\"),\n (0x1A4,\"M\",\"\u01a5\"),\n (0x1A5,\"V\"),\n (0x1A6,\"M\",\"\u0280\"),\n (0x1A7,\"M\",\"\u01a8\"),\n (0x1A8,\"V\"),\n (0x1A9,\"M\",\"\u0283\"),\n (0x1AA,\"V\"),\n (0x1AC,\"M\",\"\u01ad\"),\n (0x1AD,\"V\"),\n (0x1AE,\"M\",\"\u0288\"),\n (0x1AF,\"M\",\"\u01b0\"),\n (0x1B0,\"V\"),\n (0x1B1,\"M\",\"\u028a\"),\n (0x1B2,\"M\",\"\u028b\"),\n (0x1B3,\"M\",\"\u01b4\"),\n (0x1B4,\"V\"),\n (0x1B5,\"M\",\"\u01b6\"),\n (0x1B6,\"V\"),\n (0x1B7,\"M\",\"\u0292\"),\n (0x1B8,\"M\",\"\u01b9\"),\n (0x1B9,\"V\"),\n (0x1BC,\"M\",\"\u01bd\"),\n (0x1BD,\"V\"),\n (0x1C4,\"M\",\"d\u017e\"),\n (0x1C7,\"M\",\"lj\"),\n (0x1CA,\"M\",\"nj\"),\n (0x1CD,\"M\",\"\u01ce\"),\n (0x1CE,\"V\"),\n (0x1CF,\"M\",\"\u01d0\"),\n (0x1D0,\"V\"),\n (0x1D1,\"M\",\"\u01d2\"),\n (0x1D2,\"V\"),\n (0x1D3,\"M\",\"\u01d4\"),\n (0x1D4,\"V\"),\n (0x1D5,\"M\",\"\u01d6\"),\n (0x1D6,\"V\"),\n (0x1D7,\"M\",\"\u01d8\"),\n (0x1D8,\"V\"),\n (0x1D9,\"M\",\"\u01da\"),\n (0x1DA,\"V\"),\n (0x1DB,\"M\",\"\u01dc\"),\n (0x1DC,\"V\"),\n (0x1DE,\"M\",\"\u01df\"),\n (0x1DF,\"V\"),\n (0x1E0,\"M\",\"\u01e1\"),\n (0x1E1,\"V\"),\n (0x1E2,\"M\",\"\u01e3\"),\n (0x1E3,\"V\"),\n (0x1E4,\"M\",\"\u01e5\"),\n (0x1E5,\"V\"),\n (0x1E6,\"M\",\"\u01e7\"),\n (0x1E7,\"V\"),\n (0x1E8,\"M\",\"\u01e9\"),\n (0x1E9,\"V\"),\n (0x1EA,\"M\",\"\u01eb\"),\n (0x1EB,\"V\"),\n (0x1EC,\"M\",\"\u01ed\"),\n (0x1ED,\"V\"),\n (0x1EE,\"M\",\"\u01ef\"),\n (0x1EF,\"V\"),\n (0x1F1,\"M\",\"dz\"),\n (0x1F4,\"M\",\"\u01f5\"),\n (0x1F5,\"V\"),\n (0x1F6,\"M\",\"\u0195\"),\n (0x1F7,\"M\",\"\u01bf\"),\n (0x1F8,\"M\",\"\u01f9\"),\n (0x1F9,\"V\"),\n (0x1FA,\"M\",\"\u01fb\"),\n (0x1FB,\"V\"),\n (0x1FC,\"M\",\"\u01fd\"),\n (0x1FD,\"V\"),\n (0x1FE,\"M\",\"\u01ff\"),\n (0x1FF,\"V\"),\n (0x200,\"M\",\"\u0201\"),\n (0x201,\"V\"),\n (0x202,\"M\",\"\u0203\"),\n (0x203,\"V\"),\n (0x204,\"M\",\"\u0205\"),\n (0x205,\"V\"),\n (0x206,\"M\",\"\u0207\"),\n (0x207,\"V\"),\n (0x208,\"M\",\"\u0209\"),\n (0x209,\"V\"),\n (0x20A,\"M\",\"\u020b\"),\n (0x20B,\"V\"),\n (0x20C,\"M\",\"\u020d\"),\n ]\n \n \ndef _seg_5()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x20D,\"V\"),\n (0x20E,\"M\",\"\u020f\"),\n (0x20F,\"V\"),\n (0x210,\"M\",\"\u0211\"),\n (0x211,\"V\"),\n (0x212,\"M\",\"\u0213\"),\n (0x213,\"V\"),\n (0x214,\"M\",\"\u0215\"),\n (0x215,\"V\"),\n (0x216,\"M\",\"\u0217\"),\n (0x217,\"V\"),\n (0x218,\"M\",\"\u0219\"),\n (0x219,\"V\"),\n (0x21A,\"M\",\"\u021b\"),\n (0x21B,\"V\"),\n (0x21C,\"M\",\"\u021d\"),\n (0x21D,\"V\"),\n (0x21E,\"M\",\"\u021f\"),\n (0x21F,\"V\"),\n (0x220,\"M\",\"\u019e\"),\n (0x221,\"V\"),\n (0x222,\"M\",\"\u0223\"),\n (0x223,\"V\"),\n (0x224,\"M\",\"\u0225\"),\n (0x225,\"V\"),\n (0x226,\"M\",\"\u0227\"),\n (0x227,\"V\"),\n (0x228,\"M\",\"\u0229\"),\n (0x229,\"V\"),\n (0x22A,\"M\",\"\u022b\"),\n (0x22B,\"V\"),\n (0x22C,\"M\",\"\u022d\"),\n (0x22D,\"V\"),\n (0x22E,\"M\",\"\u022f\"),\n (0x22F,\"V\"),\n (0x230,\"M\",\"\u0231\"),\n (0x231,\"V\"),\n (0x232,\"M\",\"\u0233\"),\n (0x233,\"V\"),\n (0x23A,\"M\",\"\u2c65\"),\n (0x23B,\"M\",\"\u023c\"),\n (0x23C,\"V\"),\n (0x23D,\"M\",\"\u019a\"),\n (0x23E,\"M\",\"\u2c66\"),\n (0x23F,\"V\"),\n (0x241,\"M\",\"\u0242\"),\n (0x242,\"V\"),\n (0x243,\"M\",\"\u0180\"),\n (0x244,\"M\",\"\u0289\"),\n (0x245,\"M\",\"\u028c\"),\n (0x246,\"M\",\"\u0247\"),\n (0x247,\"V\"),\n (0x248,\"M\",\"\u0249\"),\n (0x249,\"V\"),\n (0x24A,\"M\",\"\u024b\"),\n (0x24B,\"V\"),\n (0x24C,\"M\",\"\u024d\"),\n (0x24D,\"V\"),\n (0x24E,\"M\",\"\u024f\"),\n (0x24F,\"V\"),\n (0x2B0,\"M\",\"h\"),\n (0x2B1,\"M\",\"\u0266\"),\n (0x2B2,\"M\",\"j\"),\n (0x2B3,\"M\",\"r\"),\n (0x2B4,\"M\",\"\u0279\"),\n (0x2B5,\"M\",\"\u027b\"),\n (0x2B6,\"M\",\"\u0281\"),\n (0x2B7,\"M\",\"w\"),\n (0x2B8,\"M\",\"y\"),\n (0x2B9,\"V\"),\n (0x2D8,\"3\",\" \u0306\"),\n (0x2D9,\"3\",\" \u0307\"),\n (0x2DA,\"3\",\" \u030a\"),\n (0x2DB,\"3\",\" \u0328\"),\n (0x2DC,\"3\",\" \u0303\"),\n (0x2DD,\"3\",\" \u030b\"),\n (0x2DE,\"V\"),\n (0x2E0,\"M\",\"\u0263\"),\n (0x2E1,\"M\",\"l\"),\n (0x2E2,\"M\",\"s\"),\n (0x2E3,\"M\",\"x\"),\n (0x2E4,\"M\",\"\u0295\"),\n (0x2E5,\"V\"),\n (0x340,\"M\",\"\u0300\"),\n (0x341,\"M\",\"\u0301\"),\n (0x342,\"V\"),\n (0x343,\"M\",\"\u0313\"),\n (0x344,\"M\",\"\u0308\u0301\"),\n (0x345,\"M\",\"\u03b9\"),\n (0x346,\"V\"),\n (0x34F,\"I\"),\n (0x350,\"V\"),\n (0x370,\"M\",\"\u0371\"),\n (0x371,\"V\"),\n (0x372,\"M\",\"\u0373\"),\n (0x373,\"V\"),\n (0x374,\"M\",\"\u02b9\"),\n (0x375,\"V\"),\n (0x376,\"M\",\"\u0377\"),\n (0x377,\"V\"),\n ]\n \n \ndef _seg_6()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x378,\"X\"),\n (0x37A,\"3\",\" \u03b9\"),\n (0x37B,\"V\"),\n (0x37E,\"3\",\";\"),\n (0x37F,\"M\",\"\u03f3\"),\n (0x380,\"X\"),\n (0x384,\"3\",\" \u0301\"),\n (0x385,\"3\",\" \u0308\u0301\"),\n (0x386,\"M\",\"\u03ac\"),\n (0x387,\"M\",\"\u00b7\"),\n (0x388,\"M\",\"\u03ad\"),\n (0x389,\"M\",\"\u03ae\"),\n (0x38A,\"M\",\"\u03af\"),\n (0x38B,\"X\"),\n (0x38C,\"M\",\"\u03cc\"),\n (0x38D,\"X\"),\n (0x38E,\"M\",\"\u03cd\"),\n (0x38F,\"M\",\"\u03ce\"),\n (0x390,\"V\"),\n (0x391,\"M\",\"\u03b1\"),\n (0x392,\"M\",\"\u03b2\"),\n (0x393,\"M\",\"\u03b3\"),\n (0x394,\"M\",\"\u03b4\"),\n (0x395,\"M\",\"\u03b5\"),\n (0x396,\"M\",\"\u03b6\"),\n (0x397,\"M\",\"\u03b7\"),\n (0x398,\"M\",\"\u03b8\"),\n (0x399,\"M\",\"\u03b9\"),\n (0x39A,\"M\",\"\u03ba\"),\n (0x39B,\"M\",\"\u03bb\"),\n (0x39C,\"M\",\"\u03bc\"),\n (0x39D,\"M\",\"\u03bd\"),\n (0x39E,\"M\",\"\u03be\"),\n (0x39F,\"M\",\"\u03bf\"),\n (0x3A0,\"M\",\"\u03c0\"),\n (0x3A1,\"M\",\"\u03c1\"),\n (0x3A2,\"X\"),\n (0x3A3,\"M\",\"\u03c3\"),\n (0x3A4,\"M\",\"\u03c4\"),\n (0x3A5,\"M\",\"\u03c5\"),\n (0x3A6,\"M\",\"\u03c6\"),\n (0x3A7,\"M\",\"\u03c7\"),\n (0x3A8,\"M\",\"\u03c8\"),\n (0x3A9,\"M\",\"\u03c9\"),\n (0x3AA,\"M\",\"\u03ca\"),\n (0x3AB,\"M\",\"\u03cb\"),\n (0x3AC,\"V\"),\n (0x3C2,\"D\",\"\u03c3\"),\n (0x3C3,\"V\"),\n (0x3CF,\"M\",\"\u03d7\"),\n (0x3D0,\"M\",\"\u03b2\"),\n (0x3D1,\"M\",\"\u03b8\"),\n (0x3D2,\"M\",\"\u03c5\"),\n (0x3D3,\"M\",\"\u03cd\"),\n (0x3D4,\"M\",\"\u03cb\"),\n (0x3D5,\"M\",\"\u03c6\"),\n (0x3D6,\"M\",\"\u03c0\"),\n (0x3D7,\"V\"),\n (0x3D8,\"M\",\"\u03d9\"),\n (0x3D9,\"V\"),\n (0x3DA,\"M\",\"\u03db\"),\n (0x3DB,\"V\"),\n (0x3DC,\"M\",\"\u03dd\"),\n (0x3DD,\"V\"),\n (0x3DE,\"M\",\"\u03df\"),\n (0x3DF,\"V\"),\n (0x3E0,\"M\",\"\u03e1\"),\n (0x3E1,\"V\"),\n (0x3E2,\"M\",\"\u03e3\"),\n (0x3E3,\"V\"),\n (0x3E4,\"M\",\"\u03e5\"),\n (0x3E5,\"V\"),\n (0x3E6,\"M\",\"\u03e7\"),\n (0x3E7,\"V\"),\n (0x3E8,\"M\",\"\u03e9\"),\n (0x3E9,\"V\"),\n (0x3EA,\"M\",\"\u03eb\"),\n (0x3EB,\"V\"),\n (0x3EC,\"M\",\"\u03ed\"),\n (0x3ED,\"V\"),\n (0x3EE,\"M\",\"\u03ef\"),\n (0x3EF,\"V\"),\n (0x3F0,\"M\",\"\u03ba\"),\n (0x3F1,\"M\",\"\u03c1\"),\n (0x3F2,\"M\",\"\u03c3\"),\n (0x3F3,\"V\"),\n (0x3F4,\"M\",\"\u03b8\"),\n (0x3F5,\"M\",\"\u03b5\"),\n (0x3F6,\"V\"),\n (0x3F7,\"M\",\"\u03f8\"),\n (0x3F8,\"V\"),\n (0x3F9,\"M\",\"\u03c3\"),\n (0x3FA,\"M\",\"\u03fb\"),\n (0x3FB,\"V\"),\n (0x3FD,\"M\",\"\u037b\"),\n (0x3FE,\"M\",\"\u037c\"),\n (0x3FF,\"M\",\"\u037d\"),\n (0x400,\"M\",\"\u0450\"),\n (0x401,\"M\",\"\u0451\"),\n (0x402,\"M\",\"\u0452\"),\n ]\n \n \ndef _seg_7()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x403,\"M\",\"\u0453\"),\n (0x404,\"M\",\"\u0454\"),\n (0x405,\"M\",\"\u0455\"),\n (0x406,\"M\",\"\u0456\"),\n (0x407,\"M\",\"\u0457\"),\n (0x408,\"M\",\"\u0458\"),\n (0x409,\"M\",\"\u0459\"),\n (0x40A,\"M\",\"\u045a\"),\n (0x40B,\"M\",\"\u045b\"),\n (0x40C,\"M\",\"\u045c\"),\n (0x40D,\"M\",\"\u045d\"),\n (0x40E,\"M\",\"\u045e\"),\n (0x40F,\"M\",\"\u045f\"),\n (0x410,\"M\",\"\u0430\"),\n (0x411,\"M\",\"\u0431\"),\n (0x412,\"M\",\"\u0432\"),\n (0x413,\"M\",\"\u0433\"),\n (0x414,\"M\",\"\u0434\"),\n (0x415,\"M\",\"\u0435\"),\n (0x416,\"M\",\"\u0436\"),\n (0x417,\"M\",\"\u0437\"),\n (0x418,\"M\",\"\u0438\"),\n (0x419,\"M\",\"\u0439\"),\n (0x41A,\"M\",\"\u043a\"),\n (0x41B,\"M\",\"\u043b\"),\n (0x41C,\"M\",\"\u043c\"),\n (0x41D,\"M\",\"\u043d\"),\n (0x41E,\"M\",\"\u043e\"),\n (0x41F,\"M\",\"\u043f\"),\n (0x420,\"M\",\"\u0440\"),\n (0x421,\"M\",\"\u0441\"),\n (0x422,\"M\",\"\u0442\"),\n (0x423,\"M\",\"\u0443\"),\n (0x424,\"M\",\"\u0444\"),\n (0x425,\"M\",\"\u0445\"),\n (0x426,\"M\",\"\u0446\"),\n (0x427,\"M\",\"\u0447\"),\n (0x428,\"M\",\"\u0448\"),\n (0x429,\"M\",\"\u0449\"),\n (0x42A,\"M\",\"\u044a\"),\n (0x42B,\"M\",\"\u044b\"),\n (0x42C,\"M\",\"\u044c\"),\n (0x42D,\"M\",\"\u044d\"),\n (0x42E,\"M\",\"\u044e\"),\n (0x42F,\"M\",\"\u044f\"),\n (0x430,\"V\"),\n (0x460,\"M\",\"\u0461\"),\n (0x461,\"V\"),\n (0x462,\"M\",\"\u0463\"),\n (0x463,\"V\"),\n (0x464,\"M\",\"\u0465\"),\n (0x465,\"V\"),\n (0x466,\"M\",\"\u0467\"),\n (0x467,\"V\"),\n (0x468,\"M\",\"\u0469\"),\n (0x469,\"V\"),\n (0x46A,\"M\",\"\u046b\"),\n (0x46B,\"V\"),\n (0x46C,\"M\",\"\u046d\"),\n (0x46D,\"V\"),\n (0x46E,\"M\",\"\u046f\"),\n (0x46F,\"V\"),\n (0x470,\"M\",\"\u0471\"),\n (0x471,\"V\"),\n (0x472,\"M\",\"\u0473\"),\n (0x473,\"V\"),\n (0x474,\"M\",\"\u0475\"),\n (0x475,\"V\"),\n (0x476,\"M\",\"\u0477\"),\n (0x477,\"V\"),\n (0x478,\"M\",\"\u0479\"),\n (0x479,\"V\"),\n (0x47A,\"M\",\"\u047b\"),\n (0x47B,\"V\"),\n (0x47C,\"M\",\"\u047d\"),\n (0x47D,\"V\"),\n (0x47E,\"M\",\"\u047f\"),\n (0x47F,\"V\"),\n (0x480,\"M\",\"\u0481\"),\n (0x481,\"V\"),\n (0x48A,\"M\",\"\u048b\"),\n (0x48B,\"V\"),\n (0x48C,\"M\",\"\u048d\"),\n (0x48D,\"V\"),\n (0x48E,\"M\",\"\u048f\"),\n (0x48F,\"V\"),\n (0x490,\"M\",\"\u0491\"),\n (0x491,\"V\"),\n (0x492,\"M\",\"\u0493\"),\n (0x493,\"V\"),\n (0x494,\"M\",\"\u0495\"),\n (0x495,\"V\"),\n (0x496,\"M\",\"\u0497\"),\n (0x497,\"V\"),\n (0x498,\"M\",\"\u0499\"),\n (0x499,\"V\"),\n (0x49A,\"M\",\"\u049b\"),\n (0x49B,\"V\"),\n (0x49C,\"M\",\"\u049d\"),\n (0x49D,\"V\"),\n ]\n \n \ndef _seg_8()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x49E,\"M\",\"\u049f\"),\n (0x49F,\"V\"),\n (0x4A0,\"M\",\"\u04a1\"),\n (0x4A1,\"V\"),\n (0x4A2,\"M\",\"\u04a3\"),\n (0x4A3,\"V\"),\n (0x4A4,\"M\",\"\u04a5\"),\n (0x4A5,\"V\"),\n (0x4A6,\"M\",\"\u04a7\"),\n (0x4A7,\"V\"),\n (0x4A8,\"M\",\"\u04a9\"),\n (0x4A9,\"V\"),\n (0x4AA,\"M\",\"\u04ab\"),\n (0x4AB,\"V\"),\n (0x4AC,\"M\",\"\u04ad\"),\n (0x4AD,\"V\"),\n (0x4AE,\"M\",\"\u04af\"),\n (0x4AF,\"V\"),\n (0x4B0,\"M\",\"\u04b1\"),\n (0x4B1,\"V\"),\n (0x4B2,\"M\",\"\u04b3\"),\n (0x4B3,\"V\"),\n (0x4B4,\"M\",\"\u04b5\"),\n (0x4B5,\"V\"),\n (0x4B6,\"M\",\"\u04b7\"),\n (0x4B7,\"V\"),\n (0x4B8,\"M\",\"\u04b9\"),\n (0x4B9,\"V\"),\n (0x4BA,\"M\",\"\u04bb\"),\n (0x4BB,\"V\"),\n (0x4BC,\"M\",\"\u04bd\"),\n (0x4BD,\"V\"),\n (0x4BE,\"M\",\"\u04bf\"),\n (0x4BF,\"V\"),\n (0x4C0,\"X\"),\n (0x4C1,\"M\",\"\u04c2\"),\n (0x4C2,\"V\"),\n (0x4C3,\"M\",\"\u04c4\"),\n (0x4C4,\"V\"),\n (0x4C5,\"M\",\"\u04c6\"),\n (0x4C6,\"V\"),\n (0x4C7,\"M\",\"\u04c8\"),\n (0x4C8,\"V\"),\n (0x4C9,\"M\",\"\u04ca\"),\n (0x4CA,\"V\"),\n (0x4CB,\"M\",\"\u04cc\"),\n (0x4CC,\"V\"),\n (0x4CD,\"M\",\"\u04ce\"),\n (0x4CE,\"V\"),\n (0x4D0,\"M\",\"\u04d1\"),\n (0x4D1,\"V\"),\n (0x4D2,\"M\",\"\u04d3\"),\n (0x4D3,\"V\"),\n (0x4D4,\"M\",\"\u04d5\"),\n (0x4D5,\"V\"),\n (0x4D6,\"M\",\"\u04d7\"),\n (0x4D7,\"V\"),\n (0x4D8,\"M\",\"\u04d9\"),\n (0x4D9,\"V\"),\n (0x4DA,\"M\",\"\u04db\"),\n (0x4DB,\"V\"),\n (0x4DC,\"M\",\"\u04dd\"),\n (0x4DD,\"V\"),\n (0x4DE,\"M\",\"\u04df\"),\n (0x4DF,\"V\"),\n (0x4E0,\"M\",\"\u04e1\"),\n (0x4E1,\"V\"),\n (0x4E2,\"M\",\"\u04e3\"),\n (0x4E3,\"V\"),\n (0x4E4,\"M\",\"\u04e5\"),\n (0x4E5,\"V\"),\n (0x4E6,\"M\",\"\u04e7\"),\n (0x4E7,\"V\"),\n (0x4E8,\"M\",\"\u04e9\"),\n (0x4E9,\"V\"),\n (0x4EA,\"M\",\"\u04eb\"),\n (0x4EB,\"V\"),\n (0x4EC,\"M\",\"\u04ed\"),\n (0x4ED,\"V\"),\n (0x4EE,\"M\",\"\u04ef\"),\n (0x4EF,\"V\"),\n (0x4F0,\"M\",\"\u04f1\"),\n (0x4F1,\"V\"),\n (0x4F2,\"M\",\"\u04f3\"),\n (0x4F3,\"V\"),\n (0x4F4,\"M\",\"\u04f5\"),\n (0x4F5,\"V\"),\n (0x4F6,\"M\",\"\u04f7\"),\n (0x4F7,\"V\"),\n (0x4F8,\"M\",\"\u04f9\"),\n (0x4F9,\"V\"),\n (0x4FA,\"M\",\"\u04fb\"),\n (0x4FB,\"V\"),\n (0x4FC,\"M\",\"\u04fd\"),\n (0x4FD,\"V\"),\n (0x4FE,\"M\",\"\u04ff\"),\n (0x4FF,\"V\"),\n (0x500,\"M\",\"\u0501\"),\n (0x501,\"V\"),\n (0x502,\"M\",\"\u0503\"),\n ]\n \n \ndef _seg_9()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x503,\"V\"),\n (0x504,\"M\",\"\u0505\"),\n (0x505,\"V\"),\n (0x506,\"M\",\"\u0507\"),\n (0x507,\"V\"),\n (0x508,\"M\",\"\u0509\"),\n (0x509,\"V\"),\n (0x50A,\"M\",\"\u050b\"),\n (0x50B,\"V\"),\n (0x50C,\"M\",\"\u050d\"),\n (0x50D,\"V\"),\n (0x50E,\"M\",\"\u050f\"),\n (0x50F,\"V\"),\n (0x510,\"M\",\"\u0511\"),\n (0x511,\"V\"),\n (0x512,\"M\",\"\u0513\"),\n (0x513,\"V\"),\n (0x514,\"M\",\"\u0515\"),\n (0x515,\"V\"),\n (0x516,\"M\",\"\u0517\"),\n (0x517,\"V\"),\n (0x518,\"M\",\"\u0519\"),\n (0x519,\"V\"),\n (0x51A,\"M\",\"\u051b\"),\n (0x51B,\"V\"),\n (0x51C,\"M\",\"\u051d\"),\n (0x51D,\"V\"),\n (0x51E,\"M\",\"\u051f\"),\n (0x51F,\"V\"),\n (0x520,\"M\",\"\u0521\"),\n (0x521,\"V\"),\n (0x522,\"M\",\"\u0523\"),\n (0x523,\"V\"),\n (0x524,\"M\",\"\u0525\"),\n (0x525,\"V\"),\n (0x526,\"M\",\"\u0527\"),\n (0x527,\"V\"),\n (0x528,\"M\",\"\u0529\"),\n (0x529,\"V\"),\n (0x52A,\"M\",\"\u052b\"),\n (0x52B,\"V\"),\n (0x52C,\"M\",\"\u052d\"),\n (0x52D,\"V\"),\n (0x52E,\"M\",\"\u052f\"),\n (0x52F,\"V\"),\n (0x530,\"X\"),\n (0x531,\"M\",\"\u0561\"),\n (0x532,\"M\",\"\u0562\"),\n (0x533,\"M\",\"\u0563\"),\n (0x534,\"M\",\"\u0564\"),\n (0x535,\"M\",\"\u0565\"),\n (0x536,\"M\",\"\u0566\"),\n (0x537,\"M\",\"\u0567\"),\n (0x538,\"M\",\"\u0568\"),\n (0x539,\"M\",\"\u0569\"),\n (0x53A,\"M\",\"\u056a\"),\n (0x53B,\"M\",\"\u056b\"),\n (0x53C,\"M\",\"\u056c\"),\n (0x53D,\"M\",\"\u056d\"),\n (0x53E,\"M\",\"\u056e\"),\n (0x53F,\"M\",\"\u056f\"),\n (0x540,\"M\",\"\u0570\"),\n (0x541,\"M\",\"\u0571\"),\n (0x542,\"M\",\"\u0572\"),\n (0x543,\"M\",\"\u0573\"),\n (0x544,\"M\",\"\u0574\"),\n (0x545,\"M\",\"\u0575\"),\n (0x546,\"M\",\"\u0576\"),\n (0x547,\"M\",\"\u0577\"),\n (0x548,\"M\",\"\u0578\"),\n (0x549,\"M\",\"\u0579\"),\n (0x54A,\"M\",\"\u057a\"),\n (0x54B,\"M\",\"\u057b\"),\n (0x54C,\"M\",\"\u057c\"),\n (0x54D,\"M\",\"\u057d\"),\n (0x54E,\"M\",\"\u057e\"),\n (0x54F,\"M\",\"\u057f\"),\n (0x550,\"M\",\"\u0580\"),\n (0x551,\"M\",\"\u0581\"),\n (0x552,\"M\",\"\u0582\"),\n (0x553,\"M\",\"\u0583\"),\n (0x554,\"M\",\"\u0584\"),\n (0x555,\"M\",\"\u0585\"),\n (0x556,\"M\",\"\u0586\"),\n (0x557,\"X\"),\n (0x559,\"V\"),\n (0x587,\"M\",\"\u0565\u0582\"),\n (0x588,\"V\"),\n (0x58B,\"X\"),\n (0x58D,\"V\"),\n (0x590,\"X\"),\n (0x591,\"V\"),\n (0x5C8,\"X\"),\n (0x5D0,\"V\"),\n (0x5EB,\"X\"),\n (0x5EF,\"V\"),\n (0x5F5,\"X\"),\n (0x606,\"V\"),\n (0x61C,\"X\"),\n (0x61D,\"V\"),\n ]\n \n \ndef _seg_10()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x675,\"M\",\"\u0627\u0674\"),\n (0x676,\"M\",\"\u0648\u0674\"),\n (0x677,\"M\",\"\u06c7\u0674\"),\n (0x678,\"M\",\"\u064a\u0674\"),\n (0x679,\"V\"),\n (0x6DD,\"X\"),\n (0x6DE,\"V\"),\n (0x70E,\"X\"),\n (0x710,\"V\"),\n (0x74B,\"X\"),\n (0x74D,\"V\"),\n (0x7B2,\"X\"),\n (0x7C0,\"V\"),\n (0x7FB,\"X\"),\n (0x7FD,\"V\"),\n (0x82E,\"X\"),\n (0x830,\"V\"),\n (0x83F,\"X\"),\n (0x840,\"V\"),\n (0x85C,\"X\"),\n (0x85E,\"V\"),\n (0x85F,\"X\"),\n (0x860,\"V\"),\n (0x86B,\"X\"),\n (0x870,\"V\"),\n (0x88F,\"X\"),\n (0x898,\"V\"),\n (0x8E2,\"X\"),\n (0x8E3,\"V\"),\n (0x958,\"M\",\"\u0915\u093c\"),\n (0x959,\"M\",\"\u0916\u093c\"),\n (0x95A,\"M\",\"\u0917\u093c\"),\n (0x95B,\"M\",\"\u091c\u093c\"),\n (0x95C,\"M\",\"\u0921\u093c\"),\n (0x95D,\"M\",\"\u0922\u093c\"),\n (0x95E,\"M\",\"\u092b\u093c\"),\n (0x95F,\"M\",\"\u092f\u093c\"),\n (0x960,\"V\"),\n (0x984,\"X\"),\n (0x985,\"V\"),\n (0x98D,\"X\"),\n (0x98F,\"V\"),\n (0x991,\"X\"),\n (0x993,\"V\"),\n (0x9A9,\"X\"),\n (0x9AA,\"V\"),\n (0x9B1,\"X\"),\n (0x9B2,\"V\"),\n (0x9B3,\"X\"),\n (0x9B6,\"V\"),\n (0x9BA,\"X\"),\n (0x9BC,\"V\"),\n (0x9C5,\"X\"),\n (0x9C7,\"V\"),\n (0x9C9,\"X\"),\n (0x9CB,\"V\"),\n (0x9CF,\"X\"),\n (0x9D7,\"V\"),\n (0x9D8,\"X\"),\n (0x9DC,\"M\",\"\u09a1\u09bc\"),\n (0x9DD,\"M\",\"\u09a2\u09bc\"),\n (0x9DE,\"X\"),\n (0x9DF,\"M\",\"\u09af\u09bc\"),\n (0x9E0,\"V\"),\n (0x9E4,\"X\"),\n (0x9E6,\"V\"),\n (0x9FF,\"X\"),\n (0xA01,\"V\"),\n (0xA04,\"X\"),\n (0xA05,\"V\"),\n (0xA0B,\"X\"),\n (0xA0F,\"V\"),\n (0xA11,\"X\"),\n (0xA13,\"V\"),\n (0xA29,\"X\"),\n (0xA2A,\"V\"),\n (0xA31,\"X\"),\n (0xA32,\"V\"),\n (0xA33,\"M\",\"\u0a32\u0a3c\"),\n (0xA34,\"X\"),\n (0xA35,\"V\"),\n (0xA36,\"M\",\"\u0a38\u0a3c\"),\n (0xA37,\"X\"),\n (0xA38,\"V\"),\n (0xA3A,\"X\"),\n (0xA3C,\"V\"),\n (0xA3D,\"X\"),\n (0xA3E,\"V\"),\n (0xA43,\"X\"),\n (0xA47,\"V\"),\n (0xA49,\"X\"),\n (0xA4B,\"V\"),\n (0xA4E,\"X\"),\n (0xA51,\"V\"),\n (0xA52,\"X\"),\n (0xA59,\"M\",\"\u0a16\u0a3c\"),\n (0xA5A,\"M\",\"\u0a17\u0a3c\"),\n (0xA5B,\"M\",\"\u0a1c\u0a3c\"),\n (0xA5C,\"V\"),\n (0xA5D,\"X\"),\n ]\n \n \ndef _seg_11()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xA5E,\"M\",\"\u0a2b\u0a3c\"),\n (0xA5F,\"X\"),\n (0xA66,\"V\"),\n (0xA77,\"X\"),\n (0xA81,\"V\"),\n (0xA84,\"X\"),\n (0xA85,\"V\"),\n (0xA8E,\"X\"),\n (0xA8F,\"V\"),\n (0xA92,\"X\"),\n (0xA93,\"V\"),\n (0xAA9,\"X\"),\n (0xAAA,\"V\"),\n (0xAB1,\"X\"),\n (0xAB2,\"V\"),\n (0xAB4,\"X\"),\n (0xAB5,\"V\"),\n (0xABA,\"X\"),\n (0xABC,\"V\"),\n (0xAC6,\"X\"),\n (0xAC7,\"V\"),\n (0xACA,\"X\"),\n (0xACB,\"V\"),\n (0xACE,\"X\"),\n (0xAD0,\"V\"),\n (0xAD1,\"X\"),\n (0xAE0,\"V\"),\n (0xAE4,\"X\"),\n (0xAE6,\"V\"),\n (0xAF2,\"X\"),\n (0xAF9,\"V\"),\n (0xB00,\"X\"),\n (0xB01,\"V\"),\n (0xB04,\"X\"),\n (0xB05,\"V\"),\n (0xB0D,\"X\"),\n (0xB0F,\"V\"),\n (0xB11,\"X\"),\n (0xB13,\"V\"),\n (0xB29,\"X\"),\n (0xB2A,\"V\"),\n (0xB31,\"X\"),\n (0xB32,\"V\"),\n (0xB34,\"X\"),\n (0xB35,\"V\"),\n (0xB3A,\"X\"),\n (0xB3C,\"V\"),\n (0xB45,\"X\"),\n (0xB47,\"V\"),\n (0xB49,\"X\"),\n (0xB4B,\"V\"),\n (0xB4E,\"X\"),\n (0xB55,\"V\"),\n (0xB58,\"X\"),\n (0xB5C,\"M\",\"\u0b21\u0b3c\"),\n (0xB5D,\"M\",\"\u0b22\u0b3c\"),\n (0xB5E,\"X\"),\n (0xB5F,\"V\"),\n (0xB64,\"X\"),\n (0xB66,\"V\"),\n (0xB78,\"X\"),\n (0xB82,\"V\"),\n (0xB84,\"X\"),\n (0xB85,\"V\"),\n (0xB8B,\"X\"),\n (0xB8E,\"V\"),\n (0xB91,\"X\"),\n (0xB92,\"V\"),\n (0xB96,\"X\"),\n (0xB99,\"V\"),\n (0xB9B,\"X\"),\n (0xB9C,\"V\"),\n (0xB9D,\"X\"),\n (0xB9E,\"V\"),\n (0xBA0,\"X\"),\n (0xBA3,\"V\"),\n (0xBA5,\"X\"),\n (0xBA8,\"V\"),\n (0xBAB,\"X\"),\n (0xBAE,\"V\"),\n (0xBBA,\"X\"),\n (0xBBE,\"V\"),\n (0xBC3,\"X\"),\n (0xBC6,\"V\"),\n (0xBC9,\"X\"),\n (0xBCA,\"V\"),\n (0xBCE,\"X\"),\n (0xBD0,\"V\"),\n (0xBD1,\"X\"),\n (0xBD7,\"V\"),\n (0xBD8,\"X\"),\n (0xBE6,\"V\"),\n (0xBFB,\"X\"),\n (0xC00,\"V\"),\n (0xC0D,\"X\"),\n (0xC0E,\"V\"),\n (0xC11,\"X\"),\n (0xC12,\"V\"),\n (0xC29,\"X\"),\n (0xC2A,\"V\"),\n ]\n \n \ndef _seg_12()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xC3A,\"X\"),\n (0xC3C,\"V\"),\n (0xC45,\"X\"),\n (0xC46,\"V\"),\n (0xC49,\"X\"),\n (0xC4A,\"V\"),\n (0xC4E,\"X\"),\n (0xC55,\"V\"),\n (0xC57,\"X\"),\n (0xC58,\"V\"),\n (0xC5B,\"X\"),\n (0xC5D,\"V\"),\n (0xC5E,\"X\"),\n (0xC60,\"V\"),\n (0xC64,\"X\"),\n (0xC66,\"V\"),\n (0xC70,\"X\"),\n (0xC77,\"V\"),\n (0xC8D,\"X\"),\n (0xC8E,\"V\"),\n (0xC91,\"X\"),\n (0xC92,\"V\"),\n (0xCA9,\"X\"),\n (0xCAA,\"V\"),\n (0xCB4,\"X\"),\n (0xCB5,\"V\"),\n (0xCBA,\"X\"),\n (0xCBC,\"V\"),\n (0xCC5,\"X\"),\n (0xCC6,\"V\"),\n (0xCC9,\"X\"),\n (0xCCA,\"V\"),\n (0xCCE,\"X\"),\n (0xCD5,\"V\"),\n (0xCD7,\"X\"),\n (0xCDD,\"V\"),\n (0xCDF,\"X\"),\n (0xCE0,\"V\"),\n (0xCE4,\"X\"),\n (0xCE6,\"V\"),\n (0xCF0,\"X\"),\n (0xCF1,\"V\"),\n (0xCF4,\"X\"),\n (0xD00,\"V\"),\n (0xD0D,\"X\"),\n (0xD0E,\"V\"),\n (0xD11,\"X\"),\n (0xD12,\"V\"),\n (0xD45,\"X\"),\n (0xD46,\"V\"),\n (0xD49,\"X\"),\n (0xD4A,\"V\"),\n (0xD50,\"X\"),\n (0xD54,\"V\"),\n (0xD64,\"X\"),\n (0xD66,\"V\"),\n (0xD80,\"X\"),\n (0xD81,\"V\"),\n (0xD84,\"X\"),\n (0xD85,\"V\"),\n (0xD97,\"X\"),\n (0xD9A,\"V\"),\n (0xDB2,\"X\"),\n (0xDB3,\"V\"),\n (0xDBC,\"X\"),\n (0xDBD,\"V\"),\n (0xDBE,\"X\"),\n (0xDC0,\"V\"),\n (0xDC7,\"X\"),\n (0xDCA,\"V\"),\n (0xDCB,\"X\"),\n (0xDCF,\"V\"),\n (0xDD5,\"X\"),\n (0xDD6,\"V\"),\n (0xDD7,\"X\"),\n (0xDD8,\"V\"),\n (0xDE0,\"X\"),\n (0xDE6,\"V\"),\n (0xDF0,\"X\"),\n (0xDF2,\"V\"),\n (0xDF5,\"X\"),\n (0xE01,\"V\"),\n (0xE33,\"M\",\"\u0e4d\u0e32\"),\n (0xE34,\"V\"),\n (0xE3B,\"X\"),\n (0xE3F,\"V\"),\n (0xE5C,\"X\"),\n (0xE81,\"V\"),\n (0xE83,\"X\"),\n (0xE84,\"V\"),\n (0xE85,\"X\"),\n (0xE86,\"V\"),\n (0xE8B,\"X\"),\n (0xE8C,\"V\"),\n (0xEA4,\"X\"),\n (0xEA5,\"V\"),\n (0xEA6,\"X\"),\n (0xEA7,\"V\"),\n (0xEB3,\"M\",\"\u0ecd\u0eb2\"),\n (0xEB4,\"V\"),\n ]\n \n \ndef _seg_13()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xEBE,\"X\"),\n (0xEC0,\"V\"),\n (0xEC5,\"X\"),\n (0xEC6,\"V\"),\n (0xEC7,\"X\"),\n (0xEC8,\"V\"),\n (0xECF,\"X\"),\n (0xED0,\"V\"),\n (0xEDA,\"X\"),\n (0xEDC,\"M\",\"\u0eab\u0e99\"),\n (0xEDD,\"M\",\"\u0eab\u0ea1\"),\n (0xEDE,\"V\"),\n (0xEE0,\"X\"),\n (0xF00,\"V\"),\n (0xF0C,\"M\",\"\u0f0b\"),\n (0xF0D,\"V\"),\n (0xF43,\"M\",\"\u0f42\u0fb7\"),\n (0xF44,\"V\"),\n (0xF48,\"X\"),\n (0xF49,\"V\"),\n (0xF4D,\"M\",\"\u0f4c\u0fb7\"),\n (0xF4E,\"V\"),\n (0xF52,\"M\",\"\u0f51\u0fb7\"),\n (0xF53,\"V\"),\n (0xF57,\"M\",\"\u0f56\u0fb7\"),\n (0xF58,\"V\"),\n (0xF5C,\"M\",\"\u0f5b\u0fb7\"),\n (0xF5D,\"V\"),\n (0xF69,\"M\",\"\u0f40\u0fb5\"),\n (0xF6A,\"V\"),\n (0xF6D,\"X\"),\n (0xF71,\"V\"),\n (0xF73,\"M\",\"\u0f71\u0f72\"),\n (0xF74,\"V\"),\n (0xF75,\"M\",\"\u0f71\u0f74\"),\n (0xF76,\"M\",\"\u0fb2\u0f80\"),\n (0xF77,\"M\",\"\u0fb2\u0f71\u0f80\"),\n (0xF78,\"M\",\"\u0fb3\u0f80\"),\n (0xF79,\"M\",\"\u0fb3\u0f71\u0f80\"),\n (0xF7A,\"V\"),\n (0xF81,\"M\",\"\u0f71\u0f80\"),\n (0xF82,\"V\"),\n (0xF93,\"M\",\"\u0f92\u0fb7\"),\n (0xF94,\"V\"),\n (0xF98,\"X\"),\n (0xF99,\"V\"),\n (0xF9D,\"M\",\"\u0f9c\u0fb7\"),\n (0xF9E,\"V\"),\n (0xFA2,\"M\",\"\u0fa1\u0fb7\"),\n (0xFA3,\"V\"),\n (0xFA7,\"M\",\"\u0fa6\u0fb7\"),\n (0xFA8,\"V\"),\n (0xFAC,\"M\",\"\u0fab\u0fb7\"),\n (0xFAD,\"V\"),\n (0xFB9,\"M\",\"\u0f90\u0fb5\"),\n (0xFBA,\"V\"),\n (0xFBD,\"X\"),\n (0xFBE,\"V\"),\n (0xFCD,\"X\"),\n (0xFCE,\"V\"),\n (0xFDB,\"X\"),\n (0x1000,\"V\"),\n (0x10A0,\"X\"),\n (0x10C7,\"M\",\"\u2d27\"),\n (0x10C8,\"X\"),\n (0x10CD,\"M\",\"\u2d2d\"),\n (0x10CE,\"X\"),\n (0x10D0,\"V\"),\n (0x10FC,\"M\",\"\u10dc\"),\n (0x10FD,\"V\"),\n (0x115F,\"X\"),\n (0x1161,\"V\"),\n (0x1249,\"X\"),\n (0x124A,\"V\"),\n (0x124E,\"X\"),\n (0x1250,\"V\"),\n (0x1257,\"X\"),\n (0x1258,\"V\"),\n (0x1259,\"X\"),\n (0x125A,\"V\"),\n (0x125E,\"X\"),\n (0x1260,\"V\"),\n (0x1289,\"X\"),\n (0x128A,\"V\"),\n (0x128E,\"X\"),\n (0x1290,\"V\"),\n (0x12B1,\"X\"),\n (0x12B2,\"V\"),\n (0x12B6,\"X\"),\n (0x12B8,\"V\"),\n (0x12BF,\"X\"),\n (0x12C0,\"V\"),\n (0x12C1,\"X\"),\n (0x12C2,\"V\"),\n (0x12C6,\"X\"),\n (0x12C8,\"V\"),\n (0x12D7,\"X\"),\n (0x12D8,\"V\"),\n (0x1311,\"X\"),\n (0x1312,\"V\"),\n ]\n \n \ndef _seg_14()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1316,\"X\"),\n (0x1318,\"V\"),\n (0x135B,\"X\"),\n (0x135D,\"V\"),\n (0x137D,\"X\"),\n (0x1380,\"V\"),\n (0x139A,\"X\"),\n (0x13A0,\"V\"),\n (0x13F6,\"X\"),\n (0x13F8,\"M\",\"\u13f0\"),\n (0x13F9,\"M\",\"\u13f1\"),\n (0x13FA,\"M\",\"\u13f2\"),\n (0x13FB,\"M\",\"\u13f3\"),\n (0x13FC,\"M\",\"\u13f4\"),\n (0x13FD,\"M\",\"\u13f5\"),\n (0x13FE,\"X\"),\n (0x1400,\"V\"),\n (0x1680,\"X\"),\n (0x1681,\"V\"),\n (0x169D,\"X\"),\n (0x16A0,\"V\"),\n (0x16F9,\"X\"),\n (0x1700,\"V\"),\n (0x1716,\"X\"),\n (0x171F,\"V\"),\n (0x1737,\"X\"),\n (0x1740,\"V\"),\n (0x1754,\"X\"),\n (0x1760,\"V\"),\n (0x176D,\"X\"),\n (0x176E,\"V\"),\n (0x1771,\"X\"),\n (0x1772,\"V\"),\n (0x1774,\"X\"),\n (0x1780,\"V\"),\n (0x17B4,\"X\"),\n (0x17B6,\"V\"),\n (0x17DE,\"X\"),\n (0x17E0,\"V\"),\n (0x17EA,\"X\"),\n (0x17F0,\"V\"),\n (0x17FA,\"X\"),\n (0x1800,\"V\"),\n (0x1806,\"X\"),\n (0x1807,\"V\"),\n (0x180B,\"I\"),\n (0x180E,\"X\"),\n (0x180F,\"I\"),\n (0x1810,\"V\"),\n (0x181A,\"X\"),\n (0x1820,\"V\"),\n (0x1879,\"X\"),\n (0x1880,\"V\"),\n (0x18AB,\"X\"),\n (0x18B0,\"V\"),\n (0x18F6,\"X\"),\n (0x1900,\"V\"),\n (0x191F,\"X\"),\n (0x1920,\"V\"),\n (0x192C,\"X\"),\n (0x1930,\"V\"),\n (0x193C,\"X\"),\n (0x1940,\"V\"),\n (0x1941,\"X\"),\n (0x1944,\"V\"),\n (0x196E,\"X\"),\n (0x1970,\"V\"),\n (0x1975,\"X\"),\n (0x1980,\"V\"),\n (0x19AC,\"X\"),\n (0x19B0,\"V\"),\n (0x19CA,\"X\"),\n (0x19D0,\"V\"),\n (0x19DB,\"X\"),\n (0x19DE,\"V\"),\n (0x1A1C,\"X\"),\n (0x1A1E,\"V\"),\n (0x1A5F,\"X\"),\n (0x1A60,\"V\"),\n (0x1A7D,\"X\"),\n (0x1A7F,\"V\"),\n (0x1A8A,\"X\"),\n (0x1A90,\"V\"),\n (0x1A9A,\"X\"),\n (0x1AA0,\"V\"),\n (0x1AAE,\"X\"),\n (0x1AB0,\"V\"),\n (0x1ACF,\"X\"),\n (0x1B00,\"V\"),\n (0x1B4D,\"X\"),\n (0x1B50,\"V\"),\n (0x1B7F,\"X\"),\n (0x1B80,\"V\"),\n (0x1BF4,\"X\"),\n (0x1BFC,\"V\"),\n (0x1C38,\"X\"),\n (0x1C3B,\"V\"),\n (0x1C4A,\"X\"),\n (0x1C4D,\"V\"),\n (0x1C80,\"M\",\"\u0432\"),\n ]\n \n \ndef _seg_15()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1C81,\"M\",\"\u0434\"),\n (0x1C82,\"M\",\"\u043e\"),\n (0x1C83,\"M\",\"\u0441\"),\n (0x1C84,\"M\",\"\u0442\"),\n (0x1C86,\"M\",\"\u044a\"),\n (0x1C87,\"M\",\"\u0463\"),\n (0x1C88,\"M\",\"\ua64b\"),\n (0x1C89,\"X\"),\n (0x1C90,\"M\",\"\u10d0\"),\n (0x1C91,\"M\",\"\u10d1\"),\n (0x1C92,\"M\",\"\u10d2\"),\n (0x1C93,\"M\",\"\u10d3\"),\n (0x1C94,\"M\",\"\u10d4\"),\n (0x1C95,\"M\",\"\u10d5\"),\n (0x1C96,\"M\",\"\u10d6\"),\n (0x1C97,\"M\",\"\u10d7\"),\n (0x1C98,\"M\",\"\u10d8\"),\n (0x1C99,\"M\",\"\u10d9\"),\n (0x1C9A,\"M\",\"\u10da\"),\n (0x1C9B,\"M\",\"\u10db\"),\n (0x1C9C,\"M\",\"\u10dc\"),\n (0x1C9D,\"M\",\"\u10dd\"),\n (0x1C9E,\"M\",\"\u10de\"),\n (0x1C9F,\"M\",\"\u10df\"),\n (0x1CA0,\"M\",\"\u10e0\"),\n (0x1CA1,\"M\",\"\u10e1\"),\n (0x1CA2,\"M\",\"\u10e2\"),\n (0x1CA3,\"M\",\"\u10e3\"),\n (0x1CA4,\"M\",\"\u10e4\"),\n (0x1CA5,\"M\",\"\u10e5\"),\n (0x1CA6,\"M\",\"\u10e6\"),\n (0x1CA7,\"M\",\"\u10e7\"),\n (0x1CA8,\"M\",\"\u10e8\"),\n (0x1CA9,\"M\",\"\u10e9\"),\n (0x1CAA,\"M\",\"\u10ea\"),\n (0x1CAB,\"M\",\"\u10eb\"),\n (0x1CAC,\"M\",\"\u10ec\"),\n (0x1CAD,\"M\",\"\u10ed\"),\n (0x1CAE,\"M\",\"\u10ee\"),\n (0x1CAF,\"M\",\"\u10ef\"),\n (0x1CB0,\"M\",\"\u10f0\"),\n (0x1CB1,\"M\",\"\u10f1\"),\n (0x1CB2,\"M\",\"\u10f2\"),\n (0x1CB3,\"M\",\"\u10f3\"),\n (0x1CB4,\"M\",\"\u10f4\"),\n (0x1CB5,\"M\",\"\u10f5\"),\n (0x1CB6,\"M\",\"\u10f6\"),\n (0x1CB7,\"M\",\"\u10f7\"),\n (0x1CB8,\"M\",\"\u10f8\"),\n (0x1CB9,\"M\",\"\u10f9\"),\n (0x1CBA,\"M\",\"\u10fa\"),\n (0x1CBB,\"X\"),\n (0x1CBD,\"M\",\"\u10fd\"),\n (0x1CBE,\"M\",\"\u10fe\"),\n (0x1CBF,\"M\",\"\u10ff\"),\n (0x1CC0,\"V\"),\n (0x1CC8,\"X\"),\n (0x1CD0,\"V\"),\n (0x1CFB,\"X\"),\n (0x1D00,\"V\"),\n (0x1D2C,\"M\",\"a\"),\n (0x1D2D,\"M\",\"\u00e6\"),\n (0x1D2E,\"M\",\"b\"),\n (0x1D2F,\"V\"),\n (0x1D30,\"M\",\"d\"),\n (0x1D31,\"M\",\"e\"),\n (0x1D32,\"M\",\"\u01dd\"),\n (0x1D33,\"M\",\"g\"),\n (0x1D34,\"M\",\"h\"),\n (0x1D35,\"M\",\"i\"),\n (0x1D36,\"M\",\"j\"),\n (0x1D37,\"M\",\"k\"),\n (0x1D38,\"M\",\"l\"),\n (0x1D39,\"M\",\"m\"),\n (0x1D3A,\"M\",\"n\"),\n (0x1D3B,\"V\"),\n (0x1D3C,\"M\",\"o\"),\n (0x1D3D,\"M\",\"\u0223\"),\n (0x1D3E,\"M\",\"p\"),\n (0x1D3F,\"M\",\"r\"),\n (0x1D40,\"M\",\"t\"),\n (0x1D41,\"M\",\"u\"),\n (0x1D42,\"M\",\"w\"),\n (0x1D43,\"M\",\"a\"),\n (0x1D44,\"M\",\"\u0250\"),\n (0x1D45,\"M\",\"\u0251\"),\n (0x1D46,\"M\",\"\u1d02\"),\n (0x1D47,\"M\",\"b\"),\n (0x1D48,\"M\",\"d\"),\n (0x1D49,\"M\",\"e\"),\n (0x1D4A,\"M\",\"\u0259\"),\n (0x1D4B,\"M\",\"\u025b\"),\n (0x1D4C,\"M\",\"\u025c\"),\n (0x1D4D,\"M\",\"g\"),\n (0x1D4E,\"V\"),\n (0x1D4F,\"M\",\"k\"),\n (0x1D50,\"M\",\"m\"),\n (0x1D51,\"M\",\"\u014b\"),\n (0x1D52,\"M\",\"o\"),\n (0x1D53,\"M\",\"\u0254\"),\n ]\n \n \ndef _seg_16()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D54,\"M\",\"\u1d16\"),\n (0x1D55,\"M\",\"\u1d17\"),\n (0x1D56,\"M\",\"p\"),\n (0x1D57,\"M\",\"t\"),\n (0x1D58,\"M\",\"u\"),\n (0x1D59,\"M\",\"\u1d1d\"),\n (0x1D5A,\"M\",\"\u026f\"),\n (0x1D5B,\"M\",\"v\"),\n (0x1D5C,\"M\",\"\u1d25\"),\n (0x1D5D,\"M\",\"\u03b2\"),\n (0x1D5E,\"M\",\"\u03b3\"),\n (0x1D5F,\"M\",\"\u03b4\"),\n (0x1D60,\"M\",\"\u03c6\"),\n (0x1D61,\"M\",\"\u03c7\"),\n (0x1D62,\"M\",\"i\"),\n (0x1D63,\"M\",\"r\"),\n (0x1D64,\"M\",\"u\"),\n (0x1D65,\"M\",\"v\"),\n (0x1D66,\"M\",\"\u03b2\"),\n (0x1D67,\"M\",\"\u03b3\"),\n (0x1D68,\"M\",\"\u03c1\"),\n (0x1D69,\"M\",\"\u03c6\"),\n (0x1D6A,\"M\",\"\u03c7\"),\n (0x1D6B,\"V\"),\n (0x1D78,\"M\",\"\u043d\"),\n (0x1D79,\"V\"),\n (0x1D9B,\"M\",\"\u0252\"),\n (0x1D9C,\"M\",\"c\"),\n (0x1D9D,\"M\",\"\u0255\"),\n (0x1D9E,\"M\",\"\u00f0\"),\n (0x1D9F,\"M\",\"\u025c\"),\n (0x1DA0,\"M\",\"f\"),\n (0x1DA1,\"M\",\"\u025f\"),\n (0x1DA2,\"M\",\"\u0261\"),\n (0x1DA3,\"M\",\"\u0265\"),\n (0x1DA4,\"M\",\"\u0268\"),\n (0x1DA5,\"M\",\"\u0269\"),\n (0x1DA6,\"M\",\"\u026a\"),\n (0x1DA7,\"M\",\"\u1d7b\"),\n (0x1DA8,\"M\",\"\u029d\"),\n (0x1DA9,\"M\",\"\u026d\"),\n (0x1DAA,\"M\",\"\u1d85\"),\n (0x1DAB,\"M\",\"\u029f\"),\n (0x1DAC,\"M\",\"\u0271\"),\n (0x1DAD,\"M\",\"\u0270\"),\n (0x1DAE,\"M\",\"\u0272\"),\n (0x1DAF,\"M\",\"\u0273\"),\n (0x1DB0,\"M\",\"\u0274\"),\n (0x1DB1,\"M\",\"\u0275\"),\n (0x1DB2,\"M\",\"\u0278\"),\n (0x1DB3,\"M\",\"\u0282\"),\n (0x1DB4,\"M\",\"\u0283\"),\n (0x1DB5,\"M\",\"\u01ab\"),\n (0x1DB6,\"M\",\"\u0289\"),\n (0x1DB7,\"M\",\"\u028a\"),\n (0x1DB8,\"M\",\"\u1d1c\"),\n (0x1DB9,\"M\",\"\u028b\"),\n (0x1DBA,\"M\",\"\u028c\"),\n (0x1DBB,\"M\",\"z\"),\n (0x1DBC,\"M\",\"\u0290\"),\n (0x1DBD,\"M\",\"\u0291\"),\n (0x1DBE,\"M\",\"\u0292\"),\n (0x1DBF,\"M\",\"\u03b8\"),\n (0x1DC0,\"V\"),\n (0x1E00,\"M\",\"\u1e01\"),\n (0x1E01,\"V\"),\n (0x1E02,\"M\",\"\u1e03\"),\n (0x1E03,\"V\"),\n (0x1E04,\"M\",\"\u1e05\"),\n (0x1E05,\"V\"),\n (0x1E06,\"M\",\"\u1e07\"),\n (0x1E07,\"V\"),\n (0x1E08,\"M\",\"\u1e09\"),\n (0x1E09,\"V\"),\n (0x1E0A,\"M\",\"\u1e0b\"),\n (0x1E0B,\"V\"),\n (0x1E0C,\"M\",\"\u1e0d\"),\n (0x1E0D,\"V\"),\n (0x1E0E,\"M\",\"\u1e0f\"),\n (0x1E0F,\"V\"),\n (0x1E10,\"M\",\"\u1e11\"),\n (0x1E11,\"V\"),\n (0x1E12,\"M\",\"\u1e13\"),\n (0x1E13,\"V\"),\n (0x1E14,\"M\",\"\u1e15\"),\n (0x1E15,\"V\"),\n (0x1E16,\"M\",\"\u1e17\"),\n (0x1E17,\"V\"),\n (0x1E18,\"M\",\"\u1e19\"),\n (0x1E19,\"V\"),\n (0x1E1A,\"M\",\"\u1e1b\"),\n (0x1E1B,\"V\"),\n (0x1E1C,\"M\",\"\u1e1d\"),\n (0x1E1D,\"V\"),\n (0x1E1E,\"M\",\"\u1e1f\"),\n (0x1E1F,\"V\"),\n (0x1E20,\"M\",\"\u1e21\"),\n (0x1E21,\"V\"),\n (0x1E22,\"M\",\"\u1e23\"),\n (0x1E23,\"V\"),\n ]\n \n \ndef _seg_17()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1E24,\"M\",\"\u1e25\"),\n (0x1E25,\"V\"),\n (0x1E26,\"M\",\"\u1e27\"),\n (0x1E27,\"V\"),\n (0x1E28,\"M\",\"\u1e29\"),\n (0x1E29,\"V\"),\n (0x1E2A,\"M\",\"\u1e2b\"),\n (0x1E2B,\"V\"),\n (0x1E2C,\"M\",\"\u1e2d\"),\n (0x1E2D,\"V\"),\n (0x1E2E,\"M\",\"\u1e2f\"),\n (0x1E2F,\"V\"),\n (0x1E30,\"M\",\"\u1e31\"),\n (0x1E31,\"V\"),\n (0x1E32,\"M\",\"\u1e33\"),\n (0x1E33,\"V\"),\n (0x1E34,\"M\",\"\u1e35\"),\n (0x1E35,\"V\"),\n (0x1E36,\"M\",\"\u1e37\"),\n (0x1E37,\"V\"),\n (0x1E38,\"M\",\"\u1e39\"),\n (0x1E39,\"V\"),\n (0x1E3A,\"M\",\"\u1e3b\"),\n (0x1E3B,\"V\"),\n (0x1E3C,\"M\",\"\u1e3d\"),\n (0x1E3D,\"V\"),\n (0x1E3E,\"M\",\"\u1e3f\"),\n (0x1E3F,\"V\"),\n (0x1E40,\"M\",\"\u1e41\"),\n (0x1E41,\"V\"),\n (0x1E42,\"M\",\"\u1e43\"),\n (0x1E43,\"V\"),\n (0x1E44,\"M\",\"\u1e45\"),\n (0x1E45,\"V\"),\n (0x1E46,\"M\",\"\u1e47\"),\n (0x1E47,\"V\"),\n (0x1E48,\"M\",\"\u1e49\"),\n (0x1E49,\"V\"),\n (0x1E4A,\"M\",\"\u1e4b\"),\n (0x1E4B,\"V\"),\n (0x1E4C,\"M\",\"\u1e4d\"),\n (0x1E4D,\"V\"),\n (0x1E4E,\"M\",\"\u1e4f\"),\n (0x1E4F,\"V\"),\n (0x1E50,\"M\",\"\u1e51\"),\n (0x1E51,\"V\"),\n (0x1E52,\"M\",\"\u1e53\"),\n (0x1E53,\"V\"),\n (0x1E54,\"M\",\"\u1e55\"),\n (0x1E55,\"V\"),\n (0x1E56,\"M\",\"\u1e57\"),\n (0x1E57,\"V\"),\n (0x1E58,\"M\",\"\u1e59\"),\n (0x1E59,\"V\"),\n (0x1E5A,\"M\",\"\u1e5b\"),\n (0x1E5B,\"V\"),\n (0x1E5C,\"M\",\"\u1e5d\"),\n (0x1E5D,\"V\"),\n (0x1E5E,\"M\",\"\u1e5f\"),\n (0x1E5F,\"V\"),\n (0x1E60,\"M\",\"\u1e61\"),\n (0x1E61,\"V\"),\n (0x1E62,\"M\",\"\u1e63\"),\n (0x1E63,\"V\"),\n (0x1E64,\"M\",\"\u1e65\"),\n (0x1E65,\"V\"),\n (0x1E66,\"M\",\"\u1e67\"),\n (0x1E67,\"V\"),\n (0x1E68,\"M\",\"\u1e69\"),\n (0x1E69,\"V\"),\n (0x1E6A,\"M\",\"\u1e6b\"),\n (0x1E6B,\"V\"),\n (0x1E6C,\"M\",\"\u1e6d\"),\n (0x1E6D,\"V\"),\n (0x1E6E,\"M\",\"\u1e6f\"),\n (0x1E6F,\"V\"),\n (0x1E70,\"M\",\"\u1e71\"),\n (0x1E71,\"V\"),\n (0x1E72,\"M\",\"\u1e73\"),\n (0x1E73,\"V\"),\n (0x1E74,\"M\",\"\u1e75\"),\n (0x1E75,\"V\"),\n (0x1E76,\"M\",\"\u1e77\"),\n (0x1E77,\"V\"),\n (0x1E78,\"M\",\"\u1e79\"),\n (0x1E79,\"V\"),\n (0x1E7A,\"M\",\"\u1e7b\"),\n (0x1E7B,\"V\"),\n (0x1E7C,\"M\",\"\u1e7d\"),\n (0x1E7D,\"V\"),\n (0x1E7E,\"M\",\"\u1e7f\"),\n (0x1E7F,\"V\"),\n (0x1E80,\"M\",\"\u1e81\"),\n (0x1E81,\"V\"),\n (0x1E82,\"M\",\"\u1e83\"),\n (0x1E83,\"V\"),\n (0x1E84,\"M\",\"\u1e85\"),\n (0x1E85,\"V\"),\n (0x1E86,\"M\",\"\u1e87\"),\n (0x1E87,\"V\"),\n ]\n \n \ndef _seg_18()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1E88,\"M\",\"\u1e89\"),\n (0x1E89,\"V\"),\n (0x1E8A,\"M\",\"\u1e8b\"),\n (0x1E8B,\"V\"),\n (0x1E8C,\"M\",\"\u1e8d\"),\n (0x1E8D,\"V\"),\n (0x1E8E,\"M\",\"\u1e8f\"),\n (0x1E8F,\"V\"),\n (0x1E90,\"M\",\"\u1e91\"),\n (0x1E91,\"V\"),\n (0x1E92,\"M\",\"\u1e93\"),\n (0x1E93,\"V\"),\n (0x1E94,\"M\",\"\u1e95\"),\n (0x1E95,\"V\"),\n (0x1E9A,\"M\",\"a\u02be\"),\n (0x1E9B,\"M\",\"\u1e61\"),\n (0x1E9C,\"V\"),\n (0x1E9E,\"M\",\"\u00df\"),\n (0x1E9F,\"V\"),\n (0x1EA0,\"M\",\"\u1ea1\"),\n (0x1EA1,\"V\"),\n (0x1EA2,\"M\",\"\u1ea3\"),\n (0x1EA3,\"V\"),\n (0x1EA4,\"M\",\"\u1ea5\"),\n (0x1EA5,\"V\"),\n (0x1EA6,\"M\",\"\u1ea7\"),\n (0x1EA7,\"V\"),\n (0x1EA8,\"M\",\"\u1ea9\"),\n (0x1EA9,\"V\"),\n (0x1EAA,\"M\",\"\u1eab\"),\n (0x1EAB,\"V\"),\n (0x1EAC,\"M\",\"\u1ead\"),\n (0x1EAD,\"V\"),\n (0x1EAE,\"M\",\"\u1eaf\"),\n (0x1EAF,\"V\"),\n (0x1EB0,\"M\",\"\u1eb1\"),\n (0x1EB1,\"V\"),\n (0x1EB2,\"M\",\"\u1eb3\"),\n (0x1EB3,\"V\"),\n (0x1EB4,\"M\",\"\u1eb5\"),\n (0x1EB5,\"V\"),\n (0x1EB6,\"M\",\"\u1eb7\"),\n (0x1EB7,\"V\"),\n (0x1EB8,\"M\",\"\u1eb9\"),\n (0x1EB9,\"V\"),\n (0x1EBA,\"M\",\"\u1ebb\"),\n (0x1EBB,\"V\"),\n (0x1EBC,\"M\",\"\u1ebd\"),\n (0x1EBD,\"V\"),\n (0x1EBE,\"M\",\"\u1ebf\"),\n (0x1EBF,\"V\"),\n (0x1EC0,\"M\",\"\u1ec1\"),\n (0x1EC1,\"V\"),\n (0x1EC2,\"M\",\"\u1ec3\"),\n (0x1EC3,\"V\"),\n (0x1EC4,\"M\",\"\u1ec5\"),\n (0x1EC5,\"V\"),\n (0x1EC6,\"M\",\"\u1ec7\"),\n (0x1EC7,\"V\"),\n (0x1EC8,\"M\",\"\u1ec9\"),\n (0x1EC9,\"V\"),\n (0x1ECA,\"M\",\"\u1ecb\"),\n (0x1ECB,\"V\"),\n (0x1ECC,\"M\",\"\u1ecd\"),\n (0x1ECD,\"V\"),\n (0x1ECE,\"M\",\"\u1ecf\"),\n (0x1ECF,\"V\"),\n (0x1ED0,\"M\",\"\u1ed1\"),\n (0x1ED1,\"V\"),\n (0x1ED2,\"M\",\"\u1ed3\"),\n (0x1ED3,\"V\"),\n (0x1ED4,\"M\",\"\u1ed5\"),\n (0x1ED5,\"V\"),\n (0x1ED6,\"M\",\"\u1ed7\"),\n (0x1ED7,\"V\"),\n (0x1ED8,\"M\",\"\u1ed9\"),\n (0x1ED9,\"V\"),\n (0x1EDA,\"M\",\"\u1edb\"),\n (0x1EDB,\"V\"),\n (0x1EDC,\"M\",\"\u1edd\"),\n (0x1EDD,\"V\"),\n (0x1EDE,\"M\",\"\u1edf\"),\n (0x1EDF,\"V\"),\n (0x1EE0,\"M\",\"\u1ee1\"),\n (0x1EE1,\"V\"),\n (0x1EE2,\"M\",\"\u1ee3\"),\n (0x1EE3,\"V\"),\n (0x1EE4,\"M\",\"\u1ee5\"),\n (0x1EE5,\"V\"),\n (0x1EE6,\"M\",\"\u1ee7\"),\n (0x1EE7,\"V\"),\n (0x1EE8,\"M\",\"\u1ee9\"),\n (0x1EE9,\"V\"),\n (0x1EEA,\"M\",\"\u1eeb\"),\n (0x1EEB,\"V\"),\n (0x1EEC,\"M\",\"\u1eed\"),\n (0x1EED,\"V\"),\n (0x1EEE,\"M\",\"\u1eef\"),\n (0x1EEF,\"V\"),\n (0x1EF0,\"M\",\"\u1ef1\"),\n ]\n \n \ndef _seg_19()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1EF1,\"V\"),\n (0x1EF2,\"M\",\"\u1ef3\"),\n (0x1EF3,\"V\"),\n (0x1EF4,\"M\",\"\u1ef5\"),\n (0x1EF5,\"V\"),\n (0x1EF6,\"M\",\"\u1ef7\"),\n (0x1EF7,\"V\"),\n (0x1EF8,\"M\",\"\u1ef9\"),\n (0x1EF9,\"V\"),\n (0x1EFA,\"M\",\"\u1efb\"),\n (0x1EFB,\"V\"),\n (0x1EFC,\"M\",\"\u1efd\"),\n (0x1EFD,\"V\"),\n (0x1EFE,\"M\",\"\u1eff\"),\n (0x1EFF,\"V\"),\n (0x1F08,\"M\",\"\u1f00\"),\n (0x1F09,\"M\",\"\u1f01\"),\n (0x1F0A,\"M\",\"\u1f02\"),\n (0x1F0B,\"M\",\"\u1f03\"),\n (0x1F0C,\"M\",\"\u1f04\"),\n (0x1F0D,\"M\",\"\u1f05\"),\n (0x1F0E,\"M\",\"\u1f06\"),\n (0x1F0F,\"M\",\"\u1f07\"),\n (0x1F10,\"V\"),\n (0x1F16,\"X\"),\n (0x1F18,\"M\",\"\u1f10\"),\n (0x1F19,\"M\",\"\u1f11\"),\n (0x1F1A,\"M\",\"\u1f12\"),\n (0x1F1B,\"M\",\"\u1f13\"),\n (0x1F1C,\"M\",\"\u1f14\"),\n (0x1F1D,\"M\",\"\u1f15\"),\n (0x1F1E,\"X\"),\n (0x1F20,\"V\"),\n (0x1F28,\"M\",\"\u1f20\"),\n (0x1F29,\"M\",\"\u1f21\"),\n (0x1F2A,\"M\",\"\u1f22\"),\n (0x1F2B,\"M\",\"\u1f23\"),\n (0x1F2C,\"M\",\"\u1f24\"),\n (0x1F2D,\"M\",\"\u1f25\"),\n (0x1F2E,\"M\",\"\u1f26\"),\n (0x1F2F,\"M\",\"\u1f27\"),\n (0x1F30,\"V\"),\n (0x1F38,\"M\",\"\u1f30\"),\n (0x1F39,\"M\",\"\u1f31\"),\n (0x1F3A,\"M\",\"\u1f32\"),\n (0x1F3B,\"M\",\"\u1f33\"),\n (0x1F3C,\"M\",\"\u1f34\"),\n (0x1F3D,\"M\",\"\u1f35\"),\n (0x1F3E,\"M\",\"\u1f36\"),\n (0x1F3F,\"M\",\"\u1f37\"),\n (0x1F40,\"V\"),\n (0x1F46,\"X\"),\n (0x1F48,\"M\",\"\u1f40\"),\n (0x1F49,\"M\",\"\u1f41\"),\n (0x1F4A,\"M\",\"\u1f42\"),\n (0x1F4B,\"M\",\"\u1f43\"),\n (0x1F4C,\"M\",\"\u1f44\"),\n (0x1F4D,\"M\",\"\u1f45\"),\n (0x1F4E,\"X\"),\n (0x1F50,\"V\"),\n (0x1F58,\"X\"),\n (0x1F59,\"M\",\"\u1f51\"),\n (0x1F5A,\"X\"),\n (0x1F5B,\"M\",\"\u1f53\"),\n (0x1F5C,\"X\"),\n (0x1F5D,\"M\",\"\u1f55\"),\n (0x1F5E,\"X\"),\n (0x1F5F,\"M\",\"\u1f57\"),\n (0x1F60,\"V\"),\n (0x1F68,\"M\",\"\u1f60\"),\n (0x1F69,\"M\",\"\u1f61\"),\n (0x1F6A,\"M\",\"\u1f62\"),\n (0x1F6B,\"M\",\"\u1f63\"),\n (0x1F6C,\"M\",\"\u1f64\"),\n (0x1F6D,\"M\",\"\u1f65\"),\n (0x1F6E,\"M\",\"\u1f66\"),\n (0x1F6F,\"M\",\"\u1f67\"),\n (0x1F70,\"V\"),\n (0x1F71,\"M\",\"\u03ac\"),\n (0x1F72,\"V\"),\n (0x1F73,\"M\",\"\u03ad\"),\n (0x1F74,\"V\"),\n (0x1F75,\"M\",\"\u03ae\"),\n (0x1F76,\"V\"),\n (0x1F77,\"M\",\"\u03af\"),\n (0x1F78,\"V\"),\n (0x1F79,\"M\",\"\u03cc\"),\n (0x1F7A,\"V\"),\n (0x1F7B,\"M\",\"\u03cd\"),\n (0x1F7C,\"V\"),\n (0x1F7D,\"M\",\"\u03ce\"),\n (0x1F7E,\"X\"),\n (0x1F80,\"M\",\"\u1f00\u03b9\"),\n (0x1F81,\"M\",\"\u1f01\u03b9\"),\n (0x1F82,\"M\",\"\u1f02\u03b9\"),\n (0x1F83,\"M\",\"\u1f03\u03b9\"),\n (0x1F84,\"M\",\"\u1f04\u03b9\"),\n (0x1F85,\"M\",\"\u1f05\u03b9\"),\n (0x1F86,\"M\",\"\u1f06\u03b9\"),\n (0x1F87,\"M\",\"\u1f07\u03b9\"),\n ]\n \n \ndef _seg_20()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1F88,\"M\",\"\u1f00\u03b9\"),\n (0x1F89,\"M\",\"\u1f01\u03b9\"),\n (0x1F8A,\"M\",\"\u1f02\u03b9\"),\n (0x1F8B,\"M\",\"\u1f03\u03b9\"),\n (0x1F8C,\"M\",\"\u1f04\u03b9\"),\n (0x1F8D,\"M\",\"\u1f05\u03b9\"),\n (0x1F8E,\"M\",\"\u1f06\u03b9\"),\n (0x1F8F,\"M\",\"\u1f07\u03b9\"),\n (0x1F90,\"M\",\"\u1f20\u03b9\"),\n (0x1F91,\"M\",\"\u1f21\u03b9\"),\n (0x1F92,\"M\",\"\u1f22\u03b9\"),\n (0x1F93,\"M\",\"\u1f23\u03b9\"),\n (0x1F94,\"M\",\"\u1f24\u03b9\"),\n (0x1F95,\"M\",\"\u1f25\u03b9\"),\n (0x1F96,\"M\",\"\u1f26\u03b9\"),\n (0x1F97,\"M\",\"\u1f27\u03b9\"),\n (0x1F98,\"M\",\"\u1f20\u03b9\"),\n (0x1F99,\"M\",\"\u1f21\u03b9\"),\n (0x1F9A,\"M\",\"\u1f22\u03b9\"),\n (0x1F9B,\"M\",\"\u1f23\u03b9\"),\n (0x1F9C,\"M\",\"\u1f24\u03b9\"),\n (0x1F9D,\"M\",\"\u1f25\u03b9\"),\n (0x1F9E,\"M\",\"\u1f26\u03b9\"),\n (0x1F9F,\"M\",\"\u1f27\u03b9\"),\n (0x1FA0,\"M\",\"\u1f60\u03b9\"),\n (0x1FA1,\"M\",\"\u1f61\u03b9\"),\n (0x1FA2,\"M\",\"\u1f62\u03b9\"),\n (0x1FA3,\"M\",\"\u1f63\u03b9\"),\n (0x1FA4,\"M\",\"\u1f64\u03b9\"),\n (0x1FA5,\"M\",\"\u1f65\u03b9\"),\n (0x1FA6,\"M\",\"\u1f66\u03b9\"),\n (0x1FA7,\"M\",\"\u1f67\u03b9\"),\n (0x1FA8,\"M\",\"\u1f60\u03b9\"),\n (0x1FA9,\"M\",\"\u1f61\u03b9\"),\n (0x1FAA,\"M\",\"\u1f62\u03b9\"),\n (0x1FAB,\"M\",\"\u1f63\u03b9\"),\n (0x1FAC,\"M\",\"\u1f64\u03b9\"),\n (0x1FAD,\"M\",\"\u1f65\u03b9\"),\n (0x1FAE,\"M\",\"\u1f66\u03b9\"),\n (0x1FAF,\"M\",\"\u1f67\u03b9\"),\n (0x1FB0,\"V\"),\n (0x1FB2,\"M\",\"\u1f70\u03b9\"),\n (0x1FB3,\"M\",\"\u03b1\u03b9\"),\n (0x1FB4,\"M\",\"\u03ac\u03b9\"),\n (0x1FB5,\"X\"),\n (0x1FB6,\"V\"),\n (0x1FB7,\"M\",\"\u1fb6\u03b9\"),\n (0x1FB8,\"M\",\"\u1fb0\"),\n (0x1FB9,\"M\",\"\u1fb1\"),\n (0x1FBA,\"M\",\"\u1f70\"),\n (0x1FBB,\"M\",\"\u03ac\"),\n (0x1FBC,\"M\",\"\u03b1\u03b9\"),\n (0x1FBD,\"3\",\" \u0313\"),\n (0x1FBE,\"M\",\"\u03b9\"),\n (0x1FBF,\"3\",\" \u0313\"),\n (0x1FC0,\"3\",\" \u0342\"),\n (0x1FC1,\"3\",\" \u0308\u0342\"),\n (0x1FC2,\"M\",\"\u1f74\u03b9\"),\n (0x1FC3,\"M\",\"\u03b7\u03b9\"),\n (0x1FC4,\"M\",\"\u03ae\u03b9\"),\n (0x1FC5,\"X\"),\n (0x1FC6,\"V\"),\n (0x1FC7,\"M\",\"\u1fc6\u03b9\"),\n (0x1FC8,\"M\",\"\u1f72\"),\n (0x1FC9,\"M\",\"\u03ad\"),\n (0x1FCA,\"M\",\"\u1f74\"),\n (0x1FCB,\"M\",\"\u03ae\"),\n (0x1FCC,\"M\",\"\u03b7\u03b9\"),\n (0x1FCD,\"3\",\" \u0313\u0300\"),\n (0x1FCE,\"3\",\" \u0313\u0301\"),\n (0x1FCF,\"3\",\" \u0313\u0342\"),\n (0x1FD0,\"V\"),\n (0x1FD3,\"M\",\"\u0390\"),\n (0x1FD4,\"X\"),\n (0x1FD6,\"V\"),\n (0x1FD8,\"M\",\"\u1fd0\"),\n (0x1FD9,\"M\",\"\u1fd1\"),\n (0x1FDA,\"M\",\"\u1f76\"),\n (0x1FDB,\"M\",\"\u03af\"),\n (0x1FDC,\"X\"),\n (0x1FDD,\"3\",\" \u0314\u0300\"),\n (0x1FDE,\"3\",\" \u0314\u0301\"),\n (0x1FDF,\"3\",\" \u0314\u0342\"),\n (0x1FE0,\"V\"),\n (0x1FE3,\"M\",\"\u03b0\"),\n (0x1FE4,\"V\"),\n (0x1FE8,\"M\",\"\u1fe0\"),\n (0x1FE9,\"M\",\"\u1fe1\"),\n (0x1FEA,\"M\",\"\u1f7a\"),\n (0x1FEB,\"M\",\"\u03cd\"),\n (0x1FEC,\"M\",\"\u1fe5\"),\n (0x1FED,\"3\",\" \u0308\u0300\"),\n (0x1FEE,\"3\",\" \u0308\u0301\"),\n (0x1FEF,\"3\",\"`\"),\n (0x1FF0,\"X\"),\n (0x1FF2,\"M\",\"\u1f7c\u03b9\"),\n (0x1FF3,\"M\",\"\u03c9\u03b9\"),\n (0x1FF4,\"M\",\"\u03ce\u03b9\"),\n (0x1FF5,\"X\"),\n (0x1FF6,\"V\"),\n ]\n \n \ndef _seg_21()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1FF7,\"M\",\"\u1ff6\u03b9\"),\n (0x1FF8,\"M\",\"\u1f78\"),\n (0x1FF9,\"M\",\"\u03cc\"),\n (0x1FFA,\"M\",\"\u1f7c\"),\n (0x1FFB,\"M\",\"\u03ce\"),\n (0x1FFC,\"M\",\"\u03c9\u03b9\"),\n (0x1FFD,\"3\",\" \u0301\"),\n (0x1FFE,\"3\",\" \u0314\"),\n (0x1FFF,\"X\"),\n (0x2000,\"3\",\" \"),\n (0x200B,\"I\"),\n (0x200C,\"D\",\"\"),\n (0x200E,\"X\"),\n (0x2010,\"V\"),\n (0x2011,\"M\",\"\u2010\"),\n (0x2012,\"V\"),\n (0x2017,\"3\",\" \u0333\"),\n (0x2018,\"V\"),\n (0x2024,\"X\"),\n (0x2027,\"V\"),\n (0x2028,\"X\"),\n (0x202F,\"3\",\" \"),\n (0x2030,\"V\"),\n (0x2033,\"M\",\"\u2032\u2032\"),\n (0x2034,\"M\",\"\u2032\u2032\u2032\"),\n (0x2035,\"V\"),\n (0x2036,\"M\",\"\u2035\u2035\"),\n (0x2037,\"M\",\"\u2035\u2035\u2035\"),\n (0x2038,\"V\"),\n (0x203C,\"3\",\"!!\"),\n (0x203D,\"V\"),\n (0x203E,\"3\",\" \u0305\"),\n (0x203F,\"V\"),\n (0x2047,\"3\",\"??\"),\n (0x2048,\"3\",\"?!\"),\n (0x2049,\"3\",\"!?\"),\n (0x204A,\"V\"),\n (0x2057,\"M\",\"\u2032\u2032\u2032\u2032\"),\n (0x2058,\"V\"),\n (0x205F,\"3\",\" \"),\n (0x2060,\"I\"),\n (0x2061,\"X\"),\n (0x2064,\"I\"),\n (0x2065,\"X\"),\n (0x2070,\"M\",\"0\"),\n (0x2071,\"M\",\"i\"),\n (0x2072,\"X\"),\n (0x2074,\"M\",\"4\"),\n (0x2075,\"M\",\"5\"),\n (0x2076,\"M\",\"6\"),\n (0x2077,\"M\",\"7\"),\n (0x2078,\"M\",\"8\"),\n (0x2079,\"M\",\"9\"),\n (0x207A,\"3\",\"+\"),\n (0x207B,\"M\",\"\u2212\"),\n (0x207C,\"3\",\"=\"),\n (0x207D,\"3\",\"(\"),\n (0x207E,\"3\",\")\"),\n (0x207F,\"M\",\"n\"),\n (0x2080,\"M\",\"0\"),\n (0x2081,\"M\",\"1\"),\n (0x2082,\"M\",\"2\"),\n (0x2083,\"M\",\"3\"),\n (0x2084,\"M\",\"4\"),\n (0x2085,\"M\",\"5\"),\n (0x2086,\"M\",\"6\"),\n (0x2087,\"M\",\"7\"),\n (0x2088,\"M\",\"8\"),\n (0x2089,\"M\",\"9\"),\n (0x208A,\"3\",\"+\"),\n (0x208B,\"M\",\"\u2212\"),\n (0x208C,\"3\",\"=\"),\n (0x208D,\"3\",\"(\"),\n (0x208E,\"3\",\")\"),\n (0x208F,\"X\"),\n (0x2090,\"M\",\"a\"),\n (0x2091,\"M\",\"e\"),\n (0x2092,\"M\",\"o\"),\n (0x2093,\"M\",\"x\"),\n (0x2094,\"M\",\"\u0259\"),\n (0x2095,\"M\",\"h\"),\n (0x2096,\"M\",\"k\"),\n (0x2097,\"M\",\"l\"),\n (0x2098,\"M\",\"m\"),\n (0x2099,\"M\",\"n\"),\n (0x209A,\"M\",\"p\"),\n (0x209B,\"M\",\"s\"),\n (0x209C,\"M\",\"t\"),\n (0x209D,\"X\"),\n (0x20A0,\"V\"),\n (0x20A8,\"M\",\"rs\"),\n (0x20A9,\"V\"),\n (0x20C1,\"X\"),\n (0x20D0,\"V\"),\n (0x20F1,\"X\"),\n (0x2100,\"3\",\"a/c\"),\n (0x2101,\"3\",\"a/s\"),\n (0x2102,\"M\",\"c\"),\n (0x2103,\"M\",\"\u00b0c\"),\n (0x2104,\"V\"),\n ]\n \n \ndef _seg_22()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2105,\"3\",\"c/o\"),\n (0x2106,\"3\",\"c/u\"),\n (0x2107,\"M\",\"\u025b\"),\n (0x2108,\"V\"),\n (0x2109,\"M\",\"\u00b0f\"),\n (0x210A,\"M\",\"g\"),\n (0x210B,\"M\",\"h\"),\n (0x210F,\"M\",\"\u0127\"),\n (0x2110,\"M\",\"i\"),\n (0x2112,\"M\",\"l\"),\n (0x2114,\"V\"),\n (0x2115,\"M\",\"n\"),\n (0x2116,\"M\",\"no\"),\n (0x2117,\"V\"),\n (0x2119,\"M\",\"p\"),\n (0x211A,\"M\",\"q\"),\n (0x211B,\"M\",\"r\"),\n (0x211E,\"V\"),\n (0x2120,\"M\",\"sm\"),\n (0x2121,\"M\",\"tel\"),\n (0x2122,\"M\",\"tm\"),\n (0x2123,\"V\"),\n (0x2124,\"M\",\"z\"),\n (0x2125,\"V\"),\n (0x2126,\"M\",\"\u03c9\"),\n (0x2127,\"V\"),\n (0x2128,\"M\",\"z\"),\n (0x2129,\"V\"),\n (0x212A,\"M\",\"k\"),\n (0x212B,\"M\",\"\u00e5\"),\n (0x212C,\"M\",\"b\"),\n (0x212D,\"M\",\"c\"),\n (0x212E,\"V\"),\n (0x212F,\"M\",\"e\"),\n (0x2131,\"M\",\"f\"),\n (0x2132,\"X\"),\n (0x2133,\"M\",\"m\"),\n (0x2134,\"M\",\"o\"),\n (0x2135,\"M\",\"\u05d0\"),\n (0x2136,\"M\",\"\u05d1\"),\n (0x2137,\"M\",\"\u05d2\"),\n (0x2138,\"M\",\"\u05d3\"),\n (0x2139,\"M\",\"i\"),\n (0x213A,\"V\"),\n (0x213B,\"M\",\"fax\"),\n (0x213C,\"M\",\"\u03c0\"),\n (0x213D,\"M\",\"\u03b3\"),\n (0x213F,\"M\",\"\u03c0\"),\n (0x2140,\"M\",\"\u2211\"),\n (0x2141,\"V\"),\n (0x2145,\"M\",\"d\"),\n (0x2147,\"M\",\"e\"),\n (0x2148,\"M\",\"i\"),\n (0x2149,\"M\",\"j\"),\n (0x214A,\"V\"),\n (0x2150,\"M\",\"1\u20447\"),\n (0x2151,\"M\",\"1\u20449\"),\n (0x2152,\"M\",\"1\u204410\"),\n (0x2153,\"M\",\"1\u20443\"),\n (0x2154,\"M\",\"2\u20443\"),\n (0x2155,\"M\",\"1\u20445\"),\n (0x2156,\"M\",\"2\u20445\"),\n (0x2157,\"M\",\"3\u20445\"),\n (0x2158,\"M\",\"4\u20445\"),\n (0x2159,\"M\",\"1\u20446\"),\n (0x215A,\"M\",\"5\u20446\"),\n (0x215B,\"M\",\"1\u20448\"),\n (0x215C,\"M\",\"3\u20448\"),\n (0x215D,\"M\",\"5\u20448\"),\n (0x215E,\"M\",\"7\u20448\"),\n (0x215F,\"M\",\"1\u2044\"),\n (0x2160,\"M\",\"i\"),\n (0x2161,\"M\",\"ii\"),\n (0x2162,\"M\",\"iii\"),\n (0x2163,\"M\",\"iv\"),\n (0x2164,\"M\",\"v\"),\n (0x2165,\"M\",\"vi\"),\n (0x2166,\"M\",\"vii\"),\n (0x2167,\"M\",\"viii\"),\n (0x2168,\"M\",\"ix\"),\n (0x2169,\"M\",\"x\"),\n (0x216A,\"M\",\"xi\"),\n (0x216B,\"M\",\"xii\"),\n (0x216C,\"M\",\"l\"),\n (0x216D,\"M\",\"c\"),\n (0x216E,\"M\",\"d\"),\n (0x216F,\"M\",\"m\"),\n (0x2170,\"M\",\"i\"),\n (0x2171,\"M\",\"ii\"),\n (0x2172,\"M\",\"iii\"),\n (0x2173,\"M\",\"iv\"),\n (0x2174,\"M\",\"v\"),\n (0x2175,\"M\",\"vi\"),\n (0x2176,\"M\",\"vii\"),\n (0x2177,\"M\",\"viii\"),\n (0x2178,\"M\",\"ix\"),\n (0x2179,\"M\",\"x\"),\n (0x217A,\"M\",\"xi\"),\n (0x217B,\"M\",\"xii\"),\n (0x217C,\"M\",\"l\"),\n ]\n \n \ndef _seg_23()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x217D,\"M\",\"c\"),\n (0x217E,\"M\",\"d\"),\n (0x217F,\"M\",\"m\"),\n (0x2180,\"V\"),\n (0x2183,\"X\"),\n (0x2184,\"V\"),\n (0x2189,\"M\",\"0\u20443\"),\n (0x218A,\"V\"),\n (0x218C,\"X\"),\n (0x2190,\"V\"),\n (0x222C,\"M\",\"\u222b\u222b\"),\n (0x222D,\"M\",\"\u222b\u222b\u222b\"),\n (0x222E,\"V\"),\n (0x222F,\"M\",\"\u222e\u222e\"),\n (0x2230,\"M\",\"\u222e\u222e\u222e\"),\n (0x2231,\"V\"),\n (0x2329,\"M\",\"\u3008\"),\n (0x232A,\"M\",\"\u3009\"),\n (0x232B,\"V\"),\n (0x2427,\"X\"),\n (0x2440,\"V\"),\n (0x244B,\"X\"),\n (0x2460,\"M\",\"1\"),\n (0x2461,\"M\",\"2\"),\n (0x2462,\"M\",\"3\"),\n (0x2463,\"M\",\"4\"),\n (0x2464,\"M\",\"5\"),\n (0x2465,\"M\",\"6\"),\n (0x2466,\"M\",\"7\"),\n (0x2467,\"M\",\"8\"),\n (0x2468,\"M\",\"9\"),\n (0x2469,\"M\",\"10\"),\n (0x246A,\"M\",\"11\"),\n (0x246B,\"M\",\"12\"),\n (0x246C,\"M\",\"13\"),\n (0x246D,\"M\",\"14\"),\n (0x246E,\"M\",\"15\"),\n (0x246F,\"M\",\"16\"),\n (0x2470,\"M\",\"17\"),\n (0x2471,\"M\",\"18\"),\n (0x2472,\"M\",\"19\"),\n (0x2473,\"M\",\"20\"),\n (0x2474,\"3\",\"(1)\"),\n (0x2475,\"3\",\"(2)\"),\n (0x2476,\"3\",\"(3)\"),\n (0x2477,\"3\",\"(4)\"),\n (0x2478,\"3\",\"(5)\"),\n (0x2479,\"3\",\"(6)\"),\n (0x247A,\"3\",\"(7)\"),\n (0x247B,\"3\",\"(8)\"),\n (0x247C,\"3\",\"(9)\"),\n (0x247D,\"3\",\"(10)\"),\n (0x247E,\"3\",\"(11)\"),\n (0x247F,\"3\",\"(12)\"),\n (0x2480,\"3\",\"(13)\"),\n (0x2481,\"3\",\"(14)\"),\n (0x2482,\"3\",\"(15)\"),\n (0x2483,\"3\",\"(16)\"),\n (0x2484,\"3\",\"(17)\"),\n (0x2485,\"3\",\"(18)\"),\n (0x2486,\"3\",\"(19)\"),\n (0x2487,\"3\",\"(20)\"),\n (0x2488,\"X\"),\n (0x249C,\"3\",\"(a)\"),\n (0x249D,\"3\",\"(b)\"),\n (0x249E,\"3\",\"(c)\"),\n (0x249F,\"3\",\"(d)\"),\n (0x24A0,\"3\",\"(e)\"),\n (0x24A1,\"3\",\"(f)\"),\n (0x24A2,\"3\",\"(g)\"),\n (0x24A3,\"3\",\"(h)\"),\n (0x24A4,\"3\",\"(i)\"),\n (0x24A5,\"3\",\"(j)\"),\n (0x24A6,\"3\",\"(k)\"),\n (0x24A7,\"3\",\"(l)\"),\n (0x24A8,\"3\",\"(m)\"),\n (0x24A9,\"3\",\"(n)\"),\n (0x24AA,\"3\",\"(o)\"),\n (0x24AB,\"3\",\"(p)\"),\n (0x24AC,\"3\",\"(q)\"),\n (0x24AD,\"3\",\"(r)\"),\n (0x24AE,\"3\",\"(s)\"),\n (0x24AF,\"3\",\"(t)\"),\n (0x24B0,\"3\",\"(u)\"),\n (0x24B1,\"3\",\"(v)\"),\n (0x24B2,\"3\",\"(w)\"),\n (0x24B3,\"3\",\"(x)\"),\n (0x24B4,\"3\",\"(y)\"),\n (0x24B5,\"3\",\"(z)\"),\n (0x24B6,\"M\",\"a\"),\n (0x24B7,\"M\",\"b\"),\n (0x24B8,\"M\",\"c\"),\n (0x24B9,\"M\",\"d\"),\n (0x24BA,\"M\",\"e\"),\n (0x24BB,\"M\",\"f\"),\n (0x24BC,\"M\",\"g\"),\n (0x24BD,\"M\",\"h\"),\n (0x24BE,\"M\",\"i\"),\n (0x24BF,\"M\",\"j\"),\n (0x24C0,\"M\",\"k\"),\n ]\n \n \ndef _seg_24()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x24C1,\"M\",\"l\"),\n (0x24C2,\"M\",\"m\"),\n (0x24C3,\"M\",\"n\"),\n (0x24C4,\"M\",\"o\"),\n (0x24C5,\"M\",\"p\"),\n (0x24C6,\"M\",\"q\"),\n (0x24C7,\"M\",\"r\"),\n (0x24C8,\"M\",\"s\"),\n (0x24C9,\"M\",\"t\"),\n (0x24CA,\"M\",\"u\"),\n (0x24CB,\"M\",\"v\"),\n (0x24CC,\"M\",\"w\"),\n (0x24CD,\"M\",\"x\"),\n (0x24CE,\"M\",\"y\"),\n (0x24CF,\"M\",\"z\"),\n (0x24D0,\"M\",\"a\"),\n (0x24D1,\"M\",\"b\"),\n (0x24D2,\"M\",\"c\"),\n (0x24D3,\"M\",\"d\"),\n (0x24D4,\"M\",\"e\"),\n (0x24D5,\"M\",\"f\"),\n (0x24D6,\"M\",\"g\"),\n (0x24D7,\"M\",\"h\"),\n (0x24D8,\"M\",\"i\"),\n (0x24D9,\"M\",\"j\"),\n (0x24DA,\"M\",\"k\"),\n (0x24DB,\"M\",\"l\"),\n (0x24DC,\"M\",\"m\"),\n (0x24DD,\"M\",\"n\"),\n (0x24DE,\"M\",\"o\"),\n (0x24DF,\"M\",\"p\"),\n (0x24E0,\"M\",\"q\"),\n (0x24E1,\"M\",\"r\"),\n (0x24E2,\"M\",\"s\"),\n (0x24E3,\"M\",\"t\"),\n (0x24E4,\"M\",\"u\"),\n (0x24E5,\"M\",\"v\"),\n (0x24E6,\"M\",\"w\"),\n (0x24E7,\"M\",\"x\"),\n (0x24E8,\"M\",\"y\"),\n (0x24E9,\"M\",\"z\"),\n (0x24EA,\"M\",\"0\"),\n (0x24EB,\"V\"),\n (0x2A0C,\"M\",\"\u222b\u222b\u222b\u222b\"),\n (0x2A0D,\"V\"),\n (0x2A74,\"3\",\"::=\"),\n (0x2A75,\"3\",\"==\"),\n (0x2A76,\"3\",\"===\"),\n (0x2A77,\"V\"),\n (0x2ADC,\"M\",\"\u2add\u0338\"),\n (0x2ADD,\"V\"),\n (0x2B74,\"X\"),\n (0x2B76,\"V\"),\n (0x2B96,\"X\"),\n (0x2B97,\"V\"),\n (0x2C00,\"M\",\"\u2c30\"),\n (0x2C01,\"M\",\"\u2c31\"),\n (0x2C02,\"M\",\"\u2c32\"),\n (0x2C03,\"M\",\"\u2c33\"),\n (0x2C04,\"M\",\"\u2c34\"),\n (0x2C05,\"M\",\"\u2c35\"),\n (0x2C06,\"M\",\"\u2c36\"),\n (0x2C07,\"M\",\"\u2c37\"),\n (0x2C08,\"M\",\"\u2c38\"),\n (0x2C09,\"M\",\"\u2c39\"),\n (0x2C0A,\"M\",\"\u2c3a\"),\n (0x2C0B,\"M\",\"\u2c3b\"),\n (0x2C0C,\"M\",\"\u2c3c\"),\n (0x2C0D,\"M\",\"\u2c3d\"),\n (0x2C0E,\"M\",\"\u2c3e\"),\n (0x2C0F,\"M\",\"\u2c3f\"),\n (0x2C10,\"M\",\"\u2c40\"),\n (0x2C11,\"M\",\"\u2c41\"),\n (0x2C12,\"M\",\"\u2c42\"),\n (0x2C13,\"M\",\"\u2c43\"),\n (0x2C14,\"M\",\"\u2c44\"),\n (0x2C15,\"M\",\"\u2c45\"),\n (0x2C16,\"M\",\"\u2c46\"),\n (0x2C17,\"M\",\"\u2c47\"),\n (0x2C18,\"M\",\"\u2c48\"),\n (0x2C19,\"M\",\"\u2c49\"),\n (0x2C1A,\"M\",\"\u2c4a\"),\n (0x2C1B,\"M\",\"\u2c4b\"),\n (0x2C1C,\"M\",\"\u2c4c\"),\n (0x2C1D,\"M\",\"\u2c4d\"),\n (0x2C1E,\"M\",\"\u2c4e\"),\n (0x2C1F,\"M\",\"\u2c4f\"),\n (0x2C20,\"M\",\"\u2c50\"),\n (0x2C21,\"M\",\"\u2c51\"),\n (0x2C22,\"M\",\"\u2c52\"),\n (0x2C23,\"M\",\"\u2c53\"),\n (0x2C24,\"M\",\"\u2c54\"),\n (0x2C25,\"M\",\"\u2c55\"),\n (0x2C26,\"M\",\"\u2c56\"),\n (0x2C27,\"M\",\"\u2c57\"),\n (0x2C28,\"M\",\"\u2c58\"),\n (0x2C29,\"M\",\"\u2c59\"),\n (0x2C2A,\"M\",\"\u2c5a\"),\n (0x2C2B,\"M\",\"\u2c5b\"),\n (0x2C2C,\"M\",\"\u2c5c\"),\n ]\n \n \ndef _seg_25()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2C2D,\"M\",\"\u2c5d\"),\n (0x2C2E,\"M\",\"\u2c5e\"),\n (0x2C2F,\"M\",\"\u2c5f\"),\n (0x2C30,\"V\"),\n (0x2C60,\"M\",\"\u2c61\"),\n (0x2C61,\"V\"),\n (0x2C62,\"M\",\"\u026b\"),\n (0x2C63,\"M\",\"\u1d7d\"),\n (0x2C64,\"M\",\"\u027d\"),\n (0x2C65,\"V\"),\n (0x2C67,\"M\",\"\u2c68\"),\n (0x2C68,\"V\"),\n (0x2C69,\"M\",\"\u2c6a\"),\n (0x2C6A,\"V\"),\n (0x2C6B,\"M\",\"\u2c6c\"),\n (0x2C6C,\"V\"),\n (0x2C6D,\"M\",\"\u0251\"),\n (0x2C6E,\"M\",\"\u0271\"),\n (0x2C6F,\"M\",\"\u0250\"),\n (0x2C70,\"M\",\"\u0252\"),\n (0x2C71,\"V\"),\n (0x2C72,\"M\",\"\u2c73\"),\n (0x2C73,\"V\"),\n (0x2C75,\"M\",\"\u2c76\"),\n (0x2C76,\"V\"),\n (0x2C7C,\"M\",\"j\"),\n (0x2C7D,\"M\",\"v\"),\n (0x2C7E,\"M\",\"\u023f\"),\n (0x2C7F,\"M\",\"\u0240\"),\n (0x2C80,\"M\",\"\u2c81\"),\n (0x2C81,\"V\"),\n (0x2C82,\"M\",\"\u2c83\"),\n (0x2C83,\"V\"),\n (0x2C84,\"M\",\"\u2c85\"),\n (0x2C85,\"V\"),\n (0x2C86,\"M\",\"\u2c87\"),\n (0x2C87,\"V\"),\n (0x2C88,\"M\",\"\u2c89\"),\n (0x2C89,\"V\"),\n (0x2C8A,\"M\",\"\u2c8b\"),\n (0x2C8B,\"V\"),\n (0x2C8C,\"M\",\"\u2c8d\"),\n (0x2C8D,\"V\"),\n (0x2C8E,\"M\",\"\u2c8f\"),\n (0x2C8F,\"V\"),\n (0x2C90,\"M\",\"\u2c91\"),\n (0x2C91,\"V\"),\n (0x2C92,\"M\",\"\u2c93\"),\n (0x2C93,\"V\"),\n (0x2C94,\"M\",\"\u2c95\"),\n (0x2C95,\"V\"),\n (0x2C96,\"M\",\"\u2c97\"),\n (0x2C97,\"V\"),\n (0x2C98,\"M\",\"\u2c99\"),\n (0x2C99,\"V\"),\n (0x2C9A,\"M\",\"\u2c9b\"),\n (0x2C9B,\"V\"),\n (0x2C9C,\"M\",\"\u2c9d\"),\n (0x2C9D,\"V\"),\n (0x2C9E,\"M\",\"\u2c9f\"),\n (0x2C9F,\"V\"),\n (0x2CA0,\"M\",\"\u2ca1\"),\n (0x2CA1,\"V\"),\n (0x2CA2,\"M\",\"\u2ca3\"),\n (0x2CA3,\"V\"),\n (0x2CA4,\"M\",\"\u2ca5\"),\n (0x2CA5,\"V\"),\n (0x2CA6,\"M\",\"\u2ca7\"),\n (0x2CA7,\"V\"),\n (0x2CA8,\"M\",\"\u2ca9\"),\n (0x2CA9,\"V\"),\n (0x2CAA,\"M\",\"\u2cab\"),\n (0x2CAB,\"V\"),\n (0x2CAC,\"M\",\"\u2cad\"),\n (0x2CAD,\"V\"),\n (0x2CAE,\"M\",\"\u2caf\"),\n (0x2CAF,\"V\"),\n (0x2CB0,\"M\",\"\u2cb1\"),\n (0x2CB1,\"V\"),\n (0x2CB2,\"M\",\"\u2cb3\"),\n (0x2CB3,\"V\"),\n (0x2CB4,\"M\",\"\u2cb5\"),\n (0x2CB5,\"V\"),\n (0x2CB6,\"M\",\"\u2cb7\"),\n (0x2CB7,\"V\"),\n (0x2CB8,\"M\",\"\u2cb9\"),\n (0x2CB9,\"V\"),\n (0x2CBA,\"M\",\"\u2cbb\"),\n (0x2CBB,\"V\"),\n (0x2CBC,\"M\",\"\u2cbd\"),\n (0x2CBD,\"V\"),\n (0x2CBE,\"M\",\"\u2cbf\"),\n (0x2CBF,\"V\"),\n (0x2CC0,\"M\",\"\u2cc1\"),\n (0x2CC1,\"V\"),\n (0x2CC2,\"M\",\"\u2cc3\"),\n (0x2CC3,\"V\"),\n (0x2CC4,\"M\",\"\u2cc5\"),\n (0x2CC5,\"V\"),\n (0x2CC6,\"M\",\"\u2cc7\"),\n ]\n \n \ndef _seg_26()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2CC7,\"V\"),\n (0x2CC8,\"M\",\"\u2cc9\"),\n (0x2CC9,\"V\"),\n (0x2CCA,\"M\",\"\u2ccb\"),\n (0x2CCB,\"V\"),\n (0x2CCC,\"M\",\"\u2ccd\"),\n (0x2CCD,\"V\"),\n (0x2CCE,\"M\",\"\u2ccf\"),\n (0x2CCF,\"V\"),\n (0x2CD0,\"M\",\"\u2cd1\"),\n (0x2CD1,\"V\"),\n (0x2CD2,\"M\",\"\u2cd3\"),\n (0x2CD3,\"V\"),\n (0x2CD4,\"M\",\"\u2cd5\"),\n (0x2CD5,\"V\"),\n (0x2CD6,\"M\",\"\u2cd7\"),\n (0x2CD7,\"V\"),\n (0x2CD8,\"M\",\"\u2cd9\"),\n (0x2CD9,\"V\"),\n (0x2CDA,\"M\",\"\u2cdb\"),\n (0x2CDB,\"V\"),\n (0x2CDC,\"M\",\"\u2cdd\"),\n (0x2CDD,\"V\"),\n (0x2CDE,\"M\",\"\u2cdf\"),\n (0x2CDF,\"V\"),\n (0x2CE0,\"M\",\"\u2ce1\"),\n (0x2CE1,\"V\"),\n (0x2CE2,\"M\",\"\u2ce3\"),\n (0x2CE3,\"V\"),\n (0x2CEB,\"M\",\"\u2cec\"),\n (0x2CEC,\"V\"),\n (0x2CED,\"M\",\"\u2cee\"),\n (0x2CEE,\"V\"),\n (0x2CF2,\"M\",\"\u2cf3\"),\n (0x2CF3,\"V\"),\n (0x2CF4,\"X\"),\n (0x2CF9,\"V\"),\n (0x2D26,\"X\"),\n (0x2D27,\"V\"),\n (0x2D28,\"X\"),\n (0x2D2D,\"V\"),\n (0x2D2E,\"X\"),\n (0x2D30,\"V\"),\n (0x2D68,\"X\"),\n (0x2D6F,\"M\",\"\u2d61\"),\n (0x2D70,\"V\"),\n (0x2D71,\"X\"),\n (0x2D7F,\"V\"),\n (0x2D97,\"X\"),\n (0x2DA0,\"V\"),\n (0x2DA7,\"X\"),\n (0x2DA8,\"V\"),\n (0x2DAF,\"X\"),\n (0x2DB0,\"V\"),\n (0x2DB7,\"X\"),\n (0x2DB8,\"V\"),\n (0x2DBF,\"X\"),\n (0x2DC0,\"V\"),\n (0x2DC7,\"X\"),\n (0x2DC8,\"V\"),\n (0x2DCF,\"X\"),\n (0x2DD0,\"V\"),\n (0x2DD7,\"X\"),\n (0x2DD8,\"V\"),\n (0x2DDF,\"X\"),\n (0x2DE0,\"V\"),\n (0x2E5E,\"X\"),\n (0x2E80,\"V\"),\n (0x2E9A,\"X\"),\n (0x2E9B,\"V\"),\n (0x2E9F,\"M\",\"\u6bcd\"),\n (0x2EA0,\"V\"),\n (0x2EF3,\"M\",\"\u9f9f\"),\n (0x2EF4,\"X\"),\n (0x2F00,\"M\",\"\u4e00\"),\n (0x2F01,\"M\",\"\u4e28\"),\n (0x2F02,\"M\",\"\u4e36\"),\n (0x2F03,\"M\",\"\u4e3f\"),\n (0x2F04,\"M\",\"\u4e59\"),\n (0x2F05,\"M\",\"\u4e85\"),\n (0x2F06,\"M\",\"\u4e8c\"),\n (0x2F07,\"M\",\"\u4ea0\"),\n (0x2F08,\"M\",\"\u4eba\"),\n (0x2F09,\"M\",\"\u513f\"),\n (0x2F0A,\"M\",\"\u5165\"),\n (0x2F0B,\"M\",\"\u516b\"),\n (0x2F0C,\"M\",\"\u5182\"),\n (0x2F0D,\"M\",\"\u5196\"),\n (0x2F0E,\"M\",\"\u51ab\"),\n (0x2F0F,\"M\",\"\u51e0\"),\n (0x2F10,\"M\",\"\u51f5\"),\n (0x2F11,\"M\",\"\u5200\"),\n (0x2F12,\"M\",\"\u529b\"),\n (0x2F13,\"M\",\"\u52f9\"),\n (0x2F14,\"M\",\"\u5315\"),\n (0x2F15,\"M\",\"\u531a\"),\n (0x2F16,\"M\",\"\u5338\"),\n (0x2F17,\"M\",\"\u5341\"),\n (0x2F18,\"M\",\"\u535c\"),\n (0x2F19,\"M\",\"\u5369\"),\n ]\n \n \ndef _seg_27()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F1A,\"M\",\"\u5382\"),\n (0x2F1B,\"M\",\"\u53b6\"),\n (0x2F1C,\"M\",\"\u53c8\"),\n (0x2F1D,\"M\",\"\u53e3\"),\n (0x2F1E,\"M\",\"\u56d7\"),\n (0x2F1F,\"M\",\"\u571f\"),\n (0x2F20,\"M\",\"\u58eb\"),\n (0x2F21,\"M\",\"\u5902\"),\n (0x2F22,\"M\",\"\u590a\"),\n (0x2F23,\"M\",\"\u5915\"),\n (0x2F24,\"M\",\"\u5927\"),\n (0x2F25,\"M\",\"\u5973\"),\n (0x2F26,\"M\",\"\u5b50\"),\n (0x2F27,\"M\",\"\u5b80\"),\n (0x2F28,\"M\",\"\u5bf8\"),\n (0x2F29,\"M\",\"\u5c0f\"),\n (0x2F2A,\"M\",\"\u5c22\"),\n (0x2F2B,\"M\",\"\u5c38\"),\n (0x2F2C,\"M\",\"\u5c6e\"),\n (0x2F2D,\"M\",\"\u5c71\"),\n (0x2F2E,\"M\",\"\u5ddb\"),\n (0x2F2F,\"M\",\"\u5de5\"),\n (0x2F30,\"M\",\"\u5df1\"),\n (0x2F31,\"M\",\"\u5dfe\"),\n (0x2F32,\"M\",\"\u5e72\"),\n (0x2F33,\"M\",\"\u5e7a\"),\n (0x2F34,\"M\",\"\u5e7f\"),\n (0x2F35,\"M\",\"\u5ef4\"),\n (0x2F36,\"M\",\"\u5efe\"),\n (0x2F37,\"M\",\"\u5f0b\"),\n (0x2F38,\"M\",\"\u5f13\"),\n (0x2F39,\"M\",\"\u5f50\"),\n (0x2F3A,\"M\",\"\u5f61\"),\n (0x2F3B,\"M\",\"\u5f73\"),\n (0x2F3C,\"M\",\"\u5fc3\"),\n (0x2F3D,\"M\",\"\u6208\"),\n (0x2F3E,\"M\",\"\u6236\"),\n (0x2F3F,\"M\",\"\u624b\"),\n (0x2F40,\"M\",\"\u652f\"),\n (0x2F41,\"M\",\"\u6534\"),\n (0x2F42,\"M\",\"\u6587\"),\n (0x2F43,\"M\",\"\u6597\"),\n (0x2F44,\"M\",\"\u65a4\"),\n (0x2F45,\"M\",\"\u65b9\"),\n (0x2F46,\"M\",\"\u65e0\"),\n (0x2F47,\"M\",\"\u65e5\"),\n (0x2F48,\"M\",\"\u66f0\"),\n (0x2F49,\"M\",\"\u6708\"),\n (0x2F4A,\"M\",\"\u6728\"),\n (0x2F4B,\"M\",\"\u6b20\"),\n (0x2F4C,\"M\",\"\u6b62\"),\n (0x2F4D,\"M\",\"\u6b79\"),\n (0x2F4E,\"M\",\"\u6bb3\"),\n (0x2F4F,\"M\",\"\u6bcb\"),\n (0x2F50,\"M\",\"\u6bd4\"),\n (0x2F51,\"M\",\"\u6bdb\"),\n (0x2F52,\"M\",\"\u6c0f\"),\n (0x2F53,\"M\",\"\u6c14\"),\n (0x2F54,\"M\",\"\u6c34\"),\n (0x2F55,\"M\",\"\u706b\"),\n (0x2F56,\"M\",\"\u722a\"),\n (0x2F57,\"M\",\"\u7236\"),\n (0x2F58,\"M\",\"\u723b\"),\n (0x2F59,\"M\",\"\u723f\"),\n (0x2F5A,\"M\",\"\u7247\"),\n (0x2F5B,\"M\",\"\u7259\"),\n (0x2F5C,\"M\",\"\u725b\"),\n (0x2F5D,\"M\",\"\u72ac\"),\n (0x2F5E,\"M\",\"\u7384\"),\n (0x2F5F,\"M\",\"\u7389\"),\n (0x2F60,\"M\",\"\u74dc\"),\n (0x2F61,\"M\",\"\u74e6\"),\n (0x2F62,\"M\",\"\u7518\"),\n (0x2F63,\"M\",\"\u751f\"),\n (0x2F64,\"M\",\"\u7528\"),\n (0x2F65,\"M\",\"\u7530\"),\n (0x2F66,\"M\",\"\u758b\"),\n (0x2F67,\"M\",\"\u7592\"),\n (0x2F68,\"M\",\"\u7676\"),\n (0x2F69,\"M\",\"\u767d\"),\n (0x2F6A,\"M\",\"\u76ae\"),\n (0x2F6B,\"M\",\"\u76bf\"),\n (0x2F6C,\"M\",\"\u76ee\"),\n (0x2F6D,\"M\",\"\u77db\"),\n (0x2F6E,\"M\",\"\u77e2\"),\n (0x2F6F,\"M\",\"\u77f3\"),\n (0x2F70,\"M\",\"\u793a\"),\n (0x2F71,\"M\",\"\u79b8\"),\n (0x2F72,\"M\",\"\u79be\"),\n (0x2F73,\"M\",\"\u7a74\"),\n (0x2F74,\"M\",\"\u7acb\"),\n (0x2F75,\"M\",\"\u7af9\"),\n (0x2F76,\"M\",\"\u7c73\"),\n (0x2F77,\"M\",\"\u7cf8\"),\n (0x2F78,\"M\",\"\u7f36\"),\n (0x2F79,\"M\",\"\u7f51\"),\n (0x2F7A,\"M\",\"\u7f8a\"),\n (0x2F7B,\"M\",\"\u7fbd\"),\n (0x2F7C,\"M\",\"\u8001\"),\n (0x2F7D,\"M\",\"\u800c\"),\n ]\n \n \ndef _seg_28()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F7E,\"M\",\"\u8012\"),\n (0x2F7F,\"M\",\"\u8033\"),\n (0x2F80,\"M\",\"\u807f\"),\n (0x2F81,\"M\",\"\u8089\"),\n (0x2F82,\"M\",\"\u81e3\"),\n (0x2F83,\"M\",\"\u81ea\"),\n (0x2F84,\"M\",\"\u81f3\"),\n (0x2F85,\"M\",\"\u81fc\"),\n (0x2F86,\"M\",\"\u820c\"),\n (0x2F87,\"M\",\"\u821b\"),\n (0x2F88,\"M\",\"\u821f\"),\n (0x2F89,\"M\",\"\u826e\"),\n (0x2F8A,\"M\",\"\u8272\"),\n (0x2F8B,\"M\",\"\u8278\"),\n (0x2F8C,\"M\",\"\u864d\"),\n (0x2F8D,\"M\",\"\u866b\"),\n (0x2F8E,\"M\",\"\u8840\"),\n (0x2F8F,\"M\",\"\u884c\"),\n (0x2F90,\"M\",\"\u8863\"),\n (0x2F91,\"M\",\"\u897e\"),\n (0x2F92,\"M\",\"\u898b\"),\n (0x2F93,\"M\",\"\u89d2\"),\n (0x2F94,\"M\",\"\u8a00\"),\n (0x2F95,\"M\",\"\u8c37\"),\n (0x2F96,\"M\",\"\u8c46\"),\n (0x2F97,\"M\",\"\u8c55\"),\n (0x2F98,\"M\",\"\u8c78\"),\n (0x2F99,\"M\",\"\u8c9d\"),\n (0x2F9A,\"M\",\"\u8d64\"),\n (0x2F9B,\"M\",\"\u8d70\"),\n (0x2F9C,\"M\",\"\u8db3\"),\n (0x2F9D,\"M\",\"\u8eab\"),\n (0x2F9E,\"M\",\"\u8eca\"),\n (0x2F9F,\"M\",\"\u8f9b\"),\n (0x2FA0,\"M\",\"\u8fb0\"),\n (0x2FA1,\"M\",\"\u8fb5\"),\n (0x2FA2,\"M\",\"\u9091\"),\n (0x2FA3,\"M\",\"\u9149\"),\n (0x2FA4,\"M\",\"\u91c6\"),\n (0x2FA5,\"M\",\"\u91cc\"),\n (0x2FA6,\"M\",\"\u91d1\"),\n (0x2FA7,\"M\",\"\u9577\"),\n (0x2FA8,\"M\",\"\u9580\"),\n (0x2FA9,\"M\",\"\u961c\"),\n (0x2FAA,\"M\",\"\u96b6\"),\n (0x2FAB,\"M\",\"\u96b9\"),\n (0x2FAC,\"M\",\"\u96e8\"),\n (0x2FAD,\"M\",\"\u9751\"),\n (0x2FAE,\"M\",\"\u975e\"),\n (0x2FAF,\"M\",\"\u9762\"),\n (0x2FB0,\"M\",\"\u9769\"),\n (0x2FB1,\"M\",\"\u97cb\"),\n (0x2FB2,\"M\",\"\u97ed\"),\n (0x2FB3,\"M\",\"\u97f3\"),\n (0x2FB4,\"M\",\"\u9801\"),\n (0x2FB5,\"M\",\"\u98a8\"),\n (0x2FB6,\"M\",\"\u98db\"),\n (0x2FB7,\"M\",\"\u98df\"),\n (0x2FB8,\"M\",\"\u9996\"),\n (0x2FB9,\"M\",\"\u9999\"),\n (0x2FBA,\"M\",\"\u99ac\"),\n (0x2FBB,\"M\",\"\u9aa8\"),\n (0x2FBC,\"M\",\"\u9ad8\"),\n (0x2FBD,\"M\",\"\u9adf\"),\n (0x2FBE,\"M\",\"\u9b25\"),\n (0x2FBF,\"M\",\"\u9b2f\"),\n (0x2FC0,\"M\",\"\u9b32\"),\n (0x2FC1,\"M\",\"\u9b3c\"),\n (0x2FC2,\"M\",\"\u9b5a\"),\n (0x2FC3,\"M\",\"\u9ce5\"),\n (0x2FC4,\"M\",\"\u9e75\"),\n (0x2FC5,\"M\",\"\u9e7f\"),\n (0x2FC6,\"M\",\"\u9ea5\"),\n (0x2FC7,\"M\",\"\u9ebb\"),\n (0x2FC8,\"M\",\"\u9ec3\"),\n (0x2FC9,\"M\",\"\u9ecd\"),\n (0x2FCA,\"M\",\"\u9ed1\"),\n (0x2FCB,\"M\",\"\u9ef9\"),\n (0x2FCC,\"M\",\"\u9efd\"),\n (0x2FCD,\"M\",\"\u9f0e\"),\n (0x2FCE,\"M\",\"\u9f13\"),\n (0x2FCF,\"M\",\"\u9f20\"),\n (0x2FD0,\"M\",\"\u9f3b\"),\n (0x2FD1,\"M\",\"\u9f4a\"),\n (0x2FD2,\"M\",\"\u9f52\"),\n (0x2FD3,\"M\",\"\u9f8d\"),\n (0x2FD4,\"M\",\"\u9f9c\"),\n (0x2FD5,\"M\",\"\u9fa0\"),\n (0x2FD6,\"X\"),\n (0x3000,\"3\",\" \"),\n (0x3001,\"V\"),\n (0x3002,\"M\",\".\"),\n (0x3003,\"V\"),\n (0x3036,\"M\",\"\u3012\"),\n (0x3037,\"V\"),\n (0x3038,\"M\",\"\u5341\"),\n (0x3039,\"M\",\"\u5344\"),\n (0x303A,\"M\",\"\u5345\"),\n (0x303B,\"V\"),\n (0x3040,\"X\"),\n ]\n \n \ndef _seg_29()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x3041,\"V\"),\n (0x3097,\"X\"),\n (0x3099,\"V\"),\n (0x309B,\"3\",\" \u3099\"),\n (0x309C,\"3\",\" \u309a\"),\n (0x309D,\"V\"),\n (0x309F,\"M\",\"\u3088\u308a\"),\n (0x30A0,\"V\"),\n (0x30FF,\"M\",\"\u30b3\u30c8\"),\n (0x3100,\"X\"),\n (0x3105,\"V\"),\n (0x3130,\"X\"),\n (0x3131,\"M\",\"\u1100\"),\n (0x3132,\"M\",\"\u1101\"),\n (0x3133,\"M\",\"\u11aa\"),\n (0x3134,\"M\",\"\u1102\"),\n (0x3135,\"M\",\"\u11ac\"),\n (0x3136,\"M\",\"\u11ad\"),\n (0x3137,\"M\",\"\u1103\"),\n (0x3138,\"M\",\"\u1104\"),\n (0x3139,\"M\",\"\u1105\"),\n (0x313A,\"M\",\"\u11b0\"),\n (0x313B,\"M\",\"\u11b1\"),\n (0x313C,\"M\",\"\u11b2\"),\n (0x313D,\"M\",\"\u11b3\"),\n (0x313E,\"M\",\"\u11b4\"),\n (0x313F,\"M\",\"\u11b5\"),\n (0x3140,\"M\",\"\u111a\"),\n (0x3141,\"M\",\"\u1106\"),\n (0x3142,\"M\",\"\u1107\"),\n (0x3143,\"M\",\"\u1108\"),\n (0x3144,\"M\",\"\u1121\"),\n (0x3145,\"M\",\"\u1109\"),\n (0x3146,\"M\",\"\u110a\"),\n (0x3147,\"M\",\"\u110b\"),\n (0x3148,\"M\",\"\u110c\"),\n (0x3149,\"M\",\"\u110d\"),\n (0x314A,\"M\",\"\u110e\"),\n (0x314B,\"M\",\"\u110f\"),\n (0x314C,\"M\",\"\u1110\"),\n (0x314D,\"M\",\"\u1111\"),\n (0x314E,\"M\",\"\u1112\"),\n (0x314F,\"M\",\"\u1161\"),\n (0x3150,\"M\",\"\u1162\"),\n (0x3151,\"M\",\"\u1163\"),\n (0x3152,\"M\",\"\u1164\"),\n (0x3153,\"M\",\"\u1165\"),\n (0x3154,\"M\",\"\u1166\"),\n (0x3155,\"M\",\"\u1167\"),\n (0x3156,\"M\",\"\u1168\"),\n (0x3157,\"M\",\"\u1169\"),\n (0x3158,\"M\",\"\u116a\"),\n (0x3159,\"M\",\"\u116b\"),\n (0x315A,\"M\",\"\u116c\"),\n (0x315B,\"M\",\"\u116d\"),\n (0x315C,\"M\",\"\u116e\"),\n (0x315D,\"M\",\"\u116f\"),\n (0x315E,\"M\",\"\u1170\"),\n (0x315F,\"M\",\"\u1171\"),\n (0x3160,\"M\",\"\u1172\"),\n (0x3161,\"M\",\"\u1173\"),\n (0x3162,\"M\",\"\u1174\"),\n (0x3163,\"M\",\"\u1175\"),\n (0x3164,\"X\"),\n (0x3165,\"M\",\"\u1114\"),\n (0x3166,\"M\",\"\u1115\"),\n (0x3167,\"M\",\"\u11c7\"),\n (0x3168,\"M\",\"\u11c8\"),\n (0x3169,\"M\",\"\u11cc\"),\n (0x316A,\"M\",\"\u11ce\"),\n (0x316B,\"M\",\"\u11d3\"),\n (0x316C,\"M\",\"\u11d7\"),\n (0x316D,\"M\",\"\u11d9\"),\n (0x316E,\"M\",\"\u111c\"),\n (0x316F,\"M\",\"\u11dd\"),\n (0x3170,\"M\",\"\u11df\"),\n (0x3171,\"M\",\"\u111d\"),\n (0x3172,\"M\",\"\u111e\"),\n (0x3173,\"M\",\"\u1120\"),\n (0x3174,\"M\",\"\u1122\"),\n (0x3175,\"M\",\"\u1123\"),\n (0x3176,\"M\",\"\u1127\"),\n (0x3177,\"M\",\"\u1129\"),\n (0x3178,\"M\",\"\u112b\"),\n (0x3179,\"M\",\"\u112c\"),\n (0x317A,\"M\",\"\u112d\"),\n (0x317B,\"M\",\"\u112e\"),\n (0x317C,\"M\",\"\u112f\"),\n (0x317D,\"M\",\"\u1132\"),\n (0x317E,\"M\",\"\u1136\"),\n (0x317F,\"M\",\"\u1140\"),\n (0x3180,\"M\",\"\u1147\"),\n (0x3181,\"M\",\"\u114c\"),\n (0x3182,\"M\",\"\u11f1\"),\n (0x3183,\"M\",\"\u11f2\"),\n (0x3184,\"M\",\"\u1157\"),\n (0x3185,\"M\",\"\u1158\"),\n (0x3186,\"M\",\"\u1159\"),\n (0x3187,\"M\",\"\u1184\"),\n (0x3188,\"M\",\"\u1185\"),\n ]\n \n \ndef _seg_30()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x3189,\"M\",\"\u1188\"),\n (0x318A,\"M\",\"\u1191\"),\n (0x318B,\"M\",\"\u1192\"),\n (0x318C,\"M\",\"\u1194\"),\n (0x318D,\"M\",\"\u119e\"),\n (0x318E,\"M\",\"\u11a1\"),\n (0x318F,\"X\"),\n (0x3190,\"V\"),\n (0x3192,\"M\",\"\u4e00\"),\n (0x3193,\"M\",\"\u4e8c\"),\n (0x3194,\"M\",\"\u4e09\"),\n (0x3195,\"M\",\"\u56db\"),\n (0x3196,\"M\",\"\u4e0a\"),\n (0x3197,\"M\",\"\u4e2d\"),\n (0x3198,\"M\",\"\u4e0b\"),\n (0x3199,\"M\",\"\u7532\"),\n (0x319A,\"M\",\"\u4e59\"),\n (0x319B,\"M\",\"\u4e19\"),\n (0x319C,\"M\",\"\u4e01\"),\n (0x319D,\"M\",\"\u5929\"),\n (0x319E,\"M\",\"\u5730\"),\n (0x319F,\"M\",\"\u4eba\"),\n (0x31A0,\"V\"),\n (0x31E4,\"X\"),\n (0x31F0,\"V\"),\n (0x3200,\"3\",\"(\u1100)\"),\n (0x3201,\"3\",\"(\u1102)\"),\n (0x3202,\"3\",\"(\u1103)\"),\n (0x3203,\"3\",\"(\u1105)\"),\n (0x3204,\"3\",\"(\u1106)\"),\n (0x3205,\"3\",\"(\u1107)\"),\n (0x3206,\"3\",\"(\u1109)\"),\n (0x3207,\"3\",\"(\u110b)\"),\n (0x3208,\"3\",\"(\u110c)\"),\n (0x3209,\"3\",\"(\u110e)\"),\n (0x320A,\"3\",\"(\u110f)\"),\n (0x320B,\"3\",\"(\u1110)\"),\n (0x320C,\"3\",\"(\u1111)\"),\n (0x320D,\"3\",\"(\u1112)\"),\n (0x320E,\"3\",\"(\uac00)\"),\n (0x320F,\"3\",\"(\ub098)\"),\n (0x3210,\"3\",\"(\ub2e4)\"),\n (0x3211,\"3\",\"(\ub77c)\"),\n (0x3212,\"3\",\"(\ub9c8)\"),\n (0x3213,\"3\",\"(\ubc14)\"),\n (0x3214,\"3\",\"(\uc0ac)\"),\n (0x3215,\"3\",\"(\uc544)\"),\n (0x3216,\"3\",\"(\uc790)\"),\n (0x3217,\"3\",\"(\ucc28)\"),\n (0x3218,\"3\",\"(\uce74)\"),\n (0x3219,\"3\",\"(\ud0c0)\"),\n (0x321A,\"3\",\"(\ud30c)\"),\n (0x321B,\"3\",\"(\ud558)\"),\n (0x321C,\"3\",\"(\uc8fc)\"),\n (0x321D,\"3\",\"(\uc624\uc804)\"),\n (0x321E,\"3\",\"(\uc624\ud6c4)\"),\n (0x321F,\"X\"),\n (0x3220,\"3\",\"(\u4e00)\"),\n (0x3221,\"3\",\"(\u4e8c)\"),\n (0x3222,\"3\",\"(\u4e09)\"),\n (0x3223,\"3\",\"(\u56db)\"),\n (0x3224,\"3\",\"(\u4e94)\"),\n (0x3225,\"3\",\"(\u516d)\"),\n (0x3226,\"3\",\"(\u4e03)\"),\n (0x3227,\"3\",\"(\u516b)\"),\n (0x3228,\"3\",\"(\u4e5d)\"),\n (0x3229,\"3\",\"(\u5341)\"),\n (0x322A,\"3\",\"(\u6708)\"),\n (0x322B,\"3\",\"(\u706b)\"),\n (0x322C,\"3\",\"(\u6c34)\"),\n (0x322D,\"3\",\"(\u6728)\"),\n (0x322E,\"3\",\"(\u91d1)\"),\n (0x322F,\"3\",\"(\u571f)\"),\n (0x3230,\"3\",\"(\u65e5)\"),\n (0x3231,\"3\",\"(\u682a)\"),\n (0x3232,\"3\",\"(\u6709)\"),\n (0x3233,\"3\",\"(\u793e)\"),\n (0x3234,\"3\",\"(\u540d)\"),\n (0x3235,\"3\",\"(\u7279)\"),\n (0x3236,\"3\",\"(\u8ca1)\"),\n (0x3237,\"3\",\"(\u795d)\"),\n (0x3238,\"3\",\"(\u52b4)\"),\n (0x3239,\"3\",\"(\u4ee3)\"),\n (0x323A,\"3\",\"(\u547c)\"),\n (0x323B,\"3\",\"(\u5b66)\"),\n (0x323C,\"3\",\"(\u76e3)\"),\n (0x323D,\"3\",\"(\u4f01)\"),\n (0x323E,\"3\",\"(\u8cc7)\"),\n (0x323F,\"3\",\"(\u5354)\"),\n (0x3240,\"3\",\"(\u796d)\"),\n (0x3241,\"3\",\"(\u4f11)\"),\n (0x3242,\"3\",\"(\u81ea)\"),\n (0x3243,\"3\",\"(\u81f3)\"),\n (0x3244,\"M\",\"\u554f\"),\n (0x3245,\"M\",\"\u5e7c\"),\n (0x3246,\"M\",\"\u6587\"),\n (0x3247,\"M\",\"\u7b8f\"),\n (0x3248,\"V\"),\n (0x3250,\"M\",\"pte\"),\n (0x3251,\"M\",\"21\"),\n ]\n \n \ndef _seg_31()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x3252,\"M\",\"22\"),\n (0x3253,\"M\",\"23\"),\n (0x3254,\"M\",\"24\"),\n (0x3255,\"M\",\"25\"),\n (0x3256,\"M\",\"26\"),\n (0x3257,\"M\",\"27\"),\n (0x3258,\"M\",\"28\"),\n (0x3259,\"M\",\"29\"),\n (0x325A,\"M\",\"30\"),\n (0x325B,\"M\",\"31\"),\n (0x325C,\"M\",\"32\"),\n (0x325D,\"M\",\"33\"),\n (0x325E,\"M\",\"34\"),\n (0x325F,\"M\",\"35\"),\n (0x3260,\"M\",\"\u1100\"),\n (0x3261,\"M\",\"\u1102\"),\n (0x3262,\"M\",\"\u1103\"),\n (0x3263,\"M\",\"\u1105\"),\n (0x3264,\"M\",\"\u1106\"),\n (0x3265,\"M\",\"\u1107\"),\n (0x3266,\"M\",\"\u1109\"),\n (0x3267,\"M\",\"\u110b\"),\n (0x3268,\"M\",\"\u110c\"),\n (0x3269,\"M\",\"\u110e\"),\n (0x326A,\"M\",\"\u110f\"),\n (0x326B,\"M\",\"\u1110\"),\n (0x326C,\"M\",\"\u1111\"),\n (0x326D,\"M\",\"\u1112\"),\n (0x326E,\"M\",\"\uac00\"),\n (0x326F,\"M\",\"\ub098\"),\n (0x3270,\"M\",\"\ub2e4\"),\n (0x3271,\"M\",\"\ub77c\"),\n (0x3272,\"M\",\"\ub9c8\"),\n (0x3273,\"M\",\"\ubc14\"),\n (0x3274,\"M\",\"\uc0ac\"),\n (0x3275,\"M\",\"\uc544\"),\n (0x3276,\"M\",\"\uc790\"),\n (0x3277,\"M\",\"\ucc28\"),\n (0x3278,\"M\",\"\uce74\"),\n (0x3279,\"M\",\"\ud0c0\"),\n (0x327A,\"M\",\"\ud30c\"),\n (0x327B,\"M\",\"\ud558\"),\n (0x327C,\"M\",\"\ucc38\uace0\"),\n (0x327D,\"M\",\"\uc8fc\uc758\"),\n (0x327E,\"M\",\"\uc6b0\"),\n (0x327F,\"V\"),\n (0x3280,\"M\",\"\u4e00\"),\n (0x3281,\"M\",\"\u4e8c\"),\n (0x3282,\"M\",\"\u4e09\"),\n (0x3283,\"M\",\"\u56db\"),\n (0x3284,\"M\",\"\u4e94\"),\n (0x3285,\"M\",\"\u516d\"),\n (0x3286,\"M\",\"\u4e03\"),\n (0x3287,\"M\",\"\u516b\"),\n (0x3288,\"M\",\"\u4e5d\"),\n (0x3289,\"M\",\"\u5341\"),\n (0x328A,\"M\",\"\u6708\"),\n (0x328B,\"M\",\"\u706b\"),\n (0x328C,\"M\",\"\u6c34\"),\n (0x328D,\"M\",\"\u6728\"),\n (0x328E,\"M\",\"\u91d1\"),\n (0x328F,\"M\",\"\u571f\"),\n (0x3290,\"M\",\"\u65e5\"),\n (0x3291,\"M\",\"\u682a\"),\n (0x3292,\"M\",\"\u6709\"),\n (0x3293,\"M\",\"\u793e\"),\n (0x3294,\"M\",\"\u540d\"),\n (0x3295,\"M\",\"\u7279\"),\n (0x3296,\"M\",\"\u8ca1\"),\n (0x3297,\"M\",\"\u795d\"),\n (0x3298,\"M\",\"\u52b4\"),\n (0x3299,\"M\",\"\u79d8\"),\n (0x329A,\"M\",\"\u7537\"),\n (0x329B,\"M\",\"\u5973\"),\n (0x329C,\"M\",\"\u9069\"),\n (0x329D,\"M\",\"\u512a\"),\n (0x329E,\"M\",\"\u5370\"),\n (0x329F,\"M\",\"\u6ce8\"),\n (0x32A0,\"M\",\"\u9805\"),\n (0x32A1,\"M\",\"\u4f11\"),\n (0x32A2,\"M\",\"\u5199\"),\n (0x32A3,\"M\",\"\u6b63\"),\n (0x32A4,\"M\",\"\u4e0a\"),\n (0x32A5,\"M\",\"\u4e2d\"),\n (0x32A6,\"M\",\"\u4e0b\"),\n (0x32A7,\"M\",\"\u5de6\"),\n (0x32A8,\"M\",\"\u53f3\"),\n (0x32A9,\"M\",\"\u533b\"),\n (0x32AA,\"M\",\"\u5b97\"),\n (0x32AB,\"M\",\"\u5b66\"),\n (0x32AC,\"M\",\"\u76e3\"),\n (0x32AD,\"M\",\"\u4f01\"),\n (0x32AE,\"M\",\"\u8cc7\"),\n (0x32AF,\"M\",\"\u5354\"),\n (0x32B0,\"M\",\"\u591c\"),\n (0x32B1,\"M\",\"36\"),\n (0x32B2,\"M\",\"37\"),\n (0x32B3,\"M\",\"38\"),\n (0x32B4,\"M\",\"39\"),\n (0x32B5,\"M\",\"40\"),\n ]\n \n \ndef _seg_32()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x32B6,\"M\",\"41\"),\n (0x32B7,\"M\",\"42\"),\n (0x32B8,\"M\",\"43\"),\n (0x32B9,\"M\",\"44\"),\n (0x32BA,\"M\",\"45\"),\n (0x32BB,\"M\",\"46\"),\n (0x32BC,\"M\",\"47\"),\n (0x32BD,\"M\",\"48\"),\n (0x32BE,\"M\",\"49\"),\n (0x32BF,\"M\",\"50\"),\n (0x32C0,\"M\",\"1\u6708\"),\n (0x32C1,\"M\",\"2\u6708\"),\n (0x32C2,\"M\",\"3\u6708\"),\n (0x32C3,\"M\",\"4\u6708\"),\n (0x32C4,\"M\",\"5\u6708\"),\n (0x32C5,\"M\",\"6\u6708\"),\n (0x32C6,\"M\",\"7\u6708\"),\n (0x32C7,\"M\",\"8\u6708\"),\n (0x32C8,\"M\",\"9\u6708\"),\n (0x32C9,\"M\",\"10\u6708\"),\n (0x32CA,\"M\",\"11\u6708\"),\n (0x32CB,\"M\",\"12\u6708\"),\n (0x32CC,\"M\",\"hg\"),\n (0x32CD,\"M\",\"erg\"),\n (0x32CE,\"M\",\"ev\"),\n (0x32CF,\"M\",\"ltd\"),\n (0x32D0,\"M\",\"\u30a2\"),\n (0x32D1,\"M\",\"\u30a4\"),\n (0x32D2,\"M\",\"\u30a6\"),\n (0x32D3,\"M\",\"\u30a8\"),\n (0x32D4,\"M\",\"\u30aa\"),\n (0x32D5,\"M\",\"\u30ab\"),\n (0x32D6,\"M\",\"\u30ad\"),\n (0x32D7,\"M\",\"\u30af\"),\n (0x32D8,\"M\",\"\u30b1\"),\n (0x32D9,\"M\",\"\u30b3\"),\n (0x32DA,\"M\",\"\u30b5\"),\n (0x32DB,\"M\",\"\u30b7\"),\n (0x32DC,\"M\",\"\u30b9\"),\n (0x32DD,\"M\",\"\u30bb\"),\n (0x32DE,\"M\",\"\u30bd\"),\n (0x32DF,\"M\",\"\u30bf\"),\n (0x32E0,\"M\",\"\u30c1\"),\n (0x32E1,\"M\",\"\u30c4\"),\n (0x32E2,\"M\",\"\u30c6\"),\n (0x32E3,\"M\",\"\u30c8\"),\n (0x32E4,\"M\",\"\u30ca\"),\n (0x32E5,\"M\",\"\u30cb\"),\n (0x32E6,\"M\",\"\u30cc\"),\n (0x32E7,\"M\",\"\u30cd\"),\n (0x32E8,\"M\",\"\u30ce\"),\n (0x32E9,\"M\",\"\u30cf\"),\n (0x32EA,\"M\",\"\u30d2\"),\n (0x32EB,\"M\",\"\u30d5\"),\n (0x32EC,\"M\",\"\u30d8\"),\n (0x32ED,\"M\",\"\u30db\"),\n (0x32EE,\"M\",\"\u30de\"),\n (0x32EF,\"M\",\"\u30df\"),\n (0x32F0,\"M\",\"\u30e0\"),\n (0x32F1,\"M\",\"\u30e1\"),\n (0x32F2,\"M\",\"\u30e2\"),\n (0x32F3,\"M\",\"\u30e4\"),\n (0x32F4,\"M\",\"\u30e6\"),\n (0x32F5,\"M\",\"\u30e8\"),\n (0x32F6,\"M\",\"\u30e9\"),\n (0x32F7,\"M\",\"\u30ea\"),\n (0x32F8,\"M\",\"\u30eb\"),\n (0x32F9,\"M\",\"\u30ec\"),\n (0x32FA,\"M\",\"\u30ed\"),\n (0x32FB,\"M\",\"\u30ef\"),\n (0x32FC,\"M\",\"\u30f0\"),\n (0x32FD,\"M\",\"\u30f1\"),\n (0x32FE,\"M\",\"\u30f2\"),\n (0x32FF,\"M\",\"\u4ee4\u548c\"),\n (0x3300,\"M\",\"\u30a2\u30d1\u30fc\u30c8\"),\n (0x3301,\"M\",\"\u30a2\u30eb\u30d5\u30a1\"),\n (0x3302,\"M\",\"\u30a2\u30f3\u30da\u30a2\"),\n (0x3303,\"M\",\"\u30a2\u30fc\u30eb\"),\n (0x3304,\"M\",\"\u30a4\u30cb\u30f3\u30b0\"),\n (0x3305,\"M\",\"\u30a4\u30f3\u30c1\"),\n (0x3306,\"M\",\"\u30a6\u30a9\u30f3\"),\n (0x3307,\"M\",\"\u30a8\u30b9\u30af\u30fc\u30c9\"),\n (0x3308,\"M\",\"\u30a8\u30fc\u30ab\u30fc\"),\n (0x3309,\"M\",\"\u30aa\u30f3\u30b9\"),\n (0x330A,\"M\",\"\u30aa\u30fc\u30e0\"),\n (0x330B,\"M\",\"\u30ab\u30a4\u30ea\"),\n (0x330C,\"M\",\"\u30ab\u30e9\u30c3\u30c8\"),\n (0x330D,\"M\",\"\u30ab\u30ed\u30ea\u30fc\"),\n (0x330E,\"M\",\"\u30ac\u30ed\u30f3\"),\n (0x330F,\"M\",\"\u30ac\u30f3\u30de\"),\n (0x3310,\"M\",\"\u30ae\u30ac\"),\n (0x3311,\"M\",\"\u30ae\u30cb\u30fc\"),\n (0x3312,\"M\",\"\u30ad\u30e5\u30ea\u30fc\"),\n (0x3313,\"M\",\"\u30ae\u30eb\u30c0\u30fc\"),\n (0x3314,\"M\",\"\u30ad\u30ed\"),\n (0x3315,\"M\",\"\u30ad\u30ed\u30b0\u30e9\u30e0\"),\n (0x3316,\"M\",\"\u30ad\u30ed\u30e1\u30fc\u30c8\u30eb\"),\n (0x3317,\"M\",\"\u30ad\u30ed\u30ef\u30c3\u30c8\"),\n (0x3318,\"M\",\"\u30b0\u30e9\u30e0\"),\n (0x3319,\"M\",\"\u30b0\u30e9\u30e0\u30c8\u30f3\"),\n ]\n \n \ndef _seg_33()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x331A,\"M\",\"\u30af\u30eb\u30bc\u30a4\u30ed\"),\n (0x331B,\"M\",\"\u30af\u30ed\u30fc\u30cd\"),\n (0x331C,\"M\",\"\u30b1\u30fc\u30b9\"),\n (0x331D,\"M\",\"\u30b3\u30eb\u30ca\"),\n (0x331E,\"M\",\"\u30b3\u30fc\u30dd\"),\n (0x331F,\"M\",\"\u30b5\u30a4\u30af\u30eb\"),\n (0x3320,\"M\",\"\u30b5\u30f3\u30c1\u30fc\u30e0\"),\n (0x3321,\"M\",\"\u30b7\u30ea\u30f3\u30b0\"),\n (0x3322,\"M\",\"\u30bb\u30f3\u30c1\"),\n (0x3323,\"M\",\"\u30bb\u30f3\u30c8\"),\n (0x3324,\"M\",\"\u30c0\u30fc\u30b9\"),\n (0x3325,\"M\",\"\u30c7\u30b7\"),\n (0x3326,\"M\",\"\u30c9\u30eb\"),\n (0x3327,\"M\",\"\u30c8\u30f3\"),\n (0x3328,\"M\",\"\u30ca\u30ce\"),\n (0x3329,\"M\",\"\u30ce\u30c3\u30c8\"),\n (0x332A,\"M\",\"\u30cf\u30a4\u30c4\"),\n (0x332B,\"M\",\"\u30d1\u30fc\u30bb\u30f3\u30c8\"),\n (0x332C,\"M\",\"\u30d1\u30fc\u30c4\"),\n (0x332D,\"M\",\"\u30d0\u30fc\u30ec\u30eb\"),\n (0x332E,\"M\",\"\u30d4\u30a2\u30b9\u30c8\u30eb\"),\n (0x332F,\"M\",\"\u30d4\u30af\u30eb\"),\n (0x3330,\"M\",\"\u30d4\u30b3\"),\n (0x3331,\"M\",\"\u30d3\u30eb\"),\n (0x3332,\"M\",\"\u30d5\u30a1\u30e9\u30c3\u30c9\"),\n (0x3333,\"M\",\"\u30d5\u30a3\u30fc\u30c8\"),\n (0x3334,\"M\",\"\u30d6\u30c3\u30b7\u30a7\u30eb\"),\n (0x3335,\"M\",\"\u30d5\u30e9\u30f3\"),\n (0x3336,\"M\",\"\u30d8\u30af\u30bf\u30fc\u30eb\"),\n (0x3337,\"M\",\"\u30da\u30bd\"),\n (0x3338,\"M\",\"\u30da\u30cb\u30d2\"),\n (0x3339,\"M\",\"\u30d8\u30eb\u30c4\"),\n (0x333A,\"M\",\"\u30da\u30f3\u30b9\"),\n (0x333B,\"M\",\"\u30da\u30fc\u30b8\"),\n (0x333C,\"M\",\"\u30d9\u30fc\u30bf\"),\n (0x333D,\"M\",\"\u30dd\u30a4\u30f3\u30c8\"),\n (0x333E,\"M\",\"\u30dc\u30eb\u30c8\"),\n (0x333F,\"M\",\"\u30db\u30f3\"),\n (0x3340,\"M\",\"\u30dd\u30f3\u30c9\"),\n (0x3341,\"M\",\"\u30db\u30fc\u30eb\"),\n (0x3342,\"M\",\"\u30db\u30fc\u30f3\"),\n (0x3343,\"M\",\"\u30de\u30a4\u30af\u30ed\"),\n (0x3344,\"M\",\"\u30de\u30a4\u30eb\"),\n (0x3345,\"M\",\"\u30de\u30c3\u30cf\"),\n (0x3346,\"M\",\"\u30de\u30eb\u30af\"),\n (0x3347,\"M\",\"\u30de\u30f3\u30b7\u30e7\u30f3\"),\n (0x3348,\"M\",\"\u30df\u30af\u30ed\u30f3\"),\n (0x3349,\"M\",\"\u30df\u30ea\"),\n (0x334A,\"M\",\"\u30df\u30ea\u30d0\u30fc\u30eb\"),\n (0x334B,\"M\",\"\u30e1\u30ac\"),\n (0x334C,\"M\",\"\u30e1\u30ac\u30c8\u30f3\"),\n (0x334D,\"M\",\"\u30e1\u30fc\u30c8\u30eb\"),\n (0x334E,\"M\",\"\u30e4\u30fc\u30c9\"),\n (0x334F,\"M\",\"\u30e4\u30fc\u30eb\"),\n (0x3350,\"M\",\"\u30e6\u30a2\u30f3\"),\n (0x3351,\"M\",\"\u30ea\u30c3\u30c8\u30eb\"),\n (0x3352,\"M\",\"\u30ea\u30e9\"),\n (0x3353,\"M\",\"\u30eb\u30d4\u30fc\"),\n (0x3354,\"M\",\"\u30eb\u30fc\u30d6\u30eb\"),\n (0x3355,\"M\",\"\u30ec\u30e0\"),\n (0x3356,\"M\",\"\u30ec\u30f3\u30c8\u30b2\u30f3\"),\n (0x3357,\"M\",\"\u30ef\u30c3\u30c8\"),\n (0x3358,\"M\",\"0\u70b9\"),\n (0x3359,\"M\",\"1\u70b9\"),\n (0x335A,\"M\",\"2\u70b9\"),\n (0x335B,\"M\",\"3\u70b9\"),\n (0x335C,\"M\",\"4\u70b9\"),\n (0x335D,\"M\",\"5\u70b9\"),\n (0x335E,\"M\",\"6\u70b9\"),\n (0x335F,\"M\",\"7\u70b9\"),\n (0x3360,\"M\",\"8\u70b9\"),\n (0x3361,\"M\",\"9\u70b9\"),\n (0x3362,\"M\",\"10\u70b9\"),\n (0x3363,\"M\",\"11\u70b9\"),\n (0x3364,\"M\",\"12\u70b9\"),\n (0x3365,\"M\",\"13\u70b9\"),\n (0x3366,\"M\",\"14\u70b9\"),\n (0x3367,\"M\",\"15\u70b9\"),\n (0x3368,\"M\",\"16\u70b9\"),\n (0x3369,\"M\",\"17\u70b9\"),\n (0x336A,\"M\",\"18\u70b9\"),\n (0x336B,\"M\",\"19\u70b9\"),\n (0x336C,\"M\",\"20\u70b9\"),\n (0x336D,\"M\",\"21\u70b9\"),\n (0x336E,\"M\",\"22\u70b9\"),\n (0x336F,\"M\",\"23\u70b9\"),\n (0x3370,\"M\",\"24\u70b9\"),\n (0x3371,\"M\",\"hpa\"),\n (0x3372,\"M\",\"da\"),\n (0x3373,\"M\",\"au\"),\n (0x3374,\"M\",\"bar\"),\n (0x3375,\"M\",\"ov\"),\n (0x3376,\"M\",\"pc\"),\n (0x3377,\"M\",\"dm\"),\n (0x3378,\"M\",\"dm2\"),\n (0x3379,\"M\",\"dm3\"),\n (0x337A,\"M\",\"iu\"),\n (0x337B,\"M\",\"\u5e73\u6210\"),\n (0x337C,\"M\",\"\u662d\u548c\"),\n (0x337D,\"M\",\"\u5927\u6b63\"),\n ]\n \n \ndef _seg_34()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x337E,\"M\",\"\u660e\u6cbb\"),\n (0x337F,\"M\",\"\u682a\u5f0f\u4f1a\u793e\"),\n (0x3380,\"M\",\"pa\"),\n (0x3381,\"M\",\"na\"),\n (0x3382,\"M\",\"\u03bca\"),\n (0x3383,\"M\",\"ma\"),\n (0x3384,\"M\",\"ka\"),\n (0x3385,\"M\",\"kb\"),\n (0x3386,\"M\",\"mb\"),\n (0x3387,\"M\",\"gb\"),\n (0x3388,\"M\",\"cal\"),\n (0x3389,\"M\",\"kcal\"),\n (0x338A,\"M\",\"pf\"),\n (0x338B,\"M\",\"nf\"),\n (0x338C,\"M\",\"\u03bcf\"),\n (0x338D,\"M\",\"\u03bcg\"),\n (0x338E,\"M\",\"mg\"),\n (0x338F,\"M\",\"kg\"),\n (0x3390,\"M\",\"hz\"),\n (0x3391,\"M\",\"khz\"),\n (0x3392,\"M\",\"mhz\"),\n (0x3393,\"M\",\"ghz\"),\n (0x3394,\"M\",\"thz\"),\n (0x3395,\"M\",\"\u03bcl\"),\n (0x3396,\"M\",\"ml\"),\n (0x3397,\"M\",\"dl\"),\n (0x3398,\"M\",\"kl\"),\n (0x3399,\"M\",\"fm\"),\n (0x339A,\"M\",\"nm\"),\n (0x339B,\"M\",\"\u03bcm\"),\n (0x339C,\"M\",\"mm\"),\n (0x339D,\"M\",\"cm\"),\n (0x339E,\"M\",\"km\"),\n (0x339F,\"M\",\"mm2\"),\n (0x33A0,\"M\",\"cm2\"),\n (0x33A1,\"M\",\"m2\"),\n (0x33A2,\"M\",\"km2\"),\n (0x33A3,\"M\",\"mm3\"),\n (0x33A4,\"M\",\"cm3\"),\n (0x33A5,\"M\",\"m3\"),\n (0x33A6,\"M\",\"km3\"),\n (0x33A7,\"M\",\"m\u2215s\"),\n (0x33A8,\"M\",\"m\u2215s2\"),\n (0x33A9,\"M\",\"pa\"),\n (0x33AA,\"M\",\"kpa\"),\n (0x33AB,\"M\",\"mpa\"),\n (0x33AC,\"M\",\"gpa\"),\n (0x33AD,\"M\",\"rad\"),\n (0x33AE,\"M\",\"rad\u2215s\"),\n (0x33AF,\"M\",\"rad\u2215s2\"),\n (0x33B0,\"M\",\"ps\"),\n (0x33B1,\"M\",\"ns\"),\n (0x33B2,\"M\",\"\u03bcs\"),\n (0x33B3,\"M\",\"ms\"),\n (0x33B4,\"M\",\"pv\"),\n (0x33B5,\"M\",\"nv\"),\n (0x33B6,\"M\",\"\u03bcv\"),\n (0x33B7,\"M\",\"mv\"),\n (0x33B8,\"M\",\"kv\"),\n (0x33B9,\"M\",\"mv\"),\n (0x33BA,\"M\",\"pw\"),\n (0x33BB,\"M\",\"nw\"),\n (0x33BC,\"M\",\"\u03bcw\"),\n (0x33BD,\"M\",\"mw\"),\n (0x33BE,\"M\",\"kw\"),\n (0x33BF,\"M\",\"mw\"),\n (0x33C0,\"M\",\"k\u03c9\"),\n (0x33C1,\"M\",\"m\u03c9\"),\n (0x33C2,\"X\"),\n (0x33C3,\"M\",\"bq\"),\n (0x33C4,\"M\",\"cc\"),\n (0x33C5,\"M\",\"cd\"),\n (0x33C6,\"M\",\"c\u2215kg\"),\n (0x33C7,\"X\"),\n (0x33C8,\"M\",\"db\"),\n (0x33C9,\"M\",\"gy\"),\n (0x33CA,\"M\",\"ha\"),\n (0x33CB,\"M\",\"hp\"),\n (0x33CC,\"M\",\"in\"),\n (0x33CD,\"M\",\"kk\"),\n (0x33CE,\"M\",\"km\"),\n (0x33CF,\"M\",\"kt\"),\n (0x33D0,\"M\",\"lm\"),\n (0x33D1,\"M\",\"ln\"),\n (0x33D2,\"M\",\"log\"),\n (0x33D3,\"M\",\"lx\"),\n (0x33D4,\"M\",\"mb\"),\n (0x33D5,\"M\",\"mil\"),\n (0x33D6,\"M\",\"mol\"),\n (0x33D7,\"M\",\"ph\"),\n (0x33D8,\"X\"),\n (0x33D9,\"M\",\"ppm\"),\n (0x33DA,\"M\",\"pr\"),\n (0x33DB,\"M\",\"sr\"),\n (0x33DC,\"M\",\"sv\"),\n (0x33DD,\"M\",\"wb\"),\n (0x33DE,\"M\",\"v\u2215m\"),\n (0x33DF,\"M\",\"a\u2215m\"),\n (0x33E0,\"M\",\"1\u65e5\"),\n (0x33E1,\"M\",\"2\u65e5\"),\n ]\n \n \ndef _seg_35()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x33E2,\"M\",\"3\u65e5\"),\n (0x33E3,\"M\",\"4\u65e5\"),\n (0x33E4,\"M\",\"5\u65e5\"),\n (0x33E5,\"M\",\"6\u65e5\"),\n (0x33E6,\"M\",\"7\u65e5\"),\n (0x33E7,\"M\",\"8\u65e5\"),\n (0x33E8,\"M\",\"9\u65e5\"),\n (0x33E9,\"M\",\"10\u65e5\"),\n (0x33EA,\"M\",\"11\u65e5\"),\n (0x33EB,\"M\",\"12\u65e5\"),\n (0x33EC,\"M\",\"13\u65e5\"),\n (0x33ED,\"M\",\"14\u65e5\"),\n (0x33EE,\"M\",\"15\u65e5\"),\n (0x33EF,\"M\",\"16\u65e5\"),\n (0x33F0,\"M\",\"17\u65e5\"),\n (0x33F1,\"M\",\"18\u65e5\"),\n (0x33F2,\"M\",\"19\u65e5\"),\n (0x33F3,\"M\",\"20\u65e5\"),\n (0x33F4,\"M\",\"21\u65e5\"),\n (0x33F5,\"M\",\"22\u65e5\"),\n (0x33F6,\"M\",\"23\u65e5\"),\n (0x33F7,\"M\",\"24\u65e5\"),\n (0x33F8,\"M\",\"25\u65e5\"),\n (0x33F9,\"M\",\"26\u65e5\"),\n (0x33FA,\"M\",\"27\u65e5\"),\n (0x33FB,\"M\",\"28\u65e5\"),\n (0x33FC,\"M\",\"29\u65e5\"),\n (0x33FD,\"M\",\"30\u65e5\"),\n (0x33FE,\"M\",\"31\u65e5\"),\n (0x33FF,\"M\",\"gal\"),\n (0x3400,\"V\"),\n (0xA48D,\"X\"),\n (0xA490,\"V\"),\n (0xA4C7,\"X\"),\n (0xA4D0,\"V\"),\n (0xA62C,\"X\"),\n (0xA640,\"M\",\"\ua641\"),\n (0xA641,\"V\"),\n (0xA642,\"M\",\"\ua643\"),\n (0xA643,\"V\"),\n (0xA644,\"M\",\"\ua645\"),\n (0xA645,\"V\"),\n (0xA646,\"M\",\"\ua647\"),\n (0xA647,\"V\"),\n (0xA648,\"M\",\"\ua649\"),\n (0xA649,\"V\"),\n (0xA64A,\"M\",\"\ua64b\"),\n (0xA64B,\"V\"),\n (0xA64C,\"M\",\"\ua64d\"),\n (0xA64D,\"V\"),\n (0xA64E,\"M\",\"\ua64f\"),\n (0xA64F,\"V\"),\n (0xA650,\"M\",\"\ua651\"),\n (0xA651,\"V\"),\n (0xA652,\"M\",\"\ua653\"),\n (0xA653,\"V\"),\n (0xA654,\"M\",\"\ua655\"),\n (0xA655,\"V\"),\n (0xA656,\"M\",\"\ua657\"),\n (0xA657,\"V\"),\n (0xA658,\"M\",\"\ua659\"),\n (0xA659,\"V\"),\n (0xA65A,\"M\",\"\ua65b\"),\n (0xA65B,\"V\"),\n (0xA65C,\"M\",\"\ua65d\"),\n (0xA65D,\"V\"),\n (0xA65E,\"M\",\"\ua65f\"),\n (0xA65F,\"V\"),\n (0xA660,\"M\",\"\ua661\"),\n (0xA661,\"V\"),\n (0xA662,\"M\",\"\ua663\"),\n (0xA663,\"V\"),\n (0xA664,\"M\",\"\ua665\"),\n (0xA665,\"V\"),\n (0xA666,\"M\",\"\ua667\"),\n (0xA667,\"V\"),\n (0xA668,\"M\",\"\ua669\"),\n (0xA669,\"V\"),\n (0xA66A,\"M\",\"\ua66b\"),\n (0xA66B,\"V\"),\n (0xA66C,\"M\",\"\ua66d\"),\n (0xA66D,\"V\"),\n (0xA680,\"M\",\"\ua681\"),\n (0xA681,\"V\"),\n (0xA682,\"M\",\"\ua683\"),\n (0xA683,\"V\"),\n (0xA684,\"M\",\"\ua685\"),\n (0xA685,\"V\"),\n (0xA686,\"M\",\"\ua687\"),\n (0xA687,\"V\"),\n (0xA688,\"M\",\"\ua689\"),\n (0xA689,\"V\"),\n (0xA68A,\"M\",\"\ua68b\"),\n (0xA68B,\"V\"),\n (0xA68C,\"M\",\"\ua68d\"),\n (0xA68D,\"V\"),\n (0xA68E,\"M\",\"\ua68f\"),\n (0xA68F,\"V\"),\n (0xA690,\"M\",\"\ua691\"),\n (0xA691,\"V\"),\n ]\n \n \ndef _seg_36()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xA692,\"M\",\"\ua693\"),\n (0xA693,\"V\"),\n (0xA694,\"M\",\"\ua695\"),\n (0xA695,\"V\"),\n (0xA696,\"M\",\"\ua697\"),\n (0xA697,\"V\"),\n (0xA698,\"M\",\"\ua699\"),\n (0xA699,\"V\"),\n (0xA69A,\"M\",\"\ua69b\"),\n (0xA69B,\"V\"),\n (0xA69C,\"M\",\"\u044a\"),\n (0xA69D,\"M\",\"\u044c\"),\n (0xA69E,\"V\"),\n (0xA6F8,\"X\"),\n (0xA700,\"V\"),\n (0xA722,\"M\",\"\ua723\"),\n (0xA723,\"V\"),\n (0xA724,\"M\",\"\ua725\"),\n (0xA725,\"V\"),\n (0xA726,\"M\",\"\ua727\"),\n (0xA727,\"V\"),\n (0xA728,\"M\",\"\ua729\"),\n (0xA729,\"V\"),\n (0xA72A,\"M\",\"\ua72b\"),\n (0xA72B,\"V\"),\n (0xA72C,\"M\",\"\ua72d\"),\n (0xA72D,\"V\"),\n (0xA72E,\"M\",\"\ua72f\"),\n (0xA72F,\"V\"),\n (0xA732,\"M\",\"\ua733\"),\n (0xA733,\"V\"),\n (0xA734,\"M\",\"\ua735\"),\n (0xA735,\"V\"),\n (0xA736,\"M\",\"\ua737\"),\n (0xA737,\"V\"),\n (0xA738,\"M\",\"\ua739\"),\n (0xA739,\"V\"),\n (0xA73A,\"M\",\"\ua73b\"),\n (0xA73B,\"V\"),\n (0xA73C,\"M\",\"\ua73d\"),\n (0xA73D,\"V\"),\n (0xA73E,\"M\",\"\ua73f\"),\n (0xA73F,\"V\"),\n (0xA740,\"M\",\"\ua741\"),\n (0xA741,\"V\"),\n (0xA742,\"M\",\"\ua743\"),\n (0xA743,\"V\"),\n (0xA744,\"M\",\"\ua745\"),\n (0xA745,\"V\"),\n (0xA746,\"M\",\"\ua747\"),\n (0xA747,\"V\"),\n (0xA748,\"M\",\"\ua749\"),\n (0xA749,\"V\"),\n (0xA74A,\"M\",\"\ua74b\"),\n (0xA74B,\"V\"),\n (0xA74C,\"M\",\"\ua74d\"),\n (0xA74D,\"V\"),\n (0xA74E,\"M\",\"\ua74f\"),\n (0xA74F,\"V\"),\n (0xA750,\"M\",\"\ua751\"),\n (0xA751,\"V\"),\n (0xA752,\"M\",\"\ua753\"),\n (0xA753,\"V\"),\n (0xA754,\"M\",\"\ua755\"),\n (0xA755,\"V\"),\n (0xA756,\"M\",\"\ua757\"),\n (0xA757,\"V\"),\n (0xA758,\"M\",\"\ua759\"),\n (0xA759,\"V\"),\n (0xA75A,\"M\",\"\ua75b\"),\n (0xA75B,\"V\"),\n (0xA75C,\"M\",\"\ua75d\"),\n (0xA75D,\"V\"),\n (0xA75E,\"M\",\"\ua75f\"),\n (0xA75F,\"V\"),\n (0xA760,\"M\",\"\ua761\"),\n (0xA761,\"V\"),\n (0xA762,\"M\",\"\ua763\"),\n (0xA763,\"V\"),\n (0xA764,\"M\",\"\ua765\"),\n (0xA765,\"V\"),\n (0xA766,\"M\",\"\ua767\"),\n (0xA767,\"V\"),\n (0xA768,\"M\",\"\ua769\"),\n (0xA769,\"V\"),\n (0xA76A,\"M\",\"\ua76b\"),\n (0xA76B,\"V\"),\n (0xA76C,\"M\",\"\ua76d\"),\n (0xA76D,\"V\"),\n (0xA76E,\"M\",\"\ua76f\"),\n (0xA76F,\"V\"),\n (0xA770,\"M\",\"\ua76f\"),\n (0xA771,\"V\"),\n (0xA779,\"M\",\"\ua77a\"),\n (0xA77A,\"V\"),\n (0xA77B,\"M\",\"\ua77c\"),\n (0xA77C,\"V\"),\n (0xA77D,\"M\",\"\u1d79\"),\n (0xA77E,\"M\",\"\ua77f\"),\n (0xA77F,\"V\"),\n ]\n \n \ndef _seg_37()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xA780,\"M\",\"\ua781\"),\n (0xA781,\"V\"),\n (0xA782,\"M\",\"\ua783\"),\n (0xA783,\"V\"),\n (0xA784,\"M\",\"\ua785\"),\n (0xA785,\"V\"),\n (0xA786,\"M\",\"\ua787\"),\n (0xA787,\"V\"),\n (0xA78B,\"M\",\"\ua78c\"),\n (0xA78C,\"V\"),\n (0xA78D,\"M\",\"\u0265\"),\n (0xA78E,\"V\"),\n (0xA790,\"M\",\"\ua791\"),\n (0xA791,\"V\"),\n (0xA792,\"M\",\"\ua793\"),\n (0xA793,\"V\"),\n (0xA796,\"M\",\"\ua797\"),\n (0xA797,\"V\"),\n (0xA798,\"M\",\"\ua799\"),\n (0xA799,\"V\"),\n (0xA79A,\"M\",\"\ua79b\"),\n (0xA79B,\"V\"),\n (0xA79C,\"M\",\"\ua79d\"),\n (0xA79D,\"V\"),\n (0xA79E,\"M\",\"\ua79f\"),\n (0xA79F,\"V\"),\n (0xA7A0,\"M\",\"\ua7a1\"),\n (0xA7A1,\"V\"),\n (0xA7A2,\"M\",\"\ua7a3\"),\n (0xA7A3,\"V\"),\n (0xA7A4,\"M\",\"\ua7a5\"),\n (0xA7A5,\"V\"),\n (0xA7A6,\"M\",\"\ua7a7\"),\n (0xA7A7,\"V\"),\n (0xA7A8,\"M\",\"\ua7a9\"),\n (0xA7A9,\"V\"),\n (0xA7AA,\"M\",\"\u0266\"),\n (0xA7AB,\"M\",\"\u025c\"),\n (0xA7AC,\"M\",\"\u0261\"),\n (0xA7AD,\"M\",\"\u026c\"),\n (0xA7AE,\"M\",\"\u026a\"),\n (0xA7AF,\"V\"),\n (0xA7B0,\"M\",\"\u029e\"),\n (0xA7B1,\"M\",\"\u0287\"),\n (0xA7B2,\"M\",\"\u029d\"),\n (0xA7B3,\"M\",\"\uab53\"),\n (0xA7B4,\"M\",\"\ua7b5\"),\n (0xA7B5,\"V\"),\n (0xA7B6,\"M\",\"\ua7b7\"),\n (0xA7B7,\"V\"),\n (0xA7B8,\"M\",\"\ua7b9\"),\n (0xA7B9,\"V\"),\n (0xA7BA,\"M\",\"\ua7bb\"),\n (0xA7BB,\"V\"),\n (0xA7BC,\"M\",\"\ua7bd\"),\n (0xA7BD,\"V\"),\n (0xA7BE,\"M\",\"\ua7bf\"),\n (0xA7BF,\"V\"),\n (0xA7C0,\"M\",\"\ua7c1\"),\n (0xA7C1,\"V\"),\n (0xA7C2,\"M\",\"\ua7c3\"),\n (0xA7C3,\"V\"),\n (0xA7C4,\"M\",\"\ua794\"),\n (0xA7C5,\"M\",\"\u0282\"),\n (0xA7C6,\"M\",\"\u1d8e\"),\n (0xA7C7,\"M\",\"\ua7c8\"),\n (0xA7C8,\"V\"),\n (0xA7C9,\"M\",\"\ua7ca\"),\n (0xA7CA,\"V\"),\n (0xA7CB,\"X\"),\n (0xA7D0,\"M\",\"\ua7d1\"),\n (0xA7D1,\"V\"),\n (0xA7D2,\"X\"),\n (0xA7D3,\"V\"),\n (0xA7D4,\"X\"),\n (0xA7D5,\"V\"),\n (0xA7D6,\"M\",\"\ua7d7\"),\n (0xA7D7,\"V\"),\n (0xA7D8,\"M\",\"\ua7d9\"),\n (0xA7D9,\"V\"),\n (0xA7DA,\"X\"),\n (0xA7F2,\"M\",\"c\"),\n (0xA7F3,\"M\",\"f\"),\n (0xA7F4,\"M\",\"q\"),\n (0xA7F5,\"M\",\"\ua7f6\"),\n (0xA7F6,\"V\"),\n (0xA7F8,\"M\",\"\u0127\"),\n (0xA7F9,\"M\",\"\u0153\"),\n (0xA7FA,\"V\"),\n (0xA82D,\"X\"),\n (0xA830,\"V\"),\n (0xA83A,\"X\"),\n (0xA840,\"V\"),\n (0xA878,\"X\"),\n (0xA880,\"V\"),\n (0xA8C6,\"X\"),\n (0xA8CE,\"V\"),\n (0xA8DA,\"X\"),\n (0xA8E0,\"V\"),\n (0xA954,\"X\"),\n ]\n \n \ndef _seg_38()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xA95F,\"V\"),\n (0xA97D,\"X\"),\n (0xA980,\"V\"),\n (0xA9CE,\"X\"),\n (0xA9CF,\"V\"),\n (0xA9DA,\"X\"),\n (0xA9DE,\"V\"),\n (0xA9FF,\"X\"),\n (0xAA00,\"V\"),\n (0xAA37,\"X\"),\n (0xAA40,\"V\"),\n (0xAA4E,\"X\"),\n (0xAA50,\"V\"),\n (0xAA5A,\"X\"),\n (0xAA5C,\"V\"),\n (0xAAC3,\"X\"),\n (0xAADB,\"V\"),\n (0xAAF7,\"X\"),\n (0xAB01,\"V\"),\n (0xAB07,\"X\"),\n (0xAB09,\"V\"),\n (0xAB0F,\"X\"),\n (0xAB11,\"V\"),\n (0xAB17,\"X\"),\n (0xAB20,\"V\"),\n (0xAB27,\"X\"),\n (0xAB28,\"V\"),\n (0xAB2F,\"X\"),\n (0xAB30,\"V\"),\n (0xAB5C,\"M\",\"\ua727\"),\n (0xAB5D,\"M\",\"\uab37\"),\n (0xAB5E,\"M\",\"\u026b\"),\n (0xAB5F,\"M\",\"\uab52\"),\n (0xAB60,\"V\"),\n (0xAB69,\"M\",\"\u028d\"),\n (0xAB6A,\"V\"),\n (0xAB6C,\"X\"),\n (0xAB70,\"M\",\"\u13a0\"),\n (0xAB71,\"M\",\"\u13a1\"),\n (0xAB72,\"M\",\"\u13a2\"),\n (0xAB73,\"M\",\"\u13a3\"),\n (0xAB74,\"M\",\"\u13a4\"),\n (0xAB75,\"M\",\"\u13a5\"),\n (0xAB76,\"M\",\"\u13a6\"),\n (0xAB77,\"M\",\"\u13a7\"),\n (0xAB78,\"M\",\"\u13a8\"),\n (0xAB79,\"M\",\"\u13a9\"),\n (0xAB7A,\"M\",\"\u13aa\"),\n (0xAB7B,\"M\",\"\u13ab\"),\n (0xAB7C,\"M\",\"\u13ac\"),\n (0xAB7D,\"M\",\"\u13ad\"),\n (0xAB7E,\"M\",\"\u13ae\"),\n (0xAB7F,\"M\",\"\u13af\"),\n (0xAB80,\"M\",\"\u13b0\"),\n (0xAB81,\"M\",\"\u13b1\"),\n (0xAB82,\"M\",\"\u13b2\"),\n (0xAB83,\"M\",\"\u13b3\"),\n (0xAB84,\"M\",\"\u13b4\"),\n (0xAB85,\"M\",\"\u13b5\"),\n (0xAB86,\"M\",\"\u13b6\"),\n (0xAB87,\"M\",\"\u13b7\"),\n (0xAB88,\"M\",\"\u13b8\"),\n (0xAB89,\"M\",\"\u13b9\"),\n (0xAB8A,\"M\",\"\u13ba\"),\n (0xAB8B,\"M\",\"\u13bb\"),\n (0xAB8C,\"M\",\"\u13bc\"),\n (0xAB8D,\"M\",\"\u13bd\"),\n (0xAB8E,\"M\",\"\u13be\"),\n (0xAB8F,\"M\",\"\u13bf\"),\n (0xAB90,\"M\",\"\u13c0\"),\n (0xAB91,\"M\",\"\u13c1\"),\n (0xAB92,\"M\",\"\u13c2\"),\n (0xAB93,\"M\",\"\u13c3\"),\n (0xAB94,\"M\",\"\u13c4\"),\n (0xAB95,\"M\",\"\u13c5\"),\n (0xAB96,\"M\",\"\u13c6\"),\n (0xAB97,\"M\",\"\u13c7\"),\n (0xAB98,\"M\",\"\u13c8\"),\n (0xAB99,\"M\",\"\u13c9\"),\n (0xAB9A,\"M\",\"\u13ca\"),\n (0xAB9B,\"M\",\"\u13cb\"),\n (0xAB9C,\"M\",\"\u13cc\"),\n (0xAB9D,\"M\",\"\u13cd\"),\n (0xAB9E,\"M\",\"\u13ce\"),\n (0xAB9F,\"M\",\"\u13cf\"),\n (0xABA0,\"M\",\"\u13d0\"),\n (0xABA1,\"M\",\"\u13d1\"),\n (0xABA2,\"M\",\"\u13d2\"),\n (0xABA3,\"M\",\"\u13d3\"),\n (0xABA4,\"M\",\"\u13d4\"),\n (0xABA5,\"M\",\"\u13d5\"),\n (0xABA6,\"M\",\"\u13d6\"),\n (0xABA7,\"M\",\"\u13d7\"),\n (0xABA8,\"M\",\"\u13d8\"),\n (0xABA9,\"M\",\"\u13d9\"),\n (0xABAA,\"M\",\"\u13da\"),\n (0xABAB,\"M\",\"\u13db\"),\n (0xABAC,\"M\",\"\u13dc\"),\n (0xABAD,\"M\",\"\u13dd\"),\n (0xABAE,\"M\",\"\u13de\"),\n ]\n \n \ndef _seg_39()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xABAF,\"M\",\"\u13df\"),\n (0xABB0,\"M\",\"\u13e0\"),\n (0xABB1,\"M\",\"\u13e1\"),\n (0xABB2,\"M\",\"\u13e2\"),\n (0xABB3,\"M\",\"\u13e3\"),\n (0xABB4,\"M\",\"\u13e4\"),\n (0xABB5,\"M\",\"\u13e5\"),\n (0xABB6,\"M\",\"\u13e6\"),\n (0xABB7,\"M\",\"\u13e7\"),\n (0xABB8,\"M\",\"\u13e8\"),\n (0xABB9,\"M\",\"\u13e9\"),\n (0xABBA,\"M\",\"\u13ea\"),\n (0xABBB,\"M\",\"\u13eb\"),\n (0xABBC,\"M\",\"\u13ec\"),\n (0xABBD,\"M\",\"\u13ed\"),\n (0xABBE,\"M\",\"\u13ee\"),\n (0xABBF,\"M\",\"\u13ef\"),\n (0xABC0,\"V\"),\n (0xABEE,\"X\"),\n (0xABF0,\"V\"),\n (0xABFA,\"X\"),\n (0xAC00,\"V\"),\n (0xD7A4,\"X\"),\n (0xD7B0,\"V\"),\n (0xD7C7,\"X\"),\n (0xD7CB,\"V\"),\n (0xD7FC,\"X\"),\n (0xF900,\"M\",\"\u8c48\"),\n (0xF901,\"M\",\"\u66f4\"),\n (0xF902,\"M\",\"\u8eca\"),\n (0xF903,\"M\",\"\u8cc8\"),\n (0xF904,\"M\",\"\u6ed1\"),\n (0xF905,\"M\",\"\u4e32\"),\n (0xF906,\"M\",\"\u53e5\"),\n (0xF907,\"M\",\"\u9f9c\"),\n (0xF909,\"M\",\"\u5951\"),\n (0xF90A,\"M\",\"\u91d1\"),\n (0xF90B,\"M\",\"\u5587\"),\n (0xF90C,\"M\",\"\u5948\"),\n (0xF90D,\"M\",\"\u61f6\"),\n (0xF90E,\"M\",\"\u7669\"),\n (0xF90F,\"M\",\"\u7f85\"),\n (0xF910,\"M\",\"\u863f\"),\n (0xF911,\"M\",\"\u87ba\"),\n (0xF912,\"M\",\"\u88f8\"),\n (0xF913,\"M\",\"\u908f\"),\n (0xF914,\"M\",\"\u6a02\"),\n (0xF915,\"M\",\"\u6d1b\"),\n (0xF916,\"M\",\"\u70d9\"),\n (0xF917,\"M\",\"\u73de\"),\n (0xF918,\"M\",\"\u843d\"),\n (0xF919,\"M\",\"\u916a\"),\n (0xF91A,\"M\",\"\u99f1\"),\n (0xF91B,\"M\",\"\u4e82\"),\n (0xF91C,\"M\",\"\u5375\"),\n (0xF91D,\"M\",\"\u6b04\"),\n (0xF91E,\"M\",\"\u721b\"),\n (0xF91F,\"M\",\"\u862d\"),\n (0xF920,\"M\",\"\u9e1e\"),\n (0xF921,\"M\",\"\u5d50\"),\n (0xF922,\"M\",\"\u6feb\"),\n (0xF923,\"M\",\"\u85cd\"),\n (0xF924,\"M\",\"\u8964\"),\n (0xF925,\"M\",\"\u62c9\"),\n (0xF926,\"M\",\"\u81d8\"),\n (0xF927,\"M\",\"\u881f\"),\n (0xF928,\"M\",\"\u5eca\"),\n (0xF929,\"M\",\"\u6717\"),\n (0xF92A,\"M\",\"\u6d6a\"),\n (0xF92B,\"M\",\"\u72fc\"),\n (0xF92C,\"M\",\"\u90ce\"),\n (0xF92D,\"M\",\"\u4f86\"),\n (0xF92E,\"M\",\"\u51b7\"),\n (0xF92F,\"M\",\"\u52de\"),\n (0xF930,\"M\",\"\u64c4\"),\n (0xF931,\"M\",\"\u6ad3\"),\n (0xF932,\"M\",\"\u7210\"),\n (0xF933,\"M\",\"\u76e7\"),\n (0xF934,\"M\",\"\u8001\"),\n (0xF935,\"M\",\"\u8606\"),\n (0xF936,\"M\",\"\u865c\"),\n (0xF937,\"M\",\"\u8def\"),\n (0xF938,\"M\",\"\u9732\"),\n (0xF939,\"M\",\"\u9b6f\"),\n (0xF93A,\"M\",\"\u9dfa\"),\n (0xF93B,\"M\",\"\u788c\"),\n (0xF93C,\"M\",\"\u797f\"),\n (0xF93D,\"M\",\"\u7da0\"),\n (0xF93E,\"M\",\"\u83c9\"),\n (0xF93F,\"M\",\"\u9304\"),\n (0xF940,\"M\",\"\u9e7f\"),\n (0xF941,\"M\",\"\u8ad6\"),\n (0xF942,\"M\",\"\u58df\"),\n (0xF943,\"M\",\"\u5f04\"),\n (0xF944,\"M\",\"\u7c60\"),\n (0xF945,\"M\",\"\u807e\"),\n (0xF946,\"M\",\"\u7262\"),\n (0xF947,\"M\",\"\u78ca\"),\n (0xF948,\"M\",\"\u8cc2\"),\n (0xF949,\"M\",\"\u96f7\"),\n ]\n \n \ndef _seg_40()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xF94A,\"M\",\"\u58d8\"),\n (0xF94B,\"M\",\"\u5c62\"),\n (0xF94C,\"M\",\"\u6a13\"),\n (0xF94D,\"M\",\"\u6dda\"),\n (0xF94E,\"M\",\"\u6f0f\"),\n (0xF94F,\"M\",\"\u7d2f\"),\n (0xF950,\"M\",\"\u7e37\"),\n (0xF951,\"M\",\"\u964b\"),\n (0xF952,\"M\",\"\u52d2\"),\n (0xF953,\"M\",\"\u808b\"),\n (0xF954,\"M\",\"\u51dc\"),\n (0xF955,\"M\",\"\u51cc\"),\n (0xF956,\"M\",\"\u7a1c\"),\n (0xF957,\"M\",\"\u7dbe\"),\n (0xF958,\"M\",\"\u83f1\"),\n (0xF959,\"M\",\"\u9675\"),\n (0xF95A,\"M\",\"\u8b80\"),\n (0xF95B,\"M\",\"\u62cf\"),\n (0xF95C,\"M\",\"\u6a02\"),\n (0xF95D,\"M\",\"\u8afe\"),\n (0xF95E,\"M\",\"\u4e39\"),\n (0xF95F,\"M\",\"\u5be7\"),\n (0xF960,\"M\",\"\u6012\"),\n (0xF961,\"M\",\"\u7387\"),\n (0xF962,\"M\",\"\u7570\"),\n (0xF963,\"M\",\"\u5317\"),\n (0xF964,\"M\",\"\u78fb\"),\n (0xF965,\"M\",\"\u4fbf\"),\n (0xF966,\"M\",\"\u5fa9\"),\n (0xF967,\"M\",\"\u4e0d\"),\n (0xF968,\"M\",\"\u6ccc\"),\n (0xF969,\"M\",\"\u6578\"),\n (0xF96A,\"M\",\"\u7d22\"),\n (0xF96B,\"M\",\"\u53c3\"),\n (0xF96C,\"M\",\"\u585e\"),\n (0xF96D,\"M\",\"\u7701\"),\n (0xF96E,\"M\",\"\u8449\"),\n (0xF96F,\"M\",\"\u8aaa\"),\n (0xF970,\"M\",\"\u6bba\"),\n (0xF971,\"M\",\"\u8fb0\"),\n (0xF972,\"M\",\"\u6c88\"),\n (0xF973,\"M\",\"\u62fe\"),\n (0xF974,\"M\",\"\u82e5\"),\n (0xF975,\"M\",\"\u63a0\"),\n (0xF976,\"M\",\"\u7565\"),\n (0xF977,\"M\",\"\u4eae\"),\n (0xF978,\"M\",\"\u5169\"),\n (0xF979,\"M\",\"\u51c9\"),\n (0xF97A,\"M\",\"\u6881\"),\n (0xF97B,\"M\",\"\u7ce7\"),\n (0xF97C,\"M\",\"\u826f\"),\n (0xF97D,\"M\",\"\u8ad2\"),\n (0xF97E,\"M\",\"\u91cf\"),\n (0xF97F,\"M\",\"\u52f5\"),\n (0xF980,\"M\",\"\u5442\"),\n (0xF981,\"M\",\"\u5973\"),\n (0xF982,\"M\",\"\u5eec\"),\n (0xF983,\"M\",\"\u65c5\"),\n (0xF984,\"M\",\"\u6ffe\"),\n (0xF985,\"M\",\"\u792a\"),\n (0xF986,\"M\",\"\u95ad\"),\n (0xF987,\"M\",\"\u9a6a\"),\n (0xF988,\"M\",\"\u9e97\"),\n (0xF989,\"M\",\"\u9ece\"),\n (0xF98A,\"M\",\"\u529b\"),\n (0xF98B,\"M\",\"\u66c6\"),\n (0xF98C,\"M\",\"\u6b77\"),\n (0xF98D,\"M\",\"\u8f62\"),\n (0xF98E,\"M\",\"\u5e74\"),\n (0xF98F,\"M\",\"\u6190\"),\n (0xF990,\"M\",\"\u6200\"),\n (0xF991,\"M\",\"\u649a\"),\n (0xF992,\"M\",\"\u6f23\"),\n (0xF993,\"M\",\"\u7149\"),\n (0xF994,\"M\",\"\u7489\"),\n (0xF995,\"M\",\"\u79ca\"),\n (0xF996,\"M\",\"\u7df4\"),\n (0xF997,\"M\",\"\u806f\"),\n (0xF998,\"M\",\"\u8f26\"),\n (0xF999,\"M\",\"\u84ee\"),\n (0xF99A,\"M\",\"\u9023\"),\n (0xF99B,\"M\",\"\u934a\"),\n (0xF99C,\"M\",\"\u5217\"),\n (0xF99D,\"M\",\"\u52a3\"),\n (0xF99E,\"M\",\"\u54bd\"),\n (0xF99F,\"M\",\"\u70c8\"),\n (0xF9A0,\"M\",\"\u88c2\"),\n (0xF9A1,\"M\",\"\u8aaa\"),\n (0xF9A2,\"M\",\"\u5ec9\"),\n (0xF9A3,\"M\",\"\u5ff5\"),\n (0xF9A4,\"M\",\"\u637b\"),\n (0xF9A5,\"M\",\"\u6bae\"),\n (0xF9A6,\"M\",\"\u7c3e\"),\n (0xF9A7,\"M\",\"\u7375\"),\n (0xF9A8,\"M\",\"\u4ee4\"),\n (0xF9A9,\"M\",\"\u56f9\"),\n (0xF9AA,\"M\",\"\u5be7\"),\n (0xF9AB,\"M\",\"\u5dba\"),\n (0xF9AC,\"M\",\"\u601c\"),\n (0xF9AD,\"M\",\"\u73b2\"),\n ]\n \n \ndef _seg_41()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xF9AE,\"M\",\"\u7469\"),\n (0xF9AF,\"M\",\"\u7f9a\"),\n (0xF9B0,\"M\",\"\u8046\"),\n (0xF9B1,\"M\",\"\u9234\"),\n (0xF9B2,\"M\",\"\u96f6\"),\n (0xF9B3,\"M\",\"\u9748\"),\n (0xF9B4,\"M\",\"\u9818\"),\n (0xF9B5,\"M\",\"\u4f8b\"),\n (0xF9B6,\"M\",\"\u79ae\"),\n (0xF9B7,\"M\",\"\u91b4\"),\n (0xF9B8,\"M\",\"\u96b8\"),\n (0xF9B9,\"M\",\"\u60e1\"),\n (0xF9BA,\"M\",\"\u4e86\"),\n (0xF9BB,\"M\",\"\u50da\"),\n (0xF9BC,\"M\",\"\u5bee\"),\n (0xF9BD,\"M\",\"\u5c3f\"),\n (0xF9BE,\"M\",\"\u6599\"),\n (0xF9BF,\"M\",\"\u6a02\"),\n (0xF9C0,\"M\",\"\u71ce\"),\n (0xF9C1,\"M\",\"\u7642\"),\n (0xF9C2,\"M\",\"\u84fc\"),\n (0xF9C3,\"M\",\"\u907c\"),\n (0xF9C4,\"M\",\"\u9f8d\"),\n (0xF9C5,\"M\",\"\u6688\"),\n (0xF9C6,\"M\",\"\u962e\"),\n (0xF9C7,\"M\",\"\u5289\"),\n (0xF9C8,\"M\",\"\u677b\"),\n (0xF9C9,\"M\",\"\u67f3\"),\n (0xF9CA,\"M\",\"\u6d41\"),\n (0xF9CB,\"M\",\"\u6e9c\"),\n (0xF9CC,\"M\",\"\u7409\"),\n (0xF9CD,\"M\",\"\u7559\"),\n (0xF9CE,\"M\",\"\u786b\"),\n (0xF9CF,\"M\",\"\u7d10\"),\n (0xF9D0,\"M\",\"\u985e\"),\n (0xF9D1,\"M\",\"\u516d\"),\n (0xF9D2,\"M\",\"\u622e\"),\n (0xF9D3,\"M\",\"\u9678\"),\n (0xF9D4,\"M\",\"\u502b\"),\n (0xF9D5,\"M\",\"\u5d19\"),\n (0xF9D6,\"M\",\"\u6dea\"),\n (0xF9D7,\"M\",\"\u8f2a\"),\n (0xF9D8,\"M\",\"\u5f8b\"),\n (0xF9D9,\"M\",\"\u6144\"),\n (0xF9DA,\"M\",\"\u6817\"),\n (0xF9DB,\"M\",\"\u7387\"),\n (0xF9DC,\"M\",\"\u9686\"),\n (0xF9DD,\"M\",\"\u5229\"),\n (0xF9DE,\"M\",\"\u540f\"),\n (0xF9DF,\"M\",\"\u5c65\"),\n (0xF9E0,\"M\",\"\u6613\"),\n (0xF9E1,\"M\",\"\u674e\"),\n (0xF9E2,\"M\",\"\u68a8\"),\n (0xF9E3,\"M\",\"\u6ce5\"),\n (0xF9E4,\"M\",\"\u7406\"),\n (0xF9E5,\"M\",\"\u75e2\"),\n (0xF9E6,\"M\",\"\u7f79\"),\n (0xF9E7,\"M\",\"\u88cf\"),\n (0xF9E8,\"M\",\"\u88e1\"),\n (0xF9E9,\"M\",\"\u91cc\"),\n (0xF9EA,\"M\",\"\u96e2\"),\n (0xF9EB,\"M\",\"\u533f\"),\n (0xF9EC,\"M\",\"\u6eba\"),\n (0xF9ED,\"M\",\"\u541d\"),\n (0xF9EE,\"M\",\"\u71d0\"),\n (0xF9EF,\"M\",\"\u7498\"),\n (0xF9F0,\"M\",\"\u85fa\"),\n (0xF9F1,\"M\",\"\u96a3\"),\n (0xF9F2,\"M\",\"\u9c57\"),\n (0xF9F3,\"M\",\"\u9e9f\"),\n (0xF9F4,\"M\",\"\u6797\"),\n (0xF9F5,\"M\",\"\u6dcb\"),\n (0xF9F6,\"M\",\"\u81e8\"),\n (0xF9F7,\"M\",\"\u7acb\"),\n (0xF9F8,\"M\",\"\u7b20\"),\n (0xF9F9,\"M\",\"\u7c92\"),\n (0xF9FA,\"M\",\"\u72c0\"),\n (0xF9FB,\"M\",\"\u7099\"),\n (0xF9FC,\"M\",\"\u8b58\"),\n (0xF9FD,\"M\",\"\u4ec0\"),\n (0xF9FE,\"M\",\"\u8336\"),\n (0xF9FF,\"M\",\"\u523a\"),\n (0xFA00,\"M\",\"\u5207\"),\n (0xFA01,\"M\",\"\u5ea6\"),\n (0xFA02,\"M\",\"\u62d3\"),\n (0xFA03,\"M\",\"\u7cd6\"),\n (0xFA04,\"M\",\"\u5b85\"),\n (0xFA05,\"M\",\"\u6d1e\"),\n (0xFA06,\"M\",\"\u66b4\"),\n (0xFA07,\"M\",\"\u8f3b\"),\n (0xFA08,\"M\",\"\u884c\"),\n (0xFA09,\"M\",\"\u964d\"),\n (0xFA0A,\"M\",\"\u898b\"),\n (0xFA0B,\"M\",\"\u5ed3\"),\n (0xFA0C,\"M\",\"\u5140\"),\n (0xFA0D,\"M\",\"\u55c0\"),\n (0xFA0E,\"V\"),\n (0xFA10,\"M\",\"\u585a\"),\n (0xFA11,\"V\"),\n (0xFA12,\"M\",\"\u6674\"),\n ]\n \n \ndef _seg_42()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFA13,\"V\"),\n (0xFA15,\"M\",\"\u51de\"),\n (0xFA16,\"M\",\"\u732a\"),\n (0xFA17,\"M\",\"\u76ca\"),\n (0xFA18,\"M\",\"\u793c\"),\n (0xFA19,\"M\",\"\u795e\"),\n (0xFA1A,\"M\",\"\u7965\"),\n (0xFA1B,\"M\",\"\u798f\"),\n (0xFA1C,\"M\",\"\u9756\"),\n (0xFA1D,\"M\",\"\u7cbe\"),\n (0xFA1E,\"M\",\"\u7fbd\"),\n (0xFA1F,\"V\"),\n (0xFA20,\"M\",\"\u8612\"),\n (0xFA21,\"V\"),\n (0xFA22,\"M\",\"\u8af8\"),\n (0xFA23,\"V\"),\n (0xFA25,\"M\",\"\u9038\"),\n (0xFA26,\"M\",\"\u90fd\"),\n (0xFA27,\"V\"),\n (0xFA2A,\"M\",\"\u98ef\"),\n (0xFA2B,\"M\",\"\u98fc\"),\n (0xFA2C,\"M\",\"\u9928\"),\n (0xFA2D,\"M\",\"\u9db4\"),\n (0xFA2E,\"M\",\"\u90de\"),\n (0xFA2F,\"M\",\"\u96b7\"),\n (0xFA30,\"M\",\"\u4fae\"),\n (0xFA31,\"M\",\"\u50e7\"),\n (0xFA32,\"M\",\"\u514d\"),\n (0xFA33,\"M\",\"\u52c9\"),\n (0xFA34,\"M\",\"\u52e4\"),\n (0xFA35,\"M\",\"\u5351\"),\n (0xFA36,\"M\",\"\u559d\"),\n (0xFA37,\"M\",\"\u5606\"),\n (0xFA38,\"M\",\"\u5668\"),\n (0xFA39,\"M\",\"\u5840\"),\n (0xFA3A,\"M\",\"\u58a8\"),\n (0xFA3B,\"M\",\"\u5c64\"),\n (0xFA3C,\"M\",\"\u5c6e\"),\n (0xFA3D,\"M\",\"\u6094\"),\n (0xFA3E,\"M\",\"\u6168\"),\n (0xFA3F,\"M\",\"\u618e\"),\n (0xFA40,\"M\",\"\u61f2\"),\n (0xFA41,\"M\",\"\u654f\"),\n (0xFA42,\"M\",\"\u65e2\"),\n (0xFA43,\"M\",\"\u6691\"),\n (0xFA44,\"M\",\"\u6885\"),\n (0xFA45,\"M\",\"\u6d77\"),\n (0xFA46,\"M\",\"\u6e1a\"),\n (0xFA47,\"M\",\"\u6f22\"),\n (0xFA48,\"M\",\"\u716e\"),\n (0xFA49,\"M\",\"\u722b\"),\n (0xFA4A,\"M\",\"\u7422\"),\n (0xFA4B,\"M\",\"\u7891\"),\n (0xFA4C,\"M\",\"\u793e\"),\n (0xFA4D,\"M\",\"\u7949\"),\n (0xFA4E,\"M\",\"\u7948\"),\n (0xFA4F,\"M\",\"\u7950\"),\n (0xFA50,\"M\",\"\u7956\"),\n (0xFA51,\"M\",\"\u795d\"),\n (0xFA52,\"M\",\"\u798d\"),\n (0xFA53,\"M\",\"\u798e\"),\n (0xFA54,\"M\",\"\u7a40\"),\n (0xFA55,\"M\",\"\u7a81\"),\n (0xFA56,\"M\",\"\u7bc0\"),\n (0xFA57,\"M\",\"\u7df4\"),\n (0xFA58,\"M\",\"\u7e09\"),\n (0xFA59,\"M\",\"\u7e41\"),\n (0xFA5A,\"M\",\"\u7f72\"),\n (0xFA5B,\"M\",\"\u8005\"),\n (0xFA5C,\"M\",\"\u81ed\"),\n (0xFA5D,\"M\",\"\u8279\"),\n (0xFA5F,\"M\",\"\u8457\"),\n (0xFA60,\"M\",\"\u8910\"),\n (0xFA61,\"M\",\"\u8996\"),\n (0xFA62,\"M\",\"\u8b01\"),\n (0xFA63,\"M\",\"\u8b39\"),\n (0xFA64,\"M\",\"\u8cd3\"),\n (0xFA65,\"M\",\"\u8d08\"),\n (0xFA66,\"M\",\"\u8fb6\"),\n (0xFA67,\"M\",\"\u9038\"),\n (0xFA68,\"M\",\"\u96e3\"),\n (0xFA69,\"M\",\"\u97ff\"),\n (0xFA6A,\"M\",\"\u983b\"),\n (0xFA6B,\"M\",\"\u6075\"),\n (0xFA6C,\"M\",\"\ud850\udeee\"),\n (0xFA6D,\"M\",\"\u8218\"),\n (0xFA6E,\"X\"),\n (0xFA70,\"M\",\"\u4e26\"),\n (0xFA71,\"M\",\"\u51b5\"),\n (0xFA72,\"M\",\"\u5168\"),\n (0xFA73,\"M\",\"\u4f80\"),\n (0xFA74,\"M\",\"\u5145\"),\n (0xFA75,\"M\",\"\u5180\"),\n (0xFA76,\"M\",\"\u52c7\"),\n (0xFA77,\"M\",\"\u52fa\"),\n (0xFA78,\"M\",\"\u559d\"),\n (0xFA79,\"M\",\"\u5555\"),\n (0xFA7A,\"M\",\"\u5599\"),\n (0xFA7B,\"M\",\"\u55e2\"),\n (0xFA7C,\"M\",\"\u585a\"),\n ]\n \n \ndef _seg_43()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFA7D,\"M\",\"\u58b3\"),\n (0xFA7E,\"M\",\"\u5944\"),\n (0xFA7F,\"M\",\"\u5954\"),\n (0xFA80,\"M\",\"\u5a62\"),\n (0xFA81,\"M\",\"\u5b28\"),\n (0xFA82,\"M\",\"\u5ed2\"),\n (0xFA83,\"M\",\"\u5ed9\"),\n (0xFA84,\"M\",\"\u5f69\"),\n (0xFA85,\"M\",\"\u5fad\"),\n (0xFA86,\"M\",\"\u60d8\"),\n (0xFA87,\"M\",\"\u614e\"),\n (0xFA88,\"M\",\"\u6108\"),\n (0xFA89,\"M\",\"\u618e\"),\n (0xFA8A,\"M\",\"\u6160\"),\n (0xFA8B,\"M\",\"\u61f2\"),\n (0xFA8C,\"M\",\"\u6234\"),\n (0xFA8D,\"M\",\"\u63c4\"),\n (0xFA8E,\"M\",\"\u641c\"),\n (0xFA8F,\"M\",\"\u6452\"),\n (0xFA90,\"M\",\"\u6556\"),\n (0xFA91,\"M\",\"\u6674\"),\n (0xFA92,\"M\",\"\u6717\"),\n (0xFA93,\"M\",\"\u671b\"),\n (0xFA94,\"M\",\"\u6756\"),\n (0xFA95,\"M\",\"\u6b79\"),\n (0xFA96,\"M\",\"\u6bba\"),\n (0xFA97,\"M\",\"\u6d41\"),\n (0xFA98,\"M\",\"\u6edb\"),\n (0xFA99,\"M\",\"\u6ecb\"),\n (0xFA9A,\"M\",\"\u6f22\"),\n (0xFA9B,\"M\",\"\u701e\"),\n (0xFA9C,\"M\",\"\u716e\"),\n (0xFA9D,\"M\",\"\u77a7\"),\n (0xFA9E,\"M\",\"\u7235\"),\n (0xFA9F,\"M\",\"\u72af\"),\n (0xFAA0,\"M\",\"\u732a\"),\n (0xFAA1,\"M\",\"\u7471\"),\n (0xFAA2,\"M\",\"\u7506\"),\n (0xFAA3,\"M\",\"\u753b\"),\n (0xFAA4,\"M\",\"\u761d\"),\n (0xFAA5,\"M\",\"\u761f\"),\n (0xFAA6,\"M\",\"\u76ca\"),\n (0xFAA7,\"M\",\"\u76db\"),\n (0xFAA8,\"M\",\"\u76f4\"),\n (0xFAA9,\"M\",\"\u774a\"),\n (0xFAAA,\"M\",\"\u7740\"),\n (0xFAAB,\"M\",\"\u78cc\"),\n (0xFAAC,\"M\",\"\u7ab1\"),\n (0xFAAD,\"M\",\"\u7bc0\"),\n (0xFAAE,\"M\",\"\u7c7b\"),\n (0xFAAF,\"M\",\"\u7d5b\"),\n (0xFAB0,\"M\",\"\u7df4\"),\n (0xFAB1,\"M\",\"\u7f3e\"),\n (0xFAB2,\"M\",\"\u8005\"),\n (0xFAB3,\"M\",\"\u8352\"),\n (0xFAB4,\"M\",\"\u83ef\"),\n (0xFAB5,\"M\",\"\u8779\"),\n (0xFAB6,\"M\",\"\u8941\"),\n (0xFAB7,\"M\",\"\u8986\"),\n (0xFAB8,\"M\",\"\u8996\"),\n (0xFAB9,\"M\",\"\u8abf\"),\n (0xFABA,\"M\",\"\u8af8\"),\n (0xFABB,\"M\",\"\u8acb\"),\n (0xFABC,\"M\",\"\u8b01\"),\n (0xFABD,\"M\",\"\u8afe\"),\n (0xFABE,\"M\",\"\u8aed\"),\n (0xFABF,\"M\",\"\u8b39\"),\n (0xFAC0,\"M\",\"\u8b8a\"),\n (0xFAC1,\"M\",\"\u8d08\"),\n (0xFAC2,\"M\",\"\u8f38\"),\n (0xFAC3,\"M\",\"\u9072\"),\n (0xFAC4,\"M\",\"\u9199\"),\n (0xFAC5,\"M\",\"\u9276\"),\n (0xFAC6,\"M\",\"\u967c\"),\n (0xFAC7,\"M\",\"\u96e3\"),\n (0xFAC8,\"M\",\"\u9756\"),\n (0xFAC9,\"M\",\"\u97db\"),\n (0xFACA,\"M\",\"\u97ff\"),\n (0xFACB,\"M\",\"\u980b\"),\n (0xFACC,\"M\",\"\u983b\"),\n (0xFACD,\"M\",\"\u9b12\"),\n (0xFACE,\"M\",\"\u9f9c\"),\n (0xFACF,\"M\",\"\ud84a\udc4a\"),\n (0xFAD0,\"M\",\"\ud84a\udc44\"),\n (0xFAD1,\"M\",\"\ud84c\udfd5\"),\n (0xFAD2,\"M\",\"\u3b9d\"),\n (0xFAD3,\"M\",\"\u4018\"),\n (0xFAD4,\"M\",\"\u4039\"),\n (0xFAD5,\"M\",\"\ud854\ude49\"),\n (0xFAD6,\"M\",\"\ud857\udcd0\"),\n (0xFAD7,\"M\",\"\ud85f\uded3\"),\n (0xFAD8,\"M\",\"\u9f43\"),\n (0xFAD9,\"M\",\"\u9f8e\"),\n (0xFADA,\"X\"),\n (0xFB00,\"M\",\"ff\"),\n (0xFB01,\"M\",\"fi\"),\n (0xFB02,\"M\",\"fl\"),\n (0xFB03,\"M\",\"ffi\"),\n (0xFB04,\"M\",\"ffl\"),\n (0xFB05,\"M\",\"st\"),\n ]\n \n \ndef _seg_44()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFB07,\"X\"),\n (0xFB13,\"M\",\"\u0574\u0576\"),\n (0xFB14,\"M\",\"\u0574\u0565\"),\n (0xFB15,\"M\",\"\u0574\u056b\"),\n (0xFB16,\"M\",\"\u057e\u0576\"),\n (0xFB17,\"M\",\"\u0574\u056d\"),\n (0xFB18,\"X\"),\n (0xFB1D,\"M\",\"\u05d9\u05b4\"),\n (0xFB1E,\"V\"),\n (0xFB1F,\"M\",\"\u05f2\u05b7\"),\n (0xFB20,\"M\",\"\u05e2\"),\n (0xFB21,\"M\",\"\u05d0\"),\n (0xFB22,\"M\",\"\u05d3\"),\n (0xFB23,\"M\",\"\u05d4\"),\n (0xFB24,\"M\",\"\u05db\"),\n (0xFB25,\"M\",\"\u05dc\"),\n (0xFB26,\"M\",\"\u05dd\"),\n (0xFB27,\"M\",\"\u05e8\"),\n (0xFB28,\"M\",\"\u05ea\"),\n (0xFB29,\"3\",\"+\"),\n (0xFB2A,\"M\",\"\u05e9\u05c1\"),\n (0xFB2B,\"M\",\"\u05e9\u05c2\"),\n (0xFB2C,\"M\",\"\u05e9\u05bc\u05c1\"),\n (0xFB2D,\"M\",\"\u05e9\u05bc\u05c2\"),\n (0xFB2E,\"M\",\"\u05d0\u05b7\"),\n (0xFB2F,\"M\",\"\u05d0\u05b8\"),\n (0xFB30,\"M\",\"\u05d0\u05bc\"),\n (0xFB31,\"M\",\"\u05d1\u05bc\"),\n (0xFB32,\"M\",\"\u05d2\u05bc\"),\n (0xFB33,\"M\",\"\u05d3\u05bc\"),\n (0xFB34,\"M\",\"\u05d4\u05bc\"),\n (0xFB35,\"M\",\"\u05d5\u05bc\"),\n (0xFB36,\"M\",\"\u05d6\u05bc\"),\n (0xFB37,\"X\"),\n (0xFB38,\"M\",\"\u05d8\u05bc\"),\n (0xFB39,\"M\",\"\u05d9\u05bc\"),\n (0xFB3A,\"M\",\"\u05da\u05bc\"),\n (0xFB3B,\"M\",\"\u05db\u05bc\"),\n (0xFB3C,\"M\",\"\u05dc\u05bc\"),\n (0xFB3D,\"X\"),\n (0xFB3E,\"M\",\"\u05de\u05bc\"),\n (0xFB3F,\"X\"),\n (0xFB40,\"M\",\"\u05e0\u05bc\"),\n (0xFB41,\"M\",\"\u05e1\u05bc\"),\n (0xFB42,\"X\"),\n (0xFB43,\"M\",\"\u05e3\u05bc\"),\n (0xFB44,\"M\",\"\u05e4\u05bc\"),\n (0xFB45,\"X\"),\n (0xFB46,\"M\",\"\u05e6\u05bc\"),\n (0xFB47,\"M\",\"\u05e7\u05bc\"),\n (0xFB48,\"M\",\"\u05e8\u05bc\"),\n (0xFB49,\"M\",\"\u05e9\u05bc\"),\n (0xFB4A,\"M\",\"\u05ea\u05bc\"),\n (0xFB4B,\"M\",\"\u05d5\u05b9\"),\n (0xFB4C,\"M\",\"\u05d1\u05bf\"),\n (0xFB4D,\"M\",\"\u05db\u05bf\"),\n (0xFB4E,\"M\",\"\u05e4\u05bf\"),\n (0xFB4F,\"M\",\"\u05d0\u05dc\"),\n (0xFB50,\"M\",\"\u0671\"),\n (0xFB52,\"M\",\"\u067b\"),\n (0xFB56,\"M\",\"\u067e\"),\n (0xFB5A,\"M\",\"\u0680\"),\n (0xFB5E,\"M\",\"\u067a\"),\n (0xFB62,\"M\",\"\u067f\"),\n (0xFB66,\"M\",\"\u0679\"),\n (0xFB6A,\"M\",\"\u06a4\"),\n (0xFB6E,\"M\",\"\u06a6\"),\n (0xFB72,\"M\",\"\u0684\"),\n (0xFB76,\"M\",\"\u0683\"),\n (0xFB7A,\"M\",\"\u0686\"),\n (0xFB7E,\"M\",\"\u0687\"),\n (0xFB82,\"M\",\"\u068d\"),\n (0xFB84,\"M\",\"\u068c\"),\n (0xFB86,\"M\",\"\u068e\"),\n (0xFB88,\"M\",\"\u0688\"),\n (0xFB8A,\"M\",\"\u0698\"),\n (0xFB8C,\"M\",\"\u0691\"),\n (0xFB8E,\"M\",\"\u06a9\"),\n (0xFB92,\"M\",\"\u06af\"),\n (0xFB96,\"M\",\"\u06b3\"),\n (0xFB9A,\"M\",\"\u06b1\"),\n (0xFB9E,\"M\",\"\u06ba\"),\n (0xFBA0,\"M\",\"\u06bb\"),\n (0xFBA4,\"M\",\"\u06c0\"),\n (0xFBA6,\"M\",\"\u06c1\"),\n (0xFBAA,\"M\",\"\u06be\"),\n (0xFBAE,\"M\",\"\u06d2\"),\n (0xFBB0,\"M\",\"\u06d3\"),\n (0xFBB2,\"V\"),\n (0xFBC3,\"X\"),\n (0xFBD3,\"M\",\"\u06ad\"),\n (0xFBD7,\"M\",\"\u06c7\"),\n (0xFBD9,\"M\",\"\u06c6\"),\n (0xFBDB,\"M\",\"\u06c8\"),\n (0xFBDD,\"M\",\"\u06c7\u0674\"),\n (0xFBDE,\"M\",\"\u06cb\"),\n (0xFBE0,\"M\",\"\u06c5\"),\n (0xFBE2,\"M\",\"\u06c9\"),\n (0xFBE4,\"M\",\"\u06d0\"),\n (0xFBE8,\"M\",\"\u0649\"),\n ]\n \n \ndef _seg_45()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFBEA,\"M\",\"\u0626\u0627\"),\n (0xFBEC,\"M\",\"\u0626\u06d5\"),\n (0xFBEE,\"M\",\"\u0626\u0648\"),\n (0xFBF0,\"M\",\"\u0626\u06c7\"),\n (0xFBF2,\"M\",\"\u0626\u06c6\"),\n (0xFBF4,\"M\",\"\u0626\u06c8\"),\n (0xFBF6,\"M\",\"\u0626\u06d0\"),\n (0xFBF9,\"M\",\"\u0626\u0649\"),\n (0xFBFC,\"M\",\"\u06cc\"),\n (0xFC00,\"M\",\"\u0626\u062c\"),\n (0xFC01,\"M\",\"\u0626\u062d\"),\n (0xFC02,\"M\",\"\u0626\u0645\"),\n (0xFC03,\"M\",\"\u0626\u0649\"),\n (0xFC04,\"M\",\"\u0626\u064a\"),\n (0xFC05,\"M\",\"\u0628\u062c\"),\n (0xFC06,\"M\",\"\u0628\u062d\"),\n (0xFC07,\"M\",\"\u0628\u062e\"),\n (0xFC08,\"M\",\"\u0628\u0645\"),\n (0xFC09,\"M\",\"\u0628\u0649\"),\n (0xFC0A,\"M\",\"\u0628\u064a\"),\n (0xFC0B,\"M\",\"\u062a\u062c\"),\n (0xFC0C,\"M\",\"\u062a\u062d\"),\n (0xFC0D,\"M\",\"\u062a\u062e\"),\n (0xFC0E,\"M\",\"\u062a\u0645\"),\n (0xFC0F,\"M\",\"\u062a\u0649\"),\n (0xFC10,\"M\",\"\u062a\u064a\"),\n (0xFC11,\"M\",\"\u062b\u062c\"),\n (0xFC12,\"M\",\"\u062b\u0645\"),\n (0xFC13,\"M\",\"\u062b\u0649\"),\n (0xFC14,\"M\",\"\u062b\u064a\"),\n (0xFC15,\"M\",\"\u062c\u062d\"),\n (0xFC16,\"M\",\"\u062c\u0645\"),\n (0xFC17,\"M\",\"\u062d\u062c\"),\n (0xFC18,\"M\",\"\u062d\u0645\"),\n (0xFC19,\"M\",\"\u062e\u062c\"),\n (0xFC1A,\"M\",\"\u062e\u062d\"),\n (0xFC1B,\"M\",\"\u062e\u0645\"),\n (0xFC1C,\"M\",\"\u0633\u062c\"),\n (0xFC1D,\"M\",\"\u0633\u062d\"),\n (0xFC1E,\"M\",\"\u0633\u062e\"),\n (0xFC1F,\"M\",\"\u0633\u0645\"),\n (0xFC20,\"M\",\"\u0635\u062d\"),\n (0xFC21,\"M\",\"\u0635\u0645\"),\n (0xFC22,\"M\",\"\u0636\u062c\"),\n (0xFC23,\"M\",\"\u0636\u062d\"),\n (0xFC24,\"M\",\"\u0636\u062e\"),\n (0xFC25,\"M\",\"\u0636\u0645\"),\n (0xFC26,\"M\",\"\u0637\u062d\"),\n (0xFC27,\"M\",\"\u0637\u0645\"),\n (0xFC28,\"M\",\"\u0638\u0645\"),\n (0xFC29,\"M\",\"\u0639\u062c\"),\n (0xFC2A,\"M\",\"\u0639\u0645\"),\n (0xFC2B,\"M\",\"\u063a\u062c\"),\n (0xFC2C,\"M\",\"\u063a\u0645\"),\n (0xFC2D,\"M\",\"\u0641\u062c\"),\n (0xFC2E,\"M\",\"\u0641\u062d\"),\n (0xFC2F,\"M\",\"\u0641\u062e\"),\n (0xFC30,\"M\",\"\u0641\u0645\"),\n (0xFC31,\"M\",\"\u0641\u0649\"),\n (0xFC32,\"M\",\"\u0641\u064a\"),\n (0xFC33,\"M\",\"\u0642\u062d\"),\n (0xFC34,\"M\",\"\u0642\u0645\"),\n (0xFC35,\"M\",\"\u0642\u0649\"),\n (0xFC36,\"M\",\"\u0642\u064a\"),\n (0xFC37,\"M\",\"\u0643\u0627\"),\n (0xFC38,\"M\",\"\u0643\u062c\"),\n (0xFC39,\"M\",\"\u0643\u062d\"),\n (0xFC3A,\"M\",\"\u0643\u062e\"),\n (0xFC3B,\"M\",\"\u0643\u0644\"),\n (0xFC3C,\"M\",\"\u0643\u0645\"),\n (0xFC3D,\"M\",\"\u0643\u0649\"),\n (0xFC3E,\"M\",\"\u0643\u064a\"),\n (0xFC3F,\"M\",\"\u0644\u062c\"),\n (0xFC40,\"M\",\"\u0644\u062d\"),\n (0xFC41,\"M\",\"\u0644\u062e\"),\n (0xFC42,\"M\",\"\u0644\u0645\"),\n (0xFC43,\"M\",\"\u0644\u0649\"),\n (0xFC44,\"M\",\"\u0644\u064a\"),\n (0xFC45,\"M\",\"\u0645\u062c\"),\n (0xFC46,\"M\",\"\u0645\u062d\"),\n (0xFC47,\"M\",\"\u0645\u062e\"),\n (0xFC48,\"M\",\"\u0645\u0645\"),\n (0xFC49,\"M\",\"\u0645\u0649\"),\n (0xFC4A,\"M\",\"\u0645\u064a\"),\n (0xFC4B,\"M\",\"\u0646\u062c\"),\n (0xFC4C,\"M\",\"\u0646\u062d\"),\n (0xFC4D,\"M\",\"\u0646\u062e\"),\n (0xFC4E,\"M\",\"\u0646\u0645\"),\n (0xFC4F,\"M\",\"\u0646\u0649\"),\n (0xFC50,\"M\",\"\u0646\u064a\"),\n (0xFC51,\"M\",\"\u0647\u062c\"),\n (0xFC52,\"M\",\"\u0647\u0645\"),\n (0xFC53,\"M\",\"\u0647\u0649\"),\n (0xFC54,\"M\",\"\u0647\u064a\"),\n (0xFC55,\"M\",\"\u064a\u062c\"),\n (0xFC56,\"M\",\"\u064a\u062d\"),\n (0xFC57,\"M\",\"\u064a\u062e\"),\n (0xFC58,\"M\",\"\u064a\u0645\"),\n (0xFC59,\"M\",\"\u064a\u0649\"),\n (0xFC5A,\"M\",\"\u064a\u064a\"),\n ]\n \n \ndef _seg_46()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFC5B,\"M\",\"\u0630\u0670\"),\n (0xFC5C,\"M\",\"\u0631\u0670\"),\n (0xFC5D,\"M\",\"\u0649\u0670\"),\n (0xFC5E,\"3\",\" \u064c\u0651\"),\n (0xFC5F,\"3\",\" \u064d\u0651\"),\n (0xFC60,\"3\",\" \u064e\u0651\"),\n (0xFC61,\"3\",\" \u064f\u0651\"),\n (0xFC62,\"3\",\" \u0650\u0651\"),\n (0xFC63,\"3\",\" \u0651\u0670\"),\n (0xFC64,\"M\",\"\u0626\u0631\"),\n (0xFC65,\"M\",\"\u0626\u0632\"),\n (0xFC66,\"M\",\"\u0626\u0645\"),\n (0xFC67,\"M\",\"\u0626\u0646\"),\n (0xFC68,\"M\",\"\u0626\u0649\"),\n (0xFC69,\"M\",\"\u0626\u064a\"),\n (0xFC6A,\"M\",\"\u0628\u0631\"),\n (0xFC6B,\"M\",\"\u0628\u0632\"),\n (0xFC6C,\"M\",\"\u0628\u0645\"),\n (0xFC6D,\"M\",\"\u0628\u0646\"),\n (0xFC6E,\"M\",\"\u0628\u0649\"),\n (0xFC6F,\"M\",\"\u0628\u064a\"),\n (0xFC70,\"M\",\"\u062a\u0631\"),\n (0xFC71,\"M\",\"\u062a\u0632\"),\n (0xFC72,\"M\",\"\u062a\u0645\"),\n (0xFC73,\"M\",\"\u062a\u0646\"),\n (0xFC74,\"M\",\"\u062a\u0649\"),\n (0xFC75,\"M\",\"\u062a\u064a\"),\n (0xFC76,\"M\",\"\u062b\u0631\"),\n (0xFC77,\"M\",\"\u062b\u0632\"),\n (0xFC78,\"M\",\"\u062b\u0645\"),\n (0xFC79,\"M\",\"\u062b\u0646\"),\n (0xFC7A,\"M\",\"\u062b\u0649\"),\n (0xFC7B,\"M\",\"\u062b\u064a\"),\n (0xFC7C,\"M\",\"\u0641\u0649\"),\n (0xFC7D,\"M\",\"\u0641\u064a\"),\n (0xFC7E,\"M\",\"\u0642\u0649\"),\n (0xFC7F,\"M\",\"\u0642\u064a\"),\n (0xFC80,\"M\",\"\u0643\u0627\"),\n (0xFC81,\"M\",\"\u0643\u0644\"),\n (0xFC82,\"M\",\"\u0643\u0645\"),\n (0xFC83,\"M\",\"\u0643\u0649\"),\n (0xFC84,\"M\",\"\u0643\u064a\"),\n (0xFC85,\"M\",\"\u0644\u0645\"),\n (0xFC86,\"M\",\"\u0644\u0649\"),\n (0xFC87,\"M\",\"\u0644\u064a\"),\n (0xFC88,\"M\",\"\u0645\u0627\"),\n (0xFC89,\"M\",\"\u0645\u0645\"),\n (0xFC8A,\"M\",\"\u0646\u0631\"),\n (0xFC8B,\"M\",\"\u0646\u0632\"),\n (0xFC8C,\"M\",\"\u0646\u0645\"),\n (0xFC8D,\"M\",\"\u0646\u0646\"),\n (0xFC8E,\"M\",\"\u0646\u0649\"),\n (0xFC8F,\"M\",\"\u0646\u064a\"),\n (0xFC90,\"M\",\"\u0649\u0670\"),\n (0xFC91,\"M\",\"\u064a\u0631\"),\n (0xFC92,\"M\",\"\u064a\u0632\"),\n (0xFC93,\"M\",\"\u064a\u0645\"),\n (0xFC94,\"M\",\"\u064a\u0646\"),\n (0xFC95,\"M\",\"\u064a\u0649\"),\n (0xFC96,\"M\",\"\u064a\u064a\"),\n (0xFC97,\"M\",\"\u0626\u062c\"),\n (0xFC98,\"M\",\"\u0626\u062d\"),\n (0xFC99,\"M\",\"\u0626\u062e\"),\n (0xFC9A,\"M\",\"\u0626\u0645\"),\n (0xFC9B,\"M\",\"\u0626\u0647\"),\n (0xFC9C,\"M\",\"\u0628\u062c\"),\n (0xFC9D,\"M\",\"\u0628\u062d\"),\n (0xFC9E,\"M\",\"\u0628\u062e\"),\n (0xFC9F,\"M\",\"\u0628\u0645\"),\n (0xFCA0,\"M\",\"\u0628\u0647\"),\n (0xFCA1,\"M\",\"\u062a\u062c\"),\n (0xFCA2,\"M\",\"\u062a\u062d\"),\n (0xFCA3,\"M\",\"\u062a\u062e\"),\n (0xFCA4,\"M\",\"\u062a\u0645\"),\n (0xFCA5,\"M\",\"\u062a\u0647\"),\n (0xFCA6,\"M\",\"\u062b\u0645\"),\n (0xFCA7,\"M\",\"\u062c\u062d\"),\n (0xFCA8,\"M\",\"\u062c\u0645\"),\n (0xFCA9,\"M\",\"\u062d\u062c\"),\n (0xFCAA,\"M\",\"\u062d\u0645\"),\n (0xFCAB,\"M\",\"\u062e\u062c\"),\n (0xFCAC,\"M\",\"\u062e\u0645\"),\n (0xFCAD,\"M\",\"\u0633\u062c\"),\n (0xFCAE,\"M\",\"\u0633\u062d\"),\n (0xFCAF,\"M\",\"\u0633\u062e\"),\n (0xFCB0,\"M\",\"\u0633\u0645\"),\n (0xFCB1,\"M\",\"\u0635\u062d\"),\n (0xFCB2,\"M\",\"\u0635\u062e\"),\n (0xFCB3,\"M\",\"\u0635\u0645\"),\n (0xFCB4,\"M\",\"\u0636\u062c\"),\n (0xFCB5,\"M\",\"\u0636\u062d\"),\n (0xFCB6,\"M\",\"\u0636\u062e\"),\n (0xFCB7,\"M\",\"\u0636\u0645\"),\n (0xFCB8,\"M\",\"\u0637\u062d\"),\n (0xFCB9,\"M\",\"\u0638\u0645\"),\n (0xFCBA,\"M\",\"\u0639\u062c\"),\n (0xFCBB,\"M\",\"\u0639\u0645\"),\n (0xFCBC,\"M\",\"\u063a\u062c\"),\n (0xFCBD,\"M\",\"\u063a\u0645\"),\n (0xFCBE,\"M\",\"\u0641\u062c\"),\n ]\n \n \ndef _seg_47()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFCBF,\"M\",\"\u0641\u062d\"),\n (0xFCC0,\"M\",\"\u0641\u062e\"),\n (0xFCC1,\"M\",\"\u0641\u0645\"),\n (0xFCC2,\"M\",\"\u0642\u062d\"),\n (0xFCC3,\"M\",\"\u0642\u0645\"),\n (0xFCC4,\"M\",\"\u0643\u062c\"),\n (0xFCC5,\"M\",\"\u0643\u062d\"),\n (0xFCC6,\"M\",\"\u0643\u062e\"),\n (0xFCC7,\"M\",\"\u0643\u0644\"),\n (0xFCC8,\"M\",\"\u0643\u0645\"),\n (0xFCC9,\"M\",\"\u0644\u062c\"),\n (0xFCCA,\"M\",\"\u0644\u062d\"),\n (0xFCCB,\"M\",\"\u0644\u062e\"),\n (0xFCCC,\"M\",\"\u0644\u0645\"),\n (0xFCCD,\"M\",\"\u0644\u0647\"),\n (0xFCCE,\"M\",\"\u0645\u062c\"),\n (0xFCCF,\"M\",\"\u0645\u062d\"),\n (0xFCD0,\"M\",\"\u0645\u062e\"),\n (0xFCD1,\"M\",\"\u0645\u0645\"),\n (0xFCD2,\"M\",\"\u0646\u062c\"),\n (0xFCD3,\"M\",\"\u0646\u062d\"),\n (0xFCD4,\"M\",\"\u0646\u062e\"),\n (0xFCD5,\"M\",\"\u0646\u0645\"),\n (0xFCD6,\"M\",\"\u0646\u0647\"),\n (0xFCD7,\"M\",\"\u0647\u062c\"),\n (0xFCD8,\"M\",\"\u0647\u0645\"),\n (0xFCD9,\"M\",\"\u0647\u0670\"),\n (0xFCDA,\"M\",\"\u064a\u062c\"),\n (0xFCDB,\"M\",\"\u064a\u062d\"),\n (0xFCDC,\"M\",\"\u064a\u062e\"),\n (0xFCDD,\"M\",\"\u064a\u0645\"),\n (0xFCDE,\"M\",\"\u064a\u0647\"),\n (0xFCDF,\"M\",\"\u0626\u0645\"),\n (0xFCE0,\"M\",\"\u0626\u0647\"),\n (0xFCE1,\"M\",\"\u0628\u0645\"),\n (0xFCE2,\"M\",\"\u0628\u0647\"),\n (0xFCE3,\"M\",\"\u062a\u0645\"),\n (0xFCE4,\"M\",\"\u062a\u0647\"),\n (0xFCE5,\"M\",\"\u062b\u0645\"),\n (0xFCE6,\"M\",\"\u062b\u0647\"),\n (0xFCE7,\"M\",\"\u0633\u0645\"),\n (0xFCE8,\"M\",\"\u0633\u0647\"),\n (0xFCE9,\"M\",\"\u0634\u0645\"),\n (0xFCEA,\"M\",\"\u0634\u0647\"),\n (0xFCEB,\"M\",\"\u0643\u0644\"),\n (0xFCEC,\"M\",\"\u0643\u0645\"),\n (0xFCED,\"M\",\"\u0644\u0645\"),\n (0xFCEE,\"M\",\"\u0646\u0645\"),\n (0xFCEF,\"M\",\"\u0646\u0647\"),\n (0xFCF0,\"M\",\"\u064a\u0645\"),\n (0xFCF1,\"M\",\"\u064a\u0647\"),\n (0xFCF2,\"M\",\"\u0640\u064e\u0651\"),\n (0xFCF3,\"M\",\"\u0640\u064f\u0651\"),\n (0xFCF4,\"M\",\"\u0640\u0650\u0651\"),\n (0xFCF5,\"M\",\"\u0637\u0649\"),\n (0xFCF6,\"M\",\"\u0637\u064a\"),\n (0xFCF7,\"M\",\"\u0639\u0649\"),\n (0xFCF8,\"M\",\"\u0639\u064a\"),\n (0xFCF9,\"M\",\"\u063a\u0649\"),\n (0xFCFA,\"M\",\"\u063a\u064a\"),\n (0xFCFB,\"M\",\"\u0633\u0649\"),\n (0xFCFC,\"M\",\"\u0633\u064a\"),\n (0xFCFD,\"M\",\"\u0634\u0649\"),\n (0xFCFE,\"M\",\"\u0634\u064a\"),\n (0xFCFF,\"M\",\"\u062d\u0649\"),\n (0xFD00,\"M\",\"\u062d\u064a\"),\n (0xFD01,\"M\",\"\u062c\u0649\"),\n (0xFD02,\"M\",\"\u062c\u064a\"),\n (0xFD03,\"M\",\"\u062e\u0649\"),\n (0xFD04,\"M\",\"\u062e\u064a\"),\n (0xFD05,\"M\",\"\u0635\u0649\"),\n (0xFD06,\"M\",\"\u0635\u064a\"),\n (0xFD07,\"M\",\"\u0636\u0649\"),\n (0xFD08,\"M\",\"\u0636\u064a\"),\n (0xFD09,\"M\",\"\u0634\u062c\"),\n (0xFD0A,\"M\",\"\u0634\u062d\"),\n (0xFD0B,\"M\",\"\u0634\u062e\"),\n (0xFD0C,\"M\",\"\u0634\u0645\"),\n (0xFD0D,\"M\",\"\u0634\u0631\"),\n (0xFD0E,\"M\",\"\u0633\u0631\"),\n (0xFD0F,\"M\",\"\u0635\u0631\"),\n (0xFD10,\"M\",\"\u0636\u0631\"),\n (0xFD11,\"M\",\"\u0637\u0649\"),\n (0xFD12,\"M\",\"\u0637\u064a\"),\n (0xFD13,\"M\",\"\u0639\u0649\"),\n (0xFD14,\"M\",\"\u0639\u064a\"),\n (0xFD15,\"M\",\"\u063a\u0649\"),\n (0xFD16,\"M\",\"\u063a\u064a\"),\n (0xFD17,\"M\",\"\u0633\u0649\"),\n (0xFD18,\"M\",\"\u0633\u064a\"),\n (0xFD19,\"M\",\"\u0634\u0649\"),\n (0xFD1A,\"M\",\"\u0634\u064a\"),\n (0xFD1B,\"M\",\"\u062d\u0649\"),\n (0xFD1C,\"M\",\"\u062d\u064a\"),\n (0xFD1D,\"M\",\"\u062c\u0649\"),\n (0xFD1E,\"M\",\"\u062c\u064a\"),\n (0xFD1F,\"M\",\"\u062e\u0649\"),\n (0xFD20,\"M\",\"\u062e\u064a\"),\n (0xFD21,\"M\",\"\u0635\u0649\"),\n (0xFD22,\"M\",\"\u0635\u064a\"),\n ]\n \n \ndef _seg_48()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFD23,\"M\",\"\u0636\u0649\"),\n (0xFD24,\"M\",\"\u0636\u064a\"),\n (0xFD25,\"M\",\"\u0634\u062c\"),\n (0xFD26,\"M\",\"\u0634\u062d\"),\n (0xFD27,\"M\",\"\u0634\u062e\"),\n (0xFD28,\"M\",\"\u0634\u0645\"),\n (0xFD29,\"M\",\"\u0634\u0631\"),\n (0xFD2A,\"M\",\"\u0633\u0631\"),\n (0xFD2B,\"M\",\"\u0635\u0631\"),\n (0xFD2C,\"M\",\"\u0636\u0631\"),\n (0xFD2D,\"M\",\"\u0634\u062c\"),\n (0xFD2E,\"M\",\"\u0634\u062d\"),\n (0xFD2F,\"M\",\"\u0634\u062e\"),\n (0xFD30,\"M\",\"\u0634\u0645\"),\n (0xFD31,\"M\",\"\u0633\u0647\"),\n (0xFD32,\"M\",\"\u0634\u0647\"),\n (0xFD33,\"M\",\"\u0637\u0645\"),\n (0xFD34,\"M\",\"\u0633\u062c\"),\n (0xFD35,\"M\",\"\u0633\u062d\"),\n (0xFD36,\"M\",\"\u0633\u062e\"),\n (0xFD37,\"M\",\"\u0634\u062c\"),\n (0xFD38,\"M\",\"\u0634\u062d\"),\n (0xFD39,\"M\",\"\u0634\u062e\"),\n (0xFD3A,\"M\",\"\u0637\u0645\"),\n (0xFD3B,\"M\",\"\u0638\u0645\"),\n (0xFD3C,\"M\",\"\u0627\u064b\"),\n (0xFD3E,\"V\"),\n (0xFD50,\"M\",\"\u062a\u062c\u0645\"),\n (0xFD51,\"M\",\"\u062a\u062d\u062c\"),\n (0xFD53,\"M\",\"\u062a\u062d\u0645\"),\n (0xFD54,\"M\",\"\u062a\u062e\u0645\"),\n (0xFD55,\"M\",\"\u062a\u0645\u062c\"),\n (0xFD56,\"M\",\"\u062a\u0645\u062d\"),\n (0xFD57,\"M\",\"\u062a\u0645\u062e\"),\n (0xFD58,\"M\",\"\u062c\u0645\u062d\"),\n (0xFD5A,\"M\",\"\u062d\u0645\u064a\"),\n (0xFD5B,\"M\",\"\u062d\u0645\u0649\"),\n (0xFD5C,\"M\",\"\u0633\u062d\u062c\"),\n (0xFD5D,\"M\",\"\u0633\u062c\u062d\"),\n (0xFD5E,\"M\",\"\u0633\u062c\u0649\"),\n (0xFD5F,\"M\",\"\u0633\u0645\u062d\"),\n (0xFD61,\"M\",\"\u0633\u0645\u062c\"),\n (0xFD62,\"M\",\"\u0633\u0645\u0645\"),\n (0xFD64,\"M\",\"\u0635\u062d\u062d\"),\n (0xFD66,\"M\",\"\u0635\u0645\u0645\"),\n (0xFD67,\"M\",\"\u0634\u062d\u0645\"),\n (0xFD69,\"M\",\"\u0634\u062c\u064a\"),\n (0xFD6A,\"M\",\"\u0634\u0645\u062e\"),\n (0xFD6C,\"M\",\"\u0634\u0645\u0645\"),\n (0xFD6E,\"M\",\"\u0636\u062d\u0649\"),\n (0xFD6F,\"M\",\"\u0636\u062e\u0645\"),\n (0xFD71,\"M\",\"\u0637\u0645\u062d\"),\n (0xFD73,\"M\",\"\u0637\u0645\u0645\"),\n (0xFD74,\"M\",\"\u0637\u0645\u064a\"),\n (0xFD75,\"M\",\"\u0639\u062c\u0645\"),\n (0xFD76,\"M\",\"\u0639\u0645\u0645\"),\n (0xFD78,\"M\",\"\u0639\u0645\u0649\"),\n (0xFD79,\"M\",\"\u063a\u0645\u0645\"),\n (0xFD7A,\"M\",\"\u063a\u0645\u064a\"),\n (0xFD7B,\"M\",\"\u063a\u0645\u0649\"),\n (0xFD7C,\"M\",\"\u0641\u062e\u0645\"),\n (0xFD7E,\"M\",\"\u0642\u0645\u062d\"),\n (0xFD7F,\"M\",\"\u0642\u0645\u0645\"),\n (0xFD80,\"M\",\"\u0644\u062d\u0645\"),\n (0xFD81,\"M\",\"\u0644\u062d\u064a\"),\n (0xFD82,\"M\",\"\u0644\u062d\u0649\"),\n (0xFD83,\"M\",\"\u0644\u062c\u062c\"),\n (0xFD85,\"M\",\"\u0644\u062e\u0645\"),\n (0xFD87,\"M\",\"\u0644\u0645\u062d\"),\n (0xFD89,\"M\",\"\u0645\u062d\u062c\"),\n (0xFD8A,\"M\",\"\u0645\u062d\u0645\"),\n (0xFD8B,\"M\",\"\u0645\u062d\u064a\"),\n (0xFD8C,\"M\",\"\u0645\u062c\u062d\"),\n (0xFD8D,\"M\",\"\u0645\u062c\u0645\"),\n (0xFD8E,\"M\",\"\u0645\u062e\u062c\"),\n (0xFD8F,\"M\",\"\u0645\u062e\u0645\"),\n (0xFD90,\"X\"),\n (0xFD92,\"M\",\"\u0645\u062c\u062e\"),\n (0xFD93,\"M\",\"\u0647\u0645\u062c\"),\n (0xFD94,\"M\",\"\u0647\u0645\u0645\"),\n (0xFD95,\"M\",\"\u0646\u062d\u0645\"),\n (0xFD96,\"M\",\"\u0646\u062d\u0649\"),\n (0xFD97,\"M\",\"\u0646\u062c\u0645\"),\n (0xFD99,\"M\",\"\u0646\u062c\u0649\"),\n (0xFD9A,\"M\",\"\u0646\u0645\u064a\"),\n (0xFD9B,\"M\",\"\u0646\u0645\u0649\"),\n (0xFD9C,\"M\",\"\u064a\u0645\u0645\"),\n (0xFD9E,\"M\",\"\u0628\u062e\u064a\"),\n (0xFD9F,\"M\",\"\u062a\u062c\u064a\"),\n (0xFDA0,\"M\",\"\u062a\u062c\u0649\"),\n (0xFDA1,\"M\",\"\u062a\u062e\u064a\"),\n (0xFDA2,\"M\",\"\u062a\u062e\u0649\"),\n (0xFDA3,\"M\",\"\u062a\u0645\u064a\"),\n (0xFDA4,\"M\",\"\u062a\u0645\u0649\"),\n (0xFDA5,\"M\",\"\u062c\u0645\u064a\"),\n (0xFDA6,\"M\",\"\u062c\u062d\u0649\"),\n (0xFDA7,\"M\",\"\u062c\u0645\u0649\"),\n (0xFDA8,\"M\",\"\u0633\u062e\u0649\"),\n (0xFDA9,\"M\",\"\u0635\u062d\u064a\"),\n (0xFDAA,\"M\",\"\u0634\u062d\u064a\"),\n ]\n \n \ndef _seg_49()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFDAB,\"M\",\"\u0636\u062d\u064a\"),\n (0xFDAC,\"M\",\"\u0644\u062c\u064a\"),\n (0xFDAD,\"M\",\"\u0644\u0645\u064a\"),\n (0xFDAE,\"M\",\"\u064a\u062d\u064a\"),\n (0xFDAF,\"M\",\"\u064a\u062c\u064a\"),\n (0xFDB0,\"M\",\"\u064a\u0645\u064a\"),\n (0xFDB1,\"M\",\"\u0645\u0645\u064a\"),\n (0xFDB2,\"M\",\"\u0642\u0645\u064a\"),\n (0xFDB3,\"M\",\"\u0646\u062d\u064a\"),\n (0xFDB4,\"M\",\"\u0642\u0645\u062d\"),\n (0xFDB5,\"M\",\"\u0644\u062d\u0645\"),\n (0xFDB6,\"M\",\"\u0639\u0645\u064a\"),\n (0xFDB7,\"M\",\"\u0643\u0645\u064a\"),\n (0xFDB8,\"M\",\"\u0646\u062c\u062d\"),\n (0xFDB9,\"M\",\"\u0645\u062e\u064a\"),\n (0xFDBA,\"M\",\"\u0644\u062c\u0645\"),\n (0xFDBB,\"M\",\"\u0643\u0645\u0645\"),\n (0xFDBC,\"M\",\"\u0644\u062c\u0645\"),\n (0xFDBD,\"M\",\"\u0646\u062c\u062d\"),\n (0xFDBE,\"M\",\"\u062c\u062d\u064a\"),\n (0xFDBF,\"M\",\"\u062d\u062c\u064a\"),\n (0xFDC0,\"M\",\"\u0645\u062c\u064a\"),\n (0xFDC1,\"M\",\"\u0641\u0645\u064a\"),\n (0xFDC2,\"M\",\"\u0628\u062d\u064a\"),\n (0xFDC3,\"M\",\"\u0643\u0645\u0645\"),\n (0xFDC4,\"M\",\"\u0639\u062c\u0645\"),\n (0xFDC5,\"M\",\"\u0635\u0645\u0645\"),\n (0xFDC6,\"M\",\"\u0633\u062e\u064a\"),\n (0xFDC7,\"M\",\"\u0646\u062c\u064a\"),\n (0xFDC8,\"X\"),\n (0xFDCF,\"V\"),\n (0xFDD0,\"X\"),\n (0xFDF0,\"M\",\"\u0635\u0644\u06d2\"),\n (0xFDF1,\"M\",\"\u0642\u0644\u06d2\"),\n (0xFDF2,\"M\",\"\u0627\u0644\u0644\u0647\"),\n (0xFDF3,\"M\",\"\u0627\u0643\u0628\u0631\"),\n (0xFDF4,\"M\",\"\u0645\u062d\u0645\u062f\"),\n (0xFDF5,\"M\",\"\u0635\u0644\u0639\u0645\"),\n (0xFDF6,\"M\",\"\u0631\u0633\u0648\u0644\"),\n (0xFDF7,\"M\",\"\u0639\u0644\u064a\u0647\"),\n (0xFDF8,\"M\",\"\u0648\u0633\u0644\u0645\"),\n (0xFDF9,\"M\",\"\u0635\u0644\u0649\"),\n (0xFDFA,\"3\",\"\u0635\u0644\u0649 \u0627\u0644\u0644\u0647 \u0639\u0644\u064a\u0647 \u0648\u0633\u0644\u0645\"),\n (0xFDFB,\"3\",\"\u062c\u0644 \u062c\u0644\u0627\u0644\u0647\"),\n (0xFDFC,\"M\",\"\u0631\u06cc\u0627\u0644\"),\n (0xFDFD,\"V\"),\n (0xFE00,\"I\"),\n (0xFE10,\"3\",\",\"),\n (0xFE11,\"M\",\"\u3001\"),\n (0xFE12,\"X\"),\n (0xFE13,\"3\",\":\"),\n (0xFE14,\"3\",\";\"),\n (0xFE15,\"3\",\"!\"),\n (0xFE16,\"3\",\"?\"),\n (0xFE17,\"M\",\"\u3016\"),\n (0xFE18,\"M\",\"\u3017\"),\n (0xFE19,\"X\"),\n (0xFE20,\"V\"),\n (0xFE30,\"X\"),\n (0xFE31,\"M\",\"\u2014\"),\n (0xFE32,\"M\",\"\u2013\"),\n (0xFE33,\"3\",\"_\"),\n (0xFE35,\"3\",\"(\"),\n (0xFE36,\"3\",\")\"),\n (0xFE37,\"3\",\"{\"),\n (0xFE38,\"3\",\"}\"),\n (0xFE39,\"M\",\"\u3014\"),\n (0xFE3A,\"M\",\"\u3015\"),\n (0xFE3B,\"M\",\"\u3010\"),\n (0xFE3C,\"M\",\"\u3011\"),\n (0xFE3D,\"M\",\"\u300a\"),\n (0xFE3E,\"M\",\"\u300b\"),\n (0xFE3F,\"M\",\"\u3008\"),\n (0xFE40,\"M\",\"\u3009\"),\n (0xFE41,\"M\",\"\u300c\"),\n (0xFE42,\"M\",\"\u300d\"),\n (0xFE43,\"M\",\"\u300e\"),\n (0xFE44,\"M\",\"\u300f\"),\n (0xFE45,\"V\"),\n (0xFE47,\"3\",\"[\"),\n (0xFE48,\"3\",\"]\"),\n (0xFE49,\"3\",\" \u0305\"),\n (0xFE4D,\"3\",\"_\"),\n (0xFE50,\"3\",\",\"),\n (0xFE51,\"M\",\"\u3001\"),\n (0xFE52,\"X\"),\n (0xFE54,\"3\",\";\"),\n (0xFE55,\"3\",\":\"),\n (0xFE56,\"3\",\"?\"),\n (0xFE57,\"3\",\"!\"),\n (0xFE58,\"M\",\"\u2014\"),\n (0xFE59,\"3\",\"(\"),\n (0xFE5A,\"3\",\")\"),\n (0xFE5B,\"3\",\"{\"),\n (0xFE5C,\"3\",\"}\"),\n (0xFE5D,\"M\",\"\u3014\"),\n (0xFE5E,\"M\",\"\u3015\"),\n (0xFE5F,\"3\",\"#\"),\n (0xFE60,\"3\",\"&\"),\n (0xFE61,\"3\",\"*\"),\n ]\n \n \ndef _seg_50()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFE62,\"3\",\"+\"),\n (0xFE63,\"M\",\"-\"),\n (0xFE64,\"3\",\"<\"),\n (0xFE65,\"3\",\">\"),\n (0xFE66,\"3\",\"=\"),\n (0xFE67,\"X\"),\n (0xFE68,\"3\",\"\\\\\"),\n (0xFE69,\"3\",\"$\"),\n (0xFE6A,\"3\",\"%\"),\n (0xFE6B,\"3\",\"@\"),\n (0xFE6C,\"X\"),\n (0xFE70,\"3\",\" \u064b\"),\n (0xFE71,\"M\",\"\u0640\u064b\"),\n (0xFE72,\"3\",\" \u064c\"),\n (0xFE73,\"V\"),\n (0xFE74,\"3\",\" \u064d\"),\n (0xFE75,\"X\"),\n (0xFE76,\"3\",\" \u064e\"),\n (0xFE77,\"M\",\"\u0640\u064e\"),\n (0xFE78,\"3\",\" \u064f\"),\n (0xFE79,\"M\",\"\u0640\u064f\"),\n (0xFE7A,\"3\",\" \u0650\"),\n (0xFE7B,\"M\",\"\u0640\u0650\"),\n (0xFE7C,\"3\",\" \u0651\"),\n (0xFE7D,\"M\",\"\u0640\u0651\"),\n (0xFE7E,\"3\",\" \u0652\"),\n (0xFE7F,\"M\",\"\u0640\u0652\"),\n (0xFE80,\"M\",\"\u0621\"),\n (0xFE81,\"M\",\"\u0622\"),\n (0xFE83,\"M\",\"\u0623\"),\n (0xFE85,\"M\",\"\u0624\"),\n (0xFE87,\"M\",\"\u0625\"),\n (0xFE89,\"M\",\"\u0626\"),\n (0xFE8D,\"M\",\"\u0627\"),\n (0xFE8F,\"M\",\"\u0628\"),\n (0xFE93,\"M\",\"\u0629\"),\n (0xFE95,\"M\",\"\u062a\"),\n (0xFE99,\"M\",\"\u062b\"),\n (0xFE9D,\"M\",\"\u062c\"),\n (0xFEA1,\"M\",\"\u062d\"),\n (0xFEA5,\"M\",\"\u062e\"),\n (0xFEA9,\"M\",\"\u062f\"),\n (0xFEAB,\"M\",\"\u0630\"),\n (0xFEAD,\"M\",\"\u0631\"),\n (0xFEAF,\"M\",\"\u0632\"),\n (0xFEB1,\"M\",\"\u0633\"),\n (0xFEB5,\"M\",\"\u0634\"),\n (0xFEB9,\"M\",\"\u0635\"),\n (0xFEBD,\"M\",\"\u0636\"),\n (0xFEC1,\"M\",\"\u0637\"),\n (0xFEC5,\"M\",\"\u0638\"),\n (0xFEC9,\"M\",\"\u0639\"),\n (0xFECD,\"M\",\"\u063a\"),\n (0xFED1,\"M\",\"\u0641\"),\n (0xFED5,\"M\",\"\u0642\"),\n (0xFED9,\"M\",\"\u0643\"),\n (0xFEDD,\"M\",\"\u0644\"),\n (0xFEE1,\"M\",\"\u0645\"),\n (0xFEE5,\"M\",\"\u0646\"),\n (0xFEE9,\"M\",\"\u0647\"),\n (0xFEED,\"M\",\"\u0648\"),\n (0xFEEF,\"M\",\"\u0649\"),\n (0xFEF1,\"M\",\"\u064a\"),\n (0xFEF5,\"M\",\"\u0644\u0622\"),\n (0xFEF7,\"M\",\"\u0644\u0623\"),\n (0xFEF9,\"M\",\"\u0644\u0625\"),\n (0xFEFB,\"M\",\"\u0644\u0627\"),\n (0xFEFD,\"X\"),\n (0xFEFF,\"I\"),\n (0xFF00,\"X\"),\n (0xFF01,\"3\",\"!\"),\n (0xFF02,\"3\",'\"'),\n (0xFF03,\"3\",\"#\"),\n (0xFF04,\"3\",\"$\"),\n (0xFF05,\"3\",\"%\"),\n (0xFF06,\"3\",\"&\"),\n (0xFF07,\"3\",\"'\"),\n (0xFF08,\"3\",\"(\"),\n (0xFF09,\"3\",\")\"),\n (0xFF0A,\"3\",\"*\"),\n (0xFF0B,\"3\",\"+\"),\n (0xFF0C,\"3\",\",\"),\n (0xFF0D,\"M\",\"-\"),\n (0xFF0E,\"M\",\".\"),\n (0xFF0F,\"3\",\"/\"),\n (0xFF10,\"M\",\"0\"),\n (0xFF11,\"M\",\"1\"),\n (0xFF12,\"M\",\"2\"),\n (0xFF13,\"M\",\"3\"),\n (0xFF14,\"M\",\"4\"),\n (0xFF15,\"M\",\"5\"),\n (0xFF16,\"M\",\"6\"),\n (0xFF17,\"M\",\"7\"),\n (0xFF18,\"M\",\"8\"),\n (0xFF19,\"M\",\"9\"),\n (0xFF1A,\"3\",\":\"),\n (0xFF1B,\"3\",\";\"),\n (0xFF1C,\"3\",\"<\"),\n (0xFF1D,\"3\",\"=\"),\n (0xFF1E,\"3\",\">\"),\n ]\n \n \ndef _seg_51()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFF1F,\"3\",\"?\"),\n (0xFF20,\"3\",\"@\"),\n (0xFF21,\"M\",\"a\"),\n (0xFF22,\"M\",\"b\"),\n (0xFF23,\"M\",\"c\"),\n (0xFF24,\"M\",\"d\"),\n (0xFF25,\"M\",\"e\"),\n (0xFF26,\"M\",\"f\"),\n (0xFF27,\"M\",\"g\"),\n (0xFF28,\"M\",\"h\"),\n (0xFF29,\"M\",\"i\"),\n (0xFF2A,\"M\",\"j\"),\n (0xFF2B,\"M\",\"k\"),\n (0xFF2C,\"M\",\"l\"),\n (0xFF2D,\"M\",\"m\"),\n (0xFF2E,\"M\",\"n\"),\n (0xFF2F,\"M\",\"o\"),\n (0xFF30,\"M\",\"p\"),\n (0xFF31,\"M\",\"q\"),\n (0xFF32,\"M\",\"r\"),\n (0xFF33,\"M\",\"s\"),\n (0xFF34,\"M\",\"t\"),\n (0xFF35,\"M\",\"u\"),\n (0xFF36,\"M\",\"v\"),\n (0xFF37,\"M\",\"w\"),\n (0xFF38,\"M\",\"x\"),\n (0xFF39,\"M\",\"y\"),\n (0xFF3A,\"M\",\"z\"),\n (0xFF3B,\"3\",\"[\"),\n (0xFF3C,\"3\",\"\\\\\"),\n (0xFF3D,\"3\",\"]\"),\n (0xFF3E,\"3\",\"^\"),\n (0xFF3F,\"3\",\"_\"),\n (0xFF40,\"3\",\"`\"),\n (0xFF41,\"M\",\"a\"),\n (0xFF42,\"M\",\"b\"),\n (0xFF43,\"M\",\"c\"),\n (0xFF44,\"M\",\"d\"),\n (0xFF45,\"M\",\"e\"),\n (0xFF46,\"M\",\"f\"),\n (0xFF47,\"M\",\"g\"),\n (0xFF48,\"M\",\"h\"),\n (0xFF49,\"M\",\"i\"),\n (0xFF4A,\"M\",\"j\"),\n (0xFF4B,\"M\",\"k\"),\n (0xFF4C,\"M\",\"l\"),\n (0xFF4D,\"M\",\"m\"),\n (0xFF4E,\"M\",\"n\"),\n (0xFF4F,\"M\",\"o\"),\n (0xFF50,\"M\",\"p\"),\n (0xFF51,\"M\",\"q\"),\n (0xFF52,\"M\",\"r\"),\n (0xFF53,\"M\",\"s\"),\n (0xFF54,\"M\",\"t\"),\n (0xFF55,\"M\",\"u\"),\n (0xFF56,\"M\",\"v\"),\n (0xFF57,\"M\",\"w\"),\n (0xFF58,\"M\",\"x\"),\n (0xFF59,\"M\",\"y\"),\n (0xFF5A,\"M\",\"z\"),\n (0xFF5B,\"3\",\"{\"),\n (0xFF5C,\"3\",\"|\"),\n (0xFF5D,\"3\",\"}\"),\n (0xFF5E,\"3\",\"~\"),\n (0xFF5F,\"M\",\"\u2985\"),\n (0xFF60,\"M\",\"\u2986\"),\n (0xFF61,\"M\",\".\"),\n (0xFF62,\"M\",\"\u300c\"),\n (0xFF63,\"M\",\"\u300d\"),\n (0xFF64,\"M\",\"\u3001\"),\n (0xFF65,\"M\",\"\u30fb\"),\n (0xFF66,\"M\",\"\u30f2\"),\n (0xFF67,\"M\",\"\u30a1\"),\n (0xFF68,\"M\",\"\u30a3\"),\n (0xFF69,\"M\",\"\u30a5\"),\n (0xFF6A,\"M\",\"\u30a7\"),\n (0xFF6B,\"M\",\"\u30a9\"),\n (0xFF6C,\"M\",\"\u30e3\"),\n (0xFF6D,\"M\",\"\u30e5\"),\n (0xFF6E,\"M\",\"\u30e7\"),\n (0xFF6F,\"M\",\"\u30c3\"),\n (0xFF70,\"M\",\"\u30fc\"),\n (0xFF71,\"M\",\"\u30a2\"),\n (0xFF72,\"M\",\"\u30a4\"),\n (0xFF73,\"M\",\"\u30a6\"),\n (0xFF74,\"M\",\"\u30a8\"),\n (0xFF75,\"M\",\"\u30aa\"),\n (0xFF76,\"M\",\"\u30ab\"),\n (0xFF77,\"M\",\"\u30ad\"),\n (0xFF78,\"M\",\"\u30af\"),\n (0xFF79,\"M\",\"\u30b1\"),\n (0xFF7A,\"M\",\"\u30b3\"),\n (0xFF7B,\"M\",\"\u30b5\"),\n (0xFF7C,\"M\",\"\u30b7\"),\n (0xFF7D,\"M\",\"\u30b9\"),\n (0xFF7E,\"M\",\"\u30bb\"),\n (0xFF7F,\"M\",\"\u30bd\"),\n (0xFF80,\"M\",\"\u30bf\"),\n (0xFF81,\"M\",\"\u30c1\"),\n (0xFF82,\"M\",\"\u30c4\"),\n ]\n \n \ndef _seg_52()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFF83,\"M\",\"\u30c6\"),\n (0xFF84,\"M\",\"\u30c8\"),\n (0xFF85,\"M\",\"\u30ca\"),\n (0xFF86,\"M\",\"\u30cb\"),\n (0xFF87,\"M\",\"\u30cc\"),\n (0xFF88,\"M\",\"\u30cd\"),\n (0xFF89,\"M\",\"\u30ce\"),\n (0xFF8A,\"M\",\"\u30cf\"),\n (0xFF8B,\"M\",\"\u30d2\"),\n (0xFF8C,\"M\",\"\u30d5\"),\n (0xFF8D,\"M\",\"\u30d8\"),\n (0xFF8E,\"M\",\"\u30db\"),\n (0xFF8F,\"M\",\"\u30de\"),\n (0xFF90,\"M\",\"\u30df\"),\n (0xFF91,\"M\",\"\u30e0\"),\n (0xFF92,\"M\",\"\u30e1\"),\n (0xFF93,\"M\",\"\u30e2\"),\n (0xFF94,\"M\",\"\u30e4\"),\n (0xFF95,\"M\",\"\u30e6\"),\n (0xFF96,\"M\",\"\u30e8\"),\n (0xFF97,\"M\",\"\u30e9\"),\n (0xFF98,\"M\",\"\u30ea\"),\n (0xFF99,\"M\",\"\u30eb\"),\n (0xFF9A,\"M\",\"\u30ec\"),\n (0xFF9B,\"M\",\"\u30ed\"),\n (0xFF9C,\"M\",\"\u30ef\"),\n (0xFF9D,\"M\",\"\u30f3\"),\n (0xFF9E,\"M\",\"\u3099\"),\n (0xFF9F,\"M\",\"\u309a\"),\n (0xFFA0,\"X\"),\n (0xFFA1,\"M\",\"\u1100\"),\n (0xFFA2,\"M\",\"\u1101\"),\n (0xFFA3,\"M\",\"\u11aa\"),\n (0xFFA4,\"M\",\"\u1102\"),\n (0xFFA5,\"M\",\"\u11ac\"),\n (0xFFA6,\"M\",\"\u11ad\"),\n (0xFFA7,\"M\",\"\u1103\"),\n (0xFFA8,\"M\",\"\u1104\"),\n (0xFFA9,\"M\",\"\u1105\"),\n (0xFFAA,\"M\",\"\u11b0\"),\n (0xFFAB,\"M\",\"\u11b1\"),\n (0xFFAC,\"M\",\"\u11b2\"),\n (0xFFAD,\"M\",\"\u11b3\"),\n (0xFFAE,\"M\",\"\u11b4\"),\n (0xFFAF,\"M\",\"\u11b5\"),\n (0xFFB0,\"M\",\"\u111a\"),\n (0xFFB1,\"M\",\"\u1106\"),\n (0xFFB2,\"M\",\"\u1107\"),\n (0xFFB3,\"M\",\"\u1108\"),\n (0xFFB4,\"M\",\"\u1121\"),\n (0xFFB5,\"M\",\"\u1109\"),\n (0xFFB6,\"M\",\"\u110a\"),\n (0xFFB7,\"M\",\"\u110b\"),\n (0xFFB8,\"M\",\"\u110c\"),\n (0xFFB9,\"M\",\"\u110d\"),\n (0xFFBA,\"M\",\"\u110e\"),\n (0xFFBB,\"M\",\"\u110f\"),\n (0xFFBC,\"M\",\"\u1110\"),\n (0xFFBD,\"M\",\"\u1111\"),\n (0xFFBE,\"M\",\"\u1112\"),\n (0xFFBF,\"X\"),\n (0xFFC2,\"M\",\"\u1161\"),\n (0xFFC3,\"M\",\"\u1162\"),\n (0xFFC4,\"M\",\"\u1163\"),\n (0xFFC5,\"M\",\"\u1164\"),\n (0xFFC6,\"M\",\"\u1165\"),\n (0xFFC7,\"M\",\"\u1166\"),\n (0xFFC8,\"X\"),\n (0xFFCA,\"M\",\"\u1167\"),\n (0xFFCB,\"M\",\"\u1168\"),\n (0xFFCC,\"M\",\"\u1169\"),\n (0xFFCD,\"M\",\"\u116a\"),\n (0xFFCE,\"M\",\"\u116b\"),\n (0xFFCF,\"M\",\"\u116c\"),\n (0xFFD0,\"X\"),\n (0xFFD2,\"M\",\"\u116d\"),\n (0xFFD3,\"M\",\"\u116e\"),\n (0xFFD4,\"M\",\"\u116f\"),\n (0xFFD5,\"M\",\"\u1170\"),\n (0xFFD6,\"M\",\"\u1171\"),\n (0xFFD7,\"M\",\"\u1172\"),\n (0xFFD8,\"X\"),\n (0xFFDA,\"M\",\"\u1173\"),\n (0xFFDB,\"M\",\"\u1174\"),\n (0xFFDC,\"M\",\"\u1175\"),\n (0xFFDD,\"X\"),\n (0xFFE0,\"M\",\"\u00a2\"),\n (0xFFE1,\"M\",\"\u00a3\"),\n (0xFFE2,\"M\",\"\u00ac\"),\n (0xFFE3,\"3\",\" \u0304\"),\n (0xFFE4,\"M\",\"\u00a6\"),\n (0xFFE5,\"M\",\"\u00a5\"),\n (0xFFE6,\"M\",\"\u20a9\"),\n (0xFFE7,\"X\"),\n (0xFFE8,\"M\",\"\u2502\"),\n (0xFFE9,\"M\",\"\u2190\"),\n (0xFFEA,\"M\",\"\u2191\"),\n (0xFFEB,\"M\",\"\u2192\"),\n (0xFFEC,\"M\",\"\u2193\"),\n (0xFFED,\"M\",\"\u25a0\"),\n ]\n \n \ndef _seg_53()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0xFFEE,\"M\",\"\u25cb\"),\n (0xFFEF,\"X\"),\n (0x10000,\"V\"),\n (0x1000C,\"X\"),\n (0x1000D,\"V\"),\n (0x10027,\"X\"),\n (0x10028,\"V\"),\n (0x1003B,\"X\"),\n (0x1003C,\"V\"),\n (0x1003E,\"X\"),\n (0x1003F,\"V\"),\n (0x1004E,\"X\"),\n (0x10050,\"V\"),\n (0x1005E,\"X\"),\n (0x10080,\"V\"),\n (0x100FB,\"X\"),\n (0x10100,\"V\"),\n (0x10103,\"X\"),\n (0x10107,\"V\"),\n (0x10134,\"X\"),\n (0x10137,\"V\"),\n (0x1018F,\"X\"),\n (0x10190,\"V\"),\n (0x1019D,\"X\"),\n (0x101A0,\"V\"),\n (0x101A1,\"X\"),\n (0x101D0,\"V\"),\n (0x101FE,\"X\"),\n (0x10280,\"V\"),\n (0x1029D,\"X\"),\n (0x102A0,\"V\"),\n (0x102D1,\"X\"),\n (0x102E0,\"V\"),\n (0x102FC,\"X\"),\n (0x10300,\"V\"),\n (0x10324,\"X\"),\n (0x1032D,\"V\"),\n (0x1034B,\"X\"),\n (0x10350,\"V\"),\n (0x1037B,\"X\"),\n (0x10380,\"V\"),\n (0x1039E,\"X\"),\n (0x1039F,\"V\"),\n (0x103C4,\"X\"),\n (0x103C8,\"V\"),\n (0x103D6,\"X\"),\n (0x10400,\"M\",\"\ud801\udc28\"),\n (0x10401,\"M\",\"\ud801\udc29\"),\n (0x10402,\"M\",\"\ud801\udc2a\"),\n (0x10403,\"M\",\"\ud801\udc2b\"),\n (0x10404,\"M\",\"\ud801\udc2c\"),\n (0x10405,\"M\",\"\ud801\udc2d\"),\n (0x10406,\"M\",\"\ud801\udc2e\"),\n (0x10407,\"M\",\"\ud801\udc2f\"),\n (0x10408,\"M\",\"\ud801\udc30\"),\n (0x10409,\"M\",\"\ud801\udc31\"),\n (0x1040A,\"M\",\"\ud801\udc32\"),\n (0x1040B,\"M\",\"\ud801\udc33\"),\n (0x1040C,\"M\",\"\ud801\udc34\"),\n (0x1040D,\"M\",\"\ud801\udc35\"),\n (0x1040E,\"M\",\"\ud801\udc36\"),\n (0x1040F,\"M\",\"\ud801\udc37\"),\n (0x10410,\"M\",\"\ud801\udc38\"),\n (0x10411,\"M\",\"\ud801\udc39\"),\n (0x10412,\"M\",\"\ud801\udc3a\"),\n (0x10413,\"M\",\"\ud801\udc3b\"),\n (0x10414,\"M\",\"\ud801\udc3c\"),\n (0x10415,\"M\",\"\ud801\udc3d\"),\n (0x10416,\"M\",\"\ud801\udc3e\"),\n (0x10417,\"M\",\"\ud801\udc3f\"),\n (0x10418,\"M\",\"\ud801\udc40\"),\n (0x10419,\"M\",\"\ud801\udc41\"),\n (0x1041A,\"M\",\"\ud801\udc42\"),\n (0x1041B,\"M\",\"\ud801\udc43\"),\n (0x1041C,\"M\",\"\ud801\udc44\"),\n (0x1041D,\"M\",\"\ud801\udc45\"),\n (0x1041E,\"M\",\"\ud801\udc46\"),\n (0x1041F,\"M\",\"\ud801\udc47\"),\n (0x10420,\"M\",\"\ud801\udc48\"),\n (0x10421,\"M\",\"\ud801\udc49\"),\n (0x10422,\"M\",\"\ud801\udc4a\"),\n (0x10423,\"M\",\"\ud801\udc4b\"),\n (0x10424,\"M\",\"\ud801\udc4c\"),\n (0x10425,\"M\",\"\ud801\udc4d\"),\n (0x10426,\"M\",\"\ud801\udc4e\"),\n (0x10427,\"M\",\"\ud801\udc4f\"),\n (0x10428,\"V\"),\n (0x1049E,\"X\"),\n (0x104A0,\"V\"),\n (0x104AA,\"X\"),\n (0x104B0,\"M\",\"\ud801\udcd8\"),\n (0x104B1,\"M\",\"\ud801\udcd9\"),\n (0x104B2,\"M\",\"\ud801\udcda\"),\n (0x104B3,\"M\",\"\ud801\udcdb\"),\n (0x104B4,\"M\",\"\ud801\udcdc\"),\n (0x104B5,\"M\",\"\ud801\udcdd\"),\n (0x104B6,\"M\",\"\ud801\udcde\"),\n (0x104B7,\"M\",\"\ud801\udcdf\"),\n (0x104B8,\"M\",\"\ud801\udce0\"),\n (0x104B9,\"M\",\"\ud801\udce1\"),\n ]\n \n \ndef _seg_54()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x104BA,\"M\",\"\ud801\udce2\"),\n (0x104BB,\"M\",\"\ud801\udce3\"),\n (0x104BC,\"M\",\"\ud801\udce4\"),\n (0x104BD,\"M\",\"\ud801\udce5\"),\n (0x104BE,\"M\",\"\ud801\udce6\"),\n (0x104BF,\"M\",\"\ud801\udce7\"),\n (0x104C0,\"M\",\"\ud801\udce8\"),\n (0x104C1,\"M\",\"\ud801\udce9\"),\n (0x104C2,\"M\",\"\ud801\udcea\"),\n (0x104C3,\"M\",\"\ud801\udceb\"),\n (0x104C4,\"M\",\"\ud801\udcec\"),\n (0x104C5,\"M\",\"\ud801\udced\"),\n (0x104C6,\"M\",\"\ud801\udcee\"),\n (0x104C7,\"M\",\"\ud801\udcef\"),\n (0x104C8,\"M\",\"\ud801\udcf0\"),\n (0x104C9,\"M\",\"\ud801\udcf1\"),\n (0x104CA,\"M\",\"\ud801\udcf2\"),\n (0x104CB,\"M\",\"\ud801\udcf3\"),\n (0x104CC,\"M\",\"\ud801\udcf4\"),\n (0x104CD,\"M\",\"\ud801\udcf5\"),\n (0x104CE,\"M\",\"\ud801\udcf6\"),\n (0x104CF,\"M\",\"\ud801\udcf7\"),\n (0x104D0,\"M\",\"\ud801\udcf8\"),\n (0x104D1,\"M\",\"\ud801\udcf9\"),\n (0x104D2,\"M\",\"\ud801\udcfa\"),\n (0x104D3,\"M\",\"\ud801\udcfb\"),\n (0x104D4,\"X\"),\n (0x104D8,\"V\"),\n (0x104FC,\"X\"),\n (0x10500,\"V\"),\n (0x10528,\"X\"),\n (0x10530,\"V\"),\n (0x10564,\"X\"),\n (0x1056F,\"V\"),\n (0x10570,\"M\",\"\ud801\udd97\"),\n (0x10571,\"M\",\"\ud801\udd98\"),\n (0x10572,\"M\",\"\ud801\udd99\"),\n (0x10573,\"M\",\"\ud801\udd9a\"),\n (0x10574,\"M\",\"\ud801\udd9b\"),\n (0x10575,\"M\",\"\ud801\udd9c\"),\n (0x10576,\"M\",\"\ud801\udd9d\"),\n (0x10577,\"M\",\"\ud801\udd9e\"),\n (0x10578,\"M\",\"\ud801\udd9f\"),\n (0x10579,\"M\",\"\ud801\udda0\"),\n (0x1057A,\"M\",\"\ud801\udda1\"),\n (0x1057B,\"X\"),\n (0x1057C,\"M\",\"\ud801\udda3\"),\n (0x1057D,\"M\",\"\ud801\udda4\"),\n (0x1057E,\"M\",\"\ud801\udda5\"),\n (0x1057F,\"M\",\"\ud801\udda6\"),\n (0x10580,\"M\",\"\ud801\udda7\"),\n (0x10581,\"M\",\"\ud801\udda8\"),\n (0x10582,\"M\",\"\ud801\udda9\"),\n (0x10583,\"M\",\"\ud801\uddaa\"),\n (0x10584,\"M\",\"\ud801\uddab\"),\n (0x10585,\"M\",\"\ud801\uddac\"),\n (0x10586,\"M\",\"\ud801\uddad\"),\n (0x10587,\"M\",\"\ud801\uddae\"),\n (0x10588,\"M\",\"\ud801\uddaf\"),\n (0x10589,\"M\",\"\ud801\uddb0\"),\n (0x1058A,\"M\",\"\ud801\uddb1\"),\n (0x1058B,\"X\"),\n (0x1058C,\"M\",\"\ud801\uddb3\"),\n (0x1058D,\"M\",\"\ud801\uddb4\"),\n (0x1058E,\"M\",\"\ud801\uddb5\"),\n (0x1058F,\"M\",\"\ud801\uddb6\"),\n (0x10590,\"M\",\"\ud801\uddb7\"),\n (0x10591,\"M\",\"\ud801\uddb8\"),\n (0x10592,\"M\",\"\ud801\uddb9\"),\n (0x10593,\"X\"),\n (0x10594,\"M\",\"\ud801\uddbb\"),\n (0x10595,\"M\",\"\ud801\uddbc\"),\n (0x10596,\"X\"),\n (0x10597,\"V\"),\n (0x105A2,\"X\"),\n (0x105A3,\"V\"),\n (0x105B2,\"X\"),\n (0x105B3,\"V\"),\n (0x105BA,\"X\"),\n (0x105BB,\"V\"),\n (0x105BD,\"X\"),\n (0x10600,\"V\"),\n (0x10737,\"X\"),\n (0x10740,\"V\"),\n (0x10756,\"X\"),\n (0x10760,\"V\"),\n (0x10768,\"X\"),\n (0x10780,\"V\"),\n (0x10781,\"M\",\"\u02d0\"),\n (0x10782,\"M\",\"\u02d1\"),\n (0x10783,\"M\",\"\u00e6\"),\n (0x10784,\"M\",\"\u0299\"),\n (0x10785,\"M\",\"\u0253\"),\n (0x10786,\"X\"),\n (0x10787,\"M\",\"\u02a3\"),\n (0x10788,\"M\",\"\uab66\"),\n (0x10789,\"M\",\"\u02a5\"),\n (0x1078A,\"M\",\"\u02a4\"),\n (0x1078B,\"M\",\"\u0256\"),\n (0x1078C,\"M\",\"\u0257\"),\n ]\n \n \ndef _seg_55()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1078D,\"M\",\"\u1d91\"),\n (0x1078E,\"M\",\"\u0258\"),\n (0x1078F,\"M\",\"\u025e\"),\n (0x10790,\"M\",\"\u02a9\"),\n (0x10791,\"M\",\"\u0264\"),\n (0x10792,\"M\",\"\u0262\"),\n (0x10793,\"M\",\"\u0260\"),\n (0x10794,\"M\",\"\u029b\"),\n (0x10795,\"M\",\"\u0127\"),\n (0x10796,\"M\",\"\u029c\"),\n (0x10797,\"M\",\"\u0267\"),\n (0x10798,\"M\",\"\u0284\"),\n (0x10799,\"M\",\"\u02aa\"),\n (0x1079A,\"M\",\"\u02ab\"),\n (0x1079B,\"M\",\"\u026c\"),\n (0x1079C,\"M\",\"\ud837\udf04\"),\n (0x1079D,\"M\",\"\ua78e\"),\n (0x1079E,\"M\",\"\u026e\"),\n (0x1079F,\"M\",\"\ud837\udf05\"),\n (0x107A0,\"M\",\"\u028e\"),\n (0x107A1,\"M\",\"\ud837\udf06\"),\n (0x107A2,\"M\",\"\u00f8\"),\n (0x107A3,\"M\",\"\u0276\"),\n (0x107A4,\"M\",\"\u0277\"),\n (0x107A5,\"M\",\"q\"),\n (0x107A6,\"M\",\"\u027a\"),\n (0x107A7,\"M\",\"\ud837\udf08\"),\n (0x107A8,\"M\",\"\u027d\"),\n (0x107A9,\"M\",\"\u027e\"),\n (0x107AA,\"M\",\"\u0280\"),\n (0x107AB,\"M\",\"\u02a8\"),\n (0x107AC,\"M\",\"\u02a6\"),\n (0x107AD,\"M\",\"\uab67\"),\n (0x107AE,\"M\",\"\u02a7\"),\n (0x107AF,\"M\",\"\u0288\"),\n (0x107B0,\"M\",\"\u2c71\"),\n (0x107B1,\"X\"),\n (0x107B2,\"M\",\"\u028f\"),\n (0x107B3,\"M\",\"\u02a1\"),\n (0x107B4,\"M\",\"\u02a2\"),\n (0x107B5,\"M\",\"\u0298\"),\n (0x107B6,\"M\",\"\u01c0\"),\n (0x107B7,\"M\",\"\u01c1\"),\n (0x107B8,\"M\",\"\u01c2\"),\n (0x107B9,\"M\",\"\ud837\udf0a\"),\n (0x107BA,\"M\",\"\ud837\udf1e\"),\n (0x107BB,\"X\"),\n (0x10800,\"V\"),\n (0x10806,\"X\"),\n (0x10808,\"V\"),\n (0x10809,\"X\"),\n (0x1080A,\"V\"),\n (0x10836,\"X\"),\n (0x10837,\"V\"),\n (0x10839,\"X\"),\n (0x1083C,\"V\"),\n (0x1083D,\"X\"),\n (0x1083F,\"V\"),\n (0x10856,\"X\"),\n (0x10857,\"V\"),\n (0x1089F,\"X\"),\n (0x108A7,\"V\"),\n (0x108B0,\"X\"),\n (0x108E0,\"V\"),\n (0x108F3,\"X\"),\n (0x108F4,\"V\"),\n (0x108F6,\"X\"),\n (0x108FB,\"V\"),\n (0x1091C,\"X\"),\n (0x1091F,\"V\"),\n (0x1093A,\"X\"),\n (0x1093F,\"V\"),\n (0x10940,\"X\"),\n (0x10980,\"V\"),\n (0x109B8,\"X\"),\n (0x109BC,\"V\"),\n (0x109D0,\"X\"),\n (0x109D2,\"V\"),\n (0x10A04,\"X\"),\n (0x10A05,\"V\"),\n (0x10A07,\"X\"),\n (0x10A0C,\"V\"),\n (0x10A14,\"X\"),\n (0x10A15,\"V\"),\n (0x10A18,\"X\"),\n (0x10A19,\"V\"),\n (0x10A36,\"X\"),\n (0x10A38,\"V\"),\n (0x10A3B,\"X\"),\n (0x10A3F,\"V\"),\n (0x10A49,\"X\"),\n (0x10A50,\"V\"),\n (0x10A59,\"X\"),\n (0x10A60,\"V\"),\n (0x10AA0,\"X\"),\n (0x10AC0,\"V\"),\n (0x10AE7,\"X\"),\n (0x10AEB,\"V\"),\n (0x10AF7,\"X\"),\n (0x10B00,\"V\"),\n ]\n \n \ndef _seg_56()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x10B36,\"X\"),\n (0x10B39,\"V\"),\n (0x10B56,\"X\"),\n (0x10B58,\"V\"),\n (0x10B73,\"X\"),\n (0x10B78,\"V\"),\n (0x10B92,\"X\"),\n (0x10B99,\"V\"),\n (0x10B9D,\"X\"),\n (0x10BA9,\"V\"),\n (0x10BB0,\"X\"),\n (0x10C00,\"V\"),\n (0x10C49,\"X\"),\n (0x10C80,\"M\",\"\ud803\udcc0\"),\n (0x10C81,\"M\",\"\ud803\udcc1\"),\n (0x10C82,\"M\",\"\ud803\udcc2\"),\n (0x10C83,\"M\",\"\ud803\udcc3\"),\n (0x10C84,\"M\",\"\ud803\udcc4\"),\n (0x10C85,\"M\",\"\ud803\udcc5\"),\n (0x10C86,\"M\",\"\ud803\udcc6\"),\n (0x10C87,\"M\",\"\ud803\udcc7\"),\n (0x10C88,\"M\",\"\ud803\udcc8\"),\n (0x10C89,\"M\",\"\ud803\udcc9\"),\n (0x10C8A,\"M\",\"\ud803\udcca\"),\n (0x10C8B,\"M\",\"\ud803\udccb\"),\n (0x10C8C,\"M\",\"\ud803\udccc\"),\n (0x10C8D,\"M\",\"\ud803\udccd\"),\n (0x10C8E,\"M\",\"\ud803\udcce\"),\n (0x10C8F,\"M\",\"\ud803\udccf\"),\n (0x10C90,\"M\",\"\ud803\udcd0\"),\n (0x10C91,\"M\",\"\ud803\udcd1\"),\n (0x10C92,\"M\",\"\ud803\udcd2\"),\n (0x10C93,\"M\",\"\ud803\udcd3\"),\n (0x10C94,\"M\",\"\ud803\udcd4\"),\n (0x10C95,\"M\",\"\ud803\udcd5\"),\n (0x10C96,\"M\",\"\ud803\udcd6\"),\n (0x10C97,\"M\",\"\ud803\udcd7\"),\n (0x10C98,\"M\",\"\ud803\udcd8\"),\n (0x10C99,\"M\",\"\ud803\udcd9\"),\n (0x10C9A,\"M\",\"\ud803\udcda\"),\n (0x10C9B,\"M\",\"\ud803\udcdb\"),\n (0x10C9C,\"M\",\"\ud803\udcdc\"),\n (0x10C9D,\"M\",\"\ud803\udcdd\"),\n (0x10C9E,\"M\",\"\ud803\udcde\"),\n (0x10C9F,\"M\",\"\ud803\udcdf\"),\n (0x10CA0,\"M\",\"\ud803\udce0\"),\n (0x10CA1,\"M\",\"\ud803\udce1\"),\n (0x10CA2,\"M\",\"\ud803\udce2\"),\n (0x10CA3,\"M\",\"\ud803\udce3\"),\n (0x10CA4,\"M\",\"\ud803\udce4\"),\n (0x10CA5,\"M\",\"\ud803\udce5\"),\n (0x10CA6,\"M\",\"\ud803\udce6\"),\n (0x10CA7,\"M\",\"\ud803\udce7\"),\n (0x10CA8,\"M\",\"\ud803\udce8\"),\n (0x10CA9,\"M\",\"\ud803\udce9\"),\n (0x10CAA,\"M\",\"\ud803\udcea\"),\n (0x10CAB,\"M\",\"\ud803\udceb\"),\n (0x10CAC,\"M\",\"\ud803\udcec\"),\n (0x10CAD,\"M\",\"\ud803\udced\"),\n (0x10CAE,\"M\",\"\ud803\udcee\"),\n (0x10CAF,\"M\",\"\ud803\udcef\"),\n (0x10CB0,\"M\",\"\ud803\udcf0\"),\n (0x10CB1,\"M\",\"\ud803\udcf1\"),\n (0x10CB2,\"M\",\"\ud803\udcf2\"),\n (0x10CB3,\"X\"),\n (0x10CC0,\"V\"),\n (0x10CF3,\"X\"),\n (0x10CFA,\"V\"),\n (0x10D28,\"X\"),\n (0x10D30,\"V\"),\n (0x10D3A,\"X\"),\n (0x10E60,\"V\"),\n (0x10E7F,\"X\"),\n (0x10E80,\"V\"),\n (0x10EAA,\"X\"),\n (0x10EAB,\"V\"),\n (0x10EAE,\"X\"),\n (0x10EB0,\"V\"),\n (0x10EB2,\"X\"),\n (0x10EFD,\"V\"),\n (0x10F28,\"X\"),\n (0x10F30,\"V\"),\n (0x10F5A,\"X\"),\n (0x10F70,\"V\"),\n (0x10F8A,\"X\"),\n (0x10FB0,\"V\"),\n (0x10FCC,\"X\"),\n (0x10FE0,\"V\"),\n (0x10FF7,\"X\"),\n (0x11000,\"V\"),\n (0x1104E,\"X\"),\n (0x11052,\"V\"),\n (0x11076,\"X\"),\n (0x1107F,\"V\"),\n (0x110BD,\"X\"),\n (0x110BE,\"V\"),\n (0x110C3,\"X\"),\n (0x110D0,\"V\"),\n (0x110E9,\"X\"),\n (0x110F0,\"V\"),\n ]\n \n \ndef _seg_57()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x110FA,\"X\"),\n (0x11100,\"V\"),\n (0x11135,\"X\"),\n (0x11136,\"V\"),\n (0x11148,\"X\"),\n (0x11150,\"V\"),\n (0x11177,\"X\"),\n (0x11180,\"V\"),\n (0x111E0,\"X\"),\n (0x111E1,\"V\"),\n (0x111F5,\"X\"),\n (0x11200,\"V\"),\n (0x11212,\"X\"),\n (0x11213,\"V\"),\n (0x11242,\"X\"),\n (0x11280,\"V\"),\n (0x11287,\"X\"),\n (0x11288,\"V\"),\n (0x11289,\"X\"),\n (0x1128A,\"V\"),\n (0x1128E,\"X\"),\n (0x1128F,\"V\"),\n (0x1129E,\"X\"),\n (0x1129F,\"V\"),\n (0x112AA,\"X\"),\n (0x112B0,\"V\"),\n (0x112EB,\"X\"),\n (0x112F0,\"V\"),\n (0x112FA,\"X\"),\n (0x11300,\"V\"),\n (0x11304,\"X\"),\n (0x11305,\"V\"),\n (0x1130D,\"X\"),\n (0x1130F,\"V\"),\n (0x11311,\"X\"),\n (0x11313,\"V\"),\n (0x11329,\"X\"),\n (0x1132A,\"V\"),\n (0x11331,\"X\"),\n (0x11332,\"V\"),\n (0x11334,\"X\"),\n (0x11335,\"V\"),\n (0x1133A,\"X\"),\n (0x1133B,\"V\"),\n (0x11345,\"X\"),\n (0x11347,\"V\"),\n (0x11349,\"X\"),\n (0x1134B,\"V\"),\n (0x1134E,\"X\"),\n (0x11350,\"V\"),\n (0x11351,\"X\"),\n (0x11357,\"V\"),\n (0x11358,\"X\"),\n (0x1135D,\"V\"),\n (0x11364,\"X\"),\n (0x11366,\"V\"),\n (0x1136D,\"X\"),\n (0x11370,\"V\"),\n (0x11375,\"X\"),\n (0x11400,\"V\"),\n (0x1145C,\"X\"),\n (0x1145D,\"V\"),\n (0x11462,\"X\"),\n (0x11480,\"V\"),\n (0x114C8,\"X\"),\n (0x114D0,\"V\"),\n (0x114DA,\"X\"),\n (0x11580,\"V\"),\n (0x115B6,\"X\"),\n (0x115B8,\"V\"),\n (0x115DE,\"X\"),\n (0x11600,\"V\"),\n (0x11645,\"X\"),\n (0x11650,\"V\"),\n (0x1165A,\"X\"),\n (0x11660,\"V\"),\n (0x1166D,\"X\"),\n (0x11680,\"V\"),\n (0x116BA,\"X\"),\n (0x116C0,\"V\"),\n (0x116CA,\"X\"),\n (0x11700,\"V\"),\n (0x1171B,\"X\"),\n (0x1171D,\"V\"),\n (0x1172C,\"X\"),\n (0x11730,\"V\"),\n (0x11747,\"X\"),\n (0x11800,\"V\"),\n (0x1183C,\"X\"),\n (0x118A0,\"M\",\"\ud806\udcc0\"),\n (0x118A1,\"M\",\"\ud806\udcc1\"),\n (0x118A2,\"M\",\"\ud806\udcc2\"),\n (0x118A3,\"M\",\"\ud806\udcc3\"),\n (0x118A4,\"M\",\"\ud806\udcc4\"),\n (0x118A5,\"M\",\"\ud806\udcc5\"),\n (0x118A6,\"M\",\"\ud806\udcc6\"),\n (0x118A7,\"M\",\"\ud806\udcc7\"),\n (0x118A8,\"M\",\"\ud806\udcc8\"),\n (0x118A9,\"M\",\"\ud806\udcc9\"),\n (0x118AA,\"M\",\"\ud806\udcca\"),\n ]\n \n \ndef _seg_58()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x118AB,\"M\",\"\ud806\udccb\"),\n (0x118AC,\"M\",\"\ud806\udccc\"),\n (0x118AD,\"M\",\"\ud806\udccd\"),\n (0x118AE,\"M\",\"\ud806\udcce\"),\n (0x118AF,\"M\",\"\ud806\udccf\"),\n (0x118B0,\"M\",\"\ud806\udcd0\"),\n (0x118B1,\"M\",\"\ud806\udcd1\"),\n (0x118B2,\"M\",\"\ud806\udcd2\"),\n (0x118B3,\"M\",\"\ud806\udcd3\"),\n (0x118B4,\"M\",\"\ud806\udcd4\"),\n (0x118B5,\"M\",\"\ud806\udcd5\"),\n (0x118B6,\"M\",\"\ud806\udcd6\"),\n (0x118B7,\"M\",\"\ud806\udcd7\"),\n (0x118B8,\"M\",\"\ud806\udcd8\"),\n (0x118B9,\"M\",\"\ud806\udcd9\"),\n (0x118BA,\"M\",\"\ud806\udcda\"),\n (0x118BB,\"M\",\"\ud806\udcdb\"),\n (0x118BC,\"M\",\"\ud806\udcdc\"),\n (0x118BD,\"M\",\"\ud806\udcdd\"),\n (0x118BE,\"M\",\"\ud806\udcde\"),\n (0x118BF,\"M\",\"\ud806\udcdf\"),\n (0x118C0,\"V\"),\n (0x118F3,\"X\"),\n (0x118FF,\"V\"),\n (0x11907,\"X\"),\n (0x11909,\"V\"),\n (0x1190A,\"X\"),\n (0x1190C,\"V\"),\n (0x11914,\"X\"),\n (0x11915,\"V\"),\n (0x11917,\"X\"),\n (0x11918,\"V\"),\n (0x11936,\"X\"),\n (0x11937,\"V\"),\n (0x11939,\"X\"),\n (0x1193B,\"V\"),\n (0x11947,\"X\"),\n (0x11950,\"V\"),\n (0x1195A,\"X\"),\n (0x119A0,\"V\"),\n (0x119A8,\"X\"),\n (0x119AA,\"V\"),\n (0x119D8,\"X\"),\n (0x119DA,\"V\"),\n (0x119E5,\"X\"),\n (0x11A00,\"V\"),\n (0x11A48,\"X\"),\n (0x11A50,\"V\"),\n (0x11AA3,\"X\"),\n (0x11AB0,\"V\"),\n (0x11AF9,\"X\"),\n (0x11B00,\"V\"),\n (0x11B0A,\"X\"),\n (0x11C00,\"V\"),\n (0x11C09,\"X\"),\n (0x11C0A,\"V\"),\n (0x11C37,\"X\"),\n (0x11C38,\"V\"),\n (0x11C46,\"X\"),\n (0x11C50,\"V\"),\n (0x11C6D,\"X\"),\n (0x11C70,\"V\"),\n (0x11C90,\"X\"),\n (0x11C92,\"V\"),\n (0x11CA8,\"X\"),\n (0x11CA9,\"V\"),\n (0x11CB7,\"X\"),\n (0x11D00,\"V\"),\n (0x11D07,\"X\"),\n (0x11D08,\"V\"),\n (0x11D0A,\"X\"),\n (0x11D0B,\"V\"),\n (0x11D37,\"X\"),\n (0x11D3A,\"V\"),\n (0x11D3B,\"X\"),\n (0x11D3C,\"V\"),\n (0x11D3E,\"X\"),\n (0x11D3F,\"V\"),\n (0x11D48,\"X\"),\n (0x11D50,\"V\"),\n (0x11D5A,\"X\"),\n (0x11D60,\"V\"),\n (0x11D66,\"X\"),\n (0x11D67,\"V\"),\n (0x11D69,\"X\"),\n (0x11D6A,\"V\"),\n (0x11D8F,\"X\"),\n (0x11D90,\"V\"),\n (0x11D92,\"X\"),\n (0x11D93,\"V\"),\n (0x11D99,\"X\"),\n (0x11DA0,\"V\"),\n (0x11DAA,\"X\"),\n (0x11EE0,\"V\"),\n (0x11EF9,\"X\"),\n (0x11F00,\"V\"),\n (0x11F11,\"X\"),\n (0x11F12,\"V\"),\n (0x11F3B,\"X\"),\n (0x11F3E,\"V\"),\n ]\n \n \ndef _seg_59()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x11F5A,\"X\"),\n (0x11FB0,\"V\"),\n (0x11FB1,\"X\"),\n (0x11FC0,\"V\"),\n (0x11FF2,\"X\"),\n (0x11FFF,\"V\"),\n (0x1239A,\"X\"),\n (0x12400,\"V\"),\n (0x1246F,\"X\"),\n (0x12470,\"V\"),\n (0x12475,\"X\"),\n (0x12480,\"V\"),\n (0x12544,\"X\"),\n (0x12F90,\"V\"),\n (0x12FF3,\"X\"),\n (0x13000,\"V\"),\n (0x13430,\"X\"),\n (0x13440,\"V\"),\n (0x13456,\"X\"),\n (0x14400,\"V\"),\n (0x14647,\"X\"),\n (0x16800,\"V\"),\n (0x16A39,\"X\"),\n (0x16A40,\"V\"),\n (0x16A5F,\"X\"),\n (0x16A60,\"V\"),\n (0x16A6A,\"X\"),\n (0x16A6E,\"V\"),\n (0x16ABF,\"X\"),\n (0x16AC0,\"V\"),\n (0x16ACA,\"X\"),\n (0x16AD0,\"V\"),\n (0x16AEE,\"X\"),\n (0x16AF0,\"V\"),\n (0x16AF6,\"X\"),\n (0x16B00,\"V\"),\n (0x16B46,\"X\"),\n (0x16B50,\"V\"),\n (0x16B5A,\"X\"),\n (0x16B5B,\"V\"),\n (0x16B62,\"X\"),\n (0x16B63,\"V\"),\n (0x16B78,\"X\"),\n (0x16B7D,\"V\"),\n (0x16B90,\"X\"),\n (0x16E40,\"M\",\"\ud81b\ude60\"),\n (0x16E41,\"M\",\"\ud81b\ude61\"),\n (0x16E42,\"M\",\"\ud81b\ude62\"),\n (0x16E43,\"M\",\"\ud81b\ude63\"),\n (0x16E44,\"M\",\"\ud81b\ude64\"),\n (0x16E45,\"M\",\"\ud81b\ude65\"),\n (0x16E46,\"M\",\"\ud81b\ude66\"),\n (0x16E47,\"M\",\"\ud81b\ude67\"),\n (0x16E48,\"M\",\"\ud81b\ude68\"),\n (0x16E49,\"M\",\"\ud81b\ude69\"),\n (0x16E4A,\"M\",\"\ud81b\ude6a\"),\n (0x16E4B,\"M\",\"\ud81b\ude6b\"),\n (0x16E4C,\"M\",\"\ud81b\ude6c\"),\n (0x16E4D,\"M\",\"\ud81b\ude6d\"),\n (0x16E4E,\"M\",\"\ud81b\ude6e\"),\n (0x16E4F,\"M\",\"\ud81b\ude6f\"),\n (0x16E50,\"M\",\"\ud81b\ude70\"),\n (0x16E51,\"M\",\"\ud81b\ude71\"),\n (0x16E52,\"M\",\"\ud81b\ude72\"),\n (0x16E53,\"M\",\"\ud81b\ude73\"),\n (0x16E54,\"M\",\"\ud81b\ude74\"),\n (0x16E55,\"M\",\"\ud81b\ude75\"),\n (0x16E56,\"M\",\"\ud81b\ude76\"),\n (0x16E57,\"M\",\"\ud81b\ude77\"),\n (0x16E58,\"M\",\"\ud81b\ude78\"),\n (0x16E59,\"M\",\"\ud81b\ude79\"),\n (0x16E5A,\"M\",\"\ud81b\ude7a\"),\n (0x16E5B,\"M\",\"\ud81b\ude7b\"),\n (0x16E5C,\"M\",\"\ud81b\ude7c\"),\n (0x16E5D,\"M\",\"\ud81b\ude7d\"),\n (0x16E5E,\"M\",\"\ud81b\ude7e\"),\n (0x16E5F,\"M\",\"\ud81b\ude7f\"),\n (0x16E60,\"V\"),\n (0x16E9B,\"X\"),\n (0x16F00,\"V\"),\n (0x16F4B,\"X\"),\n (0x16F4F,\"V\"),\n (0x16F88,\"X\"),\n (0x16F8F,\"V\"),\n (0x16FA0,\"X\"),\n (0x16FE0,\"V\"),\n (0x16FE5,\"X\"),\n (0x16FF0,\"V\"),\n (0x16FF2,\"X\"),\n (0x17000,\"V\"),\n (0x187F8,\"X\"),\n (0x18800,\"V\"),\n (0x18CD6,\"X\"),\n (0x18D00,\"V\"),\n (0x18D09,\"X\"),\n (0x1AFF0,\"V\"),\n (0x1AFF4,\"X\"),\n (0x1AFF5,\"V\"),\n (0x1AFFC,\"X\"),\n (0x1AFFD,\"V\"),\n ]\n \n \ndef _seg_60()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1AFFF,\"X\"),\n (0x1B000,\"V\"),\n (0x1B123,\"X\"),\n (0x1B132,\"V\"),\n (0x1B133,\"X\"),\n (0x1B150,\"V\"),\n (0x1B153,\"X\"),\n (0x1B155,\"V\"),\n (0x1B156,\"X\"),\n (0x1B164,\"V\"),\n (0x1B168,\"X\"),\n (0x1B170,\"V\"),\n (0x1B2FC,\"X\"),\n (0x1BC00,\"V\"),\n (0x1BC6B,\"X\"),\n (0x1BC70,\"V\"),\n (0x1BC7D,\"X\"),\n (0x1BC80,\"V\"),\n (0x1BC89,\"X\"),\n (0x1BC90,\"V\"),\n (0x1BC9A,\"X\"),\n (0x1BC9C,\"V\"),\n (0x1BCA0,\"I\"),\n (0x1BCA4,\"X\"),\n (0x1CF00,\"V\"),\n (0x1CF2E,\"X\"),\n (0x1CF30,\"V\"),\n (0x1CF47,\"X\"),\n (0x1CF50,\"V\"),\n (0x1CFC4,\"X\"),\n (0x1D000,\"V\"),\n (0x1D0F6,\"X\"),\n (0x1D100,\"V\"),\n (0x1D127,\"X\"),\n (0x1D129,\"V\"),\n (0x1D15E,\"M\",\"\ud834\udd57\ud834\udd65\"),\n (0x1D15F,\"M\",\"\ud834\udd58\ud834\udd65\"),\n (0x1D160,\"M\",\"\ud834\udd58\ud834\udd65\ud834\udd6e\"),\n (0x1D161,\"M\",\"\ud834\udd58\ud834\udd65\ud834\udd6f\"),\n (0x1D162,\"M\",\"\ud834\udd58\ud834\udd65\ud834\udd70\"),\n (0x1D163,\"M\",\"\ud834\udd58\ud834\udd65\ud834\udd71\"),\n (0x1D164,\"M\",\"\ud834\udd58\ud834\udd65\ud834\udd72\"),\n (0x1D165,\"V\"),\n (0x1D173,\"X\"),\n (0x1D17B,\"V\"),\n (0x1D1BB,\"M\",\"\ud834\uddb9\ud834\udd65\"),\n (0x1D1BC,\"M\",\"\ud834\uddba\ud834\udd65\"),\n (0x1D1BD,\"M\",\"\ud834\uddb9\ud834\udd65\ud834\udd6e\"),\n (0x1D1BE,\"M\",\"\ud834\uddba\ud834\udd65\ud834\udd6e\"),\n (0x1D1BF,\"M\",\"\ud834\uddb9\ud834\udd65\ud834\udd6f\"),\n (0x1D1C0,\"M\",\"\ud834\uddba\ud834\udd65\ud834\udd6f\"),\n (0x1D1C1,\"V\"),\n (0x1D1EB,\"X\"),\n (0x1D200,\"V\"),\n (0x1D246,\"X\"),\n (0x1D2C0,\"V\"),\n (0x1D2D4,\"X\"),\n (0x1D2E0,\"V\"),\n (0x1D2F4,\"X\"),\n (0x1D300,\"V\"),\n (0x1D357,\"X\"),\n (0x1D360,\"V\"),\n (0x1D379,\"X\"),\n (0x1D400,\"M\",\"a\"),\n (0x1D401,\"M\",\"b\"),\n (0x1D402,\"M\",\"c\"),\n (0x1D403,\"M\",\"d\"),\n (0x1D404,\"M\",\"e\"),\n (0x1D405,\"M\",\"f\"),\n (0x1D406,\"M\",\"g\"),\n (0x1D407,\"M\",\"h\"),\n (0x1D408,\"M\",\"i\"),\n (0x1D409,\"M\",\"j\"),\n (0x1D40A,\"M\",\"k\"),\n (0x1D40B,\"M\",\"l\"),\n (0x1D40C,\"M\",\"m\"),\n (0x1D40D,\"M\",\"n\"),\n (0x1D40E,\"M\",\"o\"),\n (0x1D40F,\"M\",\"p\"),\n (0x1D410,\"M\",\"q\"),\n (0x1D411,\"M\",\"r\"),\n (0x1D412,\"M\",\"s\"),\n (0x1D413,\"M\",\"t\"),\n (0x1D414,\"M\",\"u\"),\n (0x1D415,\"M\",\"v\"),\n (0x1D416,\"M\",\"w\"),\n (0x1D417,\"M\",\"x\"),\n (0x1D418,\"M\",\"y\"),\n (0x1D419,\"M\",\"z\"),\n (0x1D41A,\"M\",\"a\"),\n (0x1D41B,\"M\",\"b\"),\n (0x1D41C,\"M\",\"c\"),\n (0x1D41D,\"M\",\"d\"),\n (0x1D41E,\"M\",\"e\"),\n (0x1D41F,\"M\",\"f\"),\n (0x1D420,\"M\",\"g\"),\n (0x1D421,\"M\",\"h\"),\n (0x1D422,\"M\",\"i\"),\n (0x1D423,\"M\",\"j\"),\n (0x1D424,\"M\",\"k\"),\n ]\n \n \ndef _seg_61()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D425,\"M\",\"l\"),\n (0x1D426,\"M\",\"m\"),\n (0x1D427,\"M\",\"n\"),\n (0x1D428,\"M\",\"o\"),\n (0x1D429,\"M\",\"p\"),\n (0x1D42A,\"M\",\"q\"),\n (0x1D42B,\"M\",\"r\"),\n (0x1D42C,\"M\",\"s\"),\n (0x1D42D,\"M\",\"t\"),\n (0x1D42E,\"M\",\"u\"),\n (0x1D42F,\"M\",\"v\"),\n (0x1D430,\"M\",\"w\"),\n (0x1D431,\"M\",\"x\"),\n (0x1D432,\"M\",\"y\"),\n (0x1D433,\"M\",\"z\"),\n (0x1D434,\"M\",\"a\"),\n (0x1D435,\"M\",\"b\"),\n (0x1D436,\"M\",\"c\"),\n (0x1D437,\"M\",\"d\"),\n (0x1D438,\"M\",\"e\"),\n (0x1D439,\"M\",\"f\"),\n (0x1D43A,\"M\",\"g\"),\n (0x1D43B,\"M\",\"h\"),\n (0x1D43C,\"M\",\"i\"),\n (0x1D43D,\"M\",\"j\"),\n (0x1D43E,\"M\",\"k\"),\n (0x1D43F,\"M\",\"l\"),\n (0x1D440,\"M\",\"m\"),\n (0x1D441,\"M\",\"n\"),\n (0x1D442,\"M\",\"o\"),\n (0x1D443,\"M\",\"p\"),\n (0x1D444,\"M\",\"q\"),\n (0x1D445,\"M\",\"r\"),\n (0x1D446,\"M\",\"s\"),\n (0x1D447,\"M\",\"t\"),\n (0x1D448,\"M\",\"u\"),\n (0x1D449,\"M\",\"v\"),\n (0x1D44A,\"M\",\"w\"),\n (0x1D44B,\"M\",\"x\"),\n (0x1D44C,\"M\",\"y\"),\n (0x1D44D,\"M\",\"z\"),\n (0x1D44E,\"M\",\"a\"),\n (0x1D44F,\"M\",\"b\"),\n (0x1D450,\"M\",\"c\"),\n (0x1D451,\"M\",\"d\"),\n (0x1D452,\"M\",\"e\"),\n (0x1D453,\"M\",\"f\"),\n (0x1D454,\"M\",\"g\"),\n (0x1D455,\"X\"),\n (0x1D456,\"M\",\"i\"),\n (0x1D457,\"M\",\"j\"),\n (0x1D458,\"M\",\"k\"),\n (0x1D459,\"M\",\"l\"),\n (0x1D45A,\"M\",\"m\"),\n (0x1D45B,\"M\",\"n\"),\n (0x1D45C,\"M\",\"o\"),\n (0x1D45D,\"M\",\"p\"),\n (0x1D45E,\"M\",\"q\"),\n (0x1D45F,\"M\",\"r\"),\n (0x1D460,\"M\",\"s\"),\n (0x1D461,\"M\",\"t\"),\n (0x1D462,\"M\",\"u\"),\n (0x1D463,\"M\",\"v\"),\n (0x1D464,\"M\",\"w\"),\n (0x1D465,\"M\",\"x\"),\n (0x1D466,\"M\",\"y\"),\n (0x1D467,\"M\",\"z\"),\n (0x1D468,\"M\",\"a\"),\n (0x1D469,\"M\",\"b\"),\n (0x1D46A,\"M\",\"c\"),\n (0x1D46B,\"M\",\"d\"),\n (0x1D46C,\"M\",\"e\"),\n (0x1D46D,\"M\",\"f\"),\n (0x1D46E,\"M\",\"g\"),\n (0x1D46F,\"M\",\"h\"),\n (0x1D470,\"M\",\"i\"),\n (0x1D471,\"M\",\"j\"),\n (0x1D472,\"M\",\"k\"),\n (0x1D473,\"M\",\"l\"),\n (0x1D474,\"M\",\"m\"),\n (0x1D475,\"M\",\"n\"),\n (0x1D476,\"M\",\"o\"),\n (0x1D477,\"M\",\"p\"),\n (0x1D478,\"M\",\"q\"),\n (0x1D479,\"M\",\"r\"),\n (0x1D47A,\"M\",\"s\"),\n (0x1D47B,\"M\",\"t\"),\n (0x1D47C,\"M\",\"u\"),\n (0x1D47D,\"M\",\"v\"),\n (0x1D47E,\"M\",\"w\"),\n (0x1D47F,\"M\",\"x\"),\n (0x1D480,\"M\",\"y\"),\n (0x1D481,\"M\",\"z\"),\n (0x1D482,\"M\",\"a\"),\n (0x1D483,\"M\",\"b\"),\n (0x1D484,\"M\",\"c\"),\n (0x1D485,\"M\",\"d\"),\n (0x1D486,\"M\",\"e\"),\n (0x1D487,\"M\",\"f\"),\n (0x1D488,\"M\",\"g\"),\n ]\n \n \ndef _seg_62()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D489,\"M\",\"h\"),\n (0x1D48A,\"M\",\"i\"),\n (0x1D48B,\"M\",\"j\"),\n (0x1D48C,\"M\",\"k\"),\n (0x1D48D,\"M\",\"l\"),\n (0x1D48E,\"M\",\"m\"),\n (0x1D48F,\"M\",\"n\"),\n (0x1D490,\"M\",\"o\"),\n (0x1D491,\"M\",\"p\"),\n (0x1D492,\"M\",\"q\"),\n (0x1D493,\"M\",\"r\"),\n (0x1D494,\"M\",\"s\"),\n (0x1D495,\"M\",\"t\"),\n (0x1D496,\"M\",\"u\"),\n (0x1D497,\"M\",\"v\"),\n (0x1D498,\"M\",\"w\"),\n (0x1D499,\"M\",\"x\"),\n (0x1D49A,\"M\",\"y\"),\n (0x1D49B,\"M\",\"z\"),\n (0x1D49C,\"M\",\"a\"),\n (0x1D49D,\"X\"),\n (0x1D49E,\"M\",\"c\"),\n (0x1D49F,\"M\",\"d\"),\n (0x1D4A0,\"X\"),\n (0x1D4A2,\"M\",\"g\"),\n (0x1D4A3,\"X\"),\n (0x1D4A5,\"M\",\"j\"),\n (0x1D4A6,\"M\",\"k\"),\n (0x1D4A7,\"X\"),\n (0x1D4A9,\"M\",\"n\"),\n (0x1D4AA,\"M\",\"o\"),\n (0x1D4AB,\"M\",\"p\"),\n (0x1D4AC,\"M\",\"q\"),\n (0x1D4AD,\"X\"),\n (0x1D4AE,\"M\",\"s\"),\n (0x1D4AF,\"M\",\"t\"),\n (0x1D4B0,\"M\",\"u\"),\n (0x1D4B1,\"M\",\"v\"),\n (0x1D4B2,\"M\",\"w\"),\n (0x1D4B3,\"M\",\"x\"),\n (0x1D4B4,\"M\",\"y\"),\n (0x1D4B5,\"M\",\"z\"),\n (0x1D4B6,\"M\",\"a\"),\n (0x1D4B7,\"M\",\"b\"),\n (0x1D4B8,\"M\",\"c\"),\n (0x1D4B9,\"M\",\"d\"),\n (0x1D4BA,\"X\"),\n (0x1D4BB,\"M\",\"f\"),\n (0x1D4BC,\"X\"),\n (0x1D4BD,\"M\",\"h\"),\n (0x1D4BE,\"M\",\"i\"),\n (0x1D4BF,\"M\",\"j\"),\n (0x1D4C0,\"M\",\"k\"),\n (0x1D4C1,\"M\",\"l\"),\n (0x1D4C2,\"M\",\"m\"),\n (0x1D4C3,\"M\",\"n\"),\n (0x1D4C4,\"X\"),\n (0x1D4C5,\"M\",\"p\"),\n (0x1D4C6,\"M\",\"q\"),\n (0x1D4C7,\"M\",\"r\"),\n (0x1D4C8,\"M\",\"s\"),\n (0x1D4C9,\"M\",\"t\"),\n (0x1D4CA,\"M\",\"u\"),\n (0x1D4CB,\"M\",\"v\"),\n (0x1D4CC,\"M\",\"w\"),\n (0x1D4CD,\"M\",\"x\"),\n (0x1D4CE,\"M\",\"y\"),\n (0x1D4CF,\"M\",\"z\"),\n (0x1D4D0,\"M\",\"a\"),\n (0x1D4D1,\"M\",\"b\"),\n (0x1D4D2,\"M\",\"c\"),\n (0x1D4D3,\"M\",\"d\"),\n (0x1D4D4,\"M\",\"e\"),\n (0x1D4D5,\"M\",\"f\"),\n (0x1D4D6,\"M\",\"g\"),\n (0x1D4D7,\"M\",\"h\"),\n (0x1D4D8,\"M\",\"i\"),\n (0x1D4D9,\"M\",\"j\"),\n (0x1D4DA,\"M\",\"k\"),\n (0x1D4DB,\"M\",\"l\"),\n (0x1D4DC,\"M\",\"m\"),\n (0x1D4DD,\"M\",\"n\"),\n (0x1D4DE,\"M\",\"o\"),\n (0x1D4DF,\"M\",\"p\"),\n (0x1D4E0,\"M\",\"q\"),\n (0x1D4E1,\"M\",\"r\"),\n (0x1D4E2,\"M\",\"s\"),\n (0x1D4E3,\"M\",\"t\"),\n (0x1D4E4,\"M\",\"u\"),\n (0x1D4E5,\"M\",\"v\"),\n (0x1D4E6,\"M\",\"w\"),\n (0x1D4E7,\"M\",\"x\"),\n (0x1D4E8,\"M\",\"y\"),\n (0x1D4E9,\"M\",\"z\"),\n (0x1D4EA,\"M\",\"a\"),\n (0x1D4EB,\"M\",\"b\"),\n (0x1D4EC,\"M\",\"c\"),\n (0x1D4ED,\"M\",\"d\"),\n (0x1D4EE,\"M\",\"e\"),\n (0x1D4EF,\"M\",\"f\"),\n ]\n \n \ndef _seg_63()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D4F0,\"M\",\"g\"),\n (0x1D4F1,\"M\",\"h\"),\n (0x1D4F2,\"M\",\"i\"),\n (0x1D4F3,\"M\",\"j\"),\n (0x1D4F4,\"M\",\"k\"),\n (0x1D4F5,\"M\",\"l\"),\n (0x1D4F6,\"M\",\"m\"),\n (0x1D4F7,\"M\",\"n\"),\n (0x1D4F8,\"M\",\"o\"),\n (0x1D4F9,\"M\",\"p\"),\n (0x1D4FA,\"M\",\"q\"),\n (0x1D4FB,\"M\",\"r\"),\n (0x1D4FC,\"M\",\"s\"),\n (0x1D4FD,\"M\",\"t\"),\n (0x1D4FE,\"M\",\"u\"),\n (0x1D4FF,\"M\",\"v\"),\n (0x1D500,\"M\",\"w\"),\n (0x1D501,\"M\",\"x\"),\n (0x1D502,\"M\",\"y\"),\n (0x1D503,\"M\",\"z\"),\n (0x1D504,\"M\",\"a\"),\n (0x1D505,\"M\",\"b\"),\n (0x1D506,\"X\"),\n (0x1D507,\"M\",\"d\"),\n (0x1D508,\"M\",\"e\"),\n (0x1D509,\"M\",\"f\"),\n (0x1D50A,\"M\",\"g\"),\n (0x1D50B,\"X\"),\n (0x1D50D,\"M\",\"j\"),\n (0x1D50E,\"M\",\"k\"),\n (0x1D50F,\"M\",\"l\"),\n (0x1D510,\"M\",\"m\"),\n (0x1D511,\"M\",\"n\"),\n (0x1D512,\"M\",\"o\"),\n (0x1D513,\"M\",\"p\"),\n (0x1D514,\"M\",\"q\"),\n (0x1D515,\"X\"),\n (0x1D516,\"M\",\"s\"),\n (0x1D517,\"M\",\"t\"),\n (0x1D518,\"M\",\"u\"),\n (0x1D519,\"M\",\"v\"),\n (0x1D51A,\"M\",\"w\"),\n (0x1D51B,\"M\",\"x\"),\n (0x1D51C,\"M\",\"y\"),\n (0x1D51D,\"X\"),\n (0x1D51E,\"M\",\"a\"),\n (0x1D51F,\"M\",\"b\"),\n (0x1D520,\"M\",\"c\"),\n (0x1D521,\"M\",\"d\"),\n (0x1D522,\"M\",\"e\"),\n (0x1D523,\"M\",\"f\"),\n (0x1D524,\"M\",\"g\"),\n (0x1D525,\"M\",\"h\"),\n (0x1D526,\"M\",\"i\"),\n (0x1D527,\"M\",\"j\"),\n (0x1D528,\"M\",\"k\"),\n (0x1D529,\"M\",\"l\"),\n (0x1D52A,\"M\",\"m\"),\n (0x1D52B,\"M\",\"n\"),\n (0x1D52C,\"M\",\"o\"),\n (0x1D52D,\"M\",\"p\"),\n (0x1D52E,\"M\",\"q\"),\n (0x1D52F,\"M\",\"r\"),\n (0x1D530,\"M\",\"s\"),\n (0x1D531,\"M\",\"t\"),\n (0x1D532,\"M\",\"u\"),\n (0x1D533,\"M\",\"v\"),\n (0x1D534,\"M\",\"w\"),\n (0x1D535,\"M\",\"x\"),\n (0x1D536,\"M\",\"y\"),\n (0x1D537,\"M\",\"z\"),\n (0x1D538,\"M\",\"a\"),\n (0x1D539,\"M\",\"b\"),\n (0x1D53A,\"X\"),\n (0x1D53B,\"M\",\"d\"),\n (0x1D53C,\"M\",\"e\"),\n (0x1D53D,\"M\",\"f\"),\n (0x1D53E,\"M\",\"g\"),\n (0x1D53F,\"X\"),\n (0x1D540,\"M\",\"i\"),\n (0x1D541,\"M\",\"j\"),\n (0x1D542,\"M\",\"k\"),\n (0x1D543,\"M\",\"l\"),\n (0x1D544,\"M\",\"m\"),\n (0x1D545,\"X\"),\n (0x1D546,\"M\",\"o\"),\n (0x1D547,\"X\"),\n (0x1D54A,\"M\",\"s\"),\n (0x1D54B,\"M\",\"t\"),\n (0x1D54C,\"M\",\"u\"),\n (0x1D54D,\"M\",\"v\"),\n (0x1D54E,\"M\",\"w\"),\n (0x1D54F,\"M\",\"x\"),\n (0x1D550,\"M\",\"y\"),\n (0x1D551,\"X\"),\n (0x1D552,\"M\",\"a\"),\n (0x1D553,\"M\",\"b\"),\n (0x1D554,\"M\",\"c\"),\n (0x1D555,\"M\",\"d\"),\n (0x1D556,\"M\",\"e\"),\n ]\n \n \ndef _seg_64()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D557,\"M\",\"f\"),\n (0x1D558,\"M\",\"g\"),\n (0x1D559,\"M\",\"h\"),\n (0x1D55A,\"M\",\"i\"),\n (0x1D55B,\"M\",\"j\"),\n (0x1D55C,\"M\",\"k\"),\n (0x1D55D,\"M\",\"l\"),\n (0x1D55E,\"M\",\"m\"),\n (0x1D55F,\"M\",\"n\"),\n (0x1D560,\"M\",\"o\"),\n (0x1D561,\"M\",\"p\"),\n (0x1D562,\"M\",\"q\"),\n (0x1D563,\"M\",\"r\"),\n (0x1D564,\"M\",\"s\"),\n (0x1D565,\"M\",\"t\"),\n (0x1D566,\"M\",\"u\"),\n (0x1D567,\"M\",\"v\"),\n (0x1D568,\"M\",\"w\"),\n (0x1D569,\"M\",\"x\"),\n (0x1D56A,\"M\",\"y\"),\n (0x1D56B,\"M\",\"z\"),\n (0x1D56C,\"M\",\"a\"),\n (0x1D56D,\"M\",\"b\"),\n (0x1D56E,\"M\",\"c\"),\n (0x1D56F,\"M\",\"d\"),\n (0x1D570,\"M\",\"e\"),\n (0x1D571,\"M\",\"f\"),\n (0x1D572,\"M\",\"g\"),\n (0x1D573,\"M\",\"h\"),\n (0x1D574,\"M\",\"i\"),\n (0x1D575,\"M\",\"j\"),\n (0x1D576,\"M\",\"k\"),\n (0x1D577,\"M\",\"l\"),\n (0x1D578,\"M\",\"m\"),\n (0x1D579,\"M\",\"n\"),\n (0x1D57A,\"M\",\"o\"),\n (0x1D57B,\"M\",\"p\"),\n (0x1D57C,\"M\",\"q\"),\n (0x1D57D,\"M\",\"r\"),\n (0x1D57E,\"M\",\"s\"),\n (0x1D57F,\"M\",\"t\"),\n (0x1D580,\"M\",\"u\"),\n (0x1D581,\"M\",\"v\"),\n (0x1D582,\"M\",\"w\"),\n (0x1D583,\"M\",\"x\"),\n (0x1D584,\"M\",\"y\"),\n (0x1D585,\"M\",\"z\"),\n (0x1D586,\"M\",\"a\"),\n (0x1D587,\"M\",\"b\"),\n (0x1D588,\"M\",\"c\"),\n (0x1D589,\"M\",\"d\"),\n (0x1D58A,\"M\",\"e\"),\n (0x1D58B,\"M\",\"f\"),\n (0x1D58C,\"M\",\"g\"),\n (0x1D58D,\"M\",\"h\"),\n (0x1D58E,\"M\",\"i\"),\n (0x1D58F,\"M\",\"j\"),\n (0x1D590,\"M\",\"k\"),\n (0x1D591,\"M\",\"l\"),\n (0x1D592,\"M\",\"m\"),\n (0x1D593,\"M\",\"n\"),\n (0x1D594,\"M\",\"o\"),\n (0x1D595,\"M\",\"p\"),\n (0x1D596,\"M\",\"q\"),\n (0x1D597,\"M\",\"r\"),\n (0x1D598,\"M\",\"s\"),\n (0x1D599,\"M\",\"t\"),\n (0x1D59A,\"M\",\"u\"),\n (0x1D59B,\"M\",\"v\"),\n (0x1D59C,\"M\",\"w\"),\n (0x1D59D,\"M\",\"x\"),\n (0x1D59E,\"M\",\"y\"),\n (0x1D59F,\"M\",\"z\"),\n (0x1D5A0,\"M\",\"a\"),\n (0x1D5A1,\"M\",\"b\"),\n (0x1D5A2,\"M\",\"c\"),\n (0x1D5A3,\"M\",\"d\"),\n (0x1D5A4,\"M\",\"e\"),\n (0x1D5A5,\"M\",\"f\"),\n (0x1D5A6,\"M\",\"g\"),\n (0x1D5A7,\"M\",\"h\"),\n (0x1D5A8,\"M\",\"i\"),\n (0x1D5A9,\"M\",\"j\"),\n (0x1D5AA,\"M\",\"k\"),\n (0x1D5AB,\"M\",\"l\"),\n (0x1D5AC,\"M\",\"m\"),\n (0x1D5AD,\"M\",\"n\"),\n (0x1D5AE,\"M\",\"o\"),\n (0x1D5AF,\"M\",\"p\"),\n (0x1D5B0,\"M\",\"q\"),\n (0x1D5B1,\"M\",\"r\"),\n (0x1D5B2,\"M\",\"s\"),\n (0x1D5B3,\"M\",\"t\"),\n (0x1D5B4,\"M\",\"u\"),\n (0x1D5B5,\"M\",\"v\"),\n (0x1D5B6,\"M\",\"w\"),\n (0x1D5B7,\"M\",\"x\"),\n (0x1D5B8,\"M\",\"y\"),\n (0x1D5B9,\"M\",\"z\"),\n (0x1D5BA,\"M\",\"a\"),\n ]\n \n \ndef _seg_65()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D5BB,\"M\",\"b\"),\n (0x1D5BC,\"M\",\"c\"),\n (0x1D5BD,\"M\",\"d\"),\n (0x1D5BE,\"M\",\"e\"),\n (0x1D5BF,\"M\",\"f\"),\n (0x1D5C0,\"M\",\"g\"),\n (0x1D5C1,\"M\",\"h\"),\n (0x1D5C2,\"M\",\"i\"),\n (0x1D5C3,\"M\",\"j\"),\n (0x1D5C4,\"M\",\"k\"),\n (0x1D5C5,\"M\",\"l\"),\n (0x1D5C6,\"M\",\"m\"),\n (0x1D5C7,\"M\",\"n\"),\n (0x1D5C8,\"M\",\"o\"),\n (0x1D5C9,\"M\",\"p\"),\n (0x1D5CA,\"M\",\"q\"),\n (0x1D5CB,\"M\",\"r\"),\n (0x1D5CC,\"M\",\"s\"),\n (0x1D5CD,\"M\",\"t\"),\n (0x1D5CE,\"M\",\"u\"),\n (0x1D5CF,\"M\",\"v\"),\n (0x1D5D0,\"M\",\"w\"),\n (0x1D5D1,\"M\",\"x\"),\n (0x1D5D2,\"M\",\"y\"),\n (0x1D5D3,\"M\",\"z\"),\n (0x1D5D4,\"M\",\"a\"),\n (0x1D5D5,\"M\",\"b\"),\n (0x1D5D6,\"M\",\"c\"),\n (0x1D5D7,\"M\",\"d\"),\n (0x1D5D8,\"M\",\"e\"),\n (0x1D5D9,\"M\",\"f\"),\n (0x1D5DA,\"M\",\"g\"),\n (0x1D5DB,\"M\",\"h\"),\n (0x1D5DC,\"M\",\"i\"),\n (0x1D5DD,\"M\",\"j\"),\n (0x1D5DE,\"M\",\"k\"),\n (0x1D5DF,\"M\",\"l\"),\n (0x1D5E0,\"M\",\"m\"),\n (0x1D5E1,\"M\",\"n\"),\n (0x1D5E2,\"M\",\"o\"),\n (0x1D5E3,\"M\",\"p\"),\n (0x1D5E4,\"M\",\"q\"),\n (0x1D5E5,\"M\",\"r\"),\n (0x1D5E6,\"M\",\"s\"),\n (0x1D5E7,\"M\",\"t\"),\n (0x1D5E8,\"M\",\"u\"),\n (0x1D5E9,\"M\",\"v\"),\n (0x1D5EA,\"M\",\"w\"),\n (0x1D5EB,\"M\",\"x\"),\n (0x1D5EC,\"M\",\"y\"),\n (0x1D5ED,\"M\",\"z\"),\n (0x1D5EE,\"M\",\"a\"),\n (0x1D5EF,\"M\",\"b\"),\n (0x1D5F0,\"M\",\"c\"),\n (0x1D5F1,\"M\",\"d\"),\n (0x1D5F2,\"M\",\"e\"),\n (0x1D5F3,\"M\",\"f\"),\n (0x1D5F4,\"M\",\"g\"),\n (0x1D5F5,\"M\",\"h\"),\n (0x1D5F6,\"M\",\"i\"),\n (0x1D5F7,\"M\",\"j\"),\n (0x1D5F8,\"M\",\"k\"),\n (0x1D5F9,\"M\",\"l\"),\n (0x1D5FA,\"M\",\"m\"),\n (0x1D5FB,\"M\",\"n\"),\n (0x1D5FC,\"M\",\"o\"),\n (0x1D5FD,\"M\",\"p\"),\n (0x1D5FE,\"M\",\"q\"),\n (0x1D5FF,\"M\",\"r\"),\n (0x1D600,\"M\",\"s\"),\n (0x1D601,\"M\",\"t\"),\n (0x1D602,\"M\",\"u\"),\n (0x1D603,\"M\",\"v\"),\n (0x1D604,\"M\",\"w\"),\n (0x1D605,\"M\",\"x\"),\n (0x1D606,\"M\",\"y\"),\n (0x1D607,\"M\",\"z\"),\n (0x1D608,\"M\",\"a\"),\n (0x1D609,\"M\",\"b\"),\n (0x1D60A,\"M\",\"c\"),\n (0x1D60B,\"M\",\"d\"),\n (0x1D60C,\"M\",\"e\"),\n (0x1D60D,\"M\",\"f\"),\n (0x1D60E,\"M\",\"g\"),\n (0x1D60F,\"M\",\"h\"),\n (0x1D610,\"M\",\"i\"),\n (0x1D611,\"M\",\"j\"),\n (0x1D612,\"M\",\"k\"),\n (0x1D613,\"M\",\"l\"),\n (0x1D614,\"M\",\"m\"),\n (0x1D615,\"M\",\"n\"),\n (0x1D616,\"M\",\"o\"),\n (0x1D617,\"M\",\"p\"),\n (0x1D618,\"M\",\"q\"),\n (0x1D619,\"M\",\"r\"),\n (0x1D61A,\"M\",\"s\"),\n (0x1D61B,\"M\",\"t\"),\n (0x1D61C,\"M\",\"u\"),\n (0x1D61D,\"M\",\"v\"),\n (0x1D61E,\"M\",\"w\"),\n ]\n \n \ndef _seg_66()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D61F,\"M\",\"x\"),\n (0x1D620,\"M\",\"y\"),\n (0x1D621,\"M\",\"z\"),\n (0x1D622,\"M\",\"a\"),\n (0x1D623,\"M\",\"b\"),\n (0x1D624,\"M\",\"c\"),\n (0x1D625,\"M\",\"d\"),\n (0x1D626,\"M\",\"e\"),\n (0x1D627,\"M\",\"f\"),\n (0x1D628,\"M\",\"g\"),\n (0x1D629,\"M\",\"h\"),\n (0x1D62A,\"M\",\"i\"),\n (0x1D62B,\"M\",\"j\"),\n (0x1D62C,\"M\",\"k\"),\n (0x1D62D,\"M\",\"l\"),\n (0x1D62E,\"M\",\"m\"),\n (0x1D62F,\"M\",\"n\"),\n (0x1D630,\"M\",\"o\"),\n (0x1D631,\"M\",\"p\"),\n (0x1D632,\"M\",\"q\"),\n (0x1D633,\"M\",\"r\"),\n (0x1D634,\"M\",\"s\"),\n (0x1D635,\"M\",\"t\"),\n (0x1D636,\"M\",\"u\"),\n (0x1D637,\"M\",\"v\"),\n (0x1D638,\"M\",\"w\"),\n (0x1D639,\"M\",\"x\"),\n (0x1D63A,\"M\",\"y\"),\n (0x1D63B,\"M\",\"z\"),\n (0x1D63C,\"M\",\"a\"),\n (0x1D63D,\"M\",\"b\"),\n (0x1D63E,\"M\",\"c\"),\n (0x1D63F,\"M\",\"d\"),\n (0x1D640,\"M\",\"e\"),\n (0x1D641,\"M\",\"f\"),\n (0x1D642,\"M\",\"g\"),\n (0x1D643,\"M\",\"h\"),\n (0x1D644,\"M\",\"i\"),\n (0x1D645,\"M\",\"j\"),\n (0x1D646,\"M\",\"k\"),\n (0x1D647,\"M\",\"l\"),\n (0x1D648,\"M\",\"m\"),\n (0x1D649,\"M\",\"n\"),\n (0x1D64A,\"M\",\"o\"),\n (0x1D64B,\"M\",\"p\"),\n (0x1D64C,\"M\",\"q\"),\n (0x1D64D,\"M\",\"r\"),\n (0x1D64E,\"M\",\"s\"),\n (0x1D64F,\"M\",\"t\"),\n (0x1D650,\"M\",\"u\"),\n (0x1D651,\"M\",\"v\"),\n (0x1D652,\"M\",\"w\"),\n (0x1D653,\"M\",\"x\"),\n (0x1D654,\"M\",\"y\"),\n (0x1D655,\"M\",\"z\"),\n (0x1D656,\"M\",\"a\"),\n (0x1D657,\"M\",\"b\"),\n (0x1D658,\"M\",\"c\"),\n (0x1D659,\"M\",\"d\"),\n (0x1D65A,\"M\",\"e\"),\n (0x1D65B,\"M\",\"f\"),\n (0x1D65C,\"M\",\"g\"),\n (0x1D65D,\"M\",\"h\"),\n (0x1D65E,\"M\",\"i\"),\n (0x1D65F,\"M\",\"j\"),\n (0x1D660,\"M\",\"k\"),\n (0x1D661,\"M\",\"l\"),\n (0x1D662,\"M\",\"m\"),\n (0x1D663,\"M\",\"n\"),\n (0x1D664,\"M\",\"o\"),\n (0x1D665,\"M\",\"p\"),\n (0x1D666,\"M\",\"q\"),\n (0x1D667,\"M\",\"r\"),\n (0x1D668,\"M\",\"s\"),\n (0x1D669,\"M\",\"t\"),\n (0x1D66A,\"M\",\"u\"),\n (0x1D66B,\"M\",\"v\"),\n (0x1D66C,\"M\",\"w\"),\n (0x1D66D,\"M\",\"x\"),\n (0x1D66E,\"M\",\"y\"),\n (0x1D66F,\"M\",\"z\"),\n (0x1D670,\"M\",\"a\"),\n (0x1D671,\"M\",\"b\"),\n (0x1D672,\"M\",\"c\"),\n (0x1D673,\"M\",\"d\"),\n (0x1D674,\"M\",\"e\"),\n (0x1D675,\"M\",\"f\"),\n (0x1D676,\"M\",\"g\"),\n (0x1D677,\"M\",\"h\"),\n (0x1D678,\"M\",\"i\"),\n (0x1D679,\"M\",\"j\"),\n (0x1D67A,\"M\",\"k\"),\n (0x1D67B,\"M\",\"l\"),\n (0x1D67C,\"M\",\"m\"),\n (0x1D67D,\"M\",\"n\"),\n (0x1D67E,\"M\",\"o\"),\n (0x1D67F,\"M\",\"p\"),\n (0x1D680,\"M\",\"q\"),\n (0x1D681,\"M\",\"r\"),\n (0x1D682,\"M\",\"s\"),\n ]\n \n \ndef _seg_67()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D683,\"M\",\"t\"),\n (0x1D684,\"M\",\"u\"),\n (0x1D685,\"M\",\"v\"),\n (0x1D686,\"M\",\"w\"),\n (0x1D687,\"M\",\"x\"),\n (0x1D688,\"M\",\"y\"),\n (0x1D689,\"M\",\"z\"),\n (0x1D68A,\"M\",\"a\"),\n (0x1D68B,\"M\",\"b\"),\n (0x1D68C,\"M\",\"c\"),\n (0x1D68D,\"M\",\"d\"),\n (0x1D68E,\"M\",\"e\"),\n (0x1D68F,\"M\",\"f\"),\n (0x1D690,\"M\",\"g\"),\n (0x1D691,\"M\",\"h\"),\n (0x1D692,\"M\",\"i\"),\n (0x1D693,\"M\",\"j\"),\n (0x1D694,\"M\",\"k\"),\n (0x1D695,\"M\",\"l\"),\n (0x1D696,\"M\",\"m\"),\n (0x1D697,\"M\",\"n\"),\n (0x1D698,\"M\",\"o\"),\n (0x1D699,\"M\",\"p\"),\n (0x1D69A,\"M\",\"q\"),\n (0x1D69B,\"M\",\"r\"),\n (0x1D69C,\"M\",\"s\"),\n (0x1D69D,\"M\",\"t\"),\n (0x1D69E,\"M\",\"u\"),\n (0x1D69F,\"M\",\"v\"),\n (0x1D6A0,\"M\",\"w\"),\n (0x1D6A1,\"M\",\"x\"),\n (0x1D6A2,\"M\",\"y\"),\n (0x1D6A3,\"M\",\"z\"),\n (0x1D6A4,\"M\",\"\u0131\"),\n (0x1D6A5,\"M\",\"\u0237\"),\n (0x1D6A6,\"X\"),\n (0x1D6A8,\"M\",\"\u03b1\"),\n (0x1D6A9,\"M\",\"\u03b2\"),\n (0x1D6AA,\"M\",\"\u03b3\"),\n (0x1D6AB,\"M\",\"\u03b4\"),\n (0x1D6AC,\"M\",\"\u03b5\"),\n (0x1D6AD,\"M\",\"\u03b6\"),\n (0x1D6AE,\"M\",\"\u03b7\"),\n (0x1D6AF,\"M\",\"\u03b8\"),\n (0x1D6B0,\"M\",\"\u03b9\"),\n (0x1D6B1,\"M\",\"\u03ba\"),\n (0x1D6B2,\"M\",\"\u03bb\"),\n (0x1D6B3,\"M\",\"\u03bc\"),\n (0x1D6B4,\"M\",\"\u03bd\"),\n (0x1D6B5,\"M\",\"\u03be\"),\n (0x1D6B6,\"M\",\"\u03bf\"),\n (0x1D6B7,\"M\",\"\u03c0\"),\n (0x1D6B8,\"M\",\"\u03c1\"),\n (0x1D6B9,\"M\",\"\u03b8\"),\n (0x1D6BA,\"M\",\"\u03c3\"),\n (0x1D6BB,\"M\",\"\u03c4\"),\n (0x1D6BC,\"M\",\"\u03c5\"),\n (0x1D6BD,\"M\",\"\u03c6\"),\n (0x1D6BE,\"M\",\"\u03c7\"),\n (0x1D6BF,\"M\",\"\u03c8\"),\n (0x1D6C0,\"M\",\"\u03c9\"),\n (0x1D6C1,\"M\",\"\u2207\"),\n (0x1D6C2,\"M\",\"\u03b1\"),\n (0x1D6C3,\"M\",\"\u03b2\"),\n (0x1D6C4,\"M\",\"\u03b3\"),\n (0x1D6C5,\"M\",\"\u03b4\"),\n (0x1D6C6,\"M\",\"\u03b5\"),\n (0x1D6C7,\"M\",\"\u03b6\"),\n (0x1D6C8,\"M\",\"\u03b7\"),\n (0x1D6C9,\"M\",\"\u03b8\"),\n (0x1D6CA,\"M\",\"\u03b9\"),\n (0x1D6CB,\"M\",\"\u03ba\"),\n (0x1D6CC,\"M\",\"\u03bb\"),\n (0x1D6CD,\"M\",\"\u03bc\"),\n (0x1D6CE,\"M\",\"\u03bd\"),\n (0x1D6CF,\"M\",\"\u03be\"),\n (0x1D6D0,\"M\",\"\u03bf\"),\n (0x1D6D1,\"M\",\"\u03c0\"),\n (0x1D6D2,\"M\",\"\u03c1\"),\n (0x1D6D3,\"M\",\"\u03c3\"),\n (0x1D6D5,\"M\",\"\u03c4\"),\n (0x1D6D6,\"M\",\"\u03c5\"),\n (0x1D6D7,\"M\",\"\u03c6\"),\n (0x1D6D8,\"M\",\"\u03c7\"),\n (0x1D6D9,\"M\",\"\u03c8\"),\n (0x1D6DA,\"M\",\"\u03c9\"),\n (0x1D6DB,\"M\",\"\u2202\"),\n (0x1D6DC,\"M\",\"\u03b5\"),\n (0x1D6DD,\"M\",\"\u03b8\"),\n (0x1D6DE,\"M\",\"\u03ba\"),\n (0x1D6DF,\"M\",\"\u03c6\"),\n (0x1D6E0,\"M\",\"\u03c1\"),\n (0x1D6E1,\"M\",\"\u03c0\"),\n (0x1D6E2,\"M\",\"\u03b1\"),\n (0x1D6E3,\"M\",\"\u03b2\"),\n (0x1D6E4,\"M\",\"\u03b3\"),\n (0x1D6E5,\"M\",\"\u03b4\"),\n (0x1D6E6,\"M\",\"\u03b5\"),\n (0x1D6E7,\"M\",\"\u03b6\"),\n (0x1D6E8,\"M\",\"\u03b7\"),\n ]\n \n \ndef _seg_68()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D6E9,\"M\",\"\u03b8\"),\n (0x1D6EA,\"M\",\"\u03b9\"),\n (0x1D6EB,\"M\",\"\u03ba\"),\n (0x1D6EC,\"M\",\"\u03bb\"),\n (0x1D6ED,\"M\",\"\u03bc\"),\n (0x1D6EE,\"M\",\"\u03bd\"),\n (0x1D6EF,\"M\",\"\u03be\"),\n (0x1D6F0,\"M\",\"\u03bf\"),\n (0x1D6F1,\"M\",\"\u03c0\"),\n (0x1D6F2,\"M\",\"\u03c1\"),\n (0x1D6F3,\"M\",\"\u03b8\"),\n (0x1D6F4,\"M\",\"\u03c3\"),\n (0x1D6F5,\"M\",\"\u03c4\"),\n (0x1D6F6,\"M\",\"\u03c5\"),\n (0x1D6F7,\"M\",\"\u03c6\"),\n (0x1D6F8,\"M\",\"\u03c7\"),\n (0x1D6F9,\"M\",\"\u03c8\"),\n (0x1D6FA,\"M\",\"\u03c9\"),\n (0x1D6FB,\"M\",\"\u2207\"),\n (0x1D6FC,\"M\",\"\u03b1\"),\n (0x1D6FD,\"M\",\"\u03b2\"),\n (0x1D6FE,\"M\",\"\u03b3\"),\n (0x1D6FF,\"M\",\"\u03b4\"),\n (0x1D700,\"M\",\"\u03b5\"),\n (0x1D701,\"M\",\"\u03b6\"),\n (0x1D702,\"M\",\"\u03b7\"),\n (0x1D703,\"M\",\"\u03b8\"),\n (0x1D704,\"M\",\"\u03b9\"),\n (0x1D705,\"M\",\"\u03ba\"),\n (0x1D706,\"M\",\"\u03bb\"),\n (0x1D707,\"M\",\"\u03bc\"),\n (0x1D708,\"M\",\"\u03bd\"),\n (0x1D709,\"M\",\"\u03be\"),\n (0x1D70A,\"M\",\"\u03bf\"),\n (0x1D70B,\"M\",\"\u03c0\"),\n (0x1D70C,\"M\",\"\u03c1\"),\n (0x1D70D,\"M\",\"\u03c3\"),\n (0x1D70F,\"M\",\"\u03c4\"),\n (0x1D710,\"M\",\"\u03c5\"),\n (0x1D711,\"M\",\"\u03c6\"),\n (0x1D712,\"M\",\"\u03c7\"),\n (0x1D713,\"M\",\"\u03c8\"),\n (0x1D714,\"M\",\"\u03c9\"),\n (0x1D715,\"M\",\"\u2202\"),\n (0x1D716,\"M\",\"\u03b5\"),\n (0x1D717,\"M\",\"\u03b8\"),\n (0x1D718,\"M\",\"\u03ba\"),\n (0x1D719,\"M\",\"\u03c6\"),\n (0x1D71A,\"M\",\"\u03c1\"),\n (0x1D71B,\"M\",\"\u03c0\"),\n (0x1D71C,\"M\",\"\u03b1\"),\n (0x1D71D,\"M\",\"\u03b2\"),\n (0x1D71E,\"M\",\"\u03b3\"),\n (0x1D71F,\"M\",\"\u03b4\"),\n (0x1D720,\"M\",\"\u03b5\"),\n (0x1D721,\"M\",\"\u03b6\"),\n (0x1D722,\"M\",\"\u03b7\"),\n (0x1D723,\"M\",\"\u03b8\"),\n (0x1D724,\"M\",\"\u03b9\"),\n (0x1D725,\"M\",\"\u03ba\"),\n (0x1D726,\"M\",\"\u03bb\"),\n (0x1D727,\"M\",\"\u03bc\"),\n (0x1D728,\"M\",\"\u03bd\"),\n (0x1D729,\"M\",\"\u03be\"),\n (0x1D72A,\"M\",\"\u03bf\"),\n (0x1D72B,\"M\",\"\u03c0\"),\n (0x1D72C,\"M\",\"\u03c1\"),\n (0x1D72D,\"M\",\"\u03b8\"),\n (0x1D72E,\"M\",\"\u03c3\"),\n (0x1D72F,\"M\",\"\u03c4\"),\n (0x1D730,\"M\",\"\u03c5\"),\n (0x1D731,\"M\",\"\u03c6\"),\n (0x1D732,\"M\",\"\u03c7\"),\n (0x1D733,\"M\",\"\u03c8\"),\n (0x1D734,\"M\",\"\u03c9\"),\n (0x1D735,\"M\",\"\u2207\"),\n (0x1D736,\"M\",\"\u03b1\"),\n (0x1D737,\"M\",\"\u03b2\"),\n (0x1D738,\"M\",\"\u03b3\"),\n (0x1D739,\"M\",\"\u03b4\"),\n (0x1D73A,\"M\",\"\u03b5\"),\n (0x1D73B,\"M\",\"\u03b6\"),\n (0x1D73C,\"M\",\"\u03b7\"),\n (0x1D73D,\"M\",\"\u03b8\"),\n (0x1D73E,\"M\",\"\u03b9\"),\n (0x1D73F,\"M\",\"\u03ba\"),\n (0x1D740,\"M\",\"\u03bb\"),\n (0x1D741,\"M\",\"\u03bc\"),\n (0x1D742,\"M\",\"\u03bd\"),\n (0x1D743,\"M\",\"\u03be\"),\n (0x1D744,\"M\",\"\u03bf\"),\n (0x1D745,\"M\",\"\u03c0\"),\n (0x1D746,\"M\",\"\u03c1\"),\n (0x1D747,\"M\",\"\u03c3\"),\n (0x1D749,\"M\",\"\u03c4\"),\n (0x1D74A,\"M\",\"\u03c5\"),\n (0x1D74B,\"M\",\"\u03c6\"),\n (0x1D74C,\"M\",\"\u03c7\"),\n (0x1D74D,\"M\",\"\u03c8\"),\n (0x1D74E,\"M\",\"\u03c9\"),\n ]\n \n \ndef _seg_69()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D74F,\"M\",\"\u2202\"),\n (0x1D750,\"M\",\"\u03b5\"),\n (0x1D751,\"M\",\"\u03b8\"),\n (0x1D752,\"M\",\"\u03ba\"),\n (0x1D753,\"M\",\"\u03c6\"),\n (0x1D754,\"M\",\"\u03c1\"),\n (0x1D755,\"M\",\"\u03c0\"),\n (0x1D756,\"M\",\"\u03b1\"),\n (0x1D757,\"M\",\"\u03b2\"),\n (0x1D758,\"M\",\"\u03b3\"),\n (0x1D759,\"M\",\"\u03b4\"),\n (0x1D75A,\"M\",\"\u03b5\"),\n (0x1D75B,\"M\",\"\u03b6\"),\n (0x1D75C,\"M\",\"\u03b7\"),\n (0x1D75D,\"M\",\"\u03b8\"),\n (0x1D75E,\"M\",\"\u03b9\"),\n (0x1D75F,\"M\",\"\u03ba\"),\n (0x1D760,\"M\",\"\u03bb\"),\n (0x1D761,\"M\",\"\u03bc\"),\n (0x1D762,\"M\",\"\u03bd\"),\n (0x1D763,\"M\",\"\u03be\"),\n (0x1D764,\"M\",\"\u03bf\"),\n (0x1D765,\"M\",\"\u03c0\"),\n (0x1D766,\"M\",\"\u03c1\"),\n (0x1D767,\"M\",\"\u03b8\"),\n (0x1D768,\"M\",\"\u03c3\"),\n (0x1D769,\"M\",\"\u03c4\"),\n (0x1D76A,\"M\",\"\u03c5\"),\n (0x1D76B,\"M\",\"\u03c6\"),\n (0x1D76C,\"M\",\"\u03c7\"),\n (0x1D76D,\"M\",\"\u03c8\"),\n (0x1D76E,\"M\",\"\u03c9\"),\n (0x1D76F,\"M\",\"\u2207\"),\n (0x1D770,\"M\",\"\u03b1\"),\n (0x1D771,\"M\",\"\u03b2\"),\n (0x1D772,\"M\",\"\u03b3\"),\n (0x1D773,\"M\",\"\u03b4\"),\n (0x1D774,\"M\",\"\u03b5\"),\n (0x1D775,\"M\",\"\u03b6\"),\n (0x1D776,\"M\",\"\u03b7\"),\n (0x1D777,\"M\",\"\u03b8\"),\n (0x1D778,\"M\",\"\u03b9\"),\n (0x1D779,\"M\",\"\u03ba\"),\n (0x1D77A,\"M\",\"\u03bb\"),\n (0x1D77B,\"M\",\"\u03bc\"),\n (0x1D77C,\"M\",\"\u03bd\"),\n (0x1D77D,\"M\",\"\u03be\"),\n (0x1D77E,\"M\",\"\u03bf\"),\n (0x1D77F,\"M\",\"\u03c0\"),\n (0x1D780,\"M\",\"\u03c1\"),\n (0x1D781,\"M\",\"\u03c3\"),\n (0x1D783,\"M\",\"\u03c4\"),\n (0x1D784,\"M\",\"\u03c5\"),\n (0x1D785,\"M\",\"\u03c6\"),\n (0x1D786,\"M\",\"\u03c7\"),\n (0x1D787,\"M\",\"\u03c8\"),\n (0x1D788,\"M\",\"\u03c9\"),\n (0x1D789,\"M\",\"\u2202\"),\n (0x1D78A,\"M\",\"\u03b5\"),\n (0x1D78B,\"M\",\"\u03b8\"),\n (0x1D78C,\"M\",\"\u03ba\"),\n (0x1D78D,\"M\",\"\u03c6\"),\n (0x1D78E,\"M\",\"\u03c1\"),\n (0x1D78F,\"M\",\"\u03c0\"),\n (0x1D790,\"M\",\"\u03b1\"),\n (0x1D791,\"M\",\"\u03b2\"),\n (0x1D792,\"M\",\"\u03b3\"),\n (0x1D793,\"M\",\"\u03b4\"),\n (0x1D794,\"M\",\"\u03b5\"),\n (0x1D795,\"M\",\"\u03b6\"),\n (0x1D796,\"M\",\"\u03b7\"),\n (0x1D797,\"M\",\"\u03b8\"),\n (0x1D798,\"M\",\"\u03b9\"),\n (0x1D799,\"M\",\"\u03ba\"),\n (0x1D79A,\"M\",\"\u03bb\"),\n (0x1D79B,\"M\",\"\u03bc\"),\n (0x1D79C,\"M\",\"\u03bd\"),\n (0x1D79D,\"M\",\"\u03be\"),\n (0x1D79E,\"M\",\"\u03bf\"),\n (0x1D79F,\"M\",\"\u03c0\"),\n (0x1D7A0,\"M\",\"\u03c1\"),\n (0x1D7A1,\"M\",\"\u03b8\"),\n (0x1D7A2,\"M\",\"\u03c3\"),\n (0x1D7A3,\"M\",\"\u03c4\"),\n (0x1D7A4,\"M\",\"\u03c5\"),\n (0x1D7A5,\"M\",\"\u03c6\"),\n (0x1D7A6,\"M\",\"\u03c7\"),\n (0x1D7A7,\"M\",\"\u03c8\"),\n (0x1D7A8,\"M\",\"\u03c9\"),\n (0x1D7A9,\"M\",\"\u2207\"),\n (0x1D7AA,\"M\",\"\u03b1\"),\n (0x1D7AB,\"M\",\"\u03b2\"),\n (0x1D7AC,\"M\",\"\u03b3\"),\n (0x1D7AD,\"M\",\"\u03b4\"),\n (0x1D7AE,\"M\",\"\u03b5\"),\n (0x1D7AF,\"M\",\"\u03b6\"),\n (0x1D7B0,\"M\",\"\u03b7\"),\n (0x1D7B1,\"M\",\"\u03b8\"),\n (0x1D7B2,\"M\",\"\u03b9\"),\n (0x1D7B3,\"M\",\"\u03ba\"),\n ]\n \n \ndef _seg_70()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1D7B4,\"M\",\"\u03bb\"),\n (0x1D7B5,\"M\",\"\u03bc\"),\n (0x1D7B6,\"M\",\"\u03bd\"),\n (0x1D7B7,\"M\",\"\u03be\"),\n (0x1D7B8,\"M\",\"\u03bf\"),\n (0x1D7B9,\"M\",\"\u03c0\"),\n (0x1D7BA,\"M\",\"\u03c1\"),\n (0x1D7BB,\"M\",\"\u03c3\"),\n (0x1D7BD,\"M\",\"\u03c4\"),\n (0x1D7BE,\"M\",\"\u03c5\"),\n (0x1D7BF,\"M\",\"\u03c6\"),\n (0x1D7C0,\"M\",\"\u03c7\"),\n (0x1D7C1,\"M\",\"\u03c8\"),\n (0x1D7C2,\"M\",\"\u03c9\"),\n (0x1D7C3,\"M\",\"\u2202\"),\n (0x1D7C4,\"M\",\"\u03b5\"),\n (0x1D7C5,\"M\",\"\u03b8\"),\n (0x1D7C6,\"M\",\"\u03ba\"),\n (0x1D7C7,\"M\",\"\u03c6\"),\n (0x1D7C8,\"M\",\"\u03c1\"),\n (0x1D7C9,\"M\",\"\u03c0\"),\n (0x1D7CA,\"M\",\"\u03dd\"),\n (0x1D7CC,\"X\"),\n (0x1D7CE,\"M\",\"0\"),\n (0x1D7CF,\"M\",\"1\"),\n (0x1D7D0,\"M\",\"2\"),\n (0x1D7D1,\"M\",\"3\"),\n (0x1D7D2,\"M\",\"4\"),\n (0x1D7D3,\"M\",\"5\"),\n (0x1D7D4,\"M\",\"6\"),\n (0x1D7D5,\"M\",\"7\"),\n (0x1D7D6,\"M\",\"8\"),\n (0x1D7D7,\"M\",\"9\"),\n (0x1D7D8,\"M\",\"0\"),\n (0x1D7D9,\"M\",\"1\"),\n (0x1D7DA,\"M\",\"2\"),\n (0x1D7DB,\"M\",\"3\"),\n (0x1D7DC,\"M\",\"4\"),\n (0x1D7DD,\"M\",\"5\"),\n (0x1D7DE,\"M\",\"6\"),\n (0x1D7DF,\"M\",\"7\"),\n (0x1D7E0,\"M\",\"8\"),\n (0x1D7E1,\"M\",\"9\"),\n (0x1D7E2,\"M\",\"0\"),\n (0x1D7E3,\"M\",\"1\"),\n (0x1D7E4,\"M\",\"2\"),\n (0x1D7E5,\"M\",\"3\"),\n (0x1D7E6,\"M\",\"4\"),\n (0x1D7E7,\"M\",\"5\"),\n (0x1D7E8,\"M\",\"6\"),\n (0x1D7E9,\"M\",\"7\"),\n (0x1D7EA,\"M\",\"8\"),\n (0x1D7EB,\"M\",\"9\"),\n (0x1D7EC,\"M\",\"0\"),\n (0x1D7ED,\"M\",\"1\"),\n (0x1D7EE,\"M\",\"2\"),\n (0x1D7EF,\"M\",\"3\"),\n (0x1D7F0,\"M\",\"4\"),\n (0x1D7F1,\"M\",\"5\"),\n (0x1D7F2,\"M\",\"6\"),\n (0x1D7F3,\"M\",\"7\"),\n (0x1D7F4,\"M\",\"8\"),\n (0x1D7F5,\"M\",\"9\"),\n (0x1D7F6,\"M\",\"0\"),\n (0x1D7F7,\"M\",\"1\"),\n (0x1D7F8,\"M\",\"2\"),\n (0x1D7F9,\"M\",\"3\"),\n (0x1D7FA,\"M\",\"4\"),\n (0x1D7FB,\"M\",\"5\"),\n (0x1D7FC,\"M\",\"6\"),\n (0x1D7FD,\"M\",\"7\"),\n (0x1D7FE,\"M\",\"8\"),\n (0x1D7FF,\"M\",\"9\"),\n (0x1D800,\"V\"),\n (0x1DA8C,\"X\"),\n (0x1DA9B,\"V\"),\n (0x1DAA0,\"X\"),\n (0x1DAA1,\"V\"),\n (0x1DAB0,\"X\"),\n (0x1DF00,\"V\"),\n (0x1DF1F,\"X\"),\n (0x1DF25,\"V\"),\n (0x1DF2B,\"X\"),\n (0x1E000,\"V\"),\n (0x1E007,\"X\"),\n (0x1E008,\"V\"),\n (0x1E019,\"X\"),\n (0x1E01B,\"V\"),\n (0x1E022,\"X\"),\n (0x1E023,\"V\"),\n (0x1E025,\"X\"),\n (0x1E026,\"V\"),\n (0x1E02B,\"X\"),\n (0x1E030,\"M\",\"\u0430\"),\n (0x1E031,\"M\",\"\u0431\"),\n (0x1E032,\"M\",\"\u0432\"),\n (0x1E033,\"M\",\"\u0433\"),\n (0x1E034,\"M\",\"\u0434\"),\n (0x1E035,\"M\",\"\u0435\"),\n (0x1E036,\"M\",\"\u0436\"),\n ]\n \n \ndef _seg_71()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1E037,\"M\",\"\u0437\"),\n (0x1E038,\"M\",\"\u0438\"),\n (0x1E039,\"M\",\"\u043a\"),\n (0x1E03A,\"M\",\"\u043b\"),\n (0x1E03B,\"M\",\"\u043c\"),\n (0x1E03C,\"M\",\"\u043e\"),\n (0x1E03D,\"M\",\"\u043f\"),\n (0x1E03E,\"M\",\"\u0440\"),\n (0x1E03F,\"M\",\"\u0441\"),\n (0x1E040,\"M\",\"\u0442\"),\n (0x1E041,\"M\",\"\u0443\"),\n (0x1E042,\"M\",\"\u0444\"),\n (0x1E043,\"M\",\"\u0445\"),\n (0x1E044,\"M\",\"\u0446\"),\n (0x1E045,\"M\",\"\u0447\"),\n (0x1E046,\"M\",\"\u0448\"),\n (0x1E047,\"M\",\"\u044b\"),\n (0x1E048,\"M\",\"\u044d\"),\n (0x1E049,\"M\",\"\u044e\"),\n (0x1E04A,\"M\",\"\ua689\"),\n (0x1E04B,\"M\",\"\u04d9\"),\n (0x1E04C,\"M\",\"\u0456\"),\n (0x1E04D,\"M\",\"\u0458\"),\n (0x1E04E,\"M\",\"\u04e9\"),\n (0x1E04F,\"M\",\"\u04af\"),\n (0x1E050,\"M\",\"\u04cf\"),\n (0x1E051,\"M\",\"\u0430\"),\n (0x1E052,\"M\",\"\u0431\"),\n (0x1E053,\"M\",\"\u0432\"),\n (0x1E054,\"M\",\"\u0433\"),\n (0x1E055,\"M\",\"\u0434\"),\n (0x1E056,\"M\",\"\u0435\"),\n (0x1E057,\"M\",\"\u0436\"),\n (0x1E058,\"M\",\"\u0437\"),\n (0x1E059,\"M\",\"\u0438\"),\n (0x1E05A,\"M\",\"\u043a\"),\n (0x1E05B,\"M\",\"\u043b\"),\n (0x1E05C,\"M\",\"\u043e\"),\n (0x1E05D,\"M\",\"\u043f\"),\n (0x1E05E,\"M\",\"\u0441\"),\n (0x1E05F,\"M\",\"\u0443\"),\n (0x1E060,\"M\",\"\u0444\"),\n (0x1E061,\"M\",\"\u0445\"),\n (0x1E062,\"M\",\"\u0446\"),\n (0x1E063,\"M\",\"\u0447\"),\n (0x1E064,\"M\",\"\u0448\"),\n (0x1E065,\"M\",\"\u044a\"),\n (0x1E066,\"M\",\"\u044b\"),\n (0x1E067,\"M\",\"\u0491\"),\n (0x1E068,\"M\",\"\u0456\"),\n (0x1E069,\"M\",\"\u0455\"),\n (0x1E06A,\"M\",\"\u045f\"),\n (0x1E06B,\"M\",\"\u04ab\"),\n (0x1E06C,\"M\",\"\ua651\"),\n (0x1E06D,\"M\",\"\u04b1\"),\n (0x1E06E,\"X\"),\n (0x1E08F,\"V\"),\n (0x1E090,\"X\"),\n (0x1E100,\"V\"),\n (0x1E12D,\"X\"),\n (0x1E130,\"V\"),\n (0x1E13E,\"X\"),\n (0x1E140,\"V\"),\n (0x1E14A,\"X\"),\n (0x1E14E,\"V\"),\n (0x1E150,\"X\"),\n (0x1E290,\"V\"),\n (0x1E2AF,\"X\"),\n (0x1E2C0,\"V\"),\n (0x1E2FA,\"X\"),\n (0x1E2FF,\"V\"),\n (0x1E300,\"X\"),\n (0x1E4D0,\"V\"),\n (0x1E4FA,\"X\"),\n (0x1E7E0,\"V\"),\n (0x1E7E7,\"X\"),\n (0x1E7E8,\"V\"),\n (0x1E7EC,\"X\"),\n (0x1E7ED,\"V\"),\n (0x1E7EF,\"X\"),\n (0x1E7F0,\"V\"),\n (0x1E7FF,\"X\"),\n (0x1E800,\"V\"),\n (0x1E8C5,\"X\"),\n (0x1E8C7,\"V\"),\n (0x1E8D7,\"X\"),\n (0x1E900,\"M\",\"\ud83a\udd22\"),\n (0x1E901,\"M\",\"\ud83a\udd23\"),\n (0x1E902,\"M\",\"\ud83a\udd24\"),\n (0x1E903,\"M\",\"\ud83a\udd25\"),\n (0x1E904,\"M\",\"\ud83a\udd26\"),\n (0x1E905,\"M\",\"\ud83a\udd27\"),\n (0x1E906,\"M\",\"\ud83a\udd28\"),\n (0x1E907,\"M\",\"\ud83a\udd29\"),\n (0x1E908,\"M\",\"\ud83a\udd2a\"),\n (0x1E909,\"M\",\"\ud83a\udd2b\"),\n (0x1E90A,\"M\",\"\ud83a\udd2c\"),\n (0x1E90B,\"M\",\"\ud83a\udd2d\"),\n (0x1E90C,\"M\",\"\ud83a\udd2e\"),\n (0x1E90D,\"M\",\"\ud83a\udd2f\"),\n ]\n \n \ndef _seg_72()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1E90E,\"M\",\"\ud83a\udd30\"),\n (0x1E90F,\"M\",\"\ud83a\udd31\"),\n (0x1E910,\"M\",\"\ud83a\udd32\"),\n (0x1E911,\"M\",\"\ud83a\udd33\"),\n (0x1E912,\"M\",\"\ud83a\udd34\"),\n (0x1E913,\"M\",\"\ud83a\udd35\"),\n (0x1E914,\"M\",\"\ud83a\udd36\"),\n (0x1E915,\"M\",\"\ud83a\udd37\"),\n (0x1E916,\"M\",\"\ud83a\udd38\"),\n (0x1E917,\"M\",\"\ud83a\udd39\"),\n (0x1E918,\"M\",\"\ud83a\udd3a\"),\n (0x1E919,\"M\",\"\ud83a\udd3b\"),\n (0x1E91A,\"M\",\"\ud83a\udd3c\"),\n (0x1E91B,\"M\",\"\ud83a\udd3d\"),\n (0x1E91C,\"M\",\"\ud83a\udd3e\"),\n (0x1E91D,\"M\",\"\ud83a\udd3f\"),\n (0x1E91E,\"M\",\"\ud83a\udd40\"),\n (0x1E91F,\"M\",\"\ud83a\udd41\"),\n (0x1E920,\"M\",\"\ud83a\udd42\"),\n (0x1E921,\"M\",\"\ud83a\udd43\"),\n (0x1E922,\"V\"),\n (0x1E94C,\"X\"),\n (0x1E950,\"V\"),\n (0x1E95A,\"X\"),\n (0x1E95E,\"V\"),\n (0x1E960,\"X\"),\n (0x1EC71,\"V\"),\n (0x1ECB5,\"X\"),\n (0x1ED01,\"V\"),\n (0x1ED3E,\"X\"),\n (0x1EE00,\"M\",\"\u0627\"),\n (0x1EE01,\"M\",\"\u0628\"),\n (0x1EE02,\"M\",\"\u062c\"),\n (0x1EE03,\"M\",\"\u062f\"),\n (0x1EE04,\"X\"),\n (0x1EE05,\"M\",\"\u0648\"),\n (0x1EE06,\"M\",\"\u0632\"),\n (0x1EE07,\"M\",\"\u062d\"),\n (0x1EE08,\"M\",\"\u0637\"),\n (0x1EE09,\"M\",\"\u064a\"),\n (0x1EE0A,\"M\",\"\u0643\"),\n (0x1EE0B,\"M\",\"\u0644\"),\n (0x1EE0C,\"M\",\"\u0645\"),\n (0x1EE0D,\"M\",\"\u0646\"),\n (0x1EE0E,\"M\",\"\u0633\"),\n (0x1EE0F,\"M\",\"\u0639\"),\n (0x1EE10,\"M\",\"\u0641\"),\n (0x1EE11,\"M\",\"\u0635\"),\n (0x1EE12,\"M\",\"\u0642\"),\n (0x1EE13,\"M\",\"\u0631\"),\n (0x1EE14,\"M\",\"\u0634\"),\n (0x1EE15,\"M\",\"\u062a\"),\n (0x1EE16,\"M\",\"\u062b\"),\n (0x1EE17,\"M\",\"\u062e\"),\n (0x1EE18,\"M\",\"\u0630\"),\n (0x1EE19,\"M\",\"\u0636\"),\n (0x1EE1A,\"M\",\"\u0638\"),\n (0x1EE1B,\"M\",\"\u063a\"),\n (0x1EE1C,\"M\",\"\u066e\"),\n (0x1EE1D,\"M\",\"\u06ba\"),\n (0x1EE1E,\"M\",\"\u06a1\"),\n (0x1EE1F,\"M\",\"\u066f\"),\n (0x1EE20,\"X\"),\n (0x1EE21,\"M\",\"\u0628\"),\n (0x1EE22,\"M\",\"\u062c\"),\n (0x1EE23,\"X\"),\n (0x1EE24,\"M\",\"\u0647\"),\n (0x1EE25,\"X\"),\n (0x1EE27,\"M\",\"\u062d\"),\n (0x1EE28,\"X\"),\n (0x1EE29,\"M\",\"\u064a\"),\n (0x1EE2A,\"M\",\"\u0643\"),\n (0x1EE2B,\"M\",\"\u0644\"),\n (0x1EE2C,\"M\",\"\u0645\"),\n (0x1EE2D,\"M\",\"\u0646\"),\n (0x1EE2E,\"M\",\"\u0633\"),\n (0x1EE2F,\"M\",\"\u0639\"),\n (0x1EE30,\"M\",\"\u0641\"),\n (0x1EE31,\"M\",\"\u0635\"),\n (0x1EE32,\"M\",\"\u0642\"),\n (0x1EE33,\"X\"),\n (0x1EE34,\"M\",\"\u0634\"),\n (0x1EE35,\"M\",\"\u062a\"),\n (0x1EE36,\"M\",\"\u062b\"),\n (0x1EE37,\"M\",\"\u062e\"),\n (0x1EE38,\"X\"),\n (0x1EE39,\"M\",\"\u0636\"),\n (0x1EE3A,\"X\"),\n (0x1EE3B,\"M\",\"\u063a\"),\n (0x1EE3C,\"X\"),\n (0x1EE42,\"M\",\"\u062c\"),\n (0x1EE43,\"X\"),\n (0x1EE47,\"M\",\"\u062d\"),\n (0x1EE48,\"X\"),\n (0x1EE49,\"M\",\"\u064a\"),\n (0x1EE4A,\"X\"),\n (0x1EE4B,\"M\",\"\u0644\"),\n (0x1EE4C,\"X\"),\n (0x1EE4D,\"M\",\"\u0646\"),\n (0x1EE4E,\"M\",\"\u0633\"),\n ]\n \n \ndef _seg_73()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1EE4F,\"M\",\"\u0639\"),\n (0x1EE50,\"X\"),\n (0x1EE51,\"M\",\"\u0635\"),\n (0x1EE52,\"M\",\"\u0642\"),\n (0x1EE53,\"X\"),\n (0x1EE54,\"M\",\"\u0634\"),\n (0x1EE55,\"X\"),\n (0x1EE57,\"M\",\"\u062e\"),\n (0x1EE58,\"X\"),\n (0x1EE59,\"M\",\"\u0636\"),\n (0x1EE5A,\"X\"),\n (0x1EE5B,\"M\",\"\u063a\"),\n (0x1EE5C,\"X\"),\n (0x1EE5D,\"M\",\"\u06ba\"),\n (0x1EE5E,\"X\"),\n (0x1EE5F,\"M\",\"\u066f\"),\n (0x1EE60,\"X\"),\n (0x1EE61,\"M\",\"\u0628\"),\n (0x1EE62,\"M\",\"\u062c\"),\n (0x1EE63,\"X\"),\n (0x1EE64,\"M\",\"\u0647\"),\n (0x1EE65,\"X\"),\n (0x1EE67,\"M\",\"\u062d\"),\n (0x1EE68,\"M\",\"\u0637\"),\n (0x1EE69,\"M\",\"\u064a\"),\n (0x1EE6A,\"M\",\"\u0643\"),\n (0x1EE6B,\"X\"),\n (0x1EE6C,\"M\",\"\u0645\"),\n (0x1EE6D,\"M\",\"\u0646\"),\n (0x1EE6E,\"M\",\"\u0633\"),\n (0x1EE6F,\"M\",\"\u0639\"),\n (0x1EE70,\"M\",\"\u0641\"),\n (0x1EE71,\"M\",\"\u0635\"),\n (0x1EE72,\"M\",\"\u0642\"),\n (0x1EE73,\"X\"),\n (0x1EE74,\"M\",\"\u0634\"),\n (0x1EE75,\"M\",\"\u062a\"),\n (0x1EE76,\"M\",\"\u062b\"),\n (0x1EE77,\"M\",\"\u062e\"),\n (0x1EE78,\"X\"),\n (0x1EE79,\"M\",\"\u0636\"),\n (0x1EE7A,\"M\",\"\u0638\"),\n (0x1EE7B,\"M\",\"\u063a\"),\n (0x1EE7C,\"M\",\"\u066e\"),\n (0x1EE7D,\"X\"),\n (0x1EE7E,\"M\",\"\u06a1\"),\n (0x1EE7F,\"X\"),\n (0x1EE80,\"M\",\"\u0627\"),\n (0x1EE81,\"M\",\"\u0628\"),\n (0x1EE82,\"M\",\"\u062c\"),\n (0x1EE83,\"M\",\"\u062f\"),\n (0x1EE84,\"M\",\"\u0647\"),\n (0x1EE85,\"M\",\"\u0648\"),\n (0x1EE86,\"M\",\"\u0632\"),\n (0x1EE87,\"M\",\"\u062d\"),\n (0x1EE88,\"M\",\"\u0637\"),\n (0x1EE89,\"M\",\"\u064a\"),\n (0x1EE8A,\"X\"),\n (0x1EE8B,\"M\",\"\u0644\"),\n (0x1EE8C,\"M\",\"\u0645\"),\n (0x1EE8D,\"M\",\"\u0646\"),\n (0x1EE8E,\"M\",\"\u0633\"),\n (0x1EE8F,\"M\",\"\u0639\"),\n (0x1EE90,\"M\",\"\u0641\"),\n (0x1EE91,\"M\",\"\u0635\"),\n (0x1EE92,\"M\",\"\u0642\"),\n (0x1EE93,\"M\",\"\u0631\"),\n (0x1EE94,\"M\",\"\u0634\"),\n (0x1EE95,\"M\",\"\u062a\"),\n (0x1EE96,\"M\",\"\u062b\"),\n (0x1EE97,\"M\",\"\u062e\"),\n (0x1EE98,\"M\",\"\u0630\"),\n (0x1EE99,\"M\",\"\u0636\"),\n (0x1EE9A,\"M\",\"\u0638\"),\n (0x1EE9B,\"M\",\"\u063a\"),\n (0x1EE9C,\"X\"),\n (0x1EEA1,\"M\",\"\u0628\"),\n (0x1EEA2,\"M\",\"\u062c\"),\n (0x1EEA3,\"M\",\"\u062f\"),\n (0x1EEA4,\"X\"),\n (0x1EEA5,\"M\",\"\u0648\"),\n (0x1EEA6,\"M\",\"\u0632\"),\n (0x1EEA7,\"M\",\"\u062d\"),\n (0x1EEA8,\"M\",\"\u0637\"),\n (0x1EEA9,\"M\",\"\u064a\"),\n (0x1EEAA,\"X\"),\n (0x1EEAB,\"M\",\"\u0644\"),\n (0x1EEAC,\"M\",\"\u0645\"),\n (0x1EEAD,\"M\",\"\u0646\"),\n (0x1EEAE,\"M\",\"\u0633\"),\n (0x1EEAF,\"M\",\"\u0639\"),\n (0x1EEB0,\"M\",\"\u0641\"),\n (0x1EEB1,\"M\",\"\u0635\"),\n (0x1EEB2,\"M\",\"\u0642\"),\n (0x1EEB3,\"M\",\"\u0631\"),\n (0x1EEB4,\"M\",\"\u0634\"),\n (0x1EEB5,\"M\",\"\u062a\"),\n (0x1EEB6,\"M\",\"\u062b\"),\n (0x1EEB7,\"M\",\"\u062e\"),\n (0x1EEB8,\"M\",\"\u0630\"),\n ]\n \n \ndef _seg_74()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1EEB9,\"M\",\"\u0636\"),\n (0x1EEBA,\"M\",\"\u0638\"),\n (0x1EEBB,\"M\",\"\u063a\"),\n (0x1EEBC,\"X\"),\n (0x1EEF0,\"V\"),\n (0x1EEF2,\"X\"),\n (0x1F000,\"V\"),\n (0x1F02C,\"X\"),\n (0x1F030,\"V\"),\n (0x1F094,\"X\"),\n (0x1F0A0,\"V\"),\n (0x1F0AF,\"X\"),\n (0x1F0B1,\"V\"),\n (0x1F0C0,\"X\"),\n (0x1F0C1,\"V\"),\n (0x1F0D0,\"X\"),\n (0x1F0D1,\"V\"),\n (0x1F0F6,\"X\"),\n (0x1F101,\"3\",\"0,\"),\n (0x1F102,\"3\",\"1,\"),\n (0x1F103,\"3\",\"2,\"),\n (0x1F104,\"3\",\"3,\"),\n (0x1F105,\"3\",\"4,\"),\n (0x1F106,\"3\",\"5,\"),\n (0x1F107,\"3\",\"6,\"),\n (0x1F108,\"3\",\"7,\"),\n (0x1F109,\"3\",\"8,\"),\n (0x1F10A,\"3\",\"9,\"),\n (0x1F10B,\"V\"),\n (0x1F110,\"3\",\"(a)\"),\n (0x1F111,\"3\",\"(b)\"),\n (0x1F112,\"3\",\"(c)\"),\n (0x1F113,\"3\",\"(d)\"),\n (0x1F114,\"3\",\"(e)\"),\n (0x1F115,\"3\",\"(f)\"),\n (0x1F116,\"3\",\"(g)\"),\n (0x1F117,\"3\",\"(h)\"),\n (0x1F118,\"3\",\"(i)\"),\n (0x1F119,\"3\",\"(j)\"),\n (0x1F11A,\"3\",\"(k)\"),\n (0x1F11B,\"3\",\"(l)\"),\n (0x1F11C,\"3\",\"(m)\"),\n (0x1F11D,\"3\",\"(n)\"),\n (0x1F11E,\"3\",\"(o)\"),\n (0x1F11F,\"3\",\"(p)\"),\n (0x1F120,\"3\",\"(q)\"),\n (0x1F121,\"3\",\"(r)\"),\n (0x1F122,\"3\",\"(s)\"),\n (0x1F123,\"3\",\"(t)\"),\n (0x1F124,\"3\",\"(u)\"),\n (0x1F125,\"3\",\"(v)\"),\n (0x1F126,\"3\",\"(w)\"),\n (0x1F127,\"3\",\"(x)\"),\n (0x1F128,\"3\",\"(y)\"),\n (0x1F129,\"3\",\"(z)\"),\n (0x1F12A,\"M\",\"\u3014s\u3015\"),\n (0x1F12B,\"M\",\"c\"),\n (0x1F12C,\"M\",\"r\"),\n (0x1F12D,\"M\",\"cd\"),\n (0x1F12E,\"M\",\"wz\"),\n (0x1F12F,\"V\"),\n (0x1F130,\"M\",\"a\"),\n (0x1F131,\"M\",\"b\"),\n (0x1F132,\"M\",\"c\"),\n (0x1F133,\"M\",\"d\"),\n (0x1F134,\"M\",\"e\"),\n (0x1F135,\"M\",\"f\"),\n (0x1F136,\"M\",\"g\"),\n (0x1F137,\"M\",\"h\"),\n (0x1F138,\"M\",\"i\"),\n (0x1F139,\"M\",\"j\"),\n (0x1F13A,\"M\",\"k\"),\n (0x1F13B,\"M\",\"l\"),\n (0x1F13C,\"M\",\"m\"),\n (0x1F13D,\"M\",\"n\"),\n (0x1F13E,\"M\",\"o\"),\n (0x1F13F,\"M\",\"p\"),\n (0x1F140,\"M\",\"q\"),\n (0x1F141,\"M\",\"r\"),\n (0x1F142,\"M\",\"s\"),\n (0x1F143,\"M\",\"t\"),\n (0x1F144,\"M\",\"u\"),\n (0x1F145,\"M\",\"v\"),\n (0x1F146,\"M\",\"w\"),\n (0x1F147,\"M\",\"x\"),\n (0x1F148,\"M\",\"y\"),\n (0x1F149,\"M\",\"z\"),\n (0x1F14A,\"M\",\"hv\"),\n (0x1F14B,\"M\",\"mv\"),\n (0x1F14C,\"M\",\"sd\"),\n (0x1F14D,\"M\",\"ss\"),\n (0x1F14E,\"M\",\"ppv\"),\n (0x1F14F,\"M\",\"wc\"),\n (0x1F150,\"V\"),\n (0x1F16A,\"M\",\"mc\"),\n (0x1F16B,\"M\",\"md\"),\n (0x1F16C,\"M\",\"mr\"),\n (0x1F16D,\"V\"),\n (0x1F190,\"M\",\"dj\"),\n (0x1F191,\"V\"),\n ]\n \n \ndef _seg_75()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1F1AE,\"X\"),\n (0x1F1E6,\"V\"),\n (0x1F200,\"M\",\"\u307b\u304b\"),\n (0x1F201,\"M\",\"\u30b3\u30b3\"),\n (0x1F202,\"M\",\"\u30b5\"),\n (0x1F203,\"X\"),\n (0x1F210,\"M\",\"\u624b\"),\n (0x1F211,\"M\",\"\u5b57\"),\n (0x1F212,\"M\",\"\u53cc\"),\n (0x1F213,\"M\",\"\u30c7\"),\n (0x1F214,\"M\",\"\u4e8c\"),\n (0x1F215,\"M\",\"\u591a\"),\n (0x1F216,\"M\",\"\u89e3\"),\n (0x1F217,\"M\",\"\u5929\"),\n (0x1F218,\"M\",\"\u4ea4\"),\n (0x1F219,\"M\",\"\u6620\"),\n (0x1F21A,\"M\",\"\u7121\"),\n (0x1F21B,\"M\",\"\u6599\"),\n (0x1F21C,\"M\",\"\u524d\"),\n (0x1F21D,\"M\",\"\u5f8c\"),\n (0x1F21E,\"M\",\"\u518d\"),\n (0x1F21F,\"M\",\"\u65b0\"),\n (0x1F220,\"M\",\"\u521d\"),\n (0x1F221,\"M\",\"\u7d42\"),\n (0x1F222,\"M\",\"\u751f\"),\n (0x1F223,\"M\",\"\u8ca9\"),\n (0x1F224,\"M\",\"\u58f0\"),\n (0x1F225,\"M\",\"\u5439\"),\n (0x1F226,\"M\",\"\u6f14\"),\n (0x1F227,\"M\",\"\u6295\"),\n (0x1F228,\"M\",\"\u6355\"),\n (0x1F229,\"M\",\"\u4e00\"),\n (0x1F22A,\"M\",\"\u4e09\"),\n (0x1F22B,\"M\",\"\u904a\"),\n (0x1F22C,\"M\",\"\u5de6\"),\n (0x1F22D,\"M\",\"\u4e2d\"),\n (0x1F22E,\"M\",\"\u53f3\"),\n (0x1F22F,\"M\",\"\u6307\"),\n (0x1F230,\"M\",\"\u8d70\"),\n (0x1F231,\"M\",\"\u6253\"),\n (0x1F232,\"M\",\"\u7981\"),\n (0x1F233,\"M\",\"\u7a7a\"),\n (0x1F234,\"M\",\"\u5408\"),\n (0x1F235,\"M\",\"\u6e80\"),\n (0x1F236,\"M\",\"\u6709\"),\n (0x1F237,\"M\",\"\u6708\"),\n (0x1F238,\"M\",\"\u7533\"),\n (0x1F239,\"M\",\"\u5272\"),\n (0x1F23A,\"M\",\"\u55b6\"),\n (0x1F23B,\"M\",\"\u914d\"),\n (0x1F23C,\"X\"),\n (0x1F240,\"M\",\"\u3014\u672c\u3015\"),\n (0x1F241,\"M\",\"\u3014\u4e09\u3015\"),\n (0x1F242,\"M\",\"\u3014\u4e8c\u3015\"),\n (0x1F243,\"M\",\"\u3014\u5b89\u3015\"),\n (0x1F244,\"M\",\"\u3014\u70b9\u3015\"),\n (0x1F245,\"M\",\"\u3014\u6253\u3015\"),\n (0x1F246,\"M\",\"\u3014\u76d7\u3015\"),\n (0x1F247,\"M\",\"\u3014\u52dd\u3015\"),\n (0x1F248,\"M\",\"\u3014\u6557\u3015\"),\n (0x1F249,\"X\"),\n (0x1F250,\"M\",\"\u5f97\"),\n (0x1F251,\"M\",\"\u53ef\"),\n (0x1F252,\"X\"),\n (0x1F260,\"V\"),\n (0x1F266,\"X\"),\n (0x1F300,\"V\"),\n (0x1F6D8,\"X\"),\n (0x1F6DC,\"V\"),\n (0x1F6ED,\"X\"),\n (0x1F6F0,\"V\"),\n (0x1F6FD,\"X\"),\n (0x1F700,\"V\"),\n (0x1F777,\"X\"),\n (0x1F77B,\"V\"),\n (0x1F7DA,\"X\"),\n (0x1F7E0,\"V\"),\n (0x1F7EC,\"X\"),\n (0x1F7F0,\"V\"),\n (0x1F7F1,\"X\"),\n (0x1F800,\"V\"),\n (0x1F80C,\"X\"),\n (0x1F810,\"V\"),\n (0x1F848,\"X\"),\n (0x1F850,\"V\"),\n (0x1F85A,\"X\"),\n (0x1F860,\"V\"),\n (0x1F888,\"X\"),\n (0x1F890,\"V\"),\n (0x1F8AE,\"X\"),\n (0x1F8B0,\"V\"),\n (0x1F8B2,\"X\"),\n (0x1F900,\"V\"),\n (0x1FA54,\"X\"),\n (0x1FA60,\"V\"),\n (0x1FA6E,\"X\"),\n (0x1FA70,\"V\"),\n (0x1FA7D,\"X\"),\n (0x1FA80,\"V\"),\n (0x1FA89,\"X\"),\n ]\n \n \ndef _seg_76()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x1FA90,\"V\"),\n (0x1FABE,\"X\"),\n (0x1FABF,\"V\"),\n (0x1FAC6,\"X\"),\n (0x1FACE,\"V\"),\n (0x1FADC,\"X\"),\n (0x1FAE0,\"V\"),\n (0x1FAE9,\"X\"),\n (0x1FAF0,\"V\"),\n (0x1FAF9,\"X\"),\n (0x1FB00,\"V\"),\n (0x1FB93,\"X\"),\n (0x1FB94,\"V\"),\n (0x1FBCB,\"X\"),\n (0x1FBF0,\"M\",\"0\"),\n (0x1FBF1,\"M\",\"1\"),\n (0x1FBF2,\"M\",\"2\"),\n (0x1FBF3,\"M\",\"3\"),\n (0x1FBF4,\"M\",\"4\"),\n (0x1FBF5,\"M\",\"5\"),\n (0x1FBF6,\"M\",\"6\"),\n (0x1FBF7,\"M\",\"7\"),\n (0x1FBF8,\"M\",\"8\"),\n (0x1FBF9,\"M\",\"9\"),\n (0x1FBFA,\"X\"),\n (0x20000,\"V\"),\n (0x2A6E0,\"X\"),\n (0x2A700,\"V\"),\n (0x2B73A,\"X\"),\n (0x2B740,\"V\"),\n (0x2B81E,\"X\"),\n (0x2B820,\"V\"),\n (0x2CEA2,\"X\"),\n (0x2CEB0,\"V\"),\n (0x2EBE1,\"X\"),\n (0x2EBF0,\"V\"),\n (0x2EE5E,\"X\"),\n (0x2F800,\"M\",\"\u4e3d\"),\n (0x2F801,\"M\",\"\u4e38\"),\n (0x2F802,\"M\",\"\u4e41\"),\n (0x2F803,\"M\",\"\ud840\udd22\"),\n (0x2F804,\"M\",\"\u4f60\"),\n (0x2F805,\"M\",\"\u4fae\"),\n (0x2F806,\"M\",\"\u4fbb\"),\n (0x2F807,\"M\",\"\u5002\"),\n (0x2F808,\"M\",\"\u507a\"),\n (0x2F809,\"M\",\"\u5099\"),\n (0x2F80A,\"M\",\"\u50e7\"),\n (0x2F80B,\"M\",\"\u50cf\"),\n (0x2F80C,\"M\",\"\u349e\"),\n (0x2F80D,\"M\",\"\ud841\ude3a\"),\n (0x2F80E,\"M\",\"\u514d\"),\n (0x2F80F,\"M\",\"\u5154\"),\n (0x2F810,\"M\",\"\u5164\"),\n (0x2F811,\"M\",\"\u5177\"),\n (0x2F812,\"M\",\"\ud841\udd1c\"),\n (0x2F813,\"M\",\"\u34b9\"),\n (0x2F814,\"M\",\"\u5167\"),\n (0x2F815,\"M\",\"\u518d\"),\n (0x2F816,\"M\",\"\ud841\udd4b\"),\n (0x2F817,\"M\",\"\u5197\"),\n (0x2F818,\"M\",\"\u51a4\"),\n (0x2F819,\"M\",\"\u4ecc\"),\n (0x2F81A,\"M\",\"\u51ac\"),\n (0x2F81B,\"M\",\"\u51b5\"),\n (0x2F81C,\"M\",\"\ud864\udddf\"),\n (0x2F81D,\"M\",\"\u51f5\"),\n (0x2F81E,\"M\",\"\u5203\"),\n (0x2F81F,\"M\",\"\u34df\"),\n (0x2F820,\"M\",\"\u523b\"),\n (0x2F821,\"M\",\"\u5246\"),\n (0x2F822,\"M\",\"\u5272\"),\n (0x2F823,\"M\",\"\u5277\"),\n (0x2F824,\"M\",\"\u3515\"),\n (0x2F825,\"M\",\"\u52c7\"),\n (0x2F826,\"M\",\"\u52c9\"),\n (0x2F827,\"M\",\"\u52e4\"),\n (0x2F828,\"M\",\"\u52fa\"),\n (0x2F829,\"M\",\"\u5305\"),\n (0x2F82A,\"M\",\"\u5306\"),\n (0x2F82B,\"M\",\"\u5317\"),\n (0x2F82C,\"M\",\"\u5349\"),\n (0x2F82D,\"M\",\"\u5351\"),\n (0x2F82E,\"M\",\"\u535a\"),\n (0x2F82F,\"M\",\"\u5373\"),\n (0x2F830,\"M\",\"\u537d\"),\n (0x2F831,\"M\",\"\u537f\"),\n (0x2F834,\"M\",\"\ud842\ude2c\"),\n (0x2F835,\"M\",\"\u7070\"),\n (0x2F836,\"M\",\"\u53ca\"),\n (0x2F837,\"M\",\"\u53df\"),\n (0x2F838,\"M\",\"\ud842\udf63\"),\n (0x2F839,\"M\",\"\u53eb\"),\n (0x2F83A,\"M\",\"\u53f1\"),\n (0x2F83B,\"M\",\"\u5406\"),\n (0x2F83C,\"M\",\"\u549e\"),\n (0x2F83D,\"M\",\"\u5438\"),\n (0x2F83E,\"M\",\"\u5448\"),\n (0x2F83F,\"M\",\"\u5468\"),\n (0x2F840,\"M\",\"\u54a2\"),\n ]\n \n \ndef _seg_77()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F841,\"M\",\"\u54f6\"),\n (0x2F842,\"M\",\"\u5510\"),\n (0x2F843,\"M\",\"\u5553\"),\n (0x2F844,\"M\",\"\u5563\"),\n (0x2F845,\"M\",\"\u5584\"),\n (0x2F847,\"M\",\"\u5599\"),\n (0x2F848,\"M\",\"\u55ab\"),\n (0x2F849,\"M\",\"\u55b3\"),\n (0x2F84A,\"M\",\"\u55c2\"),\n (0x2F84B,\"M\",\"\u5716\"),\n (0x2F84C,\"M\",\"\u5606\"),\n (0x2F84D,\"M\",\"\u5717\"),\n (0x2F84E,\"M\",\"\u5651\"),\n (0x2F84F,\"M\",\"\u5674\"),\n (0x2F850,\"M\",\"\u5207\"),\n (0x2F851,\"M\",\"\u58ee\"),\n (0x2F852,\"M\",\"\u57ce\"),\n (0x2F853,\"M\",\"\u57f4\"),\n (0x2F854,\"M\",\"\u580d\"),\n (0x2F855,\"M\",\"\u578b\"),\n (0x2F856,\"M\",\"\u5832\"),\n (0x2F857,\"M\",\"\u5831\"),\n (0x2F858,\"M\",\"\u58ac\"),\n (0x2F859,\"M\",\"\ud845\udce4\"),\n (0x2F85A,\"M\",\"\u58f2\"),\n (0x2F85B,\"M\",\"\u58f7\"),\n (0x2F85C,\"M\",\"\u5906\"),\n (0x2F85D,\"M\",\"\u591a\"),\n (0x2F85E,\"M\",\"\u5922\"),\n (0x2F85F,\"M\",\"\u5962\"),\n (0x2F860,\"M\",\"\ud845\udea8\"),\n (0x2F861,\"M\",\"\ud845\udeea\"),\n (0x2F862,\"M\",\"\u59ec\"),\n (0x2F863,\"M\",\"\u5a1b\"),\n (0x2F864,\"M\",\"\u5a27\"),\n (0x2F865,\"M\",\"\u59d8\"),\n (0x2F866,\"M\",\"\u5a66\"),\n (0x2F867,\"M\",\"\u36ee\"),\n (0x2F868,\"X\"),\n (0x2F869,\"M\",\"\u5b08\"),\n (0x2F86A,\"M\",\"\u5b3e\"),\n (0x2F86C,\"M\",\"\ud846\uddc8\"),\n (0x2F86D,\"M\",\"\u5bc3\"),\n (0x2F86E,\"M\",\"\u5bd8\"),\n (0x2F86F,\"M\",\"\u5be7\"),\n (0x2F870,\"M\",\"\u5bf3\"),\n (0x2F871,\"M\",\"\ud846\udf18\"),\n (0x2F872,\"M\",\"\u5bff\"),\n (0x2F873,\"M\",\"\u5c06\"),\n (0x2F874,\"X\"),\n (0x2F875,\"M\",\"\u5c22\"),\n (0x2F876,\"M\",\"\u3781\"),\n (0x2F877,\"M\",\"\u5c60\"),\n (0x2F878,\"M\",\"\u5c6e\"),\n (0x2F879,\"M\",\"\u5cc0\"),\n (0x2F87A,\"M\",\"\u5c8d\"),\n (0x2F87B,\"M\",\"\ud847\udde4\"),\n (0x2F87C,\"M\",\"\u5d43\"),\n (0x2F87D,\"M\",\"\ud847\udde6\"),\n (0x2F87E,\"M\",\"\u5d6e\"),\n (0x2F87F,\"M\",\"\u5d6b\"),\n (0x2F880,\"M\",\"\u5d7c\"),\n (0x2F881,\"M\",\"\u5de1\"),\n (0x2F882,\"M\",\"\u5de2\"),\n (0x2F883,\"M\",\"\u382f\"),\n (0x2F884,\"M\",\"\u5dfd\"),\n (0x2F885,\"M\",\"\u5e28\"),\n (0x2F886,\"M\",\"\u5e3d\"),\n (0x2F887,\"M\",\"\u5e69\"),\n (0x2F888,\"M\",\"\u3862\"),\n (0x2F889,\"M\",\"\ud848\udd83\"),\n (0x2F88A,\"M\",\"\u387c\"),\n (0x2F88B,\"M\",\"\u5eb0\"),\n (0x2F88C,\"M\",\"\u5eb3\"),\n (0x2F88D,\"M\",\"\u5eb6\"),\n (0x2F88E,\"M\",\"\u5eca\"),\n (0x2F88F,\"M\",\"\ud868\udf92\"),\n (0x2F890,\"M\",\"\u5efe\"),\n (0x2F891,\"M\",\"\ud848\udf31\"),\n (0x2F893,\"M\",\"\u8201\"),\n (0x2F894,\"M\",\"\u5f22\"),\n (0x2F896,\"M\",\"\u38c7\"),\n (0x2F897,\"M\",\"\ud84c\udeb8\"),\n (0x2F898,\"M\",\"\ud858\uddda\"),\n (0x2F899,\"M\",\"\u5f62\"),\n (0x2F89A,\"M\",\"\u5f6b\"),\n (0x2F89B,\"M\",\"\u38e3\"),\n (0x2F89C,\"M\",\"\u5f9a\"),\n (0x2F89D,\"M\",\"\u5fcd\"),\n (0x2F89E,\"M\",\"\u5fd7\"),\n (0x2F89F,\"M\",\"\u5ff9\"),\n (0x2F8A0,\"M\",\"\u6081\"),\n (0x2F8A1,\"M\",\"\u393a\"),\n (0x2F8A2,\"M\",\"\u391c\"),\n (0x2F8A3,\"M\",\"\u6094\"),\n (0x2F8A4,\"M\",\"\ud849\uded4\"),\n (0x2F8A5,\"M\",\"\u60c7\"),\n (0x2F8A6,\"M\",\"\u6148\"),\n (0x2F8A7,\"M\",\"\u614c\"),\n (0x2F8A8,\"M\",\"\u614e\"),\n ]\n \n \ndef _seg_78()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F8A9,\"M\",\"\u614c\"),\n (0x2F8AA,\"M\",\"\u617a\"),\n (0x2F8AB,\"M\",\"\u618e\"),\n (0x2F8AC,\"M\",\"\u61b2\"),\n (0x2F8AD,\"M\",\"\u61a4\"),\n (0x2F8AE,\"M\",\"\u61af\"),\n (0x2F8AF,\"M\",\"\u61de\"),\n (0x2F8B0,\"M\",\"\u61f2\"),\n (0x2F8B1,\"M\",\"\u61f6\"),\n (0x2F8B2,\"M\",\"\u6210\"),\n (0x2F8B3,\"M\",\"\u621b\"),\n (0x2F8B4,\"M\",\"\u625d\"),\n (0x2F8B5,\"M\",\"\u62b1\"),\n (0x2F8B6,\"M\",\"\u62d4\"),\n (0x2F8B7,\"M\",\"\u6350\"),\n (0x2F8B8,\"M\",\"\ud84a\udf0c\"),\n (0x2F8B9,\"M\",\"\u633d\"),\n (0x2F8BA,\"M\",\"\u62fc\"),\n (0x2F8BB,\"M\",\"\u6368\"),\n (0x2F8BC,\"M\",\"\u6383\"),\n (0x2F8BD,\"M\",\"\u63e4\"),\n (0x2F8BE,\"M\",\"\ud84a\udff1\"),\n (0x2F8BF,\"M\",\"\u6422\"),\n (0x2F8C0,\"M\",\"\u63c5\"),\n (0x2F8C1,\"M\",\"\u63a9\"),\n (0x2F8C2,\"M\",\"\u3a2e\"),\n (0x2F8C3,\"M\",\"\u6469\"),\n (0x2F8C4,\"M\",\"\u647e\"),\n (0x2F8C5,\"M\",\"\u649d\"),\n (0x2F8C6,\"M\",\"\u6477\"),\n (0x2F8C7,\"M\",\"\u3a6c\"),\n (0x2F8C8,\"M\",\"\u654f\"),\n (0x2F8C9,\"M\",\"\u656c\"),\n (0x2F8CA,\"M\",\"\ud84c\udc0a\"),\n (0x2F8CB,\"M\",\"\u65e3\"),\n (0x2F8CC,\"M\",\"\u66f8\"),\n (0x2F8CD,\"M\",\"\u6649\"),\n (0x2F8CE,\"M\",\"\u3b19\"),\n (0x2F8CF,\"M\",\"\u6691\"),\n (0x2F8D0,\"M\",\"\u3b08\"),\n (0x2F8D1,\"M\",\"\u3ae4\"),\n (0x2F8D2,\"M\",\"\u5192\"),\n (0x2F8D3,\"M\",\"\u5195\"),\n (0x2F8D4,\"M\",\"\u6700\"),\n (0x2F8D5,\"M\",\"\u669c\"),\n (0x2F8D6,\"M\",\"\u80ad\"),\n (0x2F8D7,\"M\",\"\u43d9\"),\n (0x2F8D8,\"M\",\"\u6717\"),\n (0x2F8D9,\"M\",\"\u671b\"),\n (0x2F8DA,\"M\",\"\u6721\"),\n (0x2F8DB,\"M\",\"\u675e\"),\n (0x2F8DC,\"M\",\"\u6753\"),\n (0x2F8DD,\"M\",\"\ud84c\udfc3\"),\n (0x2F8DE,\"M\",\"\u3b49\"),\n (0x2F8DF,\"M\",\"\u67fa\"),\n (0x2F8E0,\"M\",\"\u6785\"),\n (0x2F8E1,\"M\",\"\u6852\"),\n (0x2F8E2,\"M\",\"\u6885\"),\n (0x2F8E3,\"M\",\"\ud84d\udc6d\"),\n (0x2F8E4,\"M\",\"\u688e\"),\n (0x2F8E5,\"M\",\"\u681f\"),\n (0x2F8E6,\"M\",\"\u6914\"),\n (0x2F8E7,\"M\",\"\u3b9d\"),\n (0x2F8E8,\"M\",\"\u6942\"),\n (0x2F8E9,\"M\",\"\u69a3\"),\n (0x2F8EA,\"M\",\"\u69ea\"),\n (0x2F8EB,\"M\",\"\u6aa8\"),\n (0x2F8EC,\"M\",\"\ud84d\udea3\"),\n (0x2F8ED,\"M\",\"\u6adb\"),\n (0x2F8EE,\"M\",\"\u3c18\"),\n (0x2F8EF,\"M\",\"\u6b21\"),\n (0x2F8F0,\"M\",\"\ud84e\udca7\"),\n (0x2F8F1,\"M\",\"\u6b54\"),\n (0x2F8F2,\"M\",\"\u3c4e\"),\n (0x2F8F3,\"M\",\"\u6b72\"),\n (0x2F8F4,\"M\",\"\u6b9f\"),\n (0x2F8F5,\"M\",\"\u6bba\"),\n (0x2F8F6,\"M\",\"\u6bbb\"),\n (0x2F8F7,\"M\",\"\ud84e\ude8d\"),\n (0x2F8F8,\"M\",\"\ud847\udd0b\"),\n (0x2F8F9,\"M\",\"\ud84e\udefa\"),\n (0x2F8FA,\"M\",\"\u6c4e\"),\n (0x2F8FB,\"M\",\"\ud84f\udcbc\"),\n (0x2F8FC,\"M\",\"\u6cbf\"),\n (0x2F8FD,\"M\",\"\u6ccd\"),\n (0x2F8FE,\"M\",\"\u6c67\"),\n (0x2F8FF,\"M\",\"\u6d16\"),\n (0x2F900,\"M\",\"\u6d3e\"),\n (0x2F901,\"M\",\"\u6d77\"),\n (0x2F902,\"M\",\"\u6d41\"),\n (0x2F903,\"M\",\"\u6d69\"),\n (0x2F904,\"M\",\"\u6d78\"),\n (0x2F905,\"M\",\"\u6d85\"),\n (0x2F906,\"M\",\"\ud84f\udd1e\"),\n (0x2F907,\"M\",\"\u6d34\"),\n (0x2F908,\"M\",\"\u6e2f\"),\n (0x2F909,\"M\",\"\u6e6e\"),\n (0x2F90A,\"M\",\"\u3d33\"),\n (0x2F90B,\"M\",\"\u6ecb\"),\n (0x2F90C,\"M\",\"\u6ec7\"),\n ]\n \n \ndef _seg_79()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F90D,\"M\",\"\ud84f\uded1\"),\n (0x2F90E,\"M\",\"\u6df9\"),\n (0x2F90F,\"M\",\"\u6f6e\"),\n (0x2F910,\"M\",\"\ud84f\udf5e\"),\n (0x2F911,\"M\",\"\ud84f\udf8e\"),\n (0x2F912,\"M\",\"\u6fc6\"),\n (0x2F913,\"M\",\"\u7039\"),\n (0x2F914,\"M\",\"\u701e\"),\n (0x2F915,\"M\",\"\u701b\"),\n (0x2F916,\"M\",\"\u3d96\"),\n (0x2F917,\"M\",\"\u704a\"),\n (0x2F918,\"M\",\"\u707d\"),\n (0x2F919,\"M\",\"\u7077\"),\n (0x2F91A,\"M\",\"\u70ad\"),\n (0x2F91B,\"M\",\"\ud841\udd25\"),\n (0x2F91C,\"M\",\"\u7145\"),\n (0x2F91D,\"M\",\"\ud850\ude63\"),\n (0x2F91E,\"M\",\"\u719c\"),\n (0x2F91F,\"X\"),\n (0x2F920,\"M\",\"\u7228\"),\n (0x2F921,\"M\",\"\u7235\"),\n (0x2F922,\"M\",\"\u7250\"),\n (0x2F923,\"M\",\"\ud851\ude08\"),\n (0x2F924,\"M\",\"\u7280\"),\n (0x2F925,\"M\",\"\u7295\"),\n (0x2F926,\"M\",\"\ud851\udf35\"),\n (0x2F927,\"M\",\"\ud852\udc14\"),\n (0x2F928,\"M\",\"\u737a\"),\n (0x2F929,\"M\",\"\u738b\"),\n (0x2F92A,\"M\",\"\u3eac\"),\n (0x2F92B,\"M\",\"\u73a5\"),\n (0x2F92C,\"M\",\"\u3eb8\"),\n (0x2F92E,\"M\",\"\u7447\"),\n (0x2F92F,\"M\",\"\u745c\"),\n (0x2F930,\"M\",\"\u7471\"),\n (0x2F931,\"M\",\"\u7485\"),\n (0x2F932,\"M\",\"\u74ca\"),\n (0x2F933,\"M\",\"\u3f1b\"),\n (0x2F934,\"M\",\"\u7524\"),\n (0x2F935,\"M\",\"\ud853\udc36\"),\n (0x2F936,\"M\",\"\u753e\"),\n (0x2F937,\"M\",\"\ud853\udc92\"),\n (0x2F938,\"M\",\"\u7570\"),\n (0x2F939,\"M\",\"\ud848\udd9f\"),\n (0x2F93A,\"M\",\"\u7610\"),\n (0x2F93B,\"M\",\"\ud853\udfa1\"),\n (0x2F93C,\"M\",\"\ud853\udfb8\"),\n (0x2F93D,\"M\",\"\ud854\udc44\"),\n (0x2F93E,\"M\",\"\u3ffc\"),\n (0x2F93F,\"M\",\"\u4008\"),\n (0x2F940,\"M\",\"\u76f4\"),\n (0x2F941,\"M\",\"\ud854\udcf3\"),\n (0x2F942,\"M\",\"\ud854\udcf2\"),\n (0x2F943,\"M\",\"\ud854\udd19\"),\n (0x2F944,\"M\",\"\ud854\udd33\"),\n (0x2F945,\"M\",\"\u771e\"),\n (0x2F946,\"M\",\"\u771f\"),\n (0x2F948,\"M\",\"\u774a\"),\n (0x2F949,\"M\",\"\u4039\"),\n (0x2F94A,\"M\",\"\u778b\"),\n (0x2F94B,\"M\",\"\u4046\"),\n (0x2F94C,\"M\",\"\u4096\"),\n (0x2F94D,\"M\",\"\ud855\udc1d\"),\n (0x2F94E,\"M\",\"\u784e\"),\n (0x2F94F,\"M\",\"\u788c\"),\n (0x2F950,\"M\",\"\u78cc\"),\n (0x2F951,\"M\",\"\u40e3\"),\n (0x2F952,\"M\",\"\ud855\ude26\"),\n (0x2F953,\"M\",\"\u7956\"),\n (0x2F954,\"M\",\"\ud855\ude9a\"),\n (0x2F955,\"M\",\"\ud855\udec5\"),\n (0x2F956,\"M\",\"\u798f\"),\n (0x2F957,\"M\",\"\u79eb\"),\n (0x2F958,\"M\",\"\u412f\"),\n (0x2F959,\"M\",\"\u7a40\"),\n (0x2F95A,\"M\",\"\u7a4a\"),\n (0x2F95B,\"M\",\"\u7a4f\"),\n (0x2F95C,\"M\",\"\ud856\udd7c\"),\n (0x2F95D,\"M\",\"\ud856\udea7\"),\n (0x2F95F,\"X\"),\n (0x2F960,\"M\",\"\u4202\"),\n (0x2F961,\"M\",\"\ud856\udfab\"),\n (0x2F962,\"M\",\"\u7bc6\"),\n (0x2F963,\"M\",\"\u7bc9\"),\n (0x2F964,\"M\",\"\u4227\"),\n (0x2F965,\"M\",\"\ud857\udc80\"),\n (0x2F966,\"M\",\"\u7cd2\"),\n (0x2F967,\"M\",\"\u42a0\"),\n (0x2F968,\"M\",\"\u7ce8\"),\n (0x2F969,\"M\",\"\u7ce3\"),\n (0x2F96A,\"M\",\"\u7d00\"),\n (0x2F96B,\"M\",\"\ud857\udf86\"),\n (0x2F96C,\"M\",\"\u7d63\"),\n (0x2F96D,\"M\",\"\u4301\"),\n (0x2F96E,\"M\",\"\u7dc7\"),\n (0x2F96F,\"M\",\"\u7e02\"),\n (0x2F970,\"M\",\"\u7e45\"),\n (0x2F971,\"M\",\"\u4334\"),\n (0x2F972,\"M\",\"\ud858\ude28\"),\n (0x2F973,\"M\",\"\ud858\ude47\"),\n ]\n \n \ndef _seg_80()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F974,\"M\",\"\u4359\"),\n (0x2F975,\"M\",\"\ud858\uded9\"),\n (0x2F976,\"M\",\"\u7f7a\"),\n (0x2F977,\"M\",\"\ud858\udf3e\"),\n (0x2F978,\"M\",\"\u7f95\"),\n (0x2F979,\"M\",\"\u7ffa\"),\n (0x2F97A,\"M\",\"\u8005\"),\n (0x2F97B,\"M\",\"\ud859\udcda\"),\n (0x2F97C,\"M\",\"\ud859\udd23\"),\n (0x2F97D,\"M\",\"\u8060\"),\n (0x2F97E,\"M\",\"\ud859\udda8\"),\n (0x2F97F,\"M\",\"\u8070\"),\n (0x2F980,\"M\",\"\ud84c\udf5f\"),\n (0x2F981,\"M\",\"\u43d5\"),\n (0x2F982,\"M\",\"\u80b2\"),\n (0x2F983,\"M\",\"\u8103\"),\n (0x2F984,\"M\",\"\u440b\"),\n (0x2F985,\"M\",\"\u813e\"),\n (0x2F986,\"M\",\"\u5ab5\"),\n (0x2F987,\"M\",\"\ud859\udfa7\"),\n (0x2F988,\"M\",\"\ud859\udfb5\"),\n (0x2F989,\"M\",\"\ud84c\udf93\"),\n (0x2F98A,\"M\",\"\ud84c\udf9c\"),\n (0x2F98B,\"M\",\"\u8201\"),\n (0x2F98C,\"M\",\"\u8204\"),\n (0x2F98D,\"M\",\"\u8f9e\"),\n (0x2F98E,\"M\",\"\u446b\"),\n (0x2F98F,\"M\",\"\u8291\"),\n (0x2F990,\"M\",\"\u828b\"),\n (0x2F991,\"M\",\"\u829d\"),\n (0x2F992,\"M\",\"\u52b3\"),\n (0x2F993,\"M\",\"\u82b1\"),\n (0x2F994,\"M\",\"\u82b3\"),\n (0x2F995,\"M\",\"\u82bd\"),\n (0x2F996,\"M\",\"\u82e6\"),\n (0x2F997,\"M\",\"\ud85a\udf3c\"),\n (0x2F998,\"M\",\"\u82e5\"),\n (0x2F999,\"M\",\"\u831d\"),\n (0x2F99A,\"M\",\"\u8363\"),\n (0x2F99B,\"M\",\"\u83ad\"),\n (0x2F99C,\"M\",\"\u8323\"),\n (0x2F99D,\"M\",\"\u83bd\"),\n (0x2F99E,\"M\",\"\u83e7\"),\n (0x2F99F,\"M\",\"\u8457\"),\n (0x2F9A0,\"M\",\"\u8353\"),\n (0x2F9A1,\"M\",\"\u83ca\"),\n (0x2F9A2,\"M\",\"\u83cc\"),\n (0x2F9A3,\"M\",\"\u83dc\"),\n (0x2F9A4,\"M\",\"\ud85b\udc36\"),\n (0x2F9A5,\"M\",\"\ud85b\udd6b\"),\n (0x2F9A6,\"M\",\"\ud85b\udcd5\"),\n (0x2F9A7,\"M\",\"\u452b\"),\n (0x2F9A8,\"M\",\"\u84f1\"),\n (0x2F9A9,\"M\",\"\u84f3\"),\n (0x2F9AA,\"M\",\"\u8516\"),\n (0x2F9AB,\"M\",\"\ud85c\udfca\"),\n (0x2F9AC,\"M\",\"\u8564\"),\n (0x2F9AD,\"M\",\"\ud85b\udf2c\"),\n (0x2F9AE,\"M\",\"\u455d\"),\n (0x2F9AF,\"M\",\"\u4561\"),\n (0x2F9B0,\"M\",\"\ud85b\udfb1\"),\n (0x2F9B1,\"M\",\"\ud85c\udcd2\"),\n (0x2F9B2,\"M\",\"\u456b\"),\n (0x2F9B3,\"M\",\"\u8650\"),\n (0x2F9B4,\"M\",\"\u865c\"),\n (0x2F9B5,\"M\",\"\u8667\"),\n (0x2F9B6,\"M\",\"\u8669\"),\n (0x2F9B7,\"M\",\"\u86a9\"),\n (0x2F9B8,\"M\",\"\u8688\"),\n (0x2F9B9,\"M\",\"\u870e\"),\n (0x2F9BA,\"M\",\"\u86e2\"),\n (0x2F9BB,\"M\",\"\u8779\"),\n (0x2F9BC,\"M\",\"\u8728\"),\n (0x2F9BD,\"M\",\"\u876b\"),\n (0x2F9BE,\"M\",\"\u8786\"),\n (0x2F9BF,\"X\"),\n (0x2F9C0,\"M\",\"\u87e1\"),\n (0x2F9C1,\"M\",\"\u8801\"),\n (0x2F9C2,\"M\",\"\u45f9\"),\n (0x2F9C3,\"M\",\"\u8860\"),\n (0x2F9C4,\"M\",\"\u8863\"),\n (0x2F9C5,\"M\",\"\ud85d\ude67\"),\n (0x2F9C6,\"M\",\"\u88d7\"),\n (0x2F9C7,\"M\",\"\u88de\"),\n (0x2F9C8,\"M\",\"\u4635\"),\n (0x2F9C9,\"M\",\"\u88fa\"),\n (0x2F9CA,\"M\",\"\u34bb\"),\n (0x2F9CB,\"M\",\"\ud85e\udcae\"),\n (0x2F9CC,\"M\",\"\ud85e\udd66\"),\n (0x2F9CD,\"M\",\"\u46be\"),\n (0x2F9CE,\"M\",\"\u46c7\"),\n (0x2F9CF,\"M\",\"\u8aa0\"),\n (0x2F9D0,\"M\",\"\u8aed\"),\n (0x2F9D1,\"M\",\"\u8b8a\"),\n (0x2F9D2,\"M\",\"\u8c55\"),\n (0x2F9D3,\"M\",\"\ud85f\udca8\"),\n (0x2F9D4,\"M\",\"\u8cab\"),\n (0x2F9D5,\"M\",\"\u8cc1\"),\n (0x2F9D6,\"M\",\"\u8d1b\"),\n (0x2F9D7,\"M\",\"\u8d77\"),\n ]\n \n \ndef _seg_81()->List[Union[Tuple[int,str],Tuple[int,str,str]]]:\n return[\n (0x2F9D8,\"M\",\"\ud85f\udf2f\"),\n (0x2F9D9,\"M\",\"\ud842\udc04\"),\n (0x2F9DA,\"M\",\"\u8dcb\"),\n (0x2F9DB,\"M\",\"\u8dbc\"),\n (0x2F9DC,\"M\",\"\u8df0\"),\n (0x2F9DD,\"M\",\"\ud842\udcde\"),\n (0x2F9DE,\"M\",\"\u8ed4\"),\n (0x2F9DF,\"M\",\"\u8f38\"),\n (0x2F9E0,\"M\",\"\ud861\uddd2\"),\n (0x2F9E1,\"M\",\"\ud861\udded\"),\n (0x2F9E2,\"M\",\"\u9094\"),\n (0x2F9E3,\"M\",\"\u90f1\"),\n (0x2F9E4,\"M\",\"\u9111\"),\n (0x2F9E5,\"M\",\"\ud861\udf2e\"),\n (0x2F9E6,\"M\",\"\u911b\"),\n (0x2F9E7,\"M\",\"\u9238\"),\n (0x2F9E8,\"M\",\"\u92d7\"),\n (0x2F9E9,\"M\",\"\u92d8\"),\n (0x2F9EA,\"M\",\"\u927c\"),\n (0x2F9EB,\"M\",\"\u93f9\"),\n (0x2F9EC,\"M\",\"\u9415\"),\n (0x2F9ED,\"M\",\"\ud862\udffa\"),\n (0x2F9EE,\"M\",\"\u958b\"),\n (0x2F9EF,\"M\",\"\u4995\"),\n (0x2F9F0,\"M\",\"\u95b7\"),\n (0x2F9F1,\"M\",\"\ud863\udd77\"),\n (0x2F9F2,\"M\",\"\u49e6\"),\n (0x2F9F3,\"M\",\"\u96c3\"),\n (0x2F9F4,\"M\",\"\u5db2\"),\n (0x2F9F5,\"M\",\"\u9723\"),\n (0x2F9F6,\"M\",\"\ud864\udd45\"),\n (0x2F9F7,\"M\",\"\ud864\ude1a\"),\n (0x2F9F8,\"M\",\"\u4a6e\"),\n (0x2F9F9,\"M\",\"\u4a76\"),\n (0x2F9FA,\"M\",\"\u97e0\"),\n (0x2F9FB,\"M\",\"\ud865\udc0a\"),\n (0x2F9FC,\"M\",\"\u4ab2\"),\n (0x2F9FD,\"M\",\"\ud865\udc96\"),\n (0x2F9FE,\"M\",\"\u980b\"),\n (0x2FA00,\"M\",\"\u9829\"),\n (0x2FA01,\"M\",\"\ud865\uddb6\"),\n (0x2FA02,\"M\",\"\u98e2\"),\n (0x2FA03,\"M\",\"\u4b33\"),\n (0x2FA04,\"M\",\"\u9929\"),\n (0x2FA05,\"M\",\"\u99a7\"),\n (0x2FA06,\"M\",\"\u99c2\"),\n (0x2FA07,\"M\",\"\u99fe\"),\n (0x2FA08,\"M\",\"\u4bce\"),\n (0x2FA09,\"M\",\"\ud866\udf30\"),\n (0x2FA0A,\"M\",\"\u9b12\"),\n (0x2FA0B,\"M\",\"\u9c40\"),\n (0x2FA0C,\"M\",\"\u9cfd\"),\n (0x2FA0D,\"M\",\"\u4cce\"),\n (0x2FA0E,\"M\",\"\u4ced\"),\n (0x2FA0F,\"M\",\"\u9d67\"),\n (0x2FA10,\"M\",\"\ud868\udcce\"),\n (0x2FA11,\"M\",\"\u4cf8\"),\n (0x2FA12,\"M\",\"\ud868\udd05\"),\n (0x2FA13,\"M\",\"\ud868\ude0e\"),\n (0x2FA14,\"M\",\"\ud868\ude91\"),\n (0x2FA15,\"M\",\"\u9ebb\"),\n (0x2FA16,\"M\",\"\u4d56\"),\n (0x2FA17,\"M\",\"\u9ef9\"),\n (0x2FA18,\"M\",\"\u9efe\"),\n (0x2FA19,\"M\",\"\u9f05\"),\n (0x2FA1A,\"M\",\"\u9f0f\"),\n (0x2FA1B,\"M\",\"\u9f16\"),\n (0x2FA1C,\"M\",\"\u9f3b\"),\n (0x2FA1D,\"M\",\"\ud869\ude00\"),\n (0x2FA1E,\"X\"),\n (0x30000,\"V\"),\n (0x3134B,\"X\"),\n (0x31350,\"V\"),\n (0x323B0,\"X\"),\n (0xE0100,\"I\"),\n (0xE01F0,\"X\"),\n ]\n \n \nuts46data=tuple(\n_seg_0()\n+_seg_1()\n+_seg_2()\n+_seg_3()\n+_seg_4()\n+_seg_5()\n+_seg_6()\n+_seg_7()\n+_seg_8()\n+_seg_9()\n+_seg_10()\n+_seg_11()\n+_seg_12()\n+_seg_13()\n+_seg_14()\n+_seg_15()\n+_seg_16()\n+_seg_17()\n+_seg_18()\n+_seg_19()\n+_seg_20()\n+_seg_21()\n+_seg_22()\n+_seg_23()\n+_seg_24()\n+_seg_25()\n+_seg_26()\n+_seg_27()\n+_seg_28()\n+_seg_29()\n+_seg_30()\n+_seg_31()\n+_seg_32()\n+_seg_33()\n+_seg_34()\n+_seg_35()\n+_seg_36()\n+_seg_37()\n+_seg_38()\n+_seg_39()\n+_seg_40()\n+_seg_41()\n+_seg_42()\n+_seg_43()\n+_seg_44()\n+_seg_45()\n+_seg_46()\n+_seg_47()\n+_seg_48()\n+_seg_49()\n+_seg_50()\n+_seg_51()\n+_seg_52()\n+_seg_53()\n+_seg_54()\n+_seg_55()\n+_seg_56()\n+_seg_57()\n+_seg_58()\n+_seg_59()\n+_seg_60()\n+_seg_61()\n+_seg_62()\n+_seg_63()\n+_seg_64()\n+_seg_65()\n+_seg_66()\n+_seg_67()\n+_seg_68()\n+_seg_69()\n+_seg_70()\n+_seg_71()\n+_seg_72()\n+_seg_73()\n+_seg_74()\n+_seg_75()\n+_seg_76()\n+_seg_77()\n+_seg_78()\n+_seg_79()\n+_seg_80()\n+_seg_81()\n)\n", ["typing"]], "annotated_types": [".py", "import math\nimport sys\nimport types\nfrom dataclasses import dataclass\nfrom datetime import tzinfo\nfrom typing import TYPE_CHECKING,Any,Callable,Iterator,Optional,SupportsFloat,SupportsIndex,TypeVar,Union\n\nif sys.version_info <(3,8):\n from typing_extensions import Protocol,runtime_checkable\nelse:\n from typing import Protocol,runtime_checkable\n \nif sys.version_info <(3,9):\n from typing_extensions import Annotated,Literal\nelse:\n from typing import Annotated,Literal\n \nif sys.version_info <(3,10):\n EllipsisType=type(Ellipsis)\n KW_ONLY={}\n SLOTS={}\nelse:\n from types import EllipsisType\n \n KW_ONLY={\"kw_only\":True}\n SLOTS={\"slots\":True}\n \n \n__all__=(\n'BaseMetadata',\n'GroupedMetadata',\n'Gt',\n'Ge',\n'Lt',\n'Le',\n'Interval',\n'MultipleOf',\n'MinLen',\n'MaxLen',\n'Len',\n'Timezone',\n'Predicate',\n'LowerCase',\n'UpperCase',\n'IsDigits',\n'IsFinite',\n'IsNotFinite',\n'IsNan',\n'IsNotNan',\n'IsInfinite',\n'IsNotInfinite',\n'doc',\n'DocInfo',\n'__version__',\n)\n\n__version__='0.7.0'\n\n\nT=TypeVar('T')\n\n\n\n\n\n\n\nclass SupportsGt(Protocol):\n def __gt__(self:T,__other:T)->bool:\n  ...\n  \n  \nclass SupportsGe(Protocol):\n def __ge__(self:T,__other:T)->bool:\n  ...\n  \n  \nclass SupportsLt(Protocol):\n def __lt__(self:T,__other:T)->bool:\n  ...\n  \n  \nclass SupportsLe(Protocol):\n def __le__(self:T,__other:T)->bool:\n  ...\n  \n  \nclass SupportsMod(Protocol):\n def __mod__(self:T,__other:T)->T:\n  ...\n  \n  \nclass SupportsDiv(Protocol):\n def __div__(self:T,__other:T)->T:\n  ...\n  \n  \nclass BaseMetadata:\n ''\n\n\n\n \n \n __slots__=()\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Gt(BaseMetadata):\n ''\n\n\n\n \n \n gt:SupportsGt\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Ge(BaseMetadata):\n ''\n\n\n\n \n \n ge:SupportsGe\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Lt(BaseMetadata):\n ''\n\n\n\n \n \n lt:SupportsLt\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Le(BaseMetadata):\n ''\n\n\n\n \n \n le:SupportsLe\n \n \n@runtime_checkable\nclass GroupedMetadata(Protocol):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n @property\n def __is_annotated_types_grouped_metadata__(self)->Literal[True]:\n  return True\n  \n def __iter__(self)->Iterator[object]:\n  ...\n  \n if not TYPE_CHECKING:\n  __slots__=()\n  \n  def __init_subclass__(cls,*args:Any,**kwargs:Any)->None:\n  \n   super().__init_subclass__(*args,**kwargs)\n   if cls.__iter__ is GroupedMetadata.__iter__:\n    raise TypeError(\"Can't subclass GroupedMetadata without implementing __iter__\")\n    \n  def __iter__(self)->Iterator[object]:\n   raise NotImplementedError\n   \n   \n@dataclass(frozen=True,**KW_ONLY,**SLOTS)\nclass Interval(GroupedMetadata):\n ''\n\n\n\n \n \n gt:Union[SupportsGt,None]=None\n ge:Union[SupportsGe,None]=None\n lt:Union[SupportsLt,None]=None\n le:Union[SupportsLe,None]=None\n \n def __iter__(self)->Iterator[BaseMetadata]:\n  ''\n  if self.gt is not None:\n   yield Gt(self.gt)\n  if self.ge is not None:\n   yield Ge(self.ge)\n  if self.lt is not None:\n   yield Lt(self.lt)\n  if self.le is not None:\n   yield Le(self.le)\n   \n   \n@dataclass(frozen=True,**SLOTS)\nclass MultipleOf(BaseMetadata):\n ''\n\n\n\n\n\n\n \n \n multiple_of:Union[SupportsDiv,SupportsMod]\n \n \n@dataclass(frozen=True,**SLOTS)\nclass MinLen(BaseMetadata):\n ''\n\n\n \n \n min_length:Annotated[int,Ge(0)]\n \n \n@dataclass(frozen=True,**SLOTS)\nclass MaxLen(BaseMetadata):\n ''\n\n\n \n \n max_length:Annotated[int,Ge(0)]\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Len(GroupedMetadata):\n ''\n\n\n\n \n \n min_length:Annotated[int,Ge(0)]=0\n max_length:Optional[Annotated[int,Ge(0)]]=None\n \n def __iter__(self)->Iterator[BaseMetadata]:\n  ''\n  if self.min_length >0:\n   yield MinLen(self.min_length)\n  if self.max_length is not None:\n   yield MaxLen(self.max_length)\n   \n   \n@dataclass(frozen=True,**SLOTS)\nclass Timezone(BaseMetadata):\n ''\n\n\n\n\n\n\n\n\n\n \n \n tz:Union[str,tzinfo,EllipsisType,None]\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Unit(BaseMetadata):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n unit:str\n \n \n@dataclass(frozen=True,**SLOTS)\nclass Predicate(BaseMetadata):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n func:Callable[[Any],bool]\n \n def __repr__(self)->str:\n  if getattr(self.func,\"__name__\",\"<lambda>\")==\"<lambda>\":\n   return f\"{self.__class__.__name__}({self.func !r})\"\n  if isinstance(self.func,(types.MethodType,types.BuiltinMethodType))and(\n  namespace :=getattr(self.func.__self__,\"__name__\",None)\n  ):\n   return f\"{self.__class__.__name__}({namespace}.{self.func.__name__})\"\n  if isinstance(self.func,type(str.isascii)):\n   return f\"{self.__class__.__name__}({self.func.__qualname__})\"\n  return f\"{self.__class__.__name__}({self.func.__name__})\"\n  \n  \n@dataclass\nclass Not:\n func:Callable[[Any],bool]\n \n def __call__(self,__v:Any)->bool:\n  return not self.func(__v)\n  \n  \n_StrType=TypeVar(\"_StrType\",bound=str)\n\nLowerCase=Annotated[_StrType,Predicate(str.islower)]\n''\n\n\n\n\nUpperCase=Annotated[_StrType,Predicate(str.isupper)]\n''\n\n\n\n\nIsDigit=Annotated[_StrType,Predicate(str.isdigit)]\nIsDigits=IsDigit\n''\n\n\n\n\nIsAscii=Annotated[_StrType,Predicate(str.isascii)]\n''\n\n\n\n\n\n_NumericType=TypeVar('_NumericType',bound=Union[SupportsFloat,SupportsIndex])\nIsFinite=Annotated[_NumericType,Predicate(math.isfinite)]\n''\nIsNotFinite=Annotated[_NumericType,Predicate(Not(math.isfinite))]\n''\nIsNan=Annotated[_NumericType,Predicate(math.isnan)]\n''\nIsNotNan=Annotated[_NumericType,Predicate(Not(math.isnan))]\n''\nIsInfinite=Annotated[_NumericType,Predicate(math.isinf)]\n''\nIsNotInfinite=Annotated[_NumericType,Predicate(Not(math.isinf))]\n''\n\ntry:\n from typing_extensions import DocInfo,doc\nexcept ImportError:\n\n @dataclass(frozen=True,**SLOTS)\n class DocInfo:\n  ''\n\n\n  \n  \n  documentation:str\n  ''  \n  \n def doc(\n documentation:str,\n )->DocInfo:\n  ''\n\n\n\n\n\n  \n  return DocInfo(documentation)\n", ["dataclasses", "datetime", "math", "sys", "types", "typing", "typing_extensions"], 1], "annotated_types.test_cases": [".py", "import math\nimport sys\nfrom datetime import date,datetime,timedelta,timezone\nfrom decimal import Decimal\nfrom typing import Any,Dict,Iterable,Iterator,List,NamedTuple,Set,Tuple\n\nif sys.version_info <(3,9):\n from typing_extensions import Annotated\nelse:\n from typing import Annotated\n \nimport annotated_types as at\n\n\nclass Case(NamedTuple):\n ''\n\n \n \n annotation:Any\n valid_cases:Iterable[Any]\n invalid_cases:Iterable[Any]\n \n \ndef cases()->Iterable[Case]:\n\n yield Case(Annotated[int,at.Gt(4)],(5,6,1000),(4,0,-1))\n yield Case(Annotated[float,at.Gt(0.5)],(0.6,0.7,0.8,0.9),(0.5,0.0,-0.1))\n yield Case(\n Annotated[datetime,at.Gt(datetime(2000,1,1))],\n [datetime(2000,1,2),datetime(2000,1,3)],\n [datetime(2000,1,1),datetime(1999,12,31)],\n )\n yield Case(\n Annotated[datetime,at.Gt(date(2000,1,1))],\n [date(2000,1,2),date(2000,1,3)],\n [date(2000,1,1),date(1999,12,31)],\n )\n yield Case(\n Annotated[datetime,at.Gt(Decimal('1.123'))],\n [Decimal('1.1231'),Decimal('123')],\n [Decimal('1.123'),Decimal('0')],\n )\n \n yield Case(Annotated[int,at.Ge(4)],(4,5,6,1000,4),(0,-1))\n yield Case(Annotated[float,at.Ge(0.5)],(0.5,0.6,0.7,0.8,0.9),(0.4,0.0,-0.1))\n yield Case(\n Annotated[datetime,at.Ge(datetime(2000,1,1))],\n [datetime(2000,1,2),datetime(2000,1,3)],\n [datetime(1998,1,1),datetime(1999,12,31)],\n )\n \n yield Case(Annotated[int,at.Lt(4)],(0,-1),(4,5,6,1000,4))\n yield Case(Annotated[float,at.Lt(0.5)],(0.4,0.0,-0.1),(0.5,0.6,0.7,0.8,0.9))\n yield Case(\n Annotated[datetime,at.Lt(datetime(2000,1,1))],\n [datetime(1999,12,31),datetime(1999,12,31)],\n [datetime(2000,1,2),datetime(2000,1,3)],\n )\n \n yield Case(Annotated[int,at.Le(4)],(4,0,-1),(5,6,1000))\n yield Case(Annotated[float,at.Le(0.5)],(0.5,0.0,-0.1),(0.6,0.7,0.8,0.9))\n yield Case(\n Annotated[datetime,at.Le(datetime(2000,1,1))],\n [datetime(2000,1,1),datetime(1999,12,31)],\n [datetime(2000,1,2),datetime(2000,1,3)],\n )\n \n \n yield Case(Annotated[int,at.Interval(gt=4)],(5,6,1000),(4,0,-1))\n yield Case(Annotated[int,at.Interval(gt=4,lt=10)],(5,6),(4,10,1000,0,-1))\n yield Case(Annotated[float,at.Interval(ge=0.5,le=1)],(0.5,0.9,1),(0.49,1.1))\n yield Case(\n Annotated[datetime,at.Interval(gt=datetime(2000,1,1),le=datetime(2000,1,3))],\n [datetime(2000,1,2),datetime(2000,1,3)],\n [datetime(2000,1,1),datetime(2000,1,4)],\n )\n \n yield Case(Annotated[int,at.MultipleOf(multiple_of=3)],(0,3,9),(1,2,4))\n yield Case(Annotated[float,at.MultipleOf(multiple_of=0.5)],(0,0.5,1,1.5),(0.4,1.1))\n \n \n \n yield Case(Annotated[str,at.MinLen(3)],('123','1234','x'*10),('','1','12'))\n yield Case(Annotated[str,at.Len(3)],('123','1234','x'*10),('','1','12'))\n yield Case(Annotated[List[int],at.MinLen(3)],([1,2,3],[1,2,3,4],[1]*10),([],[1],[1,2]))\n yield Case(Annotated[List[int],at.Len(3)],([1,2,3],[1,2,3,4],[1]*10),([],[1],[1,2]))\n \n yield Case(Annotated[str,at.MaxLen(4)],('','1234'),('12345','x'*10))\n yield Case(Annotated[str,at.Len(0,4)],('','1234'),('12345','x'*10))\n yield Case(Annotated[List[str],at.MaxLen(4)],([],['a','bcdef'],['a','b','c']),(['a']*5,['b']*10))\n yield Case(Annotated[List[str],at.Len(0,4)],([],['a','bcdef'],['a','b','c']),(['a']*5,['b']*10))\n \n yield Case(Annotated[str,at.Len(3,5)],('123','12345'),('','1','12','123456','x'*10))\n yield Case(Annotated[str,at.Len(3,3)],('123',),('12','1234'))\n \n yield Case(Annotated[Dict[int,int],at.Len(2,3)],[{1:1,2:2}],[{},{1:1},{1:1,2:2,3:3,4:4}])\n yield Case(Annotated[Set[int],at.Len(2,3)],({1,2},{1,2,3}),(set(),{1},{1,2,3,4}))\n yield Case(Annotated[Tuple[int,...],at.Len(2,3)],((1,2),(1,2,3)),((),(1,),(1,2,3,4)))\n \n \n \n yield Case(\n Annotated[datetime,at.Timezone(None)],[datetime(2000,1,1)],[datetime(2000,1,1,tzinfo=timezone.utc)]\n )\n yield Case(\n Annotated[datetime,at.Timezone(...)],[datetime(2000,1,1,tzinfo=timezone.utc)],[datetime(2000,1,1)]\n )\n yield Case(\n Annotated[datetime,at.Timezone(timezone.utc)],\n [datetime(2000,1,1,tzinfo=timezone.utc)],\n [datetime(2000,1,1),datetime(2000,1,1,tzinfo=timezone(timedelta(hours=6)))],\n )\n yield Case(\n Annotated[datetime,at.Timezone('Europe/London')],\n [datetime(2000,1,1,tzinfo=timezone(timedelta(0),name='Europe/London'))],\n [datetime(2000,1,1),datetime(2000,1,1,tzinfo=timezone(timedelta(hours=6)))],\n )\n \n \n \n yield Case(Annotated[float,at.Unit(unit='m')],(5,4.2),('5m','4.2m'))\n \n \n \n yield Case(at.LowerCase[str],['abc','foobar'],['','A','Boom'])\n yield Case(at.UpperCase[str],['ABC','DEFO'],['','a','abc','AbC'])\n yield Case(at.IsDigit[str],['123'],['','ab','a1b2'])\n yield Case(at.IsAscii[str],['123','foo bar'],['\u00a3100','\ud83d\ude0a','whatever \ud83d\udc40'])\n \n yield Case(Annotated[int,at.Predicate(lambda x:x %2 ==0)],[0,2,4],[1,3,5])\n \n yield Case(at.IsFinite[float],[1.23],[math.nan,math.inf,-math.inf])\n yield Case(at.IsNotFinite[float],[math.nan,math.inf],[1.23])\n yield Case(at.IsNan[float],[math.nan],[1.23,math.inf])\n yield Case(at.IsNotNan[float],[1.23,math.inf],[math.nan])\n yield Case(at.IsInfinite[float],[math.inf],[math.nan,1.23])\n yield Case(at.IsNotInfinite[float],[math.nan,1.23],[math.inf])\n \n \n yield Case(at.IsInfinite[Annotated[float,at.Predicate(lambda x:x >0)]],[math.inf],[-math.inf,1.23,math.nan])\n \n \n yield Case(Annotated[int,at.doc(\"A number\")],[1,2],[])\n \n \n class MyCustomGroupedMetadata(at.GroupedMetadata):\n  def __iter__(self)->Iterator[at.Predicate]:\n   yield at.Predicate(lambda x:float(x).is_integer())\n   \n yield Case(Annotated[float,MyCustomGroupedMetadata()],[0,2.0],[0.01,1.5])\n", ["annotated_types", "datetime", "decimal", "math", "sys", "typing", "typing_extensions"]], "pydantic_core": [".py", "from __future__ import annotations\n\nimport sys as _sys\nfrom typing import Any as _Any\n\nfrom._pydantic_core import(\nArgsKwargs,\nMultiHostUrl,\nPydanticCustomError,\nPydanticKnownError,\nPydanticOmit,\nPydanticSerializationError,\nPydanticSerializationUnexpectedValue,\nPydanticUndefined,\nPydanticUndefinedType,\nPydanticUseDefault,\nSchemaError,\nSchemaSerializer,\nSchemaValidator,\nSome,\nTzInfo,\nUrl,\nValidationError,\n__version__,\nfrom_json,\nto_json,\nto_jsonable_python,\nvalidate_core_schema,\n)\nfrom.core_schema import CoreConfig,CoreSchema,CoreSchemaType,ErrorType\n\nif _sys.version_info <(3,11):\n from typing_extensions import NotRequired as _NotRequired\nelse:\n from typing import NotRequired as _NotRequired\n \nif _sys.version_info <(3,12):\n from typing_extensions import TypedDict as _TypedDict\nelse:\n from typing import TypedDict as _TypedDict\n \n__all__=[\n'__version__',\n'CoreConfig',\n'CoreSchema',\n'CoreSchemaType',\n'SchemaValidator',\n'SchemaSerializer',\n'Some',\n'Url',\n'MultiHostUrl',\n'ArgsKwargs',\n'PydanticUndefined',\n'PydanticUndefinedType',\n'SchemaError',\n'ErrorDetails',\n'InitErrorDetails',\n'ValidationError',\n'PydanticCustomError',\n'PydanticKnownError',\n'PydanticOmit',\n'PydanticUseDefault',\n'PydanticSerializationError',\n'PydanticSerializationUnexpectedValue',\n'TzInfo',\n'to_json',\n'from_json',\n'to_jsonable_python',\n'validate_core_schema',\n]\n\n\nclass ErrorDetails(_TypedDict):\n type:str\n ''\n\n\n\n\n \n loc:tuple[int |str,...]\n '' \n msg:str\n '' \n input:_Any\n '' \n ctx:_NotRequired[dict[str,_Any]]\n ''\n\n\n \n \n \nclass InitErrorDetails(_TypedDict):\n type:str |PydanticCustomError\n '' \n loc:_NotRequired[tuple[int |str,...]]\n '' \n input:_Any\n '' \n ctx:_NotRequired[dict[str,_Any]]\n ''\n\n\n \n \n \nclass ErrorTypeInfo(_TypedDict):\n ''\n\n \n \n type:ErrorType\n '' \n message_template_python:str\n '' \n example_message_python:str\n '' \n message_template_json:_NotRequired[str]\n '' \n example_message_json:_NotRequired[str]\n '' \n example_context:dict[str,_Any]|None\n '' \n \n \nclass MultiHostHost(_TypedDict):\n ''\n\n \n \n username:str |None\n '' \n password:str |None\n '' \n host:str |None\n '' \n port:int |None\n '' \n", ["__future__", "pydantic_core._pydantic_core", "pydantic_core.core_schema", "sys", "typing", "typing_extensions"], 1], "pydantic_core.core_schema": [".py", "''\n\n\n\n\nfrom __future__ import annotations as _annotations\n\nimport sys\nimport warnings\nfrom collections.abc import Mapping\nfrom datetime import date,datetime,time,timedelta\nfrom decimal import Decimal\nfrom typing import TYPE_CHECKING,Any,Callable,Dict,Hashable,List,Pattern,Set,Tuple,Type,Union\n\nfrom typing_extensions import deprecated\n\nif sys.version_info <(3,12):\n from typing_extensions import TypedDict\nelse:\n from typing import TypedDict\n \nif sys.version_info <(3,11):\n from typing_extensions import Protocol,Required,TypeAlias\nelse:\n from typing import Protocol,Required,TypeAlias\n \nif sys.version_info <(3,9):\n from typing_extensions import Literal\nelse:\n from typing import Literal\n \nif TYPE_CHECKING:\n from pydantic_core import PydanticUndefined\nelse:\n\n\n\n try:\n  from pydantic_core import PydanticUndefined\n except ImportError:\n  PydanticUndefined=object()\n  \n  \nExtraBehavior=Literal['allow','forbid','ignore']\n\n\nclass CoreConfig(TypedDict,total=False):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n title:str\n strict:bool\n \n extra_fields_behavior:ExtraBehavior\n typed_dict_total:bool\n \n from_attributes:bool\n \n \n loc_by_alias:bool\n \n revalidate_instances:Literal['always','never','subclass-instances']\n \n validate_default:bool\n \n populate_by_name:bool\n \n str_max_length:int\n str_min_length:int\n str_strip_whitespace:bool\n str_to_lower:bool\n str_to_upper:bool\n \n allow_inf_nan:bool\n \n ser_json_timedelta:Literal['iso8601','float']\n ser_json_bytes:Literal['utf8','base64','hex']\n ser_json_inf_nan:Literal['null','constants','strings']\n val_json_bytes:Literal['utf8','base64','hex']\n \n hide_input_in_errors:bool\n validation_error_cause:bool\n coerce_numbers_to_str:bool\n regex_engine:Literal['rust-regex','python-re']\n cache_strings:Union[bool,Literal['all','keys','none']]\n \n \nIncExCall:TypeAlias='set[int | str] | dict[int | str, IncExCall] | None'\n\n\nclass SerializationInfo(Protocol):\n @property\n def include(self)->IncExCall:...\n \n @property\n def exclude(self)->IncExCall:...\n \n @property\n def context(self)->Any |None:\n  ''\n  \n @property\n def mode(self)->str:...\n \n @property\n def by_alias(self)->bool:...\n \n @property\n def exclude_unset(self)->bool:...\n \n @property\n def exclude_defaults(self)->bool:...\n \n @property\n def exclude_none(self)->bool:...\n \n @property\n def serialize_as_any(self)->bool:...\n \n def round_trip(self)->bool:...\n \n def mode_is_json(self)->bool:...\n \n def __str__(self)->str:...\n \n def __repr__(self)->str:...\n \n \nclass FieldSerializationInfo(SerializationInfo,Protocol):\n @property\n def field_name(self)->str:...\n \n \nclass ValidationInfo(Protocol):\n ''\n\n \n \n @property\n def context(self)->Any |None:\n  ''\n  ...\n  \n @property\n def config(self)->CoreConfig |None:\n  ''\n  ...\n  \n @property\n def mode(self)->Literal['python','json']:\n  ''\n  ...\n  \n @property\n def data(self)->Dict[str,Any]:\n  ''\n  ...\n  \n @property\n def field_name(self)->str |None:\n  ''\n\n\n  \n  ...\n  \n  \nExpectedSerializationTypes=Literal[\n'none',\n'int',\n'bool',\n'float',\n'str',\n'bytes',\n'bytearray',\n'list',\n'tuple',\n'set',\n'frozenset',\n'generator',\n'dict',\n'datetime',\n'date',\n'time',\n'timedelta',\n'url',\n'multi-host-url',\n'json',\n'uuid',\n'any',\n]\n\n\nclass SimpleSerSchema(TypedDict,total=False):\n type:Required[ExpectedSerializationTypes]\n \n \ndef simple_ser_schema(type:ExpectedSerializationTypes)->SimpleSerSchema:\n ''\n\n\n\n\n \n return SimpleSerSchema(type=type)\n \n \n \nGeneralPlainNoInfoSerializerFunction=Callable[[Any],Any]\n\nGeneralPlainInfoSerializerFunction=Callable[[Any,SerializationInfo],Any]\n\nFieldPlainNoInfoSerializerFunction=Callable[[Any,Any],Any]\n\nFieldPlainInfoSerializerFunction=Callable[[Any,Any,FieldSerializationInfo],Any]\nSerializerFunction=Union[\nGeneralPlainNoInfoSerializerFunction,\nGeneralPlainInfoSerializerFunction,\nFieldPlainNoInfoSerializerFunction,\nFieldPlainInfoSerializerFunction,\n]\n\nWhenUsed=Literal['always','unless-none','json','json-unless-none']\n''\n\n\n\n\n\n\n\n\n\nclass PlainSerializerFunctionSerSchema(TypedDict,total=False):\n type:Required[Literal['function-plain']]\n function:Required[SerializerFunction]\n is_field_serializer:bool\n info_arg:bool\n return_schema:CoreSchema\n when_used:WhenUsed\n \n \ndef plain_serializer_function_ser_schema(\nfunction:SerializerFunction,\n*,\nis_field_serializer:bool |None=None,\ninfo_arg:bool |None=None,\nreturn_schema:CoreSchema |None=None,\nwhen_used:WhenUsed='always',\n)->PlainSerializerFunctionSerSchema:\n ''\n\n\n\n\n\n\n\n\n\n \n if when_used =='always':\n \n  when_used=None\n return _dict_not_none(\n type='function-plain',\n function=function,\n is_field_serializer=is_field_serializer,\n info_arg=info_arg,\n return_schema=return_schema,\n when_used=when_used,\n )\n \n \nclass SerializerFunctionWrapHandler(Protocol):\n def __call__(self,input_value:Any,index_key:int |str |None=None,/)->Any:...\n \n \n \nGeneralWrapNoInfoSerializerFunction=Callable[[Any,SerializerFunctionWrapHandler],Any]\n\nGeneralWrapInfoSerializerFunction=Callable[[Any,SerializerFunctionWrapHandler,SerializationInfo],Any]\n\nFieldWrapNoInfoSerializerFunction=Callable[[Any,Any,SerializerFunctionWrapHandler],Any]\n\nFieldWrapInfoSerializerFunction=Callable[[Any,Any,SerializerFunctionWrapHandler,FieldSerializationInfo],Any]\nWrapSerializerFunction=Union[\nGeneralWrapNoInfoSerializerFunction,\nGeneralWrapInfoSerializerFunction,\nFieldWrapNoInfoSerializerFunction,\nFieldWrapInfoSerializerFunction,\n]\n\n\nclass WrapSerializerFunctionSerSchema(TypedDict,total=False):\n type:Required[Literal['function-wrap']]\n function:Required[WrapSerializerFunction]\n is_field_serializer:bool\n info_arg:bool\n schema:CoreSchema\n return_schema:CoreSchema\n when_used:WhenUsed\n \n \ndef wrap_serializer_function_ser_schema(\nfunction:WrapSerializerFunction,\n*,\nis_field_serializer:bool |None=None,\ninfo_arg:bool |None=None,\nschema:CoreSchema |None=None,\nreturn_schema:CoreSchema |None=None,\nwhen_used:WhenUsed='always',\n)->WrapSerializerFunctionSerSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n \n if when_used =='always':\n \n  when_used=None\n return _dict_not_none(\n type='function-wrap',\n function=function,\n is_field_serializer=is_field_serializer,\n info_arg=info_arg,\n schema=schema,\n return_schema=return_schema,\n when_used=when_used,\n )\n \n \nclass FormatSerSchema(TypedDict,total=False):\n type:Required[Literal['format']]\n formatting_string:Required[str]\n when_used:WhenUsed\n \n \ndef format_ser_schema(formatting_string:str,*,when_used:WhenUsed='json-unless-none')->FormatSerSchema:\n ''\n\n\n\n\n\n \n if when_used =='json-unless-none':\n \n  when_used=None\n return _dict_not_none(type='format',formatting_string=formatting_string,when_used=when_used)\n \n \nclass ToStringSerSchema(TypedDict,total=False):\n type:Required[Literal['to-string']]\n when_used:WhenUsed\n \n \ndef to_string_ser_schema(*,when_used:WhenUsed='json-unless-none')->ToStringSerSchema:\n ''\n\n\n\n\n \n s=dict(type='to-string')\n if when_used !='json-unless-none':\n \n  s['when_used']=when_used\n return s\n \n \nclass ModelSerSchema(TypedDict,total=False):\n type:Required[Literal['model']]\n cls:Required[Type[Any]]\n schema:Required[CoreSchema]\n \n \ndef model_ser_schema(cls:Type[Any],schema:CoreSchema)->ModelSerSchema:\n ''\n\n\n\n\n\n \n return ModelSerSchema(type='model',cls=cls,schema=schema)\n \n \nSerSchema=Union[\nSimpleSerSchema,\nPlainSerializerFunctionSerSchema,\nWrapSerializerFunctionSerSchema,\nFormatSerSchema,\nToStringSerSchema,\nModelSerSchema,\n]\n\n\nclass InvalidSchema(TypedDict,total=False):\n type:Required[Literal['invalid']]\n ref:str\n metadata:Dict[str,Any]\n \n \n serialization:SerSchema\n \n \ndef invalid_schema(ref:str |None=None,metadata:Dict[str,Any]|None=None)->InvalidSchema:\n ''\n\n\n\n\n\n\n\n \n \n return _dict_not_none(type='invalid',ref=ref,metadata=metadata)\n \n \nclass ComputedField(TypedDict,total=False):\n type:Required[Literal['computed-field']]\n property_name:Required[str]\n return_schema:Required[CoreSchema]\n alias:str\n metadata:Dict[str,Any]\n \n \ndef computed_field(\nproperty_name:str,return_schema:CoreSchema,*,alias:str |None=None,metadata:Dict[str,Any]|None=None\n)->ComputedField:\n ''\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='computed-field',property_name=property_name,return_schema=return_schema,alias=alias,metadata=metadata\n )\n \n \nclass AnySchema(TypedDict,total=False):\n type:Required[Literal['any']]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef any_schema(\n*,ref:str |None=None,metadata:Dict[str,Any]|None=None,serialization:SerSchema |None=None\n)->AnySchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='any',ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass NoneSchema(TypedDict,total=False):\n type:Required[Literal['none']]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef none_schema(\n*,ref:str |None=None,metadata:Dict[str,Any]|None=None,serialization:SerSchema |None=None\n)->NoneSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='none',ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass BoolSchema(TypedDict,total=False):\n type:Required[Literal['bool']]\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef bool_schema(\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->BoolSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='bool',strict=strict,ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass IntSchema(TypedDict,total=False):\n type:Required[Literal['int']]\n multiple_of:int\n le:int\n ge:int\n lt:int\n gt:int\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef int_schema(\n*,\nmultiple_of:int |None=None,\nle:int |None=None,\nge:int |None=None,\nlt:int |None=None,\ngt:int |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->IntSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='int',\n multiple_of=multiple_of,\n le=le,\n ge=ge,\n lt=lt,\n gt=gt,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass FloatSchema(TypedDict,total=False):\n type:Required[Literal['float']]\n allow_inf_nan:bool\n multiple_of:float\n le:float\n ge:float\n lt:float\n gt:float\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef float_schema(\n*,\nallow_inf_nan:bool |None=None,\nmultiple_of:float |None=None,\nle:float |None=None,\nge:float |None=None,\nlt:float |None=None,\ngt:float |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->FloatSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='float',\n allow_inf_nan=allow_inf_nan,\n multiple_of=multiple_of,\n le=le,\n ge=ge,\n lt=lt,\n gt=gt,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass DecimalSchema(TypedDict,total=False):\n type:Required[Literal['decimal']]\n allow_inf_nan:bool\n multiple_of:Decimal\n le:Decimal\n ge:Decimal\n lt:Decimal\n gt:Decimal\n max_digits:int\n decimal_places:int\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef decimal_schema(\n*,\nallow_inf_nan:bool |None=None,\nmultiple_of:Decimal |None=None,\nle:Decimal |None=None,\nge:Decimal |None=None,\nlt:Decimal |None=None,\ngt:Decimal |None=None,\nmax_digits:int |None=None,\ndecimal_places:int |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->DecimalSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='decimal',\n gt=gt,\n ge=ge,\n lt=lt,\n le=le,\n max_digits=max_digits,\n decimal_places=decimal_places,\n multiple_of=multiple_of,\n allow_inf_nan=allow_inf_nan,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass ComplexSchema(TypedDict,total=False):\n type:Required[Literal['complex']]\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef complex_schema(\n*,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->ComplexSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='complex',\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass StringSchema(TypedDict,total=False):\n type:Required[Literal['str']]\n pattern:Union[str,Pattern[str]]\n max_length:int\n min_length:int\n strip_whitespace:bool\n to_lower:bool\n to_upper:bool\n regex_engine:Literal['rust-regex','python-re']\n strict:bool\n coerce_numbers_to_str:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef str_schema(\n*,\npattern:str |Pattern[str]|None=None,\nmax_length:int |None=None,\nmin_length:int |None=None,\nstrip_whitespace:bool |None=None,\nto_lower:bool |None=None,\nto_upper:bool |None=None,\nregex_engine:Literal['rust-regex','python-re']|None=None,\nstrict:bool |None=None,\ncoerce_numbers_to_str:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->StringSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='str',\n pattern=pattern,\n max_length=max_length,\n min_length=min_length,\n strip_whitespace=strip_whitespace,\n to_lower=to_lower,\n to_upper=to_upper,\n regex_engine=regex_engine,\n strict=strict,\n coerce_numbers_to_str=coerce_numbers_to_str,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass BytesSchema(TypedDict,total=False):\n type:Required[Literal['bytes']]\n max_length:int\n min_length:int\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef bytes_schema(\n*,\nmax_length:int |None=None,\nmin_length:int |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->BytesSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='bytes',\n max_length=max_length,\n min_length=min_length,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass DateSchema(TypedDict,total=False):\n type:Required[Literal['date']]\n strict:bool\n le:date\n ge:date\n lt:date\n gt:date\n now_op:Literal['past','future']\n \n \n now_utc_offset:int\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef date_schema(\n*,\nstrict:bool |None=None,\nle:date |None=None,\nge:date |None=None,\nlt:date |None=None,\ngt:date |None=None,\nnow_op:Literal['past','future']|None=None,\nnow_utc_offset:int |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->DateSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='date',\n strict=strict,\n le=le,\n ge=ge,\n lt=lt,\n gt=gt,\n now_op=now_op,\n now_utc_offset=now_utc_offset,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass TimeSchema(TypedDict,total=False):\n type:Required[Literal['time']]\n strict:bool\n le:time\n ge:time\n lt:time\n gt:time\n tz_constraint:Union[Literal['aware','naive'],int]\n microseconds_precision:Literal['truncate','error']\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef time_schema(\n*,\nstrict:bool |None=None,\nle:time |None=None,\nge:time |None=None,\nlt:time |None=None,\ngt:time |None=None,\ntz_constraint:Literal['aware','naive']|int |None=None,\nmicroseconds_precision:Literal['truncate','error']='truncate',\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->TimeSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='time',\n strict=strict,\n le=le,\n ge=ge,\n lt=lt,\n gt=gt,\n tz_constraint=tz_constraint,\n microseconds_precision=microseconds_precision,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass DatetimeSchema(TypedDict,total=False):\n type:Required[Literal['datetime']]\n strict:bool\n le:datetime\n ge:datetime\n lt:datetime\n gt:datetime\n now_op:Literal['past','future']\n tz_constraint:Union[Literal['aware','naive'],int]\n \n \n now_utc_offset:int\n microseconds_precision:Literal['truncate','error']\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef datetime_schema(\n*,\nstrict:bool |None=None,\nle:datetime |None=None,\nge:datetime |None=None,\nlt:datetime |None=None,\ngt:datetime |None=None,\nnow_op:Literal['past','future']|None=None,\ntz_constraint:Literal['aware','naive']|int |None=None,\nnow_utc_offset:int |None=None,\nmicroseconds_precision:Literal['truncate','error']='truncate',\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->DatetimeSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='datetime',\n strict=strict,\n le=le,\n ge=ge,\n lt=lt,\n gt=gt,\n now_op=now_op,\n tz_constraint=tz_constraint,\n now_utc_offset=now_utc_offset,\n microseconds_precision=microseconds_precision,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass TimedeltaSchema(TypedDict,total=False):\n type:Required[Literal['timedelta']]\n strict:bool\n le:timedelta\n ge:timedelta\n lt:timedelta\n gt:timedelta\n microseconds_precision:Literal['truncate','error']\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef timedelta_schema(\n*,\nstrict:bool |None=None,\nle:timedelta |None=None,\nge:timedelta |None=None,\nlt:timedelta |None=None,\ngt:timedelta |None=None,\nmicroseconds_precision:Literal['truncate','error']='truncate',\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->TimedeltaSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='timedelta',\n strict=strict,\n le=le,\n ge=ge,\n lt=lt,\n gt=gt,\n microseconds_precision=microseconds_precision,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass LiteralSchema(TypedDict,total=False):\n type:Required[Literal['literal']]\n expected:Required[List[Any]]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef literal_schema(\nexpected:list[Any],\n*,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->LiteralSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='literal',expected=expected,ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass EnumSchema(TypedDict,total=False):\n type:Required[Literal['enum']]\n cls:Required[Any]\n members:Required[List[Any]]\n sub_type:Literal['str','int','float']\n missing:Callable[[Any],Any]\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef enum_schema(\ncls:Any,\nmembers:list[Any],\n*,\nsub_type:Literal['str','int','float']|None=None,\nmissing:Callable[[Any],Any]|None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->EnumSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='enum',\n cls=cls,\n members=members,\n sub_type=sub_type,\n missing=missing,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \n \nJsonType=Literal['null','bool','int','float','str','list','dict']\n\n\nclass IsInstanceSchema(TypedDict,total=False):\n type:Required[Literal['is-instance']]\n cls:Required[Any]\n cls_repr:str\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef is_instance_schema(\ncls:Any,\n*,\ncls_repr:str |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->IsInstanceSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='is-instance',cls=cls,cls_repr=cls_repr,ref=ref,metadata=metadata,serialization=serialization\n )\n \n \nclass IsSubclassSchema(TypedDict,total=False):\n type:Required[Literal['is-subclass']]\n cls:Required[Type[Any]]\n cls_repr:str\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef is_subclass_schema(\ncls:Type[Any],\n*,\ncls_repr:str |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->IsInstanceSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='is-subclass',cls=cls,cls_repr=cls_repr,ref=ref,metadata=metadata,serialization=serialization\n )\n \n \nclass CallableSchema(TypedDict,total=False):\n type:Required[Literal['callable']]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef callable_schema(\n*,ref:str |None=None,metadata:Dict[str,Any]|None=None,serialization:SerSchema |None=None\n)->CallableSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='callable',ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass UuidSchema(TypedDict,total=False):\n type:Required[Literal['uuid']]\n version:Literal[1,3,4,5]\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef uuid_schema(\n*,\nversion:Literal[1,3,4,5]|None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->UuidSchema:\n return _dict_not_none(\n type='uuid',version=version,strict=strict,ref=ref,metadata=metadata,serialization=serialization\n )\n \n \nclass IncExSeqSerSchema(TypedDict,total=False):\n type:Required[Literal['include-exclude-sequence']]\n include:Set[int]\n exclude:Set[int]\n \n \ndef filter_seq_schema(*,include:Set[int]|None=None,exclude:Set[int]|None=None)->IncExSeqSerSchema:\n return _dict_not_none(type='include-exclude-sequence',include=include,exclude=exclude)\n \n \nIncExSeqOrElseSerSchema=Union[IncExSeqSerSchema,SerSchema]\n\n\nclass ListSchema(TypedDict,total=False):\n type:Required[Literal['list']]\n items_schema:CoreSchema\n min_length:int\n max_length:int\n fail_fast:bool\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:IncExSeqOrElseSerSchema\n \n \ndef list_schema(\nitems_schema:CoreSchema |None=None,\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nfail_fast:bool |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:IncExSeqOrElseSerSchema |None=None,\n)->ListSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='list',\n items_schema=items_schema,\n min_length=min_length,\n max_length=max_length,\n fail_fast=fail_fast,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \n \ndef tuple_positional_schema(\nitems_schema:list[CoreSchema],\n*,\nextras_schema:CoreSchema |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:IncExSeqOrElseSerSchema |None=None,\n)->TupleSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n if extras_schema is not None:\n  variadic_item_index=len(items_schema)\n  items_schema=items_schema+[extras_schema]\n else:\n  variadic_item_index=None\n return tuple_schema(\n items_schema=items_schema,\n variadic_item_index=variadic_item_index,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \n \ndef tuple_variable_schema(\nitems_schema:CoreSchema |None=None,\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:IncExSeqOrElseSerSchema |None=None,\n)->TupleSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return tuple_schema(\n items_schema=[items_schema or any_schema()],\n variadic_item_index=0,\n min_length=min_length,\n max_length=max_length,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass TupleSchema(TypedDict,total=False):\n type:Required[Literal['tuple']]\n items_schema:Required[List[CoreSchema]]\n variadic_item_index:int\n min_length:int\n max_length:int\n fail_fast:bool\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:IncExSeqOrElseSerSchema\n \n \ndef tuple_schema(\nitems_schema:list[CoreSchema],\n*,\nvariadic_item_index:int |None=None,\nmin_length:int |None=None,\nmax_length:int |None=None,\nfail_fast:bool |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:IncExSeqOrElseSerSchema |None=None,\n)->TupleSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='tuple',\n items_schema=items_schema,\n variadic_item_index=variadic_item_index,\n min_length=min_length,\n max_length=max_length,\n fail_fast=fail_fast,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass SetSchema(TypedDict,total=False):\n type:Required[Literal['set']]\n items_schema:CoreSchema\n min_length:int\n max_length:int\n fail_fast:bool\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef set_schema(\nitems_schema:CoreSchema |None=None,\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nfail_fast:bool |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->SetSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='set',\n items_schema=items_schema,\n min_length=min_length,\n max_length=max_length,\n fail_fast=fail_fast,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass FrozenSetSchema(TypedDict,total=False):\n type:Required[Literal['frozenset']]\n items_schema:CoreSchema\n min_length:int\n max_length:int\n fail_fast:bool\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef frozenset_schema(\nitems_schema:CoreSchema |None=None,\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nfail_fast:bool |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->FrozenSetSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='frozenset',\n items_schema=items_schema,\n min_length=min_length,\n max_length=max_length,\n fail_fast=fail_fast,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass GeneratorSchema(TypedDict,total=False):\n type:Required[Literal['generator']]\n items_schema:CoreSchema\n min_length:int\n max_length:int\n ref:str\n metadata:Dict[str,Any]\n serialization:IncExSeqOrElseSerSchema\n \n \ndef generator_schema(\nitems_schema:CoreSchema |None=None,\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:IncExSeqOrElseSerSchema |None=None,\n)->GeneratorSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='generator',\n items_schema=items_schema,\n min_length=min_length,\n max_length=max_length,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nIncExDict=Set[Union[int,str]]\n\n\nclass IncExDictSerSchema(TypedDict,total=False):\n type:Required[Literal['include-exclude-dict']]\n include:IncExDict\n exclude:IncExDict\n \n \ndef filter_dict_schema(*,include:IncExDict |None=None,exclude:IncExDict |None=None)->IncExDictSerSchema:\n return _dict_not_none(type='include-exclude-dict',include=include,exclude=exclude)\n \n \nIncExDictOrElseSerSchema=Union[IncExDictSerSchema,SerSchema]\n\n\nclass DictSchema(TypedDict,total=False):\n type:Required[Literal['dict']]\n keys_schema:CoreSchema\n values_schema:CoreSchema\n min_length:int\n max_length:int\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:IncExDictOrElseSerSchema\n \n \ndef dict_schema(\nkeys_schema:CoreSchema |None=None,\nvalues_schema:CoreSchema |None=None,\n*,\nmin_length:int |None=None,\nmax_length:int |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->DictSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='dict',\n keys_schema=keys_schema,\n values_schema=values_schema,\n min_length=min_length,\n max_length=max_length,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \n \nNoInfoValidatorFunction=Callable[[Any],Any]\n\n\nclass NoInfoValidatorFunctionSchema(TypedDict):\n type:Literal['no-info']\n function:NoInfoValidatorFunction\n \n \n \nWithInfoValidatorFunction=Callable[[Any,ValidationInfo],Any]\n\n\nclass WithInfoValidatorFunctionSchema(TypedDict,total=False):\n type:Required[Literal['with-info']]\n function:Required[WithInfoValidatorFunction]\n field_name:str\n \n \nValidationFunction=Union[NoInfoValidatorFunctionSchema,WithInfoValidatorFunctionSchema]\n\n\nclass _ValidatorFunctionSchema(TypedDict,total=False):\n function:Required[ValidationFunction]\n schema:Required[CoreSchema]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \nclass BeforeValidatorFunctionSchema(_ValidatorFunctionSchema,total=False):\n type:Required[Literal['function-before']]\n json_schema_input_schema:CoreSchema\n \n \ndef no_info_before_validator_function(\nfunction:NoInfoValidatorFunction,\nschema:CoreSchema,\n*,\nref:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->BeforeValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-before',\n function={'type':'no-info','function':function},\n schema=schema,\n ref=ref,\n json_schema_input_schema=json_schema_input_schema,\n metadata=metadata,\n serialization=serialization,\n )\n \n \ndef with_info_before_validator_function(\nfunction:WithInfoValidatorFunction,\nschema:CoreSchema,\n*,\nfield_name:str |None=None,\nref:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->BeforeValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-before',\n function=_dict_not_none(type='with-info',function=function,field_name=field_name),\n schema=schema,\n ref=ref,\n json_schema_input_schema=json_schema_input_schema,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass AfterValidatorFunctionSchema(_ValidatorFunctionSchema,total=False):\n type:Required[Literal['function-after']]\n \n \ndef no_info_after_validator_function(\nfunction:NoInfoValidatorFunction,\nschema:CoreSchema,\n*,\nref:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->AfterValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-after',\n function={'type':'no-info','function':function},\n schema=schema,\n ref=ref,\n json_schema_input_schema=json_schema_input_schema,\n metadata=metadata,\n serialization=serialization,\n )\n \n \ndef with_info_after_validator_function(\nfunction:WithInfoValidatorFunction,\nschema:CoreSchema,\n*,\nfield_name:str |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->AfterValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-after',\n function=_dict_not_none(type='with-info',function=function,field_name=field_name),\n schema=schema,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass ValidatorFunctionWrapHandler(Protocol):\n def __call__(self,input_value:Any,outer_location:str |int |None=None,/)->Any:\n  ...\n  \n  \n  \nNoInfoWrapValidatorFunction=Callable[[Any,ValidatorFunctionWrapHandler],Any]\n\n\nclass NoInfoWrapValidatorFunctionSchema(TypedDict):\n type:Literal['no-info']\n function:NoInfoWrapValidatorFunction\n \n \n \nWithInfoWrapValidatorFunction=Callable[[Any,ValidatorFunctionWrapHandler,ValidationInfo],Any]\n\n\nclass WithInfoWrapValidatorFunctionSchema(TypedDict,total=False):\n type:Required[Literal['with-info']]\n function:Required[WithInfoWrapValidatorFunction]\n field_name:str\n \n \nWrapValidatorFunction=Union[NoInfoWrapValidatorFunctionSchema,WithInfoWrapValidatorFunctionSchema]\n\n\nclass WrapValidatorFunctionSchema(TypedDict,total=False):\n type:Required[Literal['function-wrap']]\n function:Required[WrapValidatorFunction]\n schema:Required[CoreSchema]\n ref:str\n json_schema_input_schema:CoreSchema\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef no_info_wrap_validator_function(\nfunction:NoInfoWrapValidatorFunction,\nschema:CoreSchema,\n*,\nref:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->WrapValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-wrap',\n function={'type':'no-info','function':function},\n schema=schema,\n json_schema_input_schema=json_schema_input_schema,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \ndef with_info_wrap_validator_function(\nfunction:WithInfoWrapValidatorFunction,\nschema:CoreSchema,\n*,\nfield_name:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->WrapValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-wrap',\n function=_dict_not_none(type='with-info',function=function,field_name=field_name),\n schema=schema,\n json_schema_input_schema=json_schema_input_schema,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass PlainValidatorFunctionSchema(TypedDict,total=False):\n type:Required[Literal['function-plain']]\n function:Required[ValidationFunction]\n ref:str\n json_schema_input_schema:CoreSchema\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef no_info_plain_validator_function(\nfunction:NoInfoValidatorFunction,\n*,\nref:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->PlainValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-plain',\n function={'type':'no-info','function':function},\n ref=ref,\n json_schema_input_schema=json_schema_input_schema,\n metadata=metadata,\n serialization=serialization,\n )\n \n \ndef with_info_plain_validator_function(\nfunction:WithInfoValidatorFunction,\n*,\nfield_name:str |None=None,\nref:str |None=None,\njson_schema_input_schema:CoreSchema |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->PlainValidatorFunctionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='function-plain',\n function=_dict_not_none(type='with-info',function=function,field_name=field_name),\n ref=ref,\n json_schema_input_schema=json_schema_input_schema,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass WithDefaultSchema(TypedDict,total=False):\n type:Required[Literal['default']]\n schema:Required[CoreSchema]\n default:Any\n default_factory:Union[Callable[[],Any],Callable[[Dict[str,Any]],Any]]\n default_factory_takes_data:bool\n on_error:Literal['raise','omit','default']\n validate_default:bool\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef with_default_schema(\nschema:CoreSchema,\n*,\ndefault:Any=PydanticUndefined,\ndefault_factory:Union[Callable[[],Any],Callable[[Dict[str,Any]],Any],None]=None,\ndefault_factory_takes_data:bool |None=None,\non_error:Literal['raise','omit','default']|None=None,\nvalidate_default:bool |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->WithDefaultSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n s=_dict_not_none(\n type='default',\n schema=schema,\n default_factory=default_factory,\n default_factory_takes_data=default_factory_takes_data,\n on_error=on_error,\n validate_default=validate_default,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n if default is not PydanticUndefined:\n  s['default']=default\n return s\n \n \nclass NullableSchema(TypedDict,total=False):\n type:Required[Literal['nullable']]\n schema:Required[CoreSchema]\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef nullable_schema(\nschema:CoreSchema,\n*,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->NullableSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='nullable',schema=schema,strict=strict,ref=ref,metadata=metadata,serialization=serialization\n )\n \n \nclass UnionSchema(TypedDict,total=False):\n type:Required[Literal['union']]\n choices:Required[List[Union[CoreSchema,Tuple[CoreSchema,str]]]]\n \n auto_collapse:bool\n custom_error_type:str\n custom_error_message:str\n custom_error_context:Dict[str,Union[str,int,float]]\n mode:Literal['smart','left_to_right']\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef union_schema(\nchoices:list[CoreSchema |tuple[CoreSchema,str]],\n*,\nauto_collapse:bool |None=None,\ncustom_error_type:str |None=None,\ncustom_error_message:str |None=None,\ncustom_error_context:dict[str,str |int]|None=None,\nmode:Literal['smart','left_to_right']|None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->UnionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='union',\n choices=choices,\n auto_collapse=auto_collapse,\n custom_error_type=custom_error_type,\n custom_error_message=custom_error_message,\n custom_error_context=custom_error_context,\n mode=mode,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass TaggedUnionSchema(TypedDict,total=False):\n type:Required[Literal['tagged-union']]\n choices:Required[Dict[Hashable,CoreSchema]]\n discriminator:Required[Union[str,List[Union[str,int]],List[List[Union[str,int]]],Callable[[Any],Hashable]]]\n custom_error_type:str\n custom_error_message:str\n custom_error_context:Dict[str,Union[str,int,float]]\n strict:bool\n from_attributes:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef tagged_union_schema(\nchoices:Dict[Any,CoreSchema],\ndiscriminator:str |list[str |int]|list[list[str |int]]|Callable[[Any],Any],\n*,\ncustom_error_type:str |None=None,\ncustom_error_message:str |None=None,\ncustom_error_context:dict[str,int |str |float]|None=None,\nstrict:bool |None=None,\nfrom_attributes:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->TaggedUnionSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='tagged-union',\n choices=choices,\n discriminator=discriminator,\n custom_error_type=custom_error_type,\n custom_error_message=custom_error_message,\n custom_error_context=custom_error_context,\n strict=strict,\n from_attributes=from_attributes,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass ChainSchema(TypedDict,total=False):\n type:Required[Literal['chain']]\n steps:Required[List[CoreSchema]]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef chain_schema(\nsteps:list[CoreSchema],\n*,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->ChainSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='chain',steps=steps,ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass LaxOrStrictSchema(TypedDict,total=False):\n type:Required[Literal['lax-or-strict']]\n lax_schema:Required[CoreSchema]\n strict_schema:Required[CoreSchema]\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef lax_or_strict_schema(\nlax_schema:CoreSchema,\nstrict_schema:CoreSchema,\n*,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->LaxOrStrictSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='lax-or-strict',\n lax_schema=lax_schema,\n strict_schema=strict_schema,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass JsonOrPythonSchema(TypedDict,total=False):\n type:Required[Literal['json-or-python']]\n json_schema:Required[CoreSchema]\n python_schema:Required[CoreSchema]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef json_or_python_schema(\njson_schema:CoreSchema,\npython_schema:CoreSchema,\n*,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->JsonOrPythonSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='json-or-python',\n json_schema=json_schema,\n python_schema=python_schema,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass TypedDictField(TypedDict,total=False):\n type:Required[Literal['typed-dict-field']]\n schema:Required[CoreSchema]\n required:bool\n validation_alias:Union[str,List[Union[str,int]],List[List[Union[str,int]]]]\n serialization_alias:str\n serialization_exclude:bool\n metadata:Dict[str,Any]\n \n \ndef typed_dict_field(\nschema:CoreSchema,\n*,\nrequired:bool |None=None,\nvalidation_alias:str |list[str |int]|list[list[str |int]]|None=None,\nserialization_alias:str |None=None,\nserialization_exclude:bool |None=None,\nmetadata:Dict[str,Any]|None=None,\n)->TypedDictField:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='typed-dict-field',\n schema=schema,\n required=required,\n validation_alias=validation_alias,\n serialization_alias=serialization_alias,\n serialization_exclude=serialization_exclude,\n metadata=metadata,\n )\n \n \nclass TypedDictSchema(TypedDict,total=False):\n type:Required[Literal['typed-dict']]\n fields:Required[Dict[str,TypedDictField]]\n cls:Type[Any]\n computed_fields:List[ComputedField]\n strict:bool\n extras_schema:CoreSchema\n \n extra_behavior:ExtraBehavior\n total:bool\n populate_by_name:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n config:CoreConfig\n \n \ndef typed_dict_schema(\nfields:Dict[str,TypedDictField],\n*,\ncls:Type[Any]|None=None,\ncomputed_fields:list[ComputedField]|None=None,\nstrict:bool |None=None,\nextras_schema:CoreSchema |None=None,\nextra_behavior:ExtraBehavior |None=None,\ntotal:bool |None=None,\npopulate_by_name:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\nconfig:CoreConfig |None=None,\n)->TypedDictSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='typed-dict',\n fields=fields,\n cls=cls,\n computed_fields=computed_fields,\n strict=strict,\n extras_schema=extras_schema,\n extra_behavior=extra_behavior,\n total=total,\n populate_by_name=populate_by_name,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n config=config,\n )\n \n \nclass ModelField(TypedDict,total=False):\n type:Required[Literal['model-field']]\n schema:Required[CoreSchema]\n validation_alias:Union[str,List[Union[str,int]],List[List[Union[str,int]]]]\n serialization_alias:str\n serialization_exclude:bool\n frozen:bool\n metadata:Dict[str,Any]\n \n \ndef model_field(\nschema:CoreSchema,\n*,\nvalidation_alias:str |list[str |int]|list[list[str |int]]|None=None,\nserialization_alias:str |None=None,\nserialization_exclude:bool |None=None,\nfrozen:bool |None=None,\nmetadata:Dict[str,Any]|None=None,\n)->ModelField:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='model-field',\n schema=schema,\n validation_alias=validation_alias,\n serialization_alias=serialization_alias,\n serialization_exclude=serialization_exclude,\n frozen=frozen,\n metadata=metadata,\n )\n \n \nclass ModelFieldsSchema(TypedDict,total=False):\n type:Required[Literal['model-fields']]\n fields:Required[Dict[str,ModelField]]\n model_name:str\n computed_fields:List[ComputedField]\n strict:bool\n extras_schema:CoreSchema\n \n extra_behavior:ExtraBehavior\n populate_by_name:bool\n from_attributes:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef model_fields_schema(\nfields:Dict[str,ModelField],\n*,\nmodel_name:str |None=None,\ncomputed_fields:list[ComputedField]|None=None,\nstrict:bool |None=None,\nextras_schema:CoreSchema |None=None,\nextra_behavior:ExtraBehavior |None=None,\npopulate_by_name:bool |None=None,\nfrom_attributes:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->ModelFieldsSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='model-fields',\n fields=fields,\n model_name=model_name,\n computed_fields=computed_fields,\n strict=strict,\n extras_schema=extras_schema,\n extra_behavior=extra_behavior,\n populate_by_name=populate_by_name,\n from_attributes=from_attributes,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass ModelSchema(TypedDict,total=False):\n type:Required[Literal['model']]\n cls:Required[Type[Any]]\n generic_origin:Type[Any]\n schema:Required[CoreSchema]\n custom_init:bool\n root_model:bool\n post_init:str\n revalidate_instances:Literal['always','never','subclass-instances']\n strict:bool\n frozen:bool\n extra_behavior:ExtraBehavior\n config:CoreConfig\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef model_schema(\ncls:Type[Any],\nschema:CoreSchema,\n*,\ngeneric_origin:Type[Any]|None=None,\ncustom_init:bool |None=None,\nroot_model:bool |None=None,\npost_init:str |None=None,\nrevalidate_instances:Literal['always','never','subclass-instances']|None=None,\nstrict:bool |None=None,\nfrozen:bool |None=None,\nextra_behavior:ExtraBehavior |None=None,\nconfig:CoreConfig |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->ModelSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='model',\n cls=cls,\n generic_origin=generic_origin,\n schema=schema,\n custom_init=custom_init,\n root_model=root_model,\n post_init=post_init,\n revalidate_instances=revalidate_instances,\n strict=strict,\n frozen=frozen,\n extra_behavior=extra_behavior,\n config=config,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass DataclassField(TypedDict,total=False):\n type:Required[Literal['dataclass-field']]\n name:Required[str]\n schema:Required[CoreSchema]\n kw_only:bool\n init:bool\n init_only:bool\n frozen:bool\n validation_alias:Union[str,List[Union[str,int]],List[List[Union[str,int]]]]\n serialization_alias:str\n serialization_exclude:bool\n metadata:Dict[str,Any]\n \n \ndef dataclass_field(\nname:str,\nschema:CoreSchema,\n*,\nkw_only:bool |None=None,\ninit:bool |None=None,\ninit_only:bool |None=None,\nvalidation_alias:str |list[str |int]|list[list[str |int]]|None=None,\nserialization_alias:str |None=None,\nserialization_exclude:bool |None=None,\nmetadata:Dict[str,Any]|None=None,\nfrozen:bool |None=None,\n)->DataclassField:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='dataclass-field',\n name=name,\n schema=schema,\n kw_only=kw_only,\n init=init,\n init_only=init_only,\n validation_alias=validation_alias,\n serialization_alias=serialization_alias,\n serialization_exclude=serialization_exclude,\n metadata=metadata,\n frozen=frozen,\n )\n \n \nclass DataclassArgsSchema(TypedDict,total=False):\n type:Required[Literal['dataclass-args']]\n dataclass_name:Required[str]\n fields:Required[List[DataclassField]]\n computed_fields:List[ComputedField]\n populate_by_name:bool\n collect_init_only:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n extra_behavior:ExtraBehavior\n \n \ndef dataclass_args_schema(\ndataclass_name:str,\nfields:list[DataclassField],\n*,\ncomputed_fields:List[ComputedField]|None=None,\npopulate_by_name:bool |None=None,\ncollect_init_only:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\nextra_behavior:ExtraBehavior |None=None,\n)->DataclassArgsSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='dataclass-args',\n dataclass_name=dataclass_name,\n fields=fields,\n computed_fields=computed_fields,\n populate_by_name=populate_by_name,\n collect_init_only=collect_init_only,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n extra_behavior=extra_behavior,\n )\n \n \nclass DataclassSchema(TypedDict,total=False):\n type:Required[Literal['dataclass']]\n cls:Required[Type[Any]]\n generic_origin:Type[Any]\n schema:Required[CoreSchema]\n fields:Required[List[str]]\n cls_name:str\n post_init:bool\n revalidate_instances:Literal['always','never','subclass-instances']\n strict:bool\n frozen:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n slots:bool\n config:CoreConfig\n \n \ndef dataclass_schema(\ncls:Type[Any],\nschema:CoreSchema,\nfields:List[str],\n*,\ngeneric_origin:Type[Any]|None=None,\ncls_name:str |None=None,\npost_init:bool |None=None,\nrevalidate_instances:Literal['always','never','subclass-instances']|None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\nfrozen:bool |None=None,\nslots:bool |None=None,\nconfig:CoreConfig |None=None,\n)->DataclassSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='dataclass',\n cls=cls,\n generic_origin=generic_origin,\n fields=fields,\n cls_name=cls_name,\n schema=schema,\n post_init=post_init,\n revalidate_instances=revalidate_instances,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n frozen=frozen,\n slots=slots,\n config=config,\n )\n \n \nclass ArgumentsParameter(TypedDict,total=False):\n name:Required[str]\n schema:Required[CoreSchema]\n mode:Literal['positional_only','positional_or_keyword','keyword_only']\n alias:Union[str,List[Union[str,int]],List[List[Union[str,int]]]]\n \n \ndef arguments_parameter(\nname:str,\nschema:CoreSchema,\n*,\nmode:Literal['positional_only','positional_or_keyword','keyword_only']|None=None,\nalias:str |list[str |int]|list[list[str |int]]|None=None,\n)->ArgumentsParameter:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(name=name,schema=schema,mode=mode,alias=alias)\n \n \nVarKwargsMode:TypeAlias=Literal['uniform','unpacked-typed-dict']\n\n\nclass ArgumentsSchema(TypedDict,total=False):\n type:Required[Literal['arguments']]\n arguments_schema:Required[List[ArgumentsParameter]]\n populate_by_name:bool\n var_args_schema:CoreSchema\n var_kwargs_mode:VarKwargsMode\n var_kwargs_schema:CoreSchema\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef arguments_schema(\narguments:list[ArgumentsParameter],\n*,\npopulate_by_name:bool |None=None,\nvar_args_schema:CoreSchema |None=None,\nvar_kwargs_mode:VarKwargsMode |None=None,\nvar_kwargs_schema:CoreSchema |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->ArgumentsSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='arguments',\n arguments_schema=arguments,\n populate_by_name=populate_by_name,\n var_args_schema=var_args_schema,\n var_kwargs_mode=var_kwargs_mode,\n var_kwargs_schema=var_kwargs_schema,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass CallSchema(TypedDict,total=False):\n type:Required[Literal['call']]\n arguments_schema:Required[CoreSchema]\n function:Required[Callable[...,Any]]\n function_name:str\n return_schema:CoreSchema\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef call_schema(\narguments:CoreSchema,\nfunction:Callable[...,Any],\n*,\nfunction_name:str |None=None,\nreturn_schema:CoreSchema |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->CallSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='call',\n arguments_schema=arguments,\n function=function,\n function_name=function_name,\n return_schema=return_schema,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass CustomErrorSchema(TypedDict,total=False):\n type:Required[Literal['custom-error']]\n schema:Required[CoreSchema]\n custom_error_type:Required[str]\n custom_error_message:str\n custom_error_context:Dict[str,Union[str,int,float]]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef custom_error_schema(\nschema:CoreSchema,\ncustom_error_type:str,\n*,\ncustom_error_message:str |None=None,\ncustom_error_context:dict[str,Any]|None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->CustomErrorSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='custom-error',\n schema=schema,\n custom_error_type=custom_error_type,\n custom_error_message=custom_error_message,\n custom_error_context=custom_error_context,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass JsonSchema(TypedDict,total=False):\n type:Required[Literal['json']]\n schema:CoreSchema\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef json_schema(\nschema:CoreSchema |None=None,\n*,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->JsonSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(type='json',schema=schema,ref=ref,metadata=metadata,serialization=serialization)\n \n \nclass UrlSchema(TypedDict,total=False):\n type:Required[Literal['url']]\n max_length:int\n allowed_schemes:List[str]\n host_required:bool\n default_host:str\n default_port:int\n default_path:str\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef url_schema(\n*,\nmax_length:int |None=None,\nallowed_schemes:list[str]|None=None,\nhost_required:bool |None=None,\ndefault_host:str |None=None,\ndefault_port:int |None=None,\ndefault_path:str |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->UrlSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='url',\n max_length=max_length,\n allowed_schemes=allowed_schemes,\n host_required=host_required,\n default_host=default_host,\n default_port=default_port,\n default_path=default_path,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass MultiHostUrlSchema(TypedDict,total=False):\n type:Required[Literal['multi-host-url']]\n max_length:int\n allowed_schemes:List[str]\n host_required:bool\n default_host:str\n default_port:int\n default_path:str\n strict:bool\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef multi_host_url_schema(\n*,\nmax_length:int |None=None,\nallowed_schemes:list[str]|None=None,\nhost_required:bool |None=None,\ndefault_host:str |None=None,\ndefault_port:int |None=None,\ndefault_path:str |None=None,\nstrict:bool |None=None,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->MultiHostUrlSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='multi-host-url',\n max_length=max_length,\n allowed_schemes=allowed_schemes,\n host_required=host_required,\n default_host=default_host,\n default_port=default_port,\n default_path=default_path,\n strict=strict,\n ref=ref,\n metadata=metadata,\n serialization=serialization,\n )\n \n \nclass DefinitionsSchema(TypedDict,total=False):\n type:Required[Literal['definitions']]\n schema:Required[CoreSchema]\n definitions:Required[List[CoreSchema]]\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef definitions_schema(schema:CoreSchema,definitions:list[CoreSchema])->DefinitionsSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return DefinitionsSchema(type='definitions',schema=schema,definitions=definitions)\n \n \nclass DefinitionReferenceSchema(TypedDict,total=False):\n type:Required[Literal['definition-ref']]\n schema_ref:Required[str]\n ref:str\n metadata:Dict[str,Any]\n serialization:SerSchema\n \n \ndef definition_reference_schema(\nschema_ref:str,\nref:str |None=None,\nmetadata:Dict[str,Any]|None=None,\nserialization:SerSchema |None=None,\n)->DefinitionReferenceSchema:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n return _dict_not_none(\n type='definition-ref',schema_ref=schema_ref,ref=ref,metadata=metadata,serialization=serialization\n )\n \n \nMYPY=False\n\n\nif not MYPY:\n CoreSchema=Union[\n InvalidSchema,\n AnySchema,\n NoneSchema,\n BoolSchema,\n IntSchema,\n FloatSchema,\n DecimalSchema,\n StringSchema,\n BytesSchema,\n DateSchema,\n TimeSchema,\n DatetimeSchema,\n TimedeltaSchema,\n LiteralSchema,\n EnumSchema,\n IsInstanceSchema,\n IsSubclassSchema,\n CallableSchema,\n ListSchema,\n TupleSchema,\n SetSchema,\n FrozenSetSchema,\n GeneratorSchema,\n DictSchema,\n AfterValidatorFunctionSchema,\n BeforeValidatorFunctionSchema,\n WrapValidatorFunctionSchema,\n PlainValidatorFunctionSchema,\n WithDefaultSchema,\n NullableSchema,\n UnionSchema,\n TaggedUnionSchema,\n ChainSchema,\n LaxOrStrictSchema,\n JsonOrPythonSchema,\n TypedDictSchema,\n ModelFieldsSchema,\n ModelSchema,\n DataclassArgsSchema,\n DataclassSchema,\n ArgumentsSchema,\n CallSchema,\n CustomErrorSchema,\n JsonSchema,\n UrlSchema,\n MultiHostUrlSchema,\n DefinitionsSchema,\n DefinitionReferenceSchema,\n UuidSchema,\n ComplexSchema,\n ]\nelif False:\n CoreSchema:TypeAlias=Mapping[str,Any]\n \n \n \nCoreSchemaType=Literal[\n'invalid',\n'any',\n'none',\n'bool',\n'int',\n'float',\n'decimal',\n'str',\n'bytes',\n'date',\n'time',\n'datetime',\n'timedelta',\n'literal',\n'enum',\n'is-instance',\n'is-subclass',\n'callable',\n'list',\n'tuple',\n'set',\n'frozenset',\n'generator',\n'dict',\n'function-after',\n'function-before',\n'function-wrap',\n'function-plain',\n'default',\n'nullable',\n'union',\n'tagged-union',\n'chain',\n'lax-or-strict',\n'json-or-python',\n'typed-dict',\n'model-fields',\n'model',\n'dataclass-args',\n'dataclass',\n'arguments',\n'call',\n'custom-error',\n'json',\n'url',\n'multi-host-url',\n'definitions',\n'definition-ref',\n'uuid',\n'complex',\n]\n\nCoreSchemaFieldType=Literal['model-field','dataclass-field','typed-dict-field','computed-field']\n\n\n\n\nErrorType=Literal[\n'no_such_attribute',\n'json_invalid',\n'json_type',\n'needs_python_object',\n'recursion_loop',\n'missing',\n'frozen_field',\n'frozen_instance',\n'extra_forbidden',\n'invalid_key',\n'get_attribute_error',\n'model_type',\n'model_attributes_type',\n'dataclass_type',\n'dataclass_exact_type',\n'none_required',\n'greater_than',\n'greater_than_equal',\n'less_than',\n'less_than_equal',\n'multiple_of',\n'finite_number',\n'too_short',\n'too_long',\n'iterable_type',\n'iteration_error',\n'string_type',\n'string_sub_type',\n'string_unicode',\n'string_too_short',\n'string_too_long',\n'string_pattern_mismatch',\n'enum',\n'dict_type',\n'mapping_type',\n'list_type',\n'tuple_type',\n'set_type',\n'bool_type',\n'bool_parsing',\n'int_type',\n'int_parsing',\n'int_parsing_size',\n'int_from_float',\n'float_type',\n'float_parsing',\n'bytes_type',\n'bytes_too_short',\n'bytes_too_long',\n'bytes_invalid_encoding',\n'value_error',\n'assertion_error',\n'literal_error',\n'date_type',\n'date_parsing',\n'date_from_datetime_parsing',\n'date_from_datetime_inexact',\n'date_past',\n'date_future',\n'time_type',\n'time_parsing',\n'datetime_type',\n'datetime_parsing',\n'datetime_object_invalid',\n'datetime_from_date_parsing',\n'datetime_past',\n'datetime_future',\n'timezone_naive',\n'timezone_aware',\n'timezone_offset',\n'time_delta_type',\n'time_delta_parsing',\n'frozen_set_type',\n'is_instance_of',\n'is_subclass_of',\n'callable_type',\n'union_tag_invalid',\n'union_tag_not_found',\n'arguments_type',\n'missing_argument',\n'unexpected_keyword_argument',\n'missing_keyword_only_argument',\n'unexpected_positional_argument',\n'missing_positional_only_argument',\n'multiple_argument_values',\n'url_type',\n'url_parsing',\n'url_syntax_violation',\n'url_too_long',\n'url_scheme',\n'uuid_type',\n'uuid_parsing',\n'uuid_version',\n'decimal_type',\n'decimal_parsing',\n'decimal_max_digits',\n'decimal_max_places',\n'decimal_whole_digits',\n'complex_type',\n'complex_str_parsing',\n]\n\n\ndef _dict_not_none(**kwargs:Any)->Any:\n return{k:v for k,v in kwargs.items()if v is not None}\n \n \n \n \n \n \n \n@deprecated('`field_before_validator_function` is deprecated, use `with_info_before_validator_function` instead.')\ndef field_before_validator_function(function:WithInfoValidatorFunction,field_name:str,schema:CoreSchema,**kwargs):\n warnings.warn(\n '`field_before_validator_function` is deprecated, use `with_info_before_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_before_validator_function(function,schema,field_name=field_name,**kwargs)\n \n \n@deprecated('`general_before_validator_function` is deprecated, use `with_info_before_validator_function` instead.')\ndef general_before_validator_function(*args,**kwargs):\n warnings.warn(\n '`general_before_validator_function` is deprecated, use `with_info_before_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_before_validator_function(*args,**kwargs)\n \n \n@deprecated('`field_after_validator_function` is deprecated, use `with_info_after_validator_function` instead.')\ndef field_after_validator_function(function:WithInfoValidatorFunction,field_name:str,schema:CoreSchema,**kwargs):\n warnings.warn(\n '`field_after_validator_function` is deprecated, use `with_info_after_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_after_validator_function(function,schema,field_name=field_name,**kwargs)\n \n \n@deprecated('`general_after_validator_function` is deprecated, use `with_info_after_validator_function` instead.')\ndef general_after_validator_function(*args,**kwargs):\n warnings.warn(\n '`general_after_validator_function` is deprecated, use `with_info_after_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_after_validator_function(*args,**kwargs)\n \n \n@deprecated('`field_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.')\ndef field_wrap_validator_function(\nfunction:WithInfoWrapValidatorFunction,field_name:str,schema:CoreSchema,**kwargs\n):\n warnings.warn(\n '`field_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_wrap_validator_function(function,schema,field_name=field_name,**kwargs)\n \n \n@deprecated('`general_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.')\ndef general_wrap_validator_function(*args,**kwargs):\n warnings.warn(\n '`general_wrap_validator_function` is deprecated, use `with_info_wrap_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_wrap_validator_function(*args,**kwargs)\n \n \n@deprecated('`field_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.')\ndef field_plain_validator_function(function:WithInfoValidatorFunction,field_name:str,**kwargs):\n warnings.warn(\n '`field_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_plain_validator_function(function,field_name=field_name,**kwargs)\n \n \n@deprecated('`general_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.')\ndef general_plain_validator_function(*args,**kwargs):\n warnings.warn(\n '`general_plain_validator_function` is deprecated, use `with_info_plain_validator_function` instead.',\n DeprecationWarning,\n )\n return with_info_plain_validator_function(*args,**kwargs)\n \n \n_deprecated_import_lookup={\n'FieldValidationInfo':ValidationInfo,\n'FieldValidatorFunction':WithInfoValidatorFunction,\n'GeneralValidatorFunction':WithInfoValidatorFunction,\n'FieldWrapValidatorFunction':WithInfoWrapValidatorFunction,\n}\n\nif TYPE_CHECKING:\n FieldValidationInfo=ValidationInfo\n \n \ndef __getattr__(attr_name:str)->object:\n new_attr=_deprecated_import_lookup.get(attr_name)\n if new_attr is None:\n  raise AttributeError(f\"module 'pydantic_core' has no attribute '{attr_name}'\")\n else:\n  import warnings\n  \n  msg=f'`{attr_name}` is deprecated, use `{new_attr.__name__}` instead.'\n  warnings.warn(msg,DeprecationWarning,stacklevel=1)\n  return new_attr\n", ["__future__", "collections.abc", "datetime", "decimal", "pydantic_core", "sys", "typing", "typing_extensions", "warnings"]], "httpx": [".py", "from.__version__ import __description__,__title__,__version__\nfrom._api import *\nfrom._auth import *\nfrom._client import *\nfrom._config import *\nfrom._content import *\nfrom._exceptions import *\nfrom._models import *\nfrom._status_codes import *\nfrom._transports import *\nfrom._types import *\nfrom._urls import *\n\ntry:\n from._main import main\nexcept ImportError:\n\n def main()->None:\n  import sys\n  \n  print(\n  \"The httpx command line client could not run because the required \"\n  \"dependencies were not installed.\\nMake sure you've installed \"\n  \"everything with: pip install 'httpx[cli]'\"\n  )\n  sys.exit(1)\n  \n  \n__all__=[\n\"__description__\",\n\"__title__\",\n\"__version__\",\n\"ASGITransport\",\n\"AsyncBaseTransport\",\n\"AsyncByteStream\",\n\"AsyncClient\",\n\"AsyncHTTPTransport\",\n\"Auth\",\n\"BaseTransport\",\n\"BasicAuth\",\n\"ByteStream\",\n\"Client\",\n\"CloseError\",\n\"codes\",\n\"ConnectError\",\n\"ConnectTimeout\",\n\"CookieConflict\",\n\"Cookies\",\n\"create_ssl_context\",\n\"DecodingError\",\n\"delete\",\n\"DigestAuth\",\n\"get\",\n\"head\",\n\"Headers\",\n\"HTTPError\",\n\"HTTPStatusError\",\n\"HTTPTransport\",\n\"InvalidURL\",\n\"Limits\",\n\"LocalProtocolError\",\n\"main\",\n\"MockTransport\",\n\"NetRCAuth\",\n\"NetworkError\",\n\"options\",\n\"patch\",\n\"PoolTimeout\",\n\"post\",\n\"ProtocolError\",\n\"Proxy\",\n\"ProxyError\",\n\"put\",\n\"QueryParams\",\n\"ReadError\",\n\"ReadTimeout\",\n\"RemoteProtocolError\",\n\"request\",\n\"Request\",\n\"RequestError\",\n\"RequestNotRead\",\n\"Response\",\n\"ResponseNotRead\",\n\"stream\",\n\"StreamClosed\",\n\"StreamConsumed\",\n\"StreamError\",\n\"SyncByteStream\",\n\"Timeout\",\n\"TimeoutException\",\n\"TooManyRedirects\",\n\"TransportError\",\n\"UnsupportedProtocol\",\n\"URL\",\n\"USE_CLIENT_DEFAULT\",\n\"WriteError\",\n\"WriteTimeout\",\n\"WSGITransport\",\n]\n\n\n__locals=locals()\nfor __name in __all__:\n if not __name.startswith(\"__\"):\n  setattr(__locals[__name],\"__module__\",\"httpx\")\n", ["httpx.__version__", "httpx._api", "httpx._auth", "httpx._client", "httpx._config", "httpx._content", "httpx._exceptions", "httpx._main", "httpx._models", "httpx._status_codes", "httpx._transports", "httpx._types", "httpx._urls", "sys"], 1], "httpx._utils": [".py", "from __future__ import annotations\n\nimport ipaddress\nimport os\nimport re\nimport typing\nfrom urllib.request import getproxies\n\nfrom._types import PrimitiveData\n\nif typing.TYPE_CHECKING:\n from._urls import URL\n \n \ndef primitive_value_to_str(value:PrimitiveData)->str:\n ''\n\n\n\n \n if value is True:\n  return \"true\"\n elif value is False:\n  return \"false\"\n elif value is None:\n  return \"\"\n return str(value)\n \n \ndef get_environment_proxies()->dict[str,str |None]:\n ''\n \n \n \n \n \n proxy_info=getproxies()\n mounts:dict[str,str |None]={}\n \n for scheme in(\"http\",\"https\",\"all\"):\n  if proxy_info.get(scheme):\n   hostname=proxy_info[scheme]\n   mounts[f\"{scheme}://\"]=(\n   hostname if \"://\"in hostname else f\"http://{hostname}\"\n   )\n   \n no_proxy_hosts=[host.strip()for host in proxy_info.get(\"no\",\"\").split(\",\")]\n for hostname in no_proxy_hosts:\n \n \n  if hostname ==\"*\":\n  \n  \n  \n  \n   return{}\n  elif hostname:\n  \n  \n  \n  \n  \n  \n  \n   if \"://\"in hostname:\n    mounts[hostname]=None\n   elif is_ipv4_hostname(hostname):\n    mounts[f\"all://{hostname}\"]=None\n   elif is_ipv6_hostname(hostname):\n    mounts[f\"all://[{hostname}]\"]=None\n   elif hostname.lower()==\"localhost\":\n    mounts[f\"all://{hostname}\"]=None\n   else:\n    mounts[f\"all://*{hostname}\"]=None\n    \n return mounts\n \n \ndef to_bytes(value:str |bytes,encoding:str=\"utf-8\")->bytes:\n return value.encode(encoding)if isinstance(value,str)else value\n \n \ndef to_str(value:str |bytes,encoding:str=\"utf-8\")->str:\n return value if isinstance(value,str)else value.decode(encoding)\n \n \ndef to_bytes_or_str(value:str,match_type_of:typing.AnyStr)->typing.AnyStr:\n return value if isinstance(match_type_of,str)else value.encode()\n \n \ndef unquote(value:str)->str:\n return value[1:-1]if value[0]==value[-1]=='\"'else value\n \n \ndef peek_filelike_length(stream:typing.Any)->int |None:\n ''\n\n\n \n try:\n \n  fd=stream.fileno()\n  \n  length=os.fstat(fd).st_size\n except(AttributeError,OSError):\n \n  try:\n  \n  \n   offset=stream.tell()\n   length=stream.seek(0,os.SEEK_END)\n   stream.seek(offset)\n  except(AttributeError,OSError):\n  \n   return None\n   \n return length\n \n \nclass URLPattern:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,pattern:str)->None:\n  from._urls import URL\n  \n  if pattern and \":\"not in pattern:\n   raise ValueError(\n   f\"Proxy keys should use proper URL forms rather \"\n   f\"than plain scheme strings. \"\n   f'Instead of \"{pattern}\", use \"{pattern}://\"'\n   )\n   \n  url=URL(pattern)\n  self.pattern=pattern\n  self.scheme=\"\"if url.scheme ==\"all\"else url.scheme\n  self.host=\"\"if url.host ==\"*\"else url.host\n  self.port=url.port\n  if not url.host or url.host ==\"*\":\n   self.host_regex:typing.Pattern[str]|None=None\n  elif url.host.startswith(\"*.\"):\n  \n   domain=re.escape(url.host[2:])\n   self.host_regex=re.compile(f\"^.+\\\\.{domain}$\")\n  elif url.host.startswith(\"*\"):\n  \n   domain=re.escape(url.host[1:])\n   self.host_regex=re.compile(f\"^(.+\\\\.)?{domain}$\")\n  else:\n  \n   domain=re.escape(url.host)\n   self.host_regex=re.compile(f\"^{domain}$\")\n   \n def matches(self,other:URL)->bool:\n  if self.scheme and self.scheme !=other.scheme:\n   return False\n  if(\n  self.host\n  and self.host_regex is not None\n  and not self.host_regex.match(other.host)\n  ):\n   return False\n  if self.port is not None and self.port !=other.port:\n   return False\n  return True\n  \n @property\n def priority(self)->tuple[int,int,int]:\n  ''\n\n\n  \n  \n  port_priority=0 if self.port is not None else 1\n  \n  host_priority=-len(self.host)\n  \n  scheme_priority=-len(self.scheme)\n  return(port_priority,host_priority,scheme_priority)\n  \n def __hash__(self)->int:\n  return hash(self.pattern)\n  \n def __lt__(self,other:URLPattern)->bool:\n  return self.priority <other.priority\n  \n def __eq__(self,other:typing.Any)->bool:\n  return isinstance(other,URLPattern)and self.pattern ==other.pattern\n  \n  \ndef is_ipv4_hostname(hostname:str)->bool:\n try:\n  ipaddress.IPv4Address(hostname.split(\"/\")[0])\n except Exception:\n  return False\n return True\n \n \ndef is_ipv6_hostname(hostname:str)->bool:\n try:\n  ipaddress.IPv6Address(hostname.split(\"/\")[0])\n except Exception:\n  return False\n return True\n", ["__future__", "httpx._types", "httpx._urls", "ipaddress", "os", "re", "typing", "urllib.request"]], "httpx._status_codes": [".py", "from __future__ import annotations\n\nfrom enum import IntEnum\n\n__all__=[\"codes\"]\n\n\nclass codes(IntEnum):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __new__(cls,value:int,phrase:str=\"\")->codes:\n  obj=int.__new__(cls,value)\n  obj._value_=value\n  \n  obj.phrase=phrase\n  return obj\n  \n def __str__(self)->str:\n  return str(self.value)\n  \n @classmethod\n def get_reason_phrase(cls,value:int)->str:\n  try:\n   return codes(value).phrase\n  except ValueError:\n   return \"\"\n   \n @classmethod\n def is_informational(cls,value:int)->bool:\n  ''\n\n  \n  return 100 <=value <=199\n  \n @classmethod\n def is_success(cls,value:int)->bool:\n  ''\n\n  \n  return 200 <=value <=299\n  \n @classmethod\n def is_redirect(cls,value:int)->bool:\n  ''\n\n  \n  return 300 <=value <=399\n  \n @classmethod\n def is_client_error(cls,value:int)->bool:\n  ''\n\n  \n  return 400 <=value <=499\n  \n @classmethod\n def is_server_error(cls,value:int)->bool:\n  ''\n\n  \n  return 500 <=value <=599\n  \n @classmethod\n def is_error(cls,value:int)->bool:\n  ''\n\n  \n  return 400 <=value <=599\n  \n  \n CONTINUE=100,\"Continue\"\n SWITCHING_PROTOCOLS=101,\"Switching Protocols\"\n PROCESSING=102,\"Processing\"\n EARLY_HINTS=103,\"Early Hints\"\n \n \n OK=200,\"OK\"\n CREATED=201,\"Created\"\n ACCEPTED=202,\"Accepted\"\n NON_AUTHORITATIVE_INFORMATION=203,\"Non-Authoritative Information\"\n NO_CONTENT=204,\"No Content\"\n RESET_CONTENT=205,\"Reset Content\"\n PARTIAL_CONTENT=206,\"Partial Content\"\n MULTI_STATUS=207,\"Multi-Status\"\n ALREADY_REPORTED=208,\"Already Reported\"\n IM_USED=226,\"IM Used\"\n \n \n MULTIPLE_CHOICES=300,\"Multiple Choices\"\n MOVED_PERMANENTLY=301,\"Moved Permanently\"\n FOUND=302,\"Found\"\n SEE_OTHER=303,\"See Other\"\n NOT_MODIFIED=304,\"Not Modified\"\n USE_PROXY=305,\"Use Proxy\"\n TEMPORARY_REDIRECT=307,\"Temporary Redirect\"\n PERMANENT_REDIRECT=308,\"Permanent Redirect\"\n \n \n BAD_REQUEST=400,\"Bad Request\"\n UNAUTHORIZED=401,\"Unauthorized\"\n PAYMENT_REQUIRED=402,\"Payment Required\"\n FORBIDDEN=403,\"Forbidden\"\n NOT_FOUND=404,\"Not Found\"\n METHOD_NOT_ALLOWED=405,\"Method Not Allowed\"\n NOT_ACCEPTABLE=406,\"Not Acceptable\"\n PROXY_AUTHENTICATION_REQUIRED=407,\"Proxy Authentication Required\"\n REQUEST_TIMEOUT=408,\"Request Timeout\"\n CONFLICT=409,\"Conflict\"\n GONE=410,\"Gone\"\n LENGTH_REQUIRED=411,\"Length Required\"\n PRECONDITION_FAILED=412,\"Precondition Failed\"\n REQUEST_ENTITY_TOO_LARGE=413,\"Request Entity Too Large\"\n REQUEST_URI_TOO_LONG=414,\"Request-URI Too Long\"\n UNSUPPORTED_MEDIA_TYPE=415,\"Unsupported Media Type\"\n REQUESTED_RANGE_NOT_SATISFIABLE=416,\"Requested Range Not Satisfiable\"\n EXPECTATION_FAILED=417,\"Expectation Failed\"\n IM_A_TEAPOT=418,\"I'm a teapot\"\n MISDIRECTED_REQUEST=421,\"Misdirected Request\"\n UNPROCESSABLE_ENTITY=422,\"Unprocessable Entity\"\n LOCKED=423,\"Locked\"\n FAILED_DEPENDENCY=424,\"Failed Dependency\"\n TOO_EARLY=425,\"Too Early\"\n UPGRADE_REQUIRED=426,\"Upgrade Required\"\n PRECONDITION_REQUIRED=428,\"Precondition Required\"\n TOO_MANY_REQUESTS=429,\"Too Many Requests\"\n REQUEST_HEADER_FIELDS_TOO_LARGE=431,\"Request Header Fields Too Large\"\n UNAVAILABLE_FOR_LEGAL_REASONS=451,\"Unavailable For Legal Reasons\"\n \n \n INTERNAL_SERVER_ERROR=500,\"Internal Server Error\"\n NOT_IMPLEMENTED=501,\"Not Implemented\"\n BAD_GATEWAY=502,\"Bad Gateway\"\n SERVICE_UNAVAILABLE=503,\"Service Unavailable\"\n GATEWAY_TIMEOUT=504,\"Gateway Timeout\"\n HTTP_VERSION_NOT_SUPPORTED=505,\"HTTP Version Not Supported\"\n VARIANT_ALSO_NEGOTIATES=506,\"Variant Also Negotiates\"\n INSUFFICIENT_STORAGE=507,\"Insufficient Storage\"\n LOOP_DETECTED=508,\"Loop Detected\"\n NOT_EXTENDED=510,\"Not Extended\"\n NETWORK_AUTHENTICATION_REQUIRED=511,\"Network Authentication Required\"\n \n \n \nfor code in codes:\n setattr(codes,code._name_.lower(),int(code))\n", ["__future__", "enum"]], "httpx._auth": [".py", "from __future__ import annotations\n\nimport hashlib\nimport os\nimport re\nimport time\nimport typing\nfrom base64 import b64encode\nfrom urllib.request import parse_http_list\n\nfrom._exceptions import ProtocolError\nfrom._models import Cookies,Request,Response\nfrom._utils import to_bytes,to_str,unquote\n\nif typing.TYPE_CHECKING:\n from hashlib import _Hash\n \n \n__all__=[\"Auth\",\"BasicAuth\",\"DigestAuth\",\"NetRCAuth\"]\n\n\nclass Auth:\n ''\n\n\n\n\n\n\n\n\n\n \n \n requires_request_body=False\n requires_response_body=False\n \n def auth_flow(self,request:Request)->typing.Generator[Request,Response,None]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  yield request\n  \n def sync_auth_flow(\n self,request:Request\n )->typing.Generator[Request,Response,None]:\n  ''\n\n\n\n\n  \n  if self.requires_request_body:\n   request.read()\n   \n  flow=self.auth_flow(request)\n  request=next(flow)\n  \n  while True:\n   response=yield request\n   if self.requires_response_body:\n    response.read()\n    \n   try:\n    request=flow.send(response)\n   except StopIteration:\n    break\n    \n async def async_auth_flow(\n self,request:Request\n )->typing.AsyncGenerator[Request,Response]:\n  ''\n\n\n\n\n  \n  if self.requires_request_body:\n   await request.aread()\n   \n  flow=self.auth_flow(request)\n  request=next(flow)\n  \n  while True:\n   response=yield request\n   if self.requires_response_body:\n    await response.aread()\n    \n   try:\n    request=flow.send(response)\n   except StopIteration:\n    break\n    \n    \nclass FunctionAuth(Auth):\n ''\n\n\n \n \n def __init__(self,func:typing.Callable[[Request],Request])->None:\n  self._func=func\n  \n def auth_flow(self,request:Request)->typing.Generator[Request,Response,None]:\n  yield self._func(request)\n  \n  \nclass BasicAuth(Auth):\n ''\n\n\n \n \n def __init__(self,username:str |bytes,password:str |bytes)->None:\n  self._auth_header=self._build_auth_header(username,password)\n  \n def auth_flow(self,request:Request)->typing.Generator[Request,Response,None]:\n  request.headers[\"Authorization\"]=self._auth_header\n  yield request\n  \n def _build_auth_header(self,username:str |bytes,password:str |bytes)->str:\n  userpass=b\":\".join((to_bytes(username),to_bytes(password)))\n  token=b64encode(userpass).decode()\n  return f\"Basic {token}\"\n  \n  \nclass NetRCAuth(Auth):\n ''\n\n \n \n def __init__(self,file:str |None=None)->None:\n \n \n  import netrc\n  \n  self._netrc_info=netrc.netrc(file)\n  \n def auth_flow(self,request:Request)->typing.Generator[Request,Response,None]:\n  auth_info=self._netrc_info.authenticators(request.url.host)\n  if auth_info is None or not auth_info[2]:\n  \n   yield request\n  else:\n  \n   request.headers[\"Authorization\"]=self._build_auth_header(\n   username=auth_info[0],password=auth_info[2]\n   )\n   yield request\n   \n def _build_auth_header(self,username:str |bytes,password:str |bytes)->str:\n  userpass=b\":\".join((to_bytes(username),to_bytes(password)))\n  token=b64encode(userpass).decode()\n  return f\"Basic {token}\"\n  \n  \nclass DigestAuth(Auth):\n _ALGORITHM_TO_HASH_FUNCTION:dict[str,typing.Callable[[bytes],_Hash]]={\n \"MD5\":hashlib.md5,\n \"MD5-SESS\":hashlib.md5,\n \"SHA\":hashlib.sha1,\n \"SHA-SESS\":hashlib.sha1,\n \"SHA-256\":hashlib.sha256,\n \"SHA-256-SESS\":hashlib.sha256,\n \"SHA-512\":hashlib.sha512,\n \"SHA-512-SESS\":hashlib.sha512,\n }\n \n def __init__(self,username:str |bytes,password:str |bytes)->None:\n  self._username=to_bytes(username)\n  self._password=to_bytes(password)\n  self._last_challenge:_DigestAuthChallenge |None=None\n  self._nonce_count=1\n  \n def auth_flow(self,request:Request)->typing.Generator[Request,Response,None]:\n  if self._last_challenge:\n   request.headers[\"Authorization\"]=self._build_auth_header(\n   request,self._last_challenge\n   )\n   \n  response=yield request\n  \n  if response.status_code !=401 or \"www-authenticate\"not in response.headers:\n  \n  \n   return\n   \n  for auth_header in response.headers.get_list(\"www-authenticate\"):\n   if auth_header.lower().startswith(\"digest \"):\n    break\n  else:\n  \n  \n   return\n   \n  self._last_challenge=self._parse_challenge(request,response,auth_header)\n  self._nonce_count=1\n  \n  request.headers[\"Authorization\"]=self._build_auth_header(\n  request,self._last_challenge\n  )\n  if response.cookies:\n   Cookies(response.cookies).set_cookie_header(request=request)\n  yield request\n  \n def _parse_challenge(\n self,request:Request,response:Response,auth_header:str\n )->_DigestAuthChallenge:\n  ''\n\n\n\n  \n  scheme,_,fields=auth_header.partition(\" \")\n  \n  \n  assert scheme.lower()==\"digest\"\n  \n  header_dict:dict[str,str]={}\n  for field in parse_http_list(fields):\n   key,value=field.strip().split(\"=\",1)\n   header_dict[key]=unquote(value)\n   \n  try:\n   realm=header_dict[\"realm\"].encode()\n   nonce=header_dict[\"nonce\"].encode()\n   algorithm=header_dict.get(\"algorithm\",\"MD5\")\n   opaque=header_dict[\"opaque\"].encode()if \"opaque\"in header_dict else None\n   qop=header_dict[\"qop\"].encode()if \"qop\"in header_dict else None\n   return _DigestAuthChallenge(\n   realm=realm,nonce=nonce,algorithm=algorithm,opaque=opaque,qop=qop\n   )\n  except KeyError as exc:\n   message=\"Malformed Digest WWW-Authenticate header\"\n   raise ProtocolError(message,request=request)from exc\n   \n def _build_auth_header(\n self,request:Request,challenge:_DigestAuthChallenge\n )->str:\n  hash_func=self._ALGORITHM_TO_HASH_FUNCTION[challenge.algorithm.upper()]\n  \n  def digest(data:bytes)->bytes:\n   return hash_func(data).hexdigest().encode()\n   \n  A1=b\":\".join((self._username,challenge.realm,self._password))\n  \n  path=request.url.raw_path\n  A2=b\":\".join((request.method.encode(),path))\n  \n  HA2=digest(A2)\n  \n  nc_value=b\"%08x\"%self._nonce_count\n  cnonce=self._get_client_nonce(self._nonce_count,challenge.nonce)\n  self._nonce_count +=1\n  \n  HA1=digest(A1)\n  if challenge.algorithm.lower().endswith(\"-sess\"):\n   HA1=digest(b\":\".join((HA1,challenge.nonce,cnonce)))\n   \n  qop=self._resolve_qop(challenge.qop,request=request)\n  if qop is None:\n  \n   digest_data=[HA1,challenge.nonce,HA2]\n  else:\n  \n   digest_data=[HA1,challenge.nonce,nc_value,cnonce,qop,HA2]\n   \n  format_args={\n  \"username\":self._username,\n  \"realm\":challenge.realm,\n  \"nonce\":challenge.nonce,\n  \"uri\":path,\n  \"response\":digest(b\":\".join(digest_data)),\n  \"algorithm\":challenge.algorithm.encode(),\n  }\n  if challenge.opaque:\n   format_args[\"opaque\"]=challenge.opaque\n  if qop:\n   format_args[\"qop\"]=b\"auth\"\n   format_args[\"nc\"]=nc_value\n   format_args[\"cnonce\"]=cnonce\n   \n  return \"Digest \"+self._get_header_value(format_args)\n  \n def _get_client_nonce(self,nonce_count:int,nonce:bytes)->bytes:\n  s=str(nonce_count).encode()\n  s +=nonce\n  s +=time.ctime().encode()\n  s +=os.urandom(8)\n  \n  return hashlib.sha1(s).hexdigest()[:16].encode()\n  \n def _get_header_value(self,header_fields:dict[str,bytes])->str:\n  NON_QUOTED_FIELDS=(\"algorithm\",\"qop\",\"nc\")\n  QUOTED_TEMPLATE='{}=\"{}\"'\n  NON_QUOTED_TEMPLATE=\"{}={}\"\n  \n  header_value=\"\"\n  for i,(field,value)in enumerate(header_fields.items()):\n   if i >0:\n    header_value +=\", \"\n   template=(\n   QUOTED_TEMPLATE\n   if field not in NON_QUOTED_FIELDS\n   else NON_QUOTED_TEMPLATE\n   )\n   header_value +=template.format(field,to_str(value))\n   \n  return header_value\n  \n def _resolve_qop(self,qop:bytes |None,request:Request)->bytes |None:\n  if qop is None:\n   return None\n  qops=re.split(b\", ?\",qop)\n  if b\"auth\"in qops:\n   return b\"auth\"\n   \n  if qops ==[b\"auth-int\"]:\n   raise NotImplementedError(\"Digest auth-int support is not yet implemented\")\n   \n  message=f'Unexpected qop value \"{qop !r}\" in digest auth'\n  raise ProtocolError(message,request=request)\n  \n  \nclass _DigestAuthChallenge(typing.NamedTuple):\n realm:bytes\n nonce:bytes\n algorithm:str\n opaque:bytes |None\n qop:bytes |None\n", ["__future__", "base64", "hashlib", "httpx._exceptions", "httpx._models", "httpx._utils", "netrc", "os", "re", "time", "typing", "urllib.request"]], "httpx._urlparse": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import annotations\n\nimport ipaddress\nimport re\nimport typing\n\nimport idna\n\nfrom._exceptions import InvalidURL\n\nMAX_URL_LENGTH=65536\n\n\nUNRESERVED_CHARACTERS=(\n\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789-._~\"\n)\nSUB_DELIMS=\"!$&'()*+,;=\"\n\nPERCENT_ENCODED_REGEX=re.compile(\"%[A-Fa-f0-9]{2}\")\n\n\n\n\n\nFRAG_SAFE=\"\".join(\n[chr(i)for i in range(0x20,0x7F)if i not in(0x20,0x22,0x3C,0x3E,0x60)]\n)\n\n\n\nQUERY_SAFE=\"\".join(\n[chr(i)for i in range(0x20,0x7F)if i not in(0x20,0x22,0x23,0x3C,0x3E)]\n)\n\n\n\nPATH_SAFE=\"\".join(\n[\nchr(i)\nfor i in range(0x20,0x7F)\nif i not in(0x20,0x22,0x23,0x3C,0x3E)+(0x3F,0x60,0x7B,0x7D)\n]\n)\n\n\n\n\nUSERNAME_SAFE=\"\".join(\n[\nchr(i)\nfor i in range(0x20,0x7F)\nif i\nnot in(0x20,0x22,0x23,0x3C,0x3E)\n+(0x3F,0x60,0x7B,0x7D)\n+(0x2F,0x3A,0x3B,0x3D,0x40,0x5B,0x5C,0x5D,0x5E,0x7C)\n]\n)\nPASSWORD_SAFE=\"\".join(\n[\nchr(i)\nfor i in range(0x20,0x7F)\nif i\nnot in(0x20,0x22,0x23,0x3C,0x3E)\n+(0x3F,0x60,0x7B,0x7D)\n+(0x2F,0x3A,0x3B,0x3D,0x40,0x5B,0x5C,0x5D,0x5E,0x7C)\n]\n)\n\n\n\nUSERINFO_SAFE=\"\".join(\n[\nchr(i)\nfor i in range(0x20,0x7F)\nif i\nnot in(0x20,0x22,0x23,0x3C,0x3E)\n+(0x3F,0x60,0x7B,0x7D)\n+(0x2F,0x3B,0x3D,0x40,0x5B,0x5C,0x5D,0x5E,0x7C)\n]\n)\n\n\n\n\n\n\n\nURL_REGEX=re.compile(\n(\nr\"(?:(?P<scheme>{scheme}):)?\"\nr\"(?://(?P<authority>{authority}))?\"\nr\"(?P<path>{path})\"\nr\"(?:\\?(?P<query>{query}))?\"\nr\"(?:#(?P<fragment>{fragment}))?\"\n).format(\nscheme=\"([a-zA-Z][a-zA-Z0-9+.-]*)?\",\nauthority=\"[^/?#]*\",\npath=\"[^?#]*\",\nquery=\"[^#]*\",\nfragment=\".*\",\n)\n)\n\n\n\n\nAUTHORITY_REGEX=re.compile(\n(\nr\"(?:(?P<userinfo>{userinfo})@)?\"r\"(?P<host>{host})\"r\":?(?P<port>{port})?\"\n).format(\nuserinfo=\".*\",\nhost=\"(\\\\[.*\\\\]|[^:@]*)\",\n\nport=\".*\",\n)\n)\n\n\n\n\n\nCOMPONENT_REGEX={\n\"scheme\":re.compile(\"([a-zA-Z][a-zA-Z0-9+.-]*)?\"),\n\"authority\":re.compile(\"[^/?#]*\"),\n\"path\":re.compile(\"[^?#]*\"),\n\"query\":re.compile(\"[^#]*\"),\n\"fragment\":re.compile(\".*\"),\n\"userinfo\":re.compile(\"[^@]*\"),\n\"host\":re.compile(\"(\\\\[.*\\\\]|[^:]*)\"),\n\"port\":re.compile(\".*\"),\n}\n\n\n\n\nIPv4_STYLE_HOSTNAME=re.compile(r\"^[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+$\")\nIPv6_STYLE_HOSTNAME=re.compile(r\"^\\[.*\\]$\")\n\n\nclass ParseResult(typing.NamedTuple):\n scheme:str\n userinfo:str\n host:str\n port:int |None\n path:str\n query:str |None\n fragment:str |None\n \n @property\n def authority(self)->str:\n  return \"\".join(\n  [\n  f\"{self.userinfo}@\"if self.userinfo else \"\",\n  f\"[{self.host}]\"if \":\"in self.host else self.host,\n  f\":{self.port}\"if self.port is not None else \"\",\n  ]\n  )\n  \n @property\n def netloc(self)->str:\n  return \"\".join(\n  [\n  f\"[{self.host}]\"if \":\"in self.host else self.host,\n  f\":{self.port}\"if self.port is not None else \"\",\n  ]\n  )\n  \n def copy_with(self,**kwargs:str |None)->ParseResult:\n  if not kwargs:\n   return self\n   \n  defaults={\n  \"scheme\":self.scheme,\n  \"authority\":self.authority,\n  \"path\":self.path,\n  \"query\":self.query,\n  \"fragment\":self.fragment,\n  }\n  defaults.update(kwargs)\n  return urlparse(\"\",**defaults)\n  \n def __str__(self)->str:\n  authority=self.authority\n  return \"\".join(\n  [\n  f\"{self.scheme}:\"if self.scheme else \"\",\n  f\"//{authority}\"if authority else \"\",\n  self.path,\n  f\"?{self.query}\"if self.query is not None else \"\",\n  f\"#{self.fragment}\"if self.fragment is not None else \"\",\n  ]\n  )\n  \n  \ndef urlparse(url:str=\"\",**kwargs:str |None)->ParseResult:\n\n\n\n\n if len(url)>MAX_URL_LENGTH:\n  raise InvalidURL(\"URL too long\")\n  \n  \n  \n if any(char.isascii()and not char.isprintable()for char in url):\n  char=next(char for char in url if char.isascii()and not char.isprintable())\n  idx=url.find(char)\n  error=(\n  f\"Invalid non-printable ASCII character in URL, {char !r} at position {idx}.\"\n  )\n  raise InvalidURL(error)\n  \n  \n  \n  \n  \n if \"port\"in kwargs:\n  port=kwargs[\"port\"]\n  kwargs[\"port\"]=str(port)if isinstance(port,int)else port\n  \n  \n if \"netloc\"in kwargs:\n  netloc=kwargs.pop(\"netloc\")or \"\"\n  kwargs[\"host\"],_,kwargs[\"port\"]=netloc.partition(\":\")\n  \n  \n if \"username\"in kwargs or \"password\"in kwargs:\n  username=quote(kwargs.pop(\"username\",\"\")or \"\",safe=USERNAME_SAFE)\n  password=quote(kwargs.pop(\"password\",\"\")or \"\",safe=PASSWORD_SAFE)\n  kwargs[\"userinfo\"]=f\"{username}:{password}\"if password else username\n  \n  \n if \"raw_path\"in kwargs:\n  raw_path=kwargs.pop(\"raw_path\")or \"\"\n  kwargs[\"path\"],seperator,kwargs[\"query\"]=raw_path.partition(\"?\")\n  if not seperator:\n   kwargs[\"query\"]=None\n   \n   \n if \"host\"in kwargs:\n  host=kwargs.get(\"host\")or \"\"\n  if \":\"in host and not(host.startswith(\"[\")and host.endswith(\"]\")):\n   kwargs[\"host\"]=f\"[{host}]\"\n   \n   \n   \n   \n for key,value in kwargs.items():\n  if value is not None:\n   if len(value)>MAX_URL_LENGTH:\n    raise InvalidURL(f\"URL component '{key}' too long\")\n    \n    \n    \n   if any(char.isascii()and not char.isprintable()for char in value):\n    char=next(\n    char for char in value if char.isascii()and not char.isprintable()\n    )\n    idx=value.find(char)\n    error=(\n    f\"Invalid non-printable ASCII character in URL {key} component, \"\n    f\"{char !r} at position {idx}.\"\n    )\n    raise InvalidURL(error)\n    \n    \n   if not COMPONENT_REGEX[key].fullmatch(value):\n    raise InvalidURL(f\"Invalid URL component '{key}'\")\n    \n    \n url_match=URL_REGEX.match(url)\n assert url_match is not None\n url_dict=url_match.groupdict()\n \n \n \n \n \n \n scheme=kwargs.get(\"scheme\",url_dict[\"scheme\"])or \"\"\n authority=kwargs.get(\"authority\",url_dict[\"authority\"])or \"\"\n path=kwargs.get(\"path\",url_dict[\"path\"])or \"\"\n query=kwargs.get(\"query\",url_dict[\"query\"])\n frag=kwargs.get(\"fragment\",url_dict[\"fragment\"])\n \n \n authority_match=AUTHORITY_REGEX.match(authority)\n assert authority_match is not None\n authority_dict=authority_match.groupdict()\n \n \n \n userinfo=kwargs.get(\"userinfo\",authority_dict[\"userinfo\"])or \"\"\n host=kwargs.get(\"host\",authority_dict[\"host\"])or \"\"\n port=kwargs.get(\"port\",authority_dict[\"port\"])\n \n \n \n \n parsed_scheme:str=scheme.lower()\n parsed_userinfo:str=quote(userinfo,safe=USERINFO_SAFE)\n parsed_host:str=encode_host(host)\n parsed_port:int |None=normalize_port(port,scheme)\n \n has_scheme=parsed_scheme !=\"\"\n has_authority=(\n parsed_userinfo !=\"\"or parsed_host !=\"\"or parsed_port is not None\n )\n validate_path(path,has_scheme=has_scheme,has_authority=has_authority)\n if has_scheme or has_authority:\n  path=normalize_path(path)\n  \n parsed_path:str=quote(path,safe=PATH_SAFE)\n parsed_query:str |None=None if query is None else quote(query,safe=QUERY_SAFE)\n parsed_frag:str |None=None if frag is None else quote(frag,safe=FRAG_SAFE)\n \n \n \n return ParseResult(\n parsed_scheme,\n parsed_userinfo,\n parsed_host,\n parsed_port,\n parsed_path,\n parsed_query,\n parsed_frag,\n )\n \n \ndef encode_host(host:str)->str:\n if not host:\n  return \"\"\n  \n elif IPv4_STYLE_HOSTNAME.match(host):\n \n \n \n \n \n  try:\n   ipaddress.IPv4Address(host)\n  except ipaddress.AddressValueError:\n   raise InvalidURL(f\"Invalid IPv4 address: {host !r}\")\n  return host\n  \n elif IPv6_STYLE_HOSTNAME.match(host):\n \n \n \n \n \n \n \n \n  try:\n   ipaddress.IPv6Address(host[1:-1])\n  except ipaddress.AddressValueError:\n   raise InvalidURL(f\"Invalid IPv6 address: {host !r}\")\n  return host[1:-1]\n  \n elif host.isascii():\n \n \n \n \n \n  WHATWG_SAFE='\"`{}%|\\\\'\n  return quote(host.lower(),safe=SUB_DELIMS+WHATWG_SAFE)\n  \n  \n try:\n  return idna.encode(host.lower()).decode(\"ascii\")\n except idna.IDNAError:\n  raise InvalidURL(f\"Invalid IDNA hostname: {host !r}\")\n  \n  \ndef normalize_port(port:str |int |None,scheme:str)->int |None:\n\n\n\n\n\n\n\n\n\n if port is None or port ==\"\":\n  return None\n  \n try:\n  port_as_int=int(port)\n except ValueError:\n  raise InvalidURL(f\"Invalid port: {port !r}\")\n  \n  \n default_port={\"ftp\":21,\"http\":80,\"https\":443,\"ws\":80,\"wss\":443}.get(\n scheme\n )\n if port_as_int ==default_port:\n  return None\n return port_as_int\n \n \ndef validate_path(path:str,has_scheme:bool,has_authority:bool)->None:\n ''\n\n\n\n\n \n if has_authority:\n \n \n  if path and not path.startswith(\"/\"):\n   raise InvalidURL(\"For absolute URLs, path must be empty or begin with '/'\")\n   \n if not has_scheme and not has_authority:\n \n \n  if path.startswith(\"//\"):\n   raise InvalidURL(\"Relative URLs cannot have a path starting with '//'\")\n   \n   \n   \n  if path.startswith(\":\"):\n   raise InvalidURL(\"Relative URLs cannot have a path starting with ':'\")\n   \n   \ndef normalize_path(path:str)->str:\n ''\n\n\n\n\n\n \n \n if \".\"not in path:\n  return path\n  \n components=path.split(\"/\")\n \n \n if \".\"not in components and \"..\"not in components:\n  return path\n  \n  \n output:list[str]=[]\n for component in components:\n  if component ==\".\":\n   pass\n  elif component ==\"..\":\n   if output and output !=[\"\"]:\n    output.pop()\n  else:\n   output.append(component)\n return \"/\".join(output)\n \n \ndef PERCENT(string:str)->str:\n return \"\".join([f\"%{byte:02X}\"for byte in string.encode(\"utf-8\")])\n \n \ndef percent_encoded(string:str,safe:str)->str:\n ''\n\n \n NON_ESCAPED_CHARS=UNRESERVED_CHARACTERS+safe\n \n \n if not string.rstrip(NON_ESCAPED_CHARS):\n  return string\n  \n return \"\".join(\n [char if char in NON_ESCAPED_CHARS else PERCENT(char)for char in string]\n )\n \n \ndef quote(string:str,safe:str)->str:\n ''\n\n\n\n\n\n\n\n\n \n parts=[]\n current_position=0\n for match in re.finditer(PERCENT_ENCODED_REGEX,string):\n  start_position,end_position=match.start(),match.end()\n  matched_text=match.group(0)\n  \n  if start_position !=current_position:\n   leading_text=string[current_position:start_position]\n   parts.append(percent_encoded(leading_text,safe=safe))\n   \n   \n  parts.append(matched_text)\n  current_position=end_position\n  \n  \n if current_position !=len(string):\n  trailing_text=string[current_position:]\n  parts.append(percent_encoded(trailing_text,safe=safe))\n  \n return \"\".join(parts)\n", ["__future__", "httpx._exceptions", "idna", "ipaddress", "re", "typing"]], "httpx._exceptions": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import annotations\n\nimport contextlib\nimport typing\n\nif typing.TYPE_CHECKING:\n from._models import Request,Response\n \n__all__=[\n\"CloseError\",\n\"ConnectError\",\n\"ConnectTimeout\",\n\"CookieConflict\",\n\"DecodingError\",\n\"HTTPError\",\n\"HTTPStatusError\",\n\"InvalidURL\",\n\"LocalProtocolError\",\n\"NetworkError\",\n\"PoolTimeout\",\n\"ProtocolError\",\n\"ProxyError\",\n\"ReadError\",\n\"ReadTimeout\",\n\"RemoteProtocolError\",\n\"RequestError\",\n\"RequestNotRead\",\n\"ResponseNotRead\",\n\"StreamClosed\",\n\"StreamConsumed\",\n\"StreamError\",\n\"TimeoutException\",\n\"TooManyRedirects\",\n\"TransportError\",\n\"UnsupportedProtocol\",\n\"WriteError\",\n\"WriteTimeout\",\n]\n\n\nclass HTTPError(Exception):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message)\n  self._request:Request |None=None\n  \n @property\n def request(self)->Request:\n  if self._request is None:\n   raise RuntimeError(\"The .request property has not been set.\")\n  return self._request\n  \n @request.setter\n def request(self,request:Request)->None:\n  self._request=request\n  \n  \nclass RequestError(HTTPError):\n ''\n\n \n \n def __init__(self,message:str,*,request:Request |None=None)->None:\n  super().__init__(message)\n  \n  \n  \n  \n  \n  \n  self._request=request\n  \n  \nclass TransportError(RequestError):\n ''\n\n \n \n \n \n \n \nclass TimeoutException(TransportError):\n ''\n\n\n\n \n \n \nclass ConnectTimeout(TimeoutException):\n ''\n\n \n \n \nclass ReadTimeout(TimeoutException):\n ''\n\n \n \n \nclass WriteTimeout(TimeoutException):\n ''\n\n \n \n \nclass PoolTimeout(TimeoutException):\n ''\n\n \n \n \n \n \n \nclass NetworkError(TransportError):\n ''\n\n\n\n \n \n \nclass ReadError(NetworkError):\n ''\n\n \n \n \nclass WriteError(NetworkError):\n ''\n\n \n \n \nclass ConnectError(NetworkError):\n ''\n\n \n \n \nclass CloseError(NetworkError):\n ''\n\n \n \n \n \n \n \nclass ProxyError(TransportError):\n ''\n\n \n \n \nclass UnsupportedProtocol(TransportError):\n ''\n\n\n\n \n \n \nclass ProtocolError(TransportError):\n ''\n\n \n \n \nclass LocalProtocolError(ProtocolError):\n ''\n\n\n\n\n\n \n \n \nclass RemoteProtocolError(ProtocolError):\n ''\n\n\n\n \n \n \n \n \n \nclass DecodingError(RequestError):\n ''\n\n \n \n \nclass TooManyRedirects(RequestError):\n ''\n\n \n \n \n \n \n \nclass HTTPStatusError(HTTPError):\n ''\n\n\n\n \n \n def __init__(self,message:str,*,request:Request,response:Response)->None:\n  super().__init__(message)\n  self.request=request\n  self.response=response\n  \n  \nclass InvalidURL(Exception):\n ''\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message)\n  \n  \nclass CookieConflict(Exception):\n ''\n\n\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message)\n  \n  \n  \n  \n  \n  \n  \n  \nclass StreamError(RuntimeError):\n ''\n\n\n\n\n \n \n def __init__(self,message:str)->None:\n  super().__init__(message)\n  \n  \nclass StreamConsumed(StreamError):\n ''\n\n\n \n \n def __init__(self)->None:\n  message=(\n  \"Attempted to read or stream some content, but the content has \"\n  \"already been streamed. For requests, this could be due to passing \"\n  \"a generator as request content, and then receiving a redirect \"\n  \"response or a secondary request as part of an authentication flow.\"\n  \"For responses, this could be due to attempting to stream the response \"\n  \"content more than once.\"\n  )\n  super().__init__(message)\n  \n  \nclass StreamClosed(StreamError):\n ''\n\n\n \n \n def __init__(self)->None:\n  message=(\n  \"Attempted to read or stream content, but the stream has \"\"been closed.\"\n  )\n  super().__init__(message)\n  \n  \nclass ResponseNotRead(StreamError):\n ''\n\n \n \n def __init__(self)->None:\n  message=(\n  \"Attempted to access streaming response content,\"\n  \" without having called `read()`.\"\n  )\n  super().__init__(message)\n  \n  \nclass RequestNotRead(StreamError):\n ''\n\n \n \n def __init__(self)->None:\n  message=(\n  \"Attempted to access streaming request content,\"\n  \" without having called `read()`.\"\n  )\n  super().__init__(message)\n  \n  \n@contextlib.contextmanager\ndef request_context(\nrequest:Request |None=None,\n)->typing.Iterator[None]:\n ''\n\n\n \n try:\n  yield\n except RequestError as exc:\n  if request is not None:\n   exc.request=request\n  raise exc\n", ["__future__", "contextlib", "httpx._models", "typing"]], "httpx._config": [".py", "from __future__ import annotations\n\nimport os\nimport typing\n\nfrom._models import Headers\nfrom._types import CertTypes,HeaderTypes,TimeoutTypes\nfrom._urls import URL\n\nif typing.TYPE_CHECKING:\n import ssl\n \n__all__=[\"Limits\",\"Proxy\",\"Timeout\",\"create_ssl_context\"]\n\n\nclass UnsetType:\n pass\n \n \nUNSET=UnsetType()\n\n\ndef create_ssl_context(\nverify:ssl.SSLContext |str |bool=True,\ncert:CertTypes |None=None,\ntrust_env:bool=True,\n)->ssl.SSLContext:\n import ssl\n import warnings\n \n import certifi\n \n if verify is True:\n  if trust_env and os.environ.get(\"SSL_CERT_FILE\"):\n   ctx=ssl.create_default_context(cafile=os.environ[\"SSL_CERT_FILE\"])\n  elif trust_env and os.environ.get(\"SSL_CERT_DIR\"):\n   ctx=ssl.create_default_context(capath=os.environ[\"SSL_CERT_DIR\"])\n  else:\n  \n   ctx=ssl.create_default_context(cafile=certifi.where())\n elif verify is False:\n  ctx=ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n  ctx.check_hostname=False\n  ctx.verify_mode=ssl.CERT_NONE\n elif isinstance(verify,str):\n  message=(\n  \"`verify=<str>` is deprecated. \"\n  \"Use `verify=ssl.create_default_context(cafile=...)` \"\n  \"or `verify=ssl.create_default_context(capath=...)` instead.\"\n  )\n  warnings.warn(message,DeprecationWarning)\n  if os.path.isdir(verify):\n   return ssl.create_default_context(capath=verify)\n  return ssl.create_default_context(cafile=verify)\n else:\n  ctx=verify\n  \n if cert:\n  message=(\n  \"`cert=...` is deprecated. Use `verify=<ssl_context>` instead,\"\n  \"with `.load_cert_chain()` to configure the certificate chain.\"\n  )\n  warnings.warn(message,DeprecationWarning)\n  if isinstance(cert,str):\n   ctx.load_cert_chain(cert)\n  else:\n   ctx.load_cert_chain(*cert)\n   \n return ctx\n \n \nclass Timeout:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n timeout:TimeoutTypes |UnsetType=UNSET,\n *,\n connect:None |float |UnsetType=UNSET,\n read:None |float |UnsetType=UNSET,\n write:None |float |UnsetType=UNSET,\n pool:None |float |UnsetType=UNSET,\n )->None:\n  if isinstance(timeout,Timeout):\n  \n   assert connect is UNSET\n   assert read is UNSET\n   assert write is UNSET\n   assert pool is UNSET\n   self.connect=timeout.connect\n   self.read=timeout.read\n   self.write=timeout.write\n   self.pool=timeout.pool\n  elif isinstance(timeout,tuple):\n  \n   self.connect=timeout[0]\n   self.read=timeout[1]\n   self.write=None if len(timeout)<3 else timeout[2]\n   self.pool=None if len(timeout)<4 else timeout[3]\n  elif not(\n  isinstance(connect,UnsetType)\n  or isinstance(read,UnsetType)\n  or isinstance(write,UnsetType)\n  or isinstance(pool,UnsetType)\n  ):\n   self.connect=connect\n   self.read=read\n   self.write=write\n   self.pool=pool\n  else:\n   if isinstance(timeout,UnsetType):\n    raise ValueError(\n    \"httpx.Timeout must either include a default, or set all \"\n    \"four parameters explicitly.\"\n    )\n   self.connect=timeout if isinstance(connect,UnsetType)else connect\n   self.read=timeout if isinstance(read,UnsetType)else read\n   self.write=timeout if isinstance(write,UnsetType)else write\n   self.pool=timeout if isinstance(pool,UnsetType)else pool\n   \n def as_dict(self)->dict[str,float |None]:\n  return{\n  \"connect\":self.connect,\n  \"read\":self.read,\n  \"write\":self.write,\n  \"pool\":self.pool,\n  }\n  \n def __eq__(self,other:typing.Any)->bool:\n  return(\n  isinstance(other,self.__class__)\n  and self.connect ==other.connect\n  and self.read ==other.read\n  and self.write ==other.write\n  and self.pool ==other.pool\n  )\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  if len({self.connect,self.read,self.write,self.pool})==1:\n   return f\"{class_name}(timeout={self.connect})\"\n  return(\n  f\"{class_name}(connect={self.connect}, \"\n  f\"read={self.read}, write={self.write}, pool={self.pool})\"\n  )\n  \n  \nclass Limits:\n ''\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n *,\n max_connections:int |None=None,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=5.0,\n )->None:\n  self.max_connections=max_connections\n  self.max_keepalive_connections=max_keepalive_connections\n  self.keepalive_expiry=keepalive_expiry\n  \n def __eq__(self,other:typing.Any)->bool:\n  return(\n  isinstance(other,self.__class__)\n  and self.max_connections ==other.max_connections\n  and self.max_keepalive_connections ==other.max_keepalive_connections\n  and self.keepalive_expiry ==other.keepalive_expiry\n  )\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  return(\n  f\"{class_name}(max_connections={self.max_connections}, \"\n  f\"max_keepalive_connections={self.max_keepalive_connections}, \"\n  f\"keepalive_expiry={self.keepalive_expiry})\"\n  )\n  \n  \nclass Proxy:\n def __init__(\n self,\n url:URL |str,\n *,\n ssl_context:ssl.SSLContext |None=None,\n auth:tuple[str,str]|None=None,\n headers:HeaderTypes |None=None,\n )->None:\n  url=URL(url)\n  headers=Headers(headers)\n  \n  if url.scheme not in(\"http\",\"https\",\"socks5\",\"socks5h\"):\n   raise ValueError(f\"Unknown scheme for proxy URL {url !r}\")\n   \n  if url.username or url.password:\n  \n   auth=(url.username,url.password)\n   url=url.copy_with(username=None,password=None)\n   \n  self.url=url\n  self.auth=auth\n  self.headers=headers\n  self.ssl_context=ssl_context\n  \n @property\n def raw_auth(self)->tuple[bytes,bytes]|None:\n \n  return(\n  None\n  if self.auth is None\n  else(self.auth[0].encode(\"utf-8\"),self.auth[1].encode(\"utf-8\"))\n  )\n  \n def __repr__(self)->str:\n \n  auth=(self.auth[0],\"********\")if self.auth else None\n  \n  \n  url_str=f\"{str(self.url)!r}\"\n  auth_str=f\", auth={auth !r}\"if auth else \"\"\n  headers_str=f\", headers={dict(self.headers)!r}\"if self.headers else \"\"\n  return f\"Proxy({url_str}{auth_str}{headers_str})\"\n  \n  \nDEFAULT_TIMEOUT_CONFIG=Timeout(timeout=5.0)\nDEFAULT_LIMITS=Limits(max_connections=100,max_keepalive_connections=20)\nDEFAULT_MAX_REDIRECTS=20\n", ["__future__", "certifi", "httpx._models", "httpx._types", "httpx._urls", "os", "ssl", "typing", "warnings"]], "httpx._main": [".py", "from __future__ import annotations\n\nimport functools\nimport json\nimport sys\nimport typing\n\nimport click\nimport pygments.lexers\nimport pygments.util\nimport rich.console\nimport rich.markup\nimport rich.progress\nimport rich.syntax\nimport rich.table\n\nfrom._client import Client\nfrom._exceptions import RequestError\nfrom._models import Response\nfrom._status_codes import codes\n\nif typing.TYPE_CHECKING:\n import httpcore\n \n \ndef print_help()->None:\n console=rich.console.Console()\n \n console.print(\"[bold]HTTPX :butterfly:\",justify=\"center\")\n console.print()\n console.print(\"A next generation HTTP client.\",justify=\"center\")\n console.print()\n console.print(\n \"Usage: [bold]httpx[/bold] [cyan]<URL> [OPTIONS][/cyan] \",justify=\"left\"\n )\n console.print()\n \n table=rich.table.Table.grid(padding=1,pad_edge=True)\n table.add_column(\"Parameter\",no_wrap=True,justify=\"left\",style=\"bold\")\n table.add_column(\"Description\")\n table.add_row(\n \"-m, --method [cyan]METHOD\",\n \"Request method, such as GET, POST, PUT, PATCH, DELETE, OPTIONS, HEAD.\\n\"\n \"[Default: GET, or POST if a request body is included]\",\n )\n table.add_row(\n \"-p, --params [cyan]<NAME VALUE> ...\",\n \"Query parameters to include in the request URL.\",\n )\n table.add_row(\n \"-c, --content [cyan]TEXT\",\"Byte content to include in the request body.\"\n )\n table.add_row(\n \"-d, --data [cyan]<NAME VALUE> ...\",\"Form data to include in the request body.\"\n )\n table.add_row(\n \"-f, --files [cyan]<NAME FILENAME> ...\",\n \"Form files to include in the request body.\",\n )\n table.add_row(\"-j, --json [cyan]TEXT\",\"JSON data to include in the request body.\")\n table.add_row(\n \"-h, --headers [cyan]<NAME VALUE> ...\",\n \"Include additional HTTP headers in the request.\",\n )\n table.add_row(\n \"--cookies [cyan]<NAME VALUE> ...\",\"Cookies to include in the request.\"\n )\n table.add_row(\n \"--auth [cyan]<USER PASS>\",\n \"Username and password to include in the request. Specify '-' for the password\"\n \" to use a password prompt. Note that using --verbose/-v will expose\"\n \" the Authorization header, including the password encoding\"\n \" in a trivially reversible format.\",\n )\n \n table.add_row(\n \"--proxy [cyan]URL\",\n \"Send the request via a proxy. Should be the URL giving the proxy address.\",\n )\n \n table.add_row(\n \"--timeout [cyan]FLOAT\",\n \"Timeout value to use for network operations, such as establishing the\"\n \" connection, reading some data, etc... [Default: 5.0]\",\n )\n \n table.add_row(\"--follow-redirects\",\"Automatically follow redirects.\")\n table.add_row(\"--no-verify\",\"Disable SSL verification.\")\n table.add_row(\n \"--http2\",\"Send the request using HTTP/2, if the remote server supports it.\"\n )\n \n table.add_row(\n \"--download [cyan]FILE\",\n \"Save the response content as a file, rather than displaying it.\",\n )\n \n table.add_row(\"-v, --verbose\",\"Verbose output. Show request as well as response.\")\n table.add_row(\"--help\",\"Show this message and exit.\")\n console.print(table)\n \n \ndef get_lexer_for_response(response:Response)->str:\n content_type=response.headers.get(\"Content-Type\")\n if content_type is not None:\n  mime_type,_,_=content_type.partition(\";\")\n  try:\n   return typing.cast(\n   str,pygments.lexers.get_lexer_for_mimetype(mime_type.strip()).name\n   )\n  except pygments.util.ClassNotFound:\n   pass\n return \"\"\n \n \ndef format_request_headers(request:httpcore.Request,http2:bool=False)->str:\n version=\"HTTP/2\"if http2 else \"HTTP/1.1\"\n headers=[\n (name.lower()if http2 else name,value)for name,value in request.headers\n ]\n method=request.method.decode(\"ascii\")\n target=request.url.target.decode(\"ascii\")\n lines=[f\"{method} {target} {version}\"]+[\n f\"{name.decode('ascii')}: {value.decode('ascii')}\"for name,value in headers\n ]\n return \"\\n\".join(lines)\n \n \ndef format_response_headers(\nhttp_version:bytes,\nstatus:int,\nreason_phrase:bytes |None,\nheaders:list[tuple[bytes,bytes]],\n)->str:\n version=http_version.decode(\"ascii\")\n reason=(\n codes.get_reason_phrase(status)\n if reason_phrase is None\n else reason_phrase.decode(\"ascii\")\n )\n lines=[f\"{version} {status} {reason}\"]+[\n f\"{name.decode('ascii')}: {value.decode('ascii')}\"for name,value in headers\n ]\n return \"\\n\".join(lines)\n \n \ndef print_request_headers(request:httpcore.Request,http2:bool=False)->None:\n console=rich.console.Console()\n http_text=format_request_headers(request,http2=http2)\n syntax=rich.syntax.Syntax(http_text,\"http\",theme=\"ansi_dark\",word_wrap=True)\n console.print(syntax)\n syntax=rich.syntax.Syntax(\"\",\"http\",theme=\"ansi_dark\",word_wrap=True)\n console.print(syntax)\n \n \ndef print_response_headers(\nhttp_version:bytes,\nstatus:int,\nreason_phrase:bytes |None,\nheaders:list[tuple[bytes,bytes]],\n)->None:\n console=rich.console.Console()\n http_text=format_response_headers(http_version,status,reason_phrase,headers)\n syntax=rich.syntax.Syntax(http_text,\"http\",theme=\"ansi_dark\",word_wrap=True)\n console.print(syntax)\n syntax=rich.syntax.Syntax(\"\",\"http\",theme=\"ansi_dark\",word_wrap=True)\n console.print(syntax)\n \n \ndef print_response(response:Response)->None:\n console=rich.console.Console()\n lexer_name=get_lexer_for_response(response)\n if lexer_name:\n  if lexer_name.lower()==\"json\":\n   try:\n    data=response.json()\n    text=json.dumps(data,indent=4)\n   except ValueError:\n    text=response.text\n  else:\n   text=response.text\n   \n  syntax=rich.syntax.Syntax(text,lexer_name,theme=\"ansi_dark\",word_wrap=True)\n  console.print(syntax)\n else:\n  console.print(f\"<{len(response.content)} bytes of binary data>\")\n  \n  \n_PCTRTT=typing.Tuple[typing.Tuple[str,str],...]\n_PCTRTTT=typing.Tuple[_PCTRTT,...]\n_PeerCertRetDictType=typing.Dict[str,typing.Union[str,_PCTRTTT,_PCTRTT]]\n\n\ndef format_certificate(cert:_PeerCertRetDictType)->str:\n lines=[]\n for key,value in cert.items():\n  if isinstance(value,(list,tuple)):\n   lines.append(f\"*   {key}:\")\n   for item in value:\n    if key in(\"subject\",\"issuer\"):\n     for sub_item in item:\n      lines.append(f\"*     {sub_item[0]}: {sub_item[1]!r}\")\n    elif isinstance(item,tuple)and len(item)==2:\n     lines.append(f\"*     {item[0]}: {item[1]!r}\")\n    else:\n     lines.append(f\"*     {item !r}\")\n  else:\n   lines.append(f\"*   {key}: {value !r}\")\n return \"\\n\".join(lines)\n \n \ndef trace(\nname:str,info:typing.Mapping[str,typing.Any],verbose:bool=False\n)->None:\n console=rich.console.Console()\n if name ==\"connection.connect_tcp.started\"and verbose:\n  host=info[\"host\"]\n  console.print(f\"* Connecting to {host !r}\")\n elif name ==\"connection.connect_tcp.complete\"and verbose:\n  stream=info[\"return_value\"]\n  server_addr=stream.get_extra_info(\"server_addr\")\n  console.print(f\"* Connected to {server_addr[0]!r} on port {server_addr[1]}\")\n elif name ==\"connection.start_tls.complete\"and verbose:\n  stream=info[\"return_value\"]\n  ssl_object=stream.get_extra_info(\"ssl_object\")\n  version=ssl_object.version()\n  cipher=ssl_object.cipher()\n  server_cert=ssl_object.getpeercert()\n  alpn=ssl_object.selected_alpn_protocol()\n  console.print(f\"* SSL established using {version !r} / {cipher[0]!r}\")\n  console.print(f\"* Selected ALPN protocol: {alpn !r}\")\n  if server_cert:\n   console.print(\"* Server certificate:\")\n   console.print(format_certificate(server_cert))\n elif name ==\"http11.send_request_headers.started\"and verbose:\n  request=info[\"request\"]\n  print_request_headers(request,http2=False)\n elif name ==\"http2.send_request_headers.started\"and verbose:\n  request=info[\"request\"]\n  print_request_headers(request,http2=True)\n elif name ==\"http11.receive_response_headers.complete\":\n  http_version,status,reason_phrase,headers=info[\"return_value\"]\n  print_response_headers(http_version,status,reason_phrase,headers)\n elif name ==\"http2.receive_response_headers.complete\":\n  status,headers=info[\"return_value\"]\n  http_version=b\"HTTP/2\"\n  reason_phrase=None\n  print_response_headers(http_version,status,reason_phrase,headers)\n  \n  \ndef download_response(response:Response,download:typing.BinaryIO)->None:\n console=rich.console.Console()\n console.print()\n content_length=response.headers.get(\"Content-Length\")\n with rich.progress.Progress(\n \"[progress.description]{task.description}\",\n \"[progress.percentage]{task.percentage:>3.0f}%\",\n rich.progress.BarColumn(bar_width=None),\n rich.progress.DownloadColumn(),\n rich.progress.TransferSpeedColumn(),\n )as progress:\n  description=f\"Downloading [bold]{rich.markup.escape(download.name)}\"\n  download_task=progress.add_task(\n  description,\n  total=int(content_length or 0),\n  start=content_length is not None,\n  )\n  for chunk in response.iter_bytes():\n   download.write(chunk)\n   progress.update(download_task,completed=response.num_bytes_downloaded)\n   \n   \ndef validate_json(\nctx:click.Context,\nparam:click.Option |click.Parameter,\nvalue:typing.Any,\n)->typing.Any:\n if value is None:\n  return None\n  \n try:\n  return json.loads(value)\n except json.JSONDecodeError:\n  raise click.BadParameter(\"Not valid JSON\")\n  \n  \ndef validate_auth(\nctx:click.Context,\nparam:click.Option |click.Parameter,\nvalue:typing.Any,\n)->typing.Any:\n if value ==(None,None):\n  return None\n  \n username,password=value\n if password ==\"-\":\n  password=click.prompt(\"Password\",hide_input=True)\n return(username,password)\n \n \ndef handle_help(\nctx:click.Context,\nparam:click.Option |click.Parameter,\nvalue:typing.Any,\n)->None:\n if not value or ctx.resilient_parsing:\n  return\n  \n print_help()\n ctx.exit()\n \n \n@click.command(add_help_option=False)\n@click.argument(\"url\",type=str)\n@click.option(\n\"--method\",\n\"-m\",\n\"method\",\ntype=str,\nhelp=(\n\"Request method, such as GET, POST, PUT, PATCH, DELETE, OPTIONS, HEAD. \"\n\"[Default: GET, or POST if a request body is included]\"\n),\n)\n@click.option(\n\"--params\",\n\"-p\",\n\"params\",\ntype=(str,str),\nmultiple=True,\nhelp=\"Query parameters to include in the request URL.\",\n)\n@click.option(\n\"--content\",\n\"-c\",\n\"content\",\ntype=str,\nhelp=\"Byte content to include in the request body.\",\n)\n@click.option(\n\"--data\",\n\"-d\",\n\"data\",\ntype=(str,str),\nmultiple=True,\nhelp=\"Form data to include in the request body.\",\n)\n@click.option(\n\"--files\",\n\"-f\",\n\"files\",\ntype=(str,click.File(mode=\"rb\")),\nmultiple=True,\nhelp=\"Form files to include in the request body.\",\n)\n@click.option(\n\"--json\",\n\"-j\",\n\"json\",\ntype=str,\ncallback=validate_json,\nhelp=\"JSON data to include in the request body.\",\n)\n@click.option(\n\"--headers\",\n\"-h\",\n\"headers\",\ntype=(str,str),\nmultiple=True,\nhelp=\"Include additional HTTP headers in the request.\",\n)\n@click.option(\n\"--cookies\",\n\"cookies\",\ntype=(str,str),\nmultiple=True,\nhelp=\"Cookies to include in the request.\",\n)\n@click.option(\n\"--auth\",\n\"auth\",\ntype=(str,str),\ndefault=(None,None),\ncallback=validate_auth,\nhelp=(\n\"Username and password to include in the request. \"\n\"Specify '-' for the password to use a password prompt. \"\n\"Note that using --verbose/-v will expose the Authorization header, \"\n\"including the password encoding in a trivially reversible format.\"\n),\n)\n@click.option(\n\"--proxy\",\n\"proxy\",\ntype=str,\ndefault=None,\nhelp=\"Send the request via a proxy. Should be the URL giving the proxy address.\",\n)\n@click.option(\n\"--timeout\",\n\"timeout\",\ntype=float,\ndefault=5.0,\nhelp=(\n\"Timeout value to use for network operations, such as establishing the \"\n\"connection, reading some data, etc... [Default: 5.0]\"\n),\n)\n@click.option(\n\"--follow-redirects\",\n\"follow_redirects\",\nis_flag=True,\ndefault=False,\nhelp=\"Automatically follow redirects.\",\n)\n@click.option(\n\"--no-verify\",\n\"verify\",\nis_flag=True,\ndefault=True,\nhelp=\"Disable SSL verification.\",\n)\n@click.option(\n\"--http2\",\n\"http2\",\ntype=bool,\nis_flag=True,\ndefault=False,\nhelp=\"Send the request using HTTP/2, if the remote server supports it.\",\n)\n@click.option(\n\"--download\",\ntype=click.File(\"wb\"),\nhelp=\"Save the response content as a file, rather than displaying it.\",\n)\n@click.option(\n\"--verbose\",\n\"-v\",\ntype=bool,\nis_flag=True,\ndefault=False,\nhelp=\"Verbose. Show request as well as response.\",\n)\n@click.option(\n\"--help\",\nis_flag=True,\nis_eager=True,\nexpose_value=False,\ncallback=handle_help,\nhelp=\"Show this message and exit.\",\n)\ndef main(\nurl:str,\nmethod:str,\nparams:list[tuple[str,str]],\ncontent:str,\ndata:list[tuple[str,str]],\nfiles:list[tuple[str,click.File]],\njson:str,\nheaders:list[tuple[str,str]],\ncookies:list[tuple[str,str]],\nauth:tuple[str,str]|None,\nproxy:str,\ntimeout:float,\nfollow_redirects:bool,\nverify:bool,\nhttp2:bool,\ndownload:typing.BinaryIO |None,\nverbose:bool,\n)->None:\n ''\n\n\n \n if not method:\n  method=\"POST\"if content or data or files or json else \"GET\"\n  \n try:\n  with Client(proxy=proxy,timeout=timeout,http2=http2,verify=verify)as client:\n   with client.stream(\n   method,\n   url,\n   params=list(params),\n   content=content,\n   data=dict(data),\n   files=files,\n   json=json,\n   headers=headers,\n   cookies=dict(cookies),\n   auth=auth,\n   follow_redirects=follow_redirects,\n   extensions={\"trace\":functools.partial(trace,verbose=verbose)},\n   )as response:\n    if download is not None:\n     download_response(response,download)\n    else:\n     response.read()\n     if response.content:\n      print_response(response)\n      \n except RequestError as exc:\n  console=rich.console.Console()\n  console.print(f\"[red]{type(exc).__name__}[/red]: {exc}\")\n  sys.exit(1)\n  \n sys.exit(0 if response.is_success else 1)\n", ["__future__", "click", "functools", "httpcore", "httpx._client", "httpx._exceptions", "httpx._models", "httpx._status_codes", "json", "pygments.lexers", "pygments.util", "rich.console", "rich.markup", "rich.progress", "rich.syntax", "rich.table", "sys", "typing"]], "httpx._types": [".py", "''\n\n\n\nfrom http.cookiejar import CookieJar\nfrom typing import(\nIO,\nTYPE_CHECKING,\nAny,\nAsyncIterable,\nAsyncIterator,\nCallable,\nDict,\nIterable,\nIterator,\nList,\nMapping,\nOptional,\nSequence,\nTuple,\nUnion,\n)\n\nif TYPE_CHECKING:\n from._auth import Auth\n from._config import Proxy,Timeout\n from._models import Cookies,Headers,Request\n from._urls import URL,QueryParams\n \n \nPrimitiveData=Optional[Union[str,int,float,bool]]\n\nURLTypes=Union[\"URL\",str]\n\nQueryParamTypes=Union[\n\"QueryParams\",\nMapping[str,Union[PrimitiveData,Sequence[PrimitiveData]]],\nList[Tuple[str,PrimitiveData]],\nTuple[Tuple[str,PrimitiveData],...],\nstr,\nbytes,\n]\n\nHeaderTypes=Union[\n\"Headers\",\nMapping[str,str],\nMapping[bytes,bytes],\nSequence[Tuple[str,str]],\nSequence[Tuple[bytes,bytes]],\n]\n\nCookieTypes=Union[\"Cookies\",CookieJar,Dict[str,str],List[Tuple[str,str]]]\n\nTimeoutTypes=Union[\nOptional[float],\nTuple[Optional[float],Optional[float],Optional[float],Optional[float]],\n\"Timeout\",\n]\nProxyTypes=Union[\"URL\",str,\"Proxy\"]\nCertTypes=Union[str,Tuple[str,str],Tuple[str,str,str]]\n\nAuthTypes=Union[\nTuple[Union[str,bytes],Union[str,bytes]],\nCallable[[\"Request\"],\"Request\"],\n\"Auth\",\n]\n\nRequestContent=Union[str,bytes,Iterable[bytes],AsyncIterable[bytes]]\nResponseContent=Union[str,bytes,Iterable[bytes],AsyncIterable[bytes]]\nResponseExtensions=Mapping[str,Any]\n\nRequestData=Mapping[str,Any]\n\nFileContent=Union[IO[bytes],bytes,str]\nFileTypes=Union[\n\nFileContent,\n\nTuple[Optional[str],FileContent],\n\nTuple[Optional[str],FileContent,Optional[str]],\n\nTuple[Optional[str],FileContent,Optional[str],Mapping[str,str]],\n]\nRequestFiles=Union[Mapping[str,FileTypes],Sequence[Tuple[str,FileTypes]]]\n\nRequestExtensions=Mapping[str,Any]\n\n__all__=[\"AsyncByteStream\",\"SyncByteStream\"]\n\n\nclass SyncByteStream:\n def __iter__(self)->Iterator[bytes]:\n  raise NotImplementedError(\n  \"The '__iter__' method must be implemented.\"\n  )\n  yield b\"\"\n  \n def close(self)->None:\n  ''\n\n\n  \n  \n  \nclass AsyncByteStream:\n async def __aiter__(self)->AsyncIterator[bytes]:\n  raise NotImplementedError(\n  \"The '__aiter__' method must be implemented.\"\n  )\n  yield b\"\"\n  \n async def aclose(self)->None:\n  pass\n", ["http.cookiejar", "httpx._auth", "httpx._config", "httpx._models", "httpx._urls", "typing"]], "httpx._models": [".py", "from __future__ import annotations\n\nimport codecs\nimport datetime\nimport email.message\nimport json as jsonlib\nimport re\nimport typing\nimport urllib.request\nfrom collections.abc import Mapping\nfrom http.cookiejar import Cookie,CookieJar\n\nfrom._content import ByteStream,UnattachedStream,encode_request,encode_response\nfrom._decoders import(\nSUPPORTED_DECODERS,\nByteChunker,\nContentDecoder,\nIdentityDecoder,\nLineDecoder,\nMultiDecoder,\nTextChunker,\nTextDecoder,\n)\nfrom._exceptions import(\nCookieConflict,\nHTTPStatusError,\nRequestNotRead,\nResponseNotRead,\nStreamClosed,\nStreamConsumed,\nrequest_context,\n)\nfrom._multipart import get_multipart_boundary_from_content_type\nfrom._status_codes import codes\nfrom._types import(\nAsyncByteStream,\nCookieTypes,\nHeaderTypes,\nQueryParamTypes,\nRequestContent,\nRequestData,\nRequestExtensions,\nRequestFiles,\nResponseContent,\nResponseExtensions,\nSyncByteStream,\n)\nfrom._urls import URL\nfrom._utils import to_bytes_or_str,to_str\n\n__all__=[\"Cookies\",\"Headers\",\"Request\",\"Response\"]\n\nSENSITIVE_HEADERS={\"authorization\",\"proxy-authorization\"}\n\n\ndef _is_known_encoding(encoding:str)->bool:\n ''\n\n \n try:\n  codecs.lookup(encoding)\n except LookupError:\n  return False\n return True\n \n \ndef _normalize_header_key(key:str |bytes,encoding:str |None=None)->bytes:\n ''\n\n \n return key if isinstance(key,bytes)else key.encode(encoding or \"ascii\")\n \n \ndef _normalize_header_value(value:str |bytes,encoding:str |None=None)->bytes:\n ''\n\n \n if isinstance(value,bytes):\n  return value\n if not isinstance(value,str):\n  raise TypeError(f\"Header value must be str or bytes, not {type(value)}\")\n return value.encode(encoding or \"ascii\")\n \n \ndef _parse_content_type_charset(content_type:str)->str |None:\n\n\n msg=email.message.Message()\n msg[\"content-type\"]=content_type\n return msg.get_content_charset(failobj=None)\n \n \ndef _parse_header_links(value:str)->list[dict[str,str]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n links:list[dict[str,str]]=[]\n replace_chars=\" '\\\"\"\n value=value.strip(replace_chars)\n if not value:\n  return links\n for val in re.split(\", *<\",value):\n  try:\n   url,params=val.split(\";\",1)\n  except ValueError:\n   url,params=val,\"\"\n  link={\"url\":url.strip(\"<> '\\\"\")}\n  for param in params.split(\";\"):\n   try:\n    key,value=param.split(\"=\")\n   except ValueError:\n    break\n   link[key.strip(replace_chars)]=value.strip(replace_chars)\n  links.append(link)\n return links\n \n \ndef _obfuscate_sensitive_headers(\nitems:typing.Iterable[tuple[typing.AnyStr,typing.AnyStr]],\n)->typing.Iterator[tuple[typing.AnyStr,typing.AnyStr]]:\n for k,v in items:\n  if to_str(k.lower())in SENSITIVE_HEADERS:\n   v=to_bytes_or_str(\"[secure]\",match_type_of=v)\n  yield k,v\n  \n  \nclass Headers(typing.MutableMapping[str,str]):\n ''\n\n \n \n def __init__(\n self,\n headers:HeaderTypes |None=None,\n encoding:str |None=None,\n )->None:\n  self._list=[]\n  \n  if isinstance(headers,Headers):\n   self._list=list(headers._list)\n  elif isinstance(headers,Mapping):\n   for k,v in headers.items():\n    bytes_key=_normalize_header_key(k,encoding)\n    bytes_value=_normalize_header_value(v,encoding)\n    self._list.append((bytes_key,bytes_key.lower(),bytes_value))\n  elif headers is not None:\n   for k,v in headers:\n    bytes_key=_normalize_header_key(k,encoding)\n    bytes_value=_normalize_header_value(v,encoding)\n    self._list.append((bytes_key,bytes_key.lower(),bytes_value))\n    \n  self._encoding=encoding\n  \n @property\n def encoding(self)->str:\n  ''\n\n\n  \n  if self._encoding is None:\n   for encoding in[\"ascii\",\"utf-8\"]:\n    for key,value in self.raw:\n     try:\n      key.decode(encoding)\n      value.decode(encoding)\n     except UnicodeDecodeError:\n      break\n    else:\n    \n    \n     self._encoding=encoding\n     break\n   else:\n   \n   \n    self._encoding=\"iso-8859-1\"\n  return self._encoding\n  \n @encoding.setter\n def encoding(self,value:str)->None:\n  self._encoding=value\n  \n @property\n def raw(self)->list[tuple[bytes,bytes]]:\n  ''\n\n  \n  return[(raw_key,value)for raw_key,_,value in self._list]\n  \n def keys(self)->typing.KeysView[str]:\n  return{key.decode(self.encoding):None for _,key,value in self._list}.keys()\n  \n def values(self)->typing.ValuesView[str]:\n  values_dict:dict[str,str]={}\n  for _,key,value in self._list:\n   str_key=key.decode(self.encoding)\n   str_value=value.decode(self.encoding)\n   if str_key in values_dict:\n    values_dict[str_key]+=f\", {str_value}\"\n   else:\n    values_dict[str_key]=str_value\n  return values_dict.values()\n  \n def items(self)->typing.ItemsView[str,str]:\n  ''\n\n\n  \n  values_dict:dict[str,str]={}\n  for _,key,value in self._list:\n   str_key=key.decode(self.encoding)\n   str_value=value.decode(self.encoding)\n   if str_key in values_dict:\n    values_dict[str_key]+=f\", {str_value}\"\n   else:\n    values_dict[str_key]=str_value\n  return values_dict.items()\n  \n def multi_items(self)->list[tuple[str,str]]:\n  ''\n\n\n\n  \n  return[\n  (key.decode(self.encoding),value.decode(self.encoding))\n  for _,key,value in self._list\n  ]\n  \n def get(self,key:str,default:typing.Any=None)->typing.Any:\n  ''\n\n\n  \n  try:\n   return self[key]\n  except KeyError:\n   return default\n   \n def get_list(self,key:str,split_commas:bool=False)->list[str]:\n  ''\n\n\n\n  \n  get_header_key=key.lower().encode(self.encoding)\n  \n  values=[\n  item_value.decode(self.encoding)\n  for _,item_key,item_value in self._list\n  if item_key.lower()==get_header_key\n  ]\n  \n  if not split_commas:\n   return values\n   \n  split_values=[]\n  for value in values:\n   split_values.extend([item.strip()for item in value.split(\",\")])\n  return split_values\n  \n def update(self,headers:HeaderTypes |None=None)->None:\n  headers=Headers(headers)\n  for key in headers.keys():\n   if key in self:\n    self.pop(key)\n  self._list.extend(headers._list)\n  \n def copy(self)->Headers:\n  return Headers(self,encoding=self.encoding)\n  \n def __getitem__(self,key:str)->str:\n  ''\n\n\n\n\n  \n  normalized_key=key.lower().encode(self.encoding)\n  \n  items=[\n  header_value.decode(self.encoding)\n  for _,header_key,header_value in self._list\n  if header_key ==normalized_key\n  ]\n  \n  if items:\n   return \", \".join(items)\n   \n  raise KeyError(key)\n  \n def __setitem__(self,key:str,value:str)->None:\n  ''\n\n\n  \n  set_key=key.encode(self._encoding or \"utf-8\")\n  set_value=value.encode(self._encoding or \"utf-8\")\n  lookup_key=set_key.lower()\n  \n  found_indexes=[\n  idx\n  for idx,(_,item_key,_)in enumerate(self._list)\n  if item_key ==lookup_key\n  ]\n  \n  for idx in reversed(found_indexes[1:]):\n   del self._list[idx]\n   \n  if found_indexes:\n   idx=found_indexes[0]\n   self._list[idx]=(set_key,lookup_key,set_value)\n  else:\n   self._list.append((set_key,lookup_key,set_value))\n   \n def __delitem__(self,key:str)->None:\n  ''\n\n  \n  del_key=key.lower().encode(self.encoding)\n  \n  pop_indexes=[\n  idx\n  for idx,(_,item_key,_)in enumerate(self._list)\n  if item_key.lower()==del_key\n  ]\n  \n  if not pop_indexes:\n   raise KeyError(key)\n   \n  for idx in reversed(pop_indexes):\n   del self._list[idx]\n   \n def __contains__(self,key:typing.Any)->bool:\n  header_key=key.lower().encode(self.encoding)\n  return header_key in[key for _,key,_ in self._list]\n  \n def __iter__(self)->typing.Iterator[typing.Any]:\n  return iter(self.keys())\n  \n def __len__(self)->int:\n  return len(self._list)\n  \n def __eq__(self,other:typing.Any)->bool:\n  try:\n   other_headers=Headers(other)\n  except ValueError:\n   return False\n   \n  self_list=[(key,value)for _,key,value in self._list]\n  other_list=[(key,value)for _,key,value in other_headers._list]\n  return sorted(self_list)==sorted(other_list)\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  \n  encoding_str=\"\"\n  if self.encoding !=\"ascii\":\n   encoding_str=f\", encoding={self.encoding !r}\"\n   \n  as_list=list(_obfuscate_sensitive_headers(self.multi_items()))\n  as_dict=dict(as_list)\n  \n  no_duplicate_keys=len(as_dict)==len(as_list)\n  if no_duplicate_keys:\n   return f\"{class_name}({as_dict !r}{encoding_str})\"\n  return f\"{class_name}({as_list !r}{encoding_str})\"\n  \n  \nclass Request:\n def __init__(\n self,\n method:str,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n stream:SyncByteStream |AsyncByteStream |None=None,\n extensions:RequestExtensions |None=None,\n )->None:\n  self.method=method.upper()\n  self.url=URL(url)if params is None else URL(url,params=params)\n  self.headers=Headers(headers)\n  self.extensions={}if extensions is None else dict(extensions)\n  \n  if cookies:\n   Cookies(cookies).set_cookie_header(self)\n   \n  if stream is None:\n   content_type:str |None=self.headers.get(\"content-type\")\n   headers,stream=encode_request(\n   content=content,\n   data=data,\n   files=files,\n   json=json,\n   boundary=get_multipart_boundary_from_content_type(\n   content_type=content_type.encode(self.headers.encoding)\n   if content_type\n   else None\n   ),\n   )\n   self._prepare(headers)\n   self.stream=stream\n   \n   if isinstance(stream,ByteStream):\n    self.read()\n  else:\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   self.stream=stream\n   \n def _prepare(self,default_headers:dict[str,str])->None:\n  for key,value in default_headers.items():\n  \n   if key.lower()==\"transfer-encoding\"and \"Content-Length\"in self.headers:\n    continue\n   self.headers.setdefault(key,value)\n   \n  auto_headers:list[tuple[bytes,bytes]]=[]\n  \n  has_host=\"Host\"in self.headers\n  has_content_length=(\n  \"Content-Length\"in self.headers or \"Transfer-Encoding\"in self.headers\n  )\n  \n  if not has_host and self.url.host:\n   auto_headers.append((b\"Host\",self.url.netloc))\n  if not has_content_length and self.method in(\"POST\",\"PUT\",\"PATCH\"):\n   auto_headers.append((b\"Content-Length\",b\"0\"))\n   \n  self.headers=Headers(auto_headers+self.headers.raw)\n  \n @property\n def content(self)->bytes:\n  if not hasattr(self,\"_content\"):\n   raise RequestNotRead()\n  return self._content\n  \n def read(self)->bytes:\n  ''\n\n  \n  if not hasattr(self,\"_content\"):\n   assert isinstance(self.stream,typing.Iterable)\n   self._content=b\"\".join(self.stream)\n   if not isinstance(self.stream,ByteStream):\n   \n   \n   \n    self.stream=ByteStream(self._content)\n  return self._content\n  \n async def aread(self)->bytes:\n  ''\n\n  \n  if not hasattr(self,\"_content\"):\n   assert isinstance(self.stream,typing.AsyncIterable)\n   self._content=b\"\".join([part async for part in self.stream])\n   if not isinstance(self.stream,ByteStream):\n   \n   \n   \n    self.stream=ByteStream(self._content)\n  return self._content\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  url=str(self.url)\n  return f\"<{class_name}({self.method !r}, {url !r})>\"\n  \n def __getstate__(self)->dict[str,typing.Any]:\n  return{\n  name:value\n  for name,value in self.__dict__.items()\n  if name not in[\"extensions\",\"stream\"]\n  }\n  \n def __setstate__(self,state:dict[str,typing.Any])->None:\n  for name,value in state.items():\n   setattr(self,name,value)\n  self.extensions={}\n  self.stream=UnattachedStream()\n  \n  \nclass Response:\n def __init__(\n self,\n status_code:int,\n *,\n headers:HeaderTypes |None=None,\n content:ResponseContent |None=None,\n text:str |None=None,\n html:str |None=None,\n json:typing.Any=None,\n stream:SyncByteStream |AsyncByteStream |None=None,\n request:Request |None=None,\n extensions:ResponseExtensions |None=None,\n history:list[Response]|None=None,\n default_encoding:str |typing.Callable[[bytes],str]=\"utf-8\",\n )->None:\n  self.status_code=status_code\n  self.headers=Headers(headers)\n  \n  self._request:Request |None=request\n  \n  \n  \n  self.next_request:Request |None=None\n  \n  self.extensions={}if extensions is None else dict(extensions)\n  self.history=[]if history is None else list(history)\n  \n  self.is_closed=False\n  self.is_stream_consumed=False\n  \n  self.default_encoding=default_encoding\n  \n  if stream is None:\n   headers,stream=encode_response(content,text,html,json)\n   self._prepare(headers)\n   self.stream=stream\n   if isinstance(stream,ByteStream):\n   \n    self.read()\n  else:\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   self.stream=stream\n   \n  self._num_bytes_downloaded=0\n  \n def _prepare(self,default_headers:dict[str,str])->None:\n  for key,value in default_headers.items():\n  \n   if key.lower()==\"transfer-encoding\"and \"content-length\"in self.headers:\n    continue\n   self.headers.setdefault(key,value)\n   \n @property\n def elapsed(self)->datetime.timedelta:\n  ''\n\n\n  \n  if not hasattr(self,\"_elapsed\"):\n   raise RuntimeError(\n   \"'.elapsed' may only be accessed after the response \"\n   \"has been read or closed.\"\n   )\n  return self._elapsed\n  \n @elapsed.setter\n def elapsed(self,elapsed:datetime.timedelta)->None:\n  self._elapsed=elapsed\n  \n @property\n def request(self)->Request:\n  ''\n\n  \n  if self._request is None:\n   raise RuntimeError(\n   \"The request instance has not been set on this response.\"\n   )\n  return self._request\n  \n @request.setter\n def request(self,value:Request)->None:\n  self._request=value\n  \n @property\n def http_version(self)->str:\n  try:\n   http_version:bytes=self.extensions[\"http_version\"]\n  except KeyError:\n   return \"HTTP/1.1\"\n  else:\n   return http_version.decode(\"ascii\",errors=\"ignore\")\n   \n @property\n def reason_phrase(self)->str:\n  try:\n   reason_phrase:bytes=self.extensions[\"reason_phrase\"]\n  except KeyError:\n   return codes.get_reason_phrase(self.status_code)\n  else:\n   return reason_phrase.decode(\"ascii\",errors=\"ignore\")\n   \n @property\n def url(self)->URL:\n  ''\n\n  \n  return self.request.url\n  \n @property\n def content(self)->bytes:\n  if not hasattr(self,\"_content\"):\n   raise ResponseNotRead()\n  return self._content\n  \n @property\n def text(self)->str:\n  if not hasattr(self,\"_text\"):\n   content=self.content\n   if not content:\n    self._text=\"\"\n   else:\n    decoder=TextDecoder(encoding=self.encoding or \"utf-8\")\n    self._text=\"\".join([decoder.decode(self.content),decoder.flush()])\n  return self._text\n  \n @property\n def encoding(self)->str |None:\n  ''\n\n\n\n\n\n\n\n\n  \n  if not hasattr(self,\"_encoding\"):\n   encoding=self.charset_encoding\n   if encoding is None or not _is_known_encoding(encoding):\n    if isinstance(self.default_encoding,str):\n     encoding=self.default_encoding\n    elif hasattr(self,\"_content\"):\n     encoding=self.default_encoding(self._content)\n   self._encoding=encoding or \"utf-8\"\n  return self._encoding\n  \n @encoding.setter\n def encoding(self,value:str)->None:\n  ''\n\n\n\n\n  \n  if hasattr(self,\"_text\"):\n   raise ValueError(\n   \"Setting encoding after `text` has been accessed is not allowed.\"\n   )\n  self._encoding=value\n  \n @property\n def charset_encoding(self)->str |None:\n  ''\n\n  \n  content_type=self.headers.get(\"Content-Type\")\n  if content_type is None:\n   return None\n   \n  return _parse_content_type_charset(content_type)\n  \n def _get_content_decoder(self)->ContentDecoder:\n  ''\n\n\n  \n  if not hasattr(self,\"_decoder\"):\n   decoders:list[ContentDecoder]=[]\n   values=self.headers.get_list(\"content-encoding\",split_commas=True)\n   for value in values:\n    value=value.strip().lower()\n    try:\n     decoder_cls=SUPPORTED_DECODERS[value]\n     decoders.append(decoder_cls())\n    except KeyError:\n     continue\n     \n   if len(decoders)==1:\n    self._decoder=decoders[0]\n   elif len(decoders)>1:\n    self._decoder=MultiDecoder(children=decoders)\n   else:\n    self._decoder=IdentityDecoder()\n    \n  return self._decoder\n  \n @property\n def is_informational(self)->bool:\n  ''\n\n  \n  return codes.is_informational(self.status_code)\n  \n @property\n def is_success(self)->bool:\n  ''\n\n  \n  return codes.is_success(self.status_code)\n  \n @property\n def is_redirect(self)->bool:\n  ''\n\n\n\n\n\n\n  \n  return codes.is_redirect(self.status_code)\n  \n @property\n def is_client_error(self)->bool:\n  ''\n\n  \n  return codes.is_client_error(self.status_code)\n  \n @property\n def is_server_error(self)->bool:\n  ''\n\n  \n  return codes.is_server_error(self.status_code)\n  \n @property\n def is_error(self)->bool:\n  ''\n\n  \n  return codes.is_error(self.status_code)\n  \n @property\n def has_redirect_location(self)->bool:\n  ''\n\n\n  \n  return(\n  self.status_code\n  in(\n  \n  codes.MOVED_PERMANENTLY,\n  \n  codes.FOUND,\n  \n  codes.SEE_OTHER,\n  \n  codes.TEMPORARY_REDIRECT,\n  \n  codes.PERMANENT_REDIRECT,\n  )\n  and \"Location\"in self.headers\n  )\n  \n def raise_for_status(self)->Response:\n  ''\n\n  \n  request=self._request\n  if request is None:\n   raise RuntimeError(\n   \"Cannot call `raise_for_status` as the request \"\n   \"instance has not been set on this response.\"\n   )\n   \n  if self.is_success:\n   return self\n   \n  if self.has_redirect_location:\n   message=(\n   \"{error_type} '{0.status_code} {0.reason_phrase}' for url '{0.url}'\\n\"\n   \"Redirect location: '{0.headers[location]}'\\n\"\n   \"For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{0.status_code}\"\n   )\n  else:\n   message=(\n   \"{error_type} '{0.status_code} {0.reason_phrase}' for url '{0.url}'\\n\"\n   \"For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{0.status_code}\"\n   )\n   \n  status_class=self.status_code //100\n  error_types={\n  1:\"Informational response\",\n  3:\"Redirect response\",\n  4:\"Client error\",\n  5:\"Server error\",\n  }\n  error_type=error_types.get(status_class,\"Invalid status code\")\n  message=message.format(self,error_type=error_type)\n  raise HTTPStatusError(message,request=request,response=self)\n  \n def json(self,**kwargs:typing.Any)->typing.Any:\n  return jsonlib.loads(self.content,**kwargs)\n  \n @property\n def cookies(self)->Cookies:\n  if not hasattr(self,\"_cookies\"):\n   self._cookies=Cookies()\n   self._cookies.extract_cookies(self)\n  return self._cookies\n  \n @property\n def links(self)->dict[str |None,dict[str,str]]:\n  ''\n\n  \n  header=self.headers.get(\"link\")\n  if header is None:\n   return{}\n   \n  return{\n  (link.get(\"rel\")or link.get(\"url\")):link\n  for link in _parse_header_links(header)\n  }\n  \n @property\n def num_bytes_downloaded(self)->int:\n  return self._num_bytes_downloaded\n  \n def __repr__(self)->str:\n  return f\"<Response [{self.status_code} {self.reason_phrase}]>\"\n  \n def __getstate__(self)->dict[str,typing.Any]:\n  return{\n  name:value\n  for name,value in self.__dict__.items()\n  if name not in[\"extensions\",\"stream\",\"is_closed\",\"_decoder\"]\n  }\n  \n def __setstate__(self,state:dict[str,typing.Any])->None:\n  for name,value in state.items():\n   setattr(self,name,value)\n  self.is_closed=True\n  self.extensions={}\n  self.stream=UnattachedStream()\n  \n def read(self)->bytes:\n  ''\n\n  \n  if not hasattr(self,\"_content\"):\n   self._content=b\"\".join(self.iter_bytes())\n  return self._content\n  \n def iter_bytes(self,chunk_size:int |None=None)->typing.Iterator[bytes]:\n  ''\n\n\n  \n  if hasattr(self,\"_content\"):\n   chunk_size=len(self._content)if chunk_size is None else chunk_size\n   for i in range(0,len(self._content),max(chunk_size,1)):\n    yield self._content[i:i+chunk_size]\n  else:\n   decoder=self._get_content_decoder()\n   chunker=ByteChunker(chunk_size=chunk_size)\n   with request_context(request=self._request):\n    for raw_bytes in self.iter_raw():\n     decoded=decoder.decode(raw_bytes)\n     for chunk in chunker.decode(decoded):\n      yield chunk\n    decoded=decoder.flush()\n    for chunk in chunker.decode(decoded):\n     yield chunk\n    for chunk in chunker.flush():\n     yield chunk\n     \n def iter_text(self,chunk_size:int |None=None)->typing.Iterator[str]:\n  ''\n\n\n\n  \n  decoder=TextDecoder(encoding=self.encoding or \"utf-8\")\n  chunker=TextChunker(chunk_size=chunk_size)\n  with request_context(request=self._request):\n   for byte_content in self.iter_bytes():\n    text_content=decoder.decode(byte_content)\n    for chunk in chunker.decode(text_content):\n     yield chunk\n   text_content=decoder.flush()\n   for chunk in chunker.decode(text_content):\n    yield chunk\n   for chunk in chunker.flush():\n    yield chunk\n    \n def iter_lines(self)->typing.Iterator[str]:\n  decoder=LineDecoder()\n  with request_context(request=self._request):\n   for text in self.iter_text():\n    for line in decoder.decode(text):\n     yield line\n   for line in decoder.flush():\n    yield line\n    \n def iter_raw(self,chunk_size:int |None=None)->typing.Iterator[bytes]:\n  ''\n\n  \n  if self.is_stream_consumed:\n   raise StreamConsumed()\n  if self.is_closed:\n   raise StreamClosed()\n  if not isinstance(self.stream,SyncByteStream):\n   raise RuntimeError(\"Attempted to call a sync iterator on an async stream.\")\n   \n  self.is_stream_consumed=True\n  self._num_bytes_downloaded=0\n  chunker=ByteChunker(chunk_size=chunk_size)\n  \n  with request_context(request=self._request):\n   for raw_stream_bytes in self.stream:\n    self._num_bytes_downloaded +=len(raw_stream_bytes)\n    for chunk in chunker.decode(raw_stream_bytes):\n     yield chunk\n     \n  for chunk in chunker.flush():\n   yield chunk\n   \n  self.close()\n  \n def close(self)->None:\n  ''\n\n\n  \n  if not isinstance(self.stream,SyncByteStream):\n   raise RuntimeError(\"Attempted to call an sync close on an async stream.\")\n   \n  if not self.is_closed:\n   self.is_closed=True\n   with request_context(request=self._request):\n    self.stream.close()\n    \n async def aread(self)->bytes:\n  ''\n\n  \n  if not hasattr(self,\"_content\"):\n   self._content=b\"\".join([part async for part in self.aiter_bytes()])\n  return self._content\n  \n async def aiter_bytes(\n self,chunk_size:int |None=None\n )->typing.AsyncIterator[bytes]:\n  ''\n\n\n  \n  if hasattr(self,\"_content\"):\n   chunk_size=len(self._content)if chunk_size is None else chunk_size\n   for i in range(0,len(self._content),max(chunk_size,1)):\n    yield self._content[i:i+chunk_size]\n  else:\n   decoder=self._get_content_decoder()\n   chunker=ByteChunker(chunk_size=chunk_size)\n   with request_context(request=self._request):\n    async for raw_bytes in self.aiter_raw():\n     decoded=decoder.decode(raw_bytes)\n     for chunk in chunker.decode(decoded):\n      yield chunk\n    decoded=decoder.flush()\n    for chunk in chunker.decode(decoded):\n     yield chunk\n    for chunk in chunker.flush():\n     yield chunk\n     \n async def aiter_text(\n self,chunk_size:int |None=None\n )->typing.AsyncIterator[str]:\n  ''\n\n\n\n  \n  decoder=TextDecoder(encoding=self.encoding or \"utf-8\")\n  chunker=TextChunker(chunk_size=chunk_size)\n  with request_context(request=self._request):\n   async for byte_content in self.aiter_bytes():\n    text_content=decoder.decode(byte_content)\n    for chunk in chunker.decode(text_content):\n     yield chunk\n   text_content=decoder.flush()\n   for chunk in chunker.decode(text_content):\n    yield chunk\n   for chunk in chunker.flush():\n    yield chunk\n    \n async def aiter_lines(self)->typing.AsyncIterator[str]:\n  decoder=LineDecoder()\n  with request_context(request=self._request):\n   async for text in self.aiter_text():\n    for line in decoder.decode(text):\n     yield line\n   for line in decoder.flush():\n    yield line\n    \n async def aiter_raw(\n self,chunk_size:int |None=None\n )->typing.AsyncIterator[bytes]:\n  ''\n\n  \n  if self.is_stream_consumed:\n   raise StreamConsumed()\n  if self.is_closed:\n   raise StreamClosed()\n  if not isinstance(self.stream,AsyncByteStream):\n   raise RuntimeError(\"Attempted to call an async iterator on an sync stream.\")\n   \n  self.is_stream_consumed=True\n  self._num_bytes_downloaded=0\n  chunker=ByteChunker(chunk_size=chunk_size)\n  \n  with request_context(request=self._request):\n   async for raw_stream_bytes in self.stream:\n    self._num_bytes_downloaded +=len(raw_stream_bytes)\n    for chunk in chunker.decode(raw_stream_bytes):\n     yield chunk\n     \n  for chunk in chunker.flush():\n   yield chunk\n   \n  await self.aclose()\n  \n async def aclose(self)->None:\n  ''\n\n\n  \n  if not isinstance(self.stream,AsyncByteStream):\n   raise RuntimeError(\"Attempted to call an async close on an sync stream.\")\n   \n  if not self.is_closed:\n   self.is_closed=True\n   with request_context(request=self._request):\n    await self.stream.aclose()\n    \n    \nclass Cookies(typing.MutableMapping[str,str]):\n ''\n\n \n \n def __init__(self,cookies:CookieTypes |None=None)->None:\n  if cookies is None or isinstance(cookies,dict):\n   self.jar=CookieJar()\n   if isinstance(cookies,dict):\n    for key,value in cookies.items():\n     self.set(key,value)\n  elif isinstance(cookies,list):\n   self.jar=CookieJar()\n   for key,value in cookies:\n    self.set(key,value)\n  elif isinstance(cookies,Cookies):\n   self.jar=CookieJar()\n   for cookie in cookies.jar:\n    self.jar.set_cookie(cookie)\n  else:\n   self.jar=cookies\n   \n def extract_cookies(self,response:Response)->None:\n  ''\n\n  \n  urllib_response=self._CookieCompatResponse(response)\n  urllib_request=self._CookieCompatRequest(response.request)\n  \n  self.jar.extract_cookies(urllib_response,urllib_request)\n  \n def set_cookie_header(self,request:Request)->None:\n  ''\n\n  \n  urllib_request=self._CookieCompatRequest(request)\n  self.jar.add_cookie_header(urllib_request)\n  \n def set(self,name:str,value:str,domain:str=\"\",path:str=\"/\")->None:\n  ''\n\n  \n  kwargs={\n  \"version\":0,\n  \"name\":name,\n  \"value\":value,\n  \"port\":None,\n  \"port_specified\":False,\n  \"domain\":domain,\n  \"domain_specified\":bool(domain),\n  \"domain_initial_dot\":domain.startswith(\".\"),\n  \"path\":path,\n  \"path_specified\":bool(path),\n  \"secure\":False,\n  \"expires\":None,\n  \"discard\":True,\n  \"comment\":None,\n  \"comment_url\":None,\n  \"rest\":{\"HttpOnly\":None},\n  \"rfc2109\":False,\n  }\n  cookie=Cookie(**kwargs)\n  self.jar.set_cookie(cookie)\n  \n def get(\n self,\n name:str,\n default:str |None=None,\n domain:str |None=None,\n path:str |None=None,\n )->str |None:\n  ''\n\n\n  \n  value=None\n  for cookie in self.jar:\n   if cookie.name ==name:\n    if domain is None or cookie.domain ==domain:\n     if path is None or cookie.path ==path:\n      if value is not None:\n       message=f\"Multiple cookies exist with name={name}\"\n       raise CookieConflict(message)\n      value=cookie.value\n      \n  if value is None:\n   return default\n  return value\n  \n def delete(\n self,\n name:str,\n domain:str |None=None,\n path:str |None=None,\n )->None:\n  ''\n\n\n  \n  if domain is not None and path is not None:\n   return self.jar.clear(domain,path,name)\n   \n  remove=[\n  cookie\n  for cookie in self.jar\n  if cookie.name ==name\n  and(domain is None or cookie.domain ==domain)\n  and(path is None or cookie.path ==path)\n  ]\n  \n  for cookie in remove:\n   self.jar.clear(cookie.domain,cookie.path,cookie.name)\n   \n def clear(self,domain:str |None=None,path:str |None=None)->None:\n  ''\n\n\n  \n  args=[]\n  if domain is not None:\n   args.append(domain)\n  if path is not None:\n   assert domain is not None\n   args.append(path)\n  self.jar.clear(*args)\n  \n def update(self,cookies:CookieTypes |None=None)->None:\n  cookies=Cookies(cookies)\n  for cookie in cookies.jar:\n   self.jar.set_cookie(cookie)\n   \n def __setitem__(self,name:str,value:str)->None:\n  return self.set(name,value)\n  \n def __getitem__(self,name:str)->str:\n  value=self.get(name)\n  if value is None:\n   raise KeyError(name)\n  return value\n  \n def __delitem__(self,name:str)->None:\n  return self.delete(name)\n  \n def __len__(self)->int:\n  return len(self.jar)\n  \n def __iter__(self)->typing.Iterator[str]:\n  return(cookie.name for cookie in self.jar)\n  \n def __bool__(self)->bool:\n  for _ in self.jar:\n   return True\n  return False\n  \n def __repr__(self)->str:\n  cookies_repr=\", \".join(\n  [\n  f\"<Cookie {cookie.name}={cookie.value} for {cookie.domain} />\"\n  for cookie in self.jar\n  ]\n  )\n  \n  return f\"<Cookies[{cookies_repr}]>\"\n  \n class _CookieCompatRequest(urllib.request.Request):\n  ''\n\n\n  \n  \n  def __init__(self,request:Request)->None:\n   super().__init__(\n   url=str(request.url),\n   headers=dict(request.headers),\n   method=request.method,\n   )\n   self.request=request\n   \n  def add_unredirected_header(self,key:str,value:str)->None:\n   super().add_unredirected_header(key,value)\n   self.request.headers[key]=value\n   \n class _CookieCompatResponse:\n  ''\n\n\n  \n  \n  def __init__(self,response:Response)->None:\n   self.response=response\n   \n  def info(self)->email.message.Message:\n   info=email.message.Message()\n   for key,value in self.response.headers.multi_items():\n   \n   \n   \n    info[key]=value\n   return info\n", ["__future__", "codecs", "collections.abc", "datetime", "email.message", "http.cookiejar", "httpx._content", "httpx._decoders", "httpx._exceptions", "httpx._multipart", "httpx._status_codes", "httpx._types", "httpx._urls", "httpx._utils", "json", "re", "typing", "urllib.request"]], "httpx.__version__": [".py", "__title__=\"httpx\"\n__description__=\"A next generation HTTP client, for Python 3.\"\n__version__=\"0.28.1\"\n", []], "httpx._urls": [".py", "from __future__ import annotations\n\nimport typing\nfrom urllib.parse import parse_qs,unquote,urlencode\n\nimport idna\n\nfrom._types import QueryParamTypes\nfrom._urlparse import urlparse\nfrom._utils import primitive_value_to_str\n\n__all__=[\"URL\",\"QueryParams\"]\n\n\nclass URL:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,url:URL |str=\"\",**kwargs:typing.Any)->None:\n  if kwargs:\n   allowed={\n   \"scheme\":str,\n   \"username\":str,\n   \"password\":str,\n   \"userinfo\":bytes,\n   \"host\":str,\n   \"port\":int,\n   \"netloc\":bytes,\n   \"path\":str,\n   \"query\":bytes,\n   \"raw_path\":bytes,\n   \"fragment\":str,\n   \"params\":object,\n   }\n   \n   \n   for key,value in kwargs.items():\n    if key not in allowed:\n     message=f\"{key !r} is an invalid keyword argument for URL()\"\n     raise TypeError(message)\n    if value is not None and not isinstance(value,allowed[key]):\n     expected=allowed[key].__name__\n     seen=type(value).__name__\n     message=f\"Argument {key !r} must be {expected} but got {seen}\"\n     raise TypeError(message)\n    if isinstance(value,bytes):\n     kwargs[key]=value.decode(\"ascii\")\n     \n   if \"params\"in kwargs:\n   \n   \n   \n   \n   \n    params=kwargs.pop(\"params\")\n    kwargs[\"query\"]=None if not params else str(QueryParams(params))\n    \n  if isinstance(url,str):\n   self._uri_reference=urlparse(url,**kwargs)\n  elif isinstance(url,URL):\n   self._uri_reference=url._uri_reference.copy_with(**kwargs)\n  else:\n   raise TypeError(\n   \"Invalid type for url.  Expected str or httpx.URL,\"\n   f\" got {type(url)}: {url !r}\"\n   )\n   \n @property\n def scheme(self)->str:\n  ''\n\n\n  \n  return self._uri_reference.scheme\n  \n @property\n def raw_scheme(self)->bytes:\n  ''\n\n\n  \n  return self._uri_reference.scheme.encode(\"ascii\")\n  \n @property\n def userinfo(self)->bytes:\n  ''\n\n\n  \n  return self._uri_reference.userinfo.encode(\"ascii\")\n  \n @property\n def username(self)->str:\n  ''\n\n\n  \n  userinfo=self._uri_reference.userinfo\n  return unquote(userinfo.partition(\":\")[0])\n  \n @property\n def password(self)->str:\n  ''\n\n\n  \n  userinfo=self._uri_reference.userinfo\n  return unquote(userinfo.partition(\":\")[2])\n  \n @property\n def host(self)->str:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  host:str=self._uri_reference.host\n  \n  if host.startswith(\"xn--\"):\n   host=idna.decode(host)\n   \n  return host\n  \n @property\n def raw_host(self)->bytes:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self._uri_reference.host.encode(\"ascii\")\n  \n @property\n def port(self)->int |None:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return self._uri_reference.port\n  \n @property\n def netloc(self)->bytes:\n  ''\n\n\n\n\n\n  \n  return self._uri_reference.netloc.encode(\"ascii\")\n  \n @property\n def path(self)->str:\n  ''\n\n\n\n\n\n\n  \n  path=self._uri_reference.path or \"/\"\n  return unquote(path)\n  \n @property\n def query(self)->bytes:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  query=self._uri_reference.query or \"\"\n  return query.encode(\"ascii\")\n  \n @property\n def params(self)->QueryParams:\n  ''\n\n\n  \n  return QueryParams(self._uri_reference.query)\n  \n @property\n def raw_path(self)->bytes:\n  ''\n\n\n\n\n\n\n\n\n  \n  path=self._uri_reference.path or \"/\"\n  if self._uri_reference.query is not None:\n   path +=\"?\"+self._uri_reference.query\n  return path.encode(\"ascii\")\n  \n @property\n def fragment(self)->str:\n  ''\n\n\n  \n  return unquote(self._uri_reference.fragment or \"\")\n  \n @property\n def is_absolute_url(self)->bool:\n  ''\n\n\n  \n  \n  \n  \n  \n  return bool(self._uri_reference.scheme and self._uri_reference.host)\n  \n @property\n def is_relative_url(self)->bool:\n  ''\n\n\n  \n  return not self.is_absolute_url\n  \n def copy_with(self,**kwargs:typing.Any)->URL:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  return URL(self,**kwargs)\n  \n def copy_set_param(self,key:str,value:typing.Any=None)->URL:\n  return self.copy_with(params=self.params.set(key,value))\n  \n def copy_add_param(self,key:str,value:typing.Any=None)->URL:\n  return self.copy_with(params=self.params.add(key,value))\n  \n def copy_remove_param(self,key:str)->URL:\n  return self.copy_with(params=self.params.remove(key))\n  \n def copy_merge_params(self,params:QueryParamTypes)->URL:\n  return self.copy_with(params=self.params.merge(params))\n  \n def join(self,url:URL |str)->URL:\n  ''\n\n\n\n\n\n\n\n  \n  from urllib.parse import urljoin\n  \n  return URL(urljoin(str(self),str(URL(url))))\n  \n def __hash__(self)->int:\n  return hash(str(self))\n  \n def __eq__(self,other:typing.Any)->bool:\n  return isinstance(other,(URL,str))and str(self)==str(URL(other))\n  \n def __str__(self)->str:\n  return str(self._uri_reference)\n  \n def __repr__(self)->str:\n  scheme,userinfo,host,port,path,query,fragment=self._uri_reference\n  \n  if \":\"in userinfo:\n  \n   userinfo=f'{userinfo.split(\":\")[0]}:[secure]'\n   \n  authority=\"\".join(\n  [\n  f\"{userinfo}@\"if userinfo else \"\",\n  f\"[{host}]\"if \":\"in host else host,\n  f\":{port}\"if port is not None else \"\",\n  ]\n  )\n  url=\"\".join(\n  [\n  f\"{self.scheme}:\"if scheme else \"\",\n  f\"//{authority}\"if authority else \"\",\n  path,\n  f\"?{query}\"if query is not None else \"\",\n  f\"#{fragment}\"if fragment is not None else \"\",\n  ]\n  )\n  \n  return f\"{self.__class__.__name__}({url !r})\"\n  \n @property\n def raw(self)->tuple[bytes,bytes,int,bytes]:\n  import collections\n  import warnings\n  \n  warnings.warn(\"URL.raw is deprecated.\")\n  RawURL=collections.namedtuple(\n  \"RawURL\",[\"raw_scheme\",\"raw_host\",\"port\",\"raw_path\"]\n  )\n  return RawURL(\n  raw_scheme=self.raw_scheme,\n  raw_host=self.raw_host,\n  port=self.port,\n  raw_path=self.raw_path,\n  )\n  \n  \nclass QueryParams(typing.Mapping[str,str]):\n ''\n\n \n \n def __init__(self,*args:QueryParamTypes |None,**kwargs:typing.Any)->None:\n  assert len(args)<2,\"Too many arguments.\"\n  assert not(args and kwargs),\"Cannot mix named and unnamed arguments.\"\n  \n  value=args[0]if args else kwargs\n  \n  if value is None or isinstance(value,(str,bytes)):\n   value=value.decode(\"ascii\")if isinstance(value,bytes)else value\n   self._dict=parse_qs(value,keep_blank_values=True)\n  elif isinstance(value,QueryParams):\n   self._dict={k:list(v)for k,v in value._dict.items()}\n  else:\n   dict_value:dict[typing.Any,list[typing.Any]]={}\n   if isinstance(value,(list,tuple)):\n   \n   \n   \n   \n    for item in value:\n     dict_value.setdefault(item[0],[]).append(item[1])\n   else:\n   \n   \n   \n   \n    dict_value={\n    k:list(v)if isinstance(v,(list,tuple))else[v]\n    for k,v in value.items()\n    }\n    \n    \n    \n    \n   self._dict={\n   str(k):[primitive_value_to_str(item)for item in v]\n   for k,v in dict_value.items()\n   }\n   \n def keys(self)->typing.KeysView[str]:\n  ''\n\n\n\n\n\n\n  \n  return self._dict.keys()\n  \n def values(self)->typing.ValuesView[str]:\n  ''\n\n\n\n\n\n\n\n  \n  return{k:v[0]for k,v in self._dict.items()}.values()\n  \n def items(self)->typing.ItemsView[str,str]:\n  ''\n\n\n\n\n\n\n\n  \n  return{k:v[0]for k,v in self._dict.items()}.items()\n  \n def multi_items(self)->list[tuple[str,str]]:\n  ''\n\n\n\n\n\n\n  \n  multi_items:list[tuple[str,str]]=[]\n  for k,v in self._dict.items():\n   multi_items.extend([(k,i)for i in v])\n  return multi_items\n  \n def get(self,key:typing.Any,default:typing.Any=None)->typing.Any:\n  ''\n\n\n\n\n\n\n\n  \n  if key in self._dict:\n   return self._dict[str(key)][0]\n  return default\n  \n def get_list(self,key:str)->list[str]:\n  ''\n\n\n\n\n\n\n  \n  return list(self._dict.get(str(key),[]))\n  \n def set(self,key:str,value:typing.Any=None)->QueryParams:\n  ''\n\n\n\n\n\n\n\n  \n  q=QueryParams()\n  q._dict=dict(self._dict)\n  q._dict[str(key)]=[primitive_value_to_str(value)]\n  return q\n  \n def add(self,key:str,value:typing.Any=None)->QueryParams:\n  ''\n\n\n\n\n\n\n\n  \n  q=QueryParams()\n  q._dict=dict(self._dict)\n  q._dict[str(key)]=q.get_list(key)+[primitive_value_to_str(value)]\n  return q\n  \n def remove(self,key:str)->QueryParams:\n  ''\n\n\n\n\n\n\n\n  \n  q=QueryParams()\n  q._dict=dict(self._dict)\n  q._dict.pop(str(key),None)\n  return q\n  \n def merge(self,params:QueryParamTypes |None=None)->QueryParams:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  q=QueryParams(params)\n  q._dict={**self._dict,**q._dict}\n  return q\n  \n def __getitem__(self,key:typing.Any)->str:\n  return self._dict[key][0]\n  \n def __contains__(self,key:typing.Any)->bool:\n  return key in self._dict\n  \n def __iter__(self)->typing.Iterator[typing.Any]:\n  return iter(self.keys())\n  \n def __len__(self)->int:\n  return len(self._dict)\n  \n def __bool__(self)->bool:\n  return bool(self._dict)\n  \n def __hash__(self)->int:\n  return hash(str(self))\n  \n def __eq__(self,other:typing.Any)->bool:\n  if not isinstance(other,self.__class__):\n   return False\n  return sorted(self.multi_items())==sorted(other.multi_items())\n  \n def __str__(self)->str:\n  return urlencode(self.multi_items())\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  query_string=str(self)\n  return f\"{class_name}({query_string !r})\"\n  \n def update(self,params:QueryParamTypes |None=None)->None:\n  raise RuntimeError(\n  \"QueryParams are immutable since 0.18.0. \"\n  \"Use `q = q.merge(...)` to create an updated copy.\"\n  )\n  \n def __setitem__(self,key:str,value:str)->None:\n  raise RuntimeError(\n  \"QueryParams are immutable since 0.18.0. \"\n  \"Use `q = q.set(key, value)` to create an updated copy.\"\n  )\n", ["__future__", "collections", "httpx._types", "httpx._urlparse", "httpx._utils", "idna", "typing", "urllib.parse", "warnings"]], "httpx._multipart": [".py", "from __future__ import annotations\n\nimport io\nimport mimetypes\nimport os\nimport re\nimport typing\nfrom pathlib import Path\n\nfrom._types import(\nAsyncByteStream,\nFileContent,\nFileTypes,\nRequestData,\nRequestFiles,\nSyncByteStream,\n)\nfrom._utils import(\npeek_filelike_length,\nprimitive_value_to_str,\nto_bytes,\n)\n\n_HTML5_FORM_ENCODING_REPLACEMENTS={'\"':\"%22\",\"\\\\\":\"\\\\\\\\\"}\n_HTML5_FORM_ENCODING_REPLACEMENTS.update(\n{chr(c):\"%{:02X}\".format(c)for c in range(0x1F+1)if c !=0x1B}\n)\n_HTML5_FORM_ENCODING_RE=re.compile(\nr\"|\".join([re.escape(c)for c in _HTML5_FORM_ENCODING_REPLACEMENTS.keys()])\n)\n\n\ndef _format_form_param(name:str,value:str)->bytes:\n ''\n\n \n \n def replacer(match:typing.Match[str])->str:\n  return _HTML5_FORM_ENCODING_REPLACEMENTS[match.group(0)]\n  \n value=_HTML5_FORM_ENCODING_RE.sub(replacer,value)\n return f'{name}=\"{value}\"'.encode()\n \n \ndef _guess_content_type(filename:str |None)->str |None:\n ''\n\n\n\n \n if filename:\n  return mimetypes.guess_type(filename)[0]or \"application/octet-stream\"\n return None\n \n \ndef get_multipart_boundary_from_content_type(\ncontent_type:bytes |None,\n)->bytes |None:\n if not content_type or not content_type.startswith(b\"multipart/form-data\"):\n  return None\n  \n  \n if b\";\"in content_type:\n  for section in content_type.split(b\";\"):\n   if section.strip().lower().startswith(b\"boundary=\"):\n    return section.strip()[len(b\"boundary=\"):].strip(b'\"')\n return None\n \n \nclass DataField:\n ''\n\n \n \n def __init__(self,name:str,value:str |bytes |int |float |None)->None:\n  if not isinstance(name,str):\n   raise TypeError(\n   f\"Invalid type for name. Expected str, got {type(name)}: {name !r}\"\n   )\n  if value is not None and not isinstance(value,(str,bytes,int,float)):\n   raise TypeError(\n   \"Invalid type for value. Expected primitive type,\"\n   f\" got {type(value)}: {value !r}\"\n   )\n  self.name=name\n  self.value:str |bytes=(\n  value if isinstance(value,bytes)else primitive_value_to_str(value)\n  )\n  \n def render_headers(self)->bytes:\n  if not hasattr(self,\"_headers\"):\n   name=_format_form_param(\"name\",self.name)\n   self._headers=b\"\".join(\n   [b\"Content-Disposition: form-data; \",name,b\"\\r\\n\\r\\n\"]\n   )\n   \n  return self._headers\n  \n def render_data(self)->bytes:\n  if not hasattr(self,\"_data\"):\n   self._data=to_bytes(self.value)\n   \n  return self._data\n  \n def get_length(self)->int:\n  headers=self.render_headers()\n  data=self.render_data()\n  return len(headers)+len(data)\n  \n def render(self)->typing.Iterator[bytes]:\n  yield self.render_headers()\n  yield self.render_data()\n  \n  \nclass FileField:\n ''\n\n \n \n CHUNK_SIZE=64 *1024\n \n def __init__(self,name:str,value:FileTypes)->None:\n  self.name=name\n  \n  fileobj:FileContent\n  \n  headers:dict[str,str]={}\n  content_type:str |None=None\n  \n  \n  \n  \n  \n  if isinstance(value,tuple):\n   if len(value)==2:\n   \n   \n    filename,fileobj=value\n   elif len(value)==3:\n    filename,fileobj,content_type=value\n   else:\n   \n    filename,fileobj,content_type,headers=value\n  else:\n   filename=Path(str(getattr(value,\"name\",\"upload\"))).name\n   fileobj=value\n   \n  if content_type is None:\n   content_type=_guess_content_type(filename)\n   \n  has_content_type_header=any(\"content-type\"in key.lower()for key in headers)\n  if content_type is not None and not has_content_type_header:\n  \n  \n  \n   headers[\"Content-Type\"]=content_type\n   \n  if isinstance(fileobj,io.StringIO):\n   raise TypeError(\n   \"Multipart file uploads require 'io.BytesIO', not 'io.StringIO'.\"\n   )\n  if isinstance(fileobj,io.TextIOBase):\n   raise TypeError(\n   \"Multipart file uploads must be opened in binary mode, not text mode.\"\n   )\n   \n  self.filename=filename\n  self.file=fileobj\n  self.headers=headers\n  \n def get_length(self)->int |None:\n  headers=self.render_headers()\n  \n  if isinstance(self.file,(str,bytes)):\n   return len(headers)+len(to_bytes(self.file))\n   \n  file_length=peek_filelike_length(self.file)\n  \n  \n  \n  if file_length is None:\n   return None\n   \n  return len(headers)+file_length\n  \n def render_headers(self)->bytes:\n  if not hasattr(self,\"_headers\"):\n   parts=[\n   b\"Content-Disposition: form-data; \",\n   _format_form_param(\"name\",self.name),\n   ]\n   if self.filename:\n    filename=_format_form_param(\"filename\",self.filename)\n    parts.extend([b\"; \",filename])\n   for header_name,header_value in self.headers.items():\n    key,val=f\"\\r\\n{header_name}: \".encode(),header_value.encode()\n    parts.extend([key,val])\n   parts.append(b\"\\r\\n\\r\\n\")\n   self._headers=b\"\".join(parts)\n   \n  return self._headers\n  \n def render_data(self)->typing.Iterator[bytes]:\n  if isinstance(self.file,(str,bytes)):\n   yield to_bytes(self.file)\n   return\n   \n  if hasattr(self.file,\"seek\"):\n   try:\n    self.file.seek(0)\n   except io.UnsupportedOperation:\n    pass\n    \n  chunk=self.file.read(self.CHUNK_SIZE)\n  while chunk:\n   yield to_bytes(chunk)\n   chunk=self.file.read(self.CHUNK_SIZE)\n   \n def render(self)->typing.Iterator[bytes]:\n  yield self.render_headers()\n  yield from self.render_data()\n  \n  \nclass MultipartStream(SyncByteStream,AsyncByteStream):\n ''\n\n \n \n def __init__(\n self,\n data:RequestData,\n files:RequestFiles,\n boundary:bytes |None=None,\n )->None:\n  if boundary is None:\n   boundary=os.urandom(16).hex().encode(\"ascii\")\n   \n  self.boundary=boundary\n  self.content_type=\"multipart/form-data; boundary=%s\"%boundary.decode(\n  \"ascii\"\n  )\n  self.fields=list(self._iter_fields(data,files))\n  \n def _iter_fields(\n self,data:RequestData,files:RequestFiles\n )->typing.Iterator[FileField |DataField]:\n  for name,value in data.items():\n   if isinstance(value,(tuple,list)):\n    for item in value:\n     yield DataField(name=name,value=item)\n   else:\n    yield DataField(name=name,value=value)\n    \n  file_items=files.items()if isinstance(files,typing.Mapping)else files\n  for name,value in file_items:\n   yield FileField(name=name,value=value)\n   \n def iter_chunks(self)->typing.Iterator[bytes]:\n  for field in self.fields:\n   yield b\"--%s\\r\\n\"%self.boundary\n   yield from field.render()\n   yield b\"\\r\\n\"\n  yield b\"--%s--\\r\\n\"%self.boundary\n  \n def get_content_length(self)->int |None:\n  ''\n\n\n  \n  boundary_length=len(self.boundary)\n  length=0\n  \n  for field in self.fields:\n   field_length=field.get_length()\n   if field_length is None:\n    return None\n    \n   length +=2+boundary_length+2\n   length +=field_length\n   length +=2\n   \n  length +=2+boundary_length+4\n  return length\n  \n  \n  \n def get_headers(self)->dict[str,str]:\n  content_length=self.get_content_length()\n  content_type=self.content_type\n  if content_length is None:\n   return{\"Transfer-Encoding\":\"chunked\",\"Content-Type\":content_type}\n  return{\"Content-Length\":str(content_length),\"Content-Type\":content_type}\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  for chunk in self.iter_chunks():\n   yield chunk\n   \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  for chunk in self.iter_chunks():\n   yield chunk\n", ["__future__", "httpx._types", "httpx._utils", "io", "mimetypes", "os", "pathlib", "re", "typing"]], "httpx._decoders": [".py", "''\n\n\n\n\n\nfrom __future__ import annotations\n\nimport codecs\nimport io\nimport typing\nimport zlib\n\nfrom._exceptions import DecodingError\n\n\ntry:\n\n import brotli\nexcept ImportError:\n try:\n \n \n  import brotlicffi as brotli\n except ImportError:\n  brotli=None\n  \n  \n  \ntry:\n import zstandard\nexcept ImportError:\n zstandard=None\n \n \nclass ContentDecoder:\n def decode(self,data:bytes)->bytes:\n  raise NotImplementedError()\n  \n def flush(self)->bytes:\n  raise NotImplementedError()\n  \n  \nclass IdentityDecoder(ContentDecoder):\n ''\n\n \n \n def decode(self,data:bytes)->bytes:\n  return data\n  \n def flush(self)->bytes:\n  return b\"\"\n  \n  \nclass DeflateDecoder(ContentDecoder):\n ''\n\n\n\n \n \n def __init__(self)->None:\n  self.first_attempt=True\n  self.decompressor=zlib.decompressobj()\n  \n def decode(self,data:bytes)->bytes:\n  was_first_attempt=self.first_attempt\n  self.first_attempt=False\n  try:\n   return self.decompressor.decompress(data)\n  except zlib.error as exc:\n   if was_first_attempt:\n    self.decompressor=zlib.decompressobj(-zlib.MAX_WBITS)\n    return self.decode(data)\n   raise DecodingError(str(exc))from exc\n   \n def flush(self)->bytes:\n  try:\n   return self.decompressor.flush()\n  except zlib.error as exc:\n   raise DecodingError(str(exc))from exc\n   \n   \nclass GZipDecoder(ContentDecoder):\n ''\n\n\n\n \n \n def __init__(self)->None:\n  self.decompressor=zlib.decompressobj(zlib.MAX_WBITS |16)\n  \n def decode(self,data:bytes)->bytes:\n  try:\n   return self.decompressor.decompress(data)\n  except zlib.error as exc:\n   raise DecodingError(str(exc))from exc\n   \n def flush(self)->bytes:\n  try:\n   return self.decompressor.flush()\n  except zlib.error as exc:\n   raise DecodingError(str(exc))from exc\n   \n   \nclass BrotliDecoder(ContentDecoder):\n ''\n\n\n\n\n\n\n \n \n def __init__(self)->None:\n  if brotli is None:\n   raise ImportError(\n   \"Using 'BrotliDecoder', but neither of the 'brotlicffi' or 'brotli' \"\n   \"packages have been installed. \"\n   \"Make sure to install httpx using `pip install httpx[brotli]`.\"\n   )from None\n   \n  self.decompressor=brotli.Decompressor()\n  self.seen_data=False\n  self._decompress:typing.Callable[[bytes],bytes]\n  if hasattr(self.decompressor,\"decompress\"):\n  \n   self._decompress=self.decompressor.decompress\n  else:\n  \n   self._decompress=self.decompressor.process\n   \n def decode(self,data:bytes)->bytes:\n  if not data:\n   return b\"\"\n  self.seen_data=True\n  try:\n   return self._decompress(data)\n  except brotli.error as exc:\n   raise DecodingError(str(exc))from exc\n   \n def flush(self)->bytes:\n  if not self.seen_data:\n   return b\"\"\n  try:\n   if hasattr(self.decompressor,\"finish\"):\n   \n   \n   \n   \n   \n    self.decompressor.finish()\n   return b\"\"\n  except brotli.error as exc:\n   raise DecodingError(str(exc))from exc\n   \n   \nclass ZStandardDecoder(ContentDecoder):\n ''\n\n\n\n\n \n \n \n def __init__(self)->None:\n  if zstandard is None:\n   raise ImportError(\n   \"Using 'ZStandardDecoder', ...\"\n   \"Make sure to install httpx using `pip install httpx[zstd]`.\"\n   )from None\n   \n  self.decompressor=zstandard.ZstdDecompressor().decompressobj()\n  self.seen_data=False\n  \n def decode(self,data:bytes)->bytes:\n  assert zstandard is not None\n  self.seen_data=True\n  output=io.BytesIO()\n  try:\n   output.write(self.decompressor.decompress(data))\n   while self.decompressor.eof and self.decompressor.unused_data:\n    unused_data=self.decompressor.unused_data\n    self.decompressor=zstandard.ZstdDecompressor().decompressobj()\n    output.write(self.decompressor.decompress(unused_data))\n  except zstandard.ZstdError as exc:\n   raise DecodingError(str(exc))from exc\n  return output.getvalue()\n  \n def flush(self)->bytes:\n  if not self.seen_data:\n   return b\"\"\n  ret=self.decompressor.flush()\n  if not self.decompressor.eof:\n   raise DecodingError(\"Zstandard data is incomplete\")\n  return bytes(ret)\n  \n  \nclass MultiDecoder(ContentDecoder):\n ''\n\n \n \n def __init__(self,children:typing.Sequence[ContentDecoder])->None:\n  ''\n\n\n  \n  \n  self.children=list(reversed(children))\n  \n def decode(self,data:bytes)->bytes:\n  for child in self.children:\n   data=child.decode(data)\n  return data\n  \n def flush(self)->bytes:\n  data=b\"\"\n  for child in self.children:\n   data=child.decode(data)+child.flush()\n  return data\n  \n  \nclass ByteChunker:\n ''\n\n \n \n def __init__(self,chunk_size:int |None=None)->None:\n  self._buffer=io.BytesIO()\n  self._chunk_size=chunk_size\n  \n def decode(self,content:bytes)->list[bytes]:\n  if self._chunk_size is None:\n   return[content]if content else[]\n   \n  self._buffer.write(content)\n  if self._buffer.tell()>=self._chunk_size:\n   value=self._buffer.getvalue()\n   chunks=[\n   value[i:i+self._chunk_size]\n   for i in range(0,len(value),self._chunk_size)\n   ]\n   if len(chunks[-1])==self._chunk_size:\n    self._buffer.seek(0)\n    self._buffer.truncate()\n    return chunks\n   else:\n    self._buffer.seek(0)\n    self._buffer.write(chunks[-1])\n    self._buffer.truncate()\n    return chunks[:-1]\n  else:\n   return[]\n   \n def flush(self)->list[bytes]:\n  value=self._buffer.getvalue()\n  self._buffer.seek(0)\n  self._buffer.truncate()\n  return[value]if value else[]\n  \n  \nclass TextChunker:\n ''\n\n \n \n def __init__(self,chunk_size:int |None=None)->None:\n  self._buffer=io.StringIO()\n  self._chunk_size=chunk_size\n  \n def decode(self,content:str)->list[str]:\n  if self._chunk_size is None:\n   return[content]if content else[]\n   \n  self._buffer.write(content)\n  if self._buffer.tell()>=self._chunk_size:\n   value=self._buffer.getvalue()\n   chunks=[\n   value[i:i+self._chunk_size]\n   for i in range(0,len(value),self._chunk_size)\n   ]\n   if len(chunks[-1])==self._chunk_size:\n    self._buffer.seek(0)\n    self._buffer.truncate()\n    return chunks\n   else:\n    self._buffer.seek(0)\n    self._buffer.write(chunks[-1])\n    self._buffer.truncate()\n    return chunks[:-1]\n  else:\n   return[]\n   \n def flush(self)->list[str]:\n  value=self._buffer.getvalue()\n  self._buffer.seek(0)\n  self._buffer.truncate()\n  return[value]if value else[]\n  \n  \nclass TextDecoder:\n ''\n\n \n \n def __init__(self,encoding:str=\"utf-8\")->None:\n  self.decoder=codecs.getincrementaldecoder(encoding)(errors=\"replace\")\n  \n def decode(self,data:bytes)->str:\n  return self.decoder.decode(data)\n  \n def flush(self)->str:\n  return self.decoder.decode(b\"\",True)\n  \n  \nclass LineDecoder:\n ''\n\n\n\n\n \n \n def __init__(self)->None:\n  self.buffer:list[str]=[]\n  self.trailing_cr:bool=False\n  \n def decode(self,text:str)->list[str]:\n \n  NEWLINE_CHARS=\"\\n\\r\\x0b\\x0c\\x1c\\x1d\\x1e\\x85\\u2028\\u2029\"\n  \n  \n  if self.trailing_cr:\n   text=\"\\r\"+text\n   self.trailing_cr=False\n  if text.endswith(\"\\r\"):\n   self.trailing_cr=True\n   text=text[:-1]\n   \n  if not text:\n  \n  \n   return[]\n   \n  trailing_newline=text[-1]in NEWLINE_CHARS\n  lines=text.splitlines()\n  \n  if len(lines)==1 and not trailing_newline:\n  \n   self.buffer.append(lines[0])\n   return[]\n   \n  if self.buffer:\n  \n  \n   lines=[\"\".join(self.buffer)+lines[0]]+lines[1:]\n   self.buffer=[]\n   \n  if not trailing_newline:\n  \n  \n   self.buffer=[lines.pop()]\n   \n  return lines\n  \n def flush(self)->list[str]:\n  if not self.buffer and not self.trailing_cr:\n   return[]\n   \n  lines=[\"\".join(self.buffer)]\n  self.buffer=[]\n  self.trailing_cr=False\n  return lines\n  \n  \nSUPPORTED_DECODERS={\n\"identity\":IdentityDecoder,\n\"gzip\":GZipDecoder,\n\"deflate\":DeflateDecoder,\n\"br\":BrotliDecoder,\n\"zstd\":ZStandardDecoder,\n}\n\n\nif brotli is None:\n SUPPORTED_DECODERS.pop(\"br\")\nif zstandard is None:\n SUPPORTED_DECODERS.pop(\"zstd\")\n", ["__future__", "brotli", "brotlicffi", "codecs", "httpx._exceptions", "io", "typing", "zlib", "zstandard"]], "httpx._api": [".py", "from __future__ import annotations\n\nimport typing\nfrom contextlib import contextmanager\n\nfrom._client import Client\nfrom._config import DEFAULT_TIMEOUT_CONFIG\nfrom._models import Response\nfrom._types import(\nAuthTypes,\nCookieTypes,\nHeaderTypes,\nProxyTypes,\nQueryParamTypes,\nRequestContent,\nRequestData,\nRequestFiles,\nTimeoutTypes,\n)\nfrom._urls import URL\n\nif typing.TYPE_CHECKING:\n import ssl\n \n \n__all__=[\n\"delete\",\n\"get\",\n\"head\",\n\"options\",\n\"patch\",\n\"post\",\n\"put\",\n\"request\",\n\"stream\",\n]\n\n\ndef request(\nmethod:str,\nurl:URL |str,\n*,\nparams:QueryParamTypes |None=None,\ncontent:RequestContent |None=None,\ndata:RequestData |None=None,\nfiles:RequestFiles |None=None,\njson:typing.Any |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n with Client(\n cookies=cookies,\n proxy=proxy,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )as client:\n  return client.request(\n  method=method,\n  url=url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  )\n  \n  \n@contextmanager\ndef stream(\nmethod:str,\nurl:URL |str,\n*,\nparams:QueryParamTypes |None=None,\ncontent:RequestContent |None=None,\ndata:RequestData |None=None,\nfiles:RequestFiles |None=None,\njson:typing.Any |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntrust_env:bool=True,\n)->typing.Iterator[Response]:\n ''\n\n\n\n\n\n\n\n\n \n with Client(\n cookies=cookies,\n proxy=proxy,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )as client:\n  with client.stream(\n  method=method,\n  url=url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  )as response:\n   yield response\n   \n   \ndef get(\nurl:URL |str,\n*,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n\n\n\n \n return request(\n \"GET\",\n url,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n \n \ndef options(\nurl:URL |str,\n*,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n\n\n\n \n return request(\n \"OPTIONS\",\n url,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n \n \ndef head(\nurl:URL |str,\n*,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n\n\n\n \n return request(\n \"HEAD\",\n url,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n \n \ndef post(\nurl:URL |str,\n*,\ncontent:RequestContent |None=None,\ndata:RequestData |None=None,\nfiles:RequestFiles |None=None,\njson:typing.Any |None=None,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n \n return request(\n \"POST\",\n url,\n content=content,\n data=data,\n files=files,\n json=json,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n \n \ndef put(\nurl:URL |str,\n*,\ncontent:RequestContent |None=None,\ndata:RequestData |None=None,\nfiles:RequestFiles |None=None,\njson:typing.Any |None=None,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n \n return request(\n \"PUT\",\n url,\n content=content,\n data=data,\n files=files,\n json=json,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n \n \ndef patch(\nurl:URL |str,\n*,\ncontent:RequestContent |None=None,\ndata:RequestData |None=None,\nfiles:RequestFiles |None=None,\njson:typing.Any |None=None,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\nverify:ssl.SSLContext |str |bool=True,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n \n return request(\n \"PATCH\",\n url,\n content=content,\n data=data,\n files=files,\n json=json,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n \n \ndef delete(\nurl:URL |str,\n*,\nparams:QueryParamTypes |None=None,\nheaders:HeaderTypes |None=None,\ncookies:CookieTypes |None=None,\nauth:AuthTypes |None=None,\nproxy:ProxyTypes |None=None,\nfollow_redirects:bool=False,\ntimeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\nverify:ssl.SSLContext |str |bool=True,\ntrust_env:bool=True,\n)->Response:\n ''\n\n\n\n\n\n\n \n return request(\n \"DELETE\",\n url,\n params=params,\n headers=headers,\n cookies=cookies,\n auth=auth,\n proxy=proxy,\n follow_redirects=follow_redirects,\n verify=verify,\n timeout=timeout,\n trust_env=trust_env,\n )\n", ["__future__", "contextlib", "httpx._client", "httpx._config", "httpx._models", "httpx._types", "httpx._urls", "ssl", "typing"]], "httpx._client": [".py", "from __future__ import annotations\n\nimport datetime\nimport enum\nimport logging\nimport time\nimport typing\nimport warnings\nfrom contextlib import asynccontextmanager,contextmanager\nfrom types import TracebackType\n\nfrom.__version__ import __version__\nfrom._auth import Auth,BasicAuth,FunctionAuth\nfrom._config import(\nDEFAULT_LIMITS,\nDEFAULT_MAX_REDIRECTS,\nDEFAULT_TIMEOUT_CONFIG,\nLimits,\nProxy,\nTimeout,\n)\nfrom._decoders import SUPPORTED_DECODERS\nfrom._exceptions import(\nInvalidURL,\nRemoteProtocolError,\nTooManyRedirects,\nrequest_context,\n)\nfrom._models import Cookies,Headers,Request,Response\nfrom._status_codes import codes\nfrom._transports.base import AsyncBaseTransport,BaseTransport\nfrom._transports.default import AsyncHTTPTransport,HTTPTransport\nfrom._types import(\nAsyncByteStream,\nAuthTypes,\nCertTypes,\nCookieTypes,\nHeaderTypes,\nProxyTypes,\nQueryParamTypes,\nRequestContent,\nRequestData,\nRequestExtensions,\nRequestFiles,\nSyncByteStream,\nTimeoutTypes,\n)\nfrom._urls import URL,QueryParams\nfrom._utils import URLPattern,get_environment_proxies\n\nif typing.TYPE_CHECKING:\n import ssl\n \n__all__=[\"USE_CLIENT_DEFAULT\",\"AsyncClient\",\"Client\"]\n\n\n\nT=typing.TypeVar(\"T\",bound=\"Client\")\nU=typing.TypeVar(\"U\",bound=\"AsyncClient\")\n\n\ndef _is_https_redirect(url:URL,location:URL)->bool:\n ''\n\n \n if url.host !=location.host:\n  return False\n  \n return(\n url.scheme ==\"http\"\n and _port_or_default(url)==80\n and location.scheme ==\"https\"\n and _port_or_default(location)==443\n )\n \n \ndef _port_or_default(url:URL)->int |None:\n if url.port is not None:\n  return url.port\n return{\"http\":80,\"https\":443}.get(url.scheme)\n \n \ndef _same_origin(url:URL,other:URL)->bool:\n ''\n\n \n return(\n url.scheme ==other.scheme\n and url.host ==other.host\n and _port_or_default(url)==_port_or_default(other)\n )\n \n \nclass UseClientDefault:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n \nUSE_CLIENT_DEFAULT=UseClientDefault()\n\n\nlogger=logging.getLogger(\"httpx\")\n\nUSER_AGENT=f\"python-httpx/{__version__}\"\nACCEPT_ENCODING=\", \".join(\n[key for key in SUPPORTED_DECODERS.keys()if key !=\"identity\"]\n)\n\n\nclass ClientState(enum.Enum):\n\n\n\n UNOPENED=1\n \n \n OPENED=2\n \n \n \n CLOSED=3\n \n \nclass BoundSyncStream(SyncByteStream):\n ''\n\n\n \n \n def __init__(\n self,stream:SyncByteStream,response:Response,start:float\n )->None:\n  self._stream=stream\n  self._response=response\n  self._start=start\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  for chunk in self._stream:\n   yield chunk\n   \n def close(self)->None:\n  elapsed=time.perf_counter()-self._start\n  self._response.elapsed=datetime.timedelta(seconds=elapsed)\n  self._stream.close()\n  \n  \nclass BoundAsyncStream(AsyncByteStream):\n ''\n\n\n \n \n def __init__(\n self,stream:AsyncByteStream,response:Response,start:float\n )->None:\n  self._stream=stream\n  self._response=response\n  self._start=start\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  async for chunk in self._stream:\n   yield chunk\n   \n async def aclose(self)->None:\n  elapsed=time.perf_counter()-self._start\n  self._response.elapsed=datetime.timedelta(seconds=elapsed)\n  await self._stream.aclose()\n  \n  \nEventHook=typing.Callable[...,typing.Any]\n\n\nclass BaseClient:\n def __init__(\n self,\n *,\n auth:AuthTypes |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n timeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\n follow_redirects:bool=False,\n max_redirects:int=DEFAULT_MAX_REDIRECTS,\n event_hooks:None |(typing.Mapping[str,list[EventHook]])=None,\n base_url:URL |str=\"\",\n trust_env:bool=True,\n default_encoding:str |typing.Callable[[bytes],str]=\"utf-8\",\n )->None:\n  event_hooks={}if event_hooks is None else event_hooks\n  \n  self._base_url=self._enforce_trailing_slash(URL(base_url))\n  \n  self._auth=self._build_auth(auth)\n  self._params=QueryParams(params)\n  self.headers=Headers(headers)\n  self._cookies=Cookies(cookies)\n  self._timeout=Timeout(timeout)\n  self.follow_redirects=follow_redirects\n  self.max_redirects=max_redirects\n  self._event_hooks={\n  \"request\":list(event_hooks.get(\"request\",[])),\n  \"response\":list(event_hooks.get(\"response\",[])),\n  }\n  self._trust_env=trust_env\n  self._default_encoding=default_encoding\n  self._state=ClientState.UNOPENED\n  \n @property\n def is_closed(self)->bool:\n  ''\n\n  \n  return self._state ==ClientState.CLOSED\n  \n @property\n def trust_env(self)->bool:\n  return self._trust_env\n  \n def _enforce_trailing_slash(self,url:URL)->URL:\n  if url.raw_path.endswith(b\"/\"):\n   return url\n  return url.copy_with(raw_path=url.raw_path+b\"/\")\n  \n def _get_proxy_map(\n self,proxy:ProxyTypes |None,allow_env_proxies:bool\n )->dict[str,Proxy |None]:\n  if proxy is None:\n   if allow_env_proxies:\n    return{\n    key:None if url is None else Proxy(url=url)\n    for key,url in get_environment_proxies().items()\n    }\n   return{}\n  else:\n   proxy=Proxy(url=proxy)if isinstance(proxy,(str,URL))else proxy\n   return{\"all://\":proxy}\n   \n @property\n def timeout(self)->Timeout:\n  return self._timeout\n  \n @timeout.setter\n def timeout(self,timeout:TimeoutTypes)->None:\n  self._timeout=Timeout(timeout)\n  \n @property\n def event_hooks(self)->dict[str,list[EventHook]]:\n  return self._event_hooks\n  \n @event_hooks.setter\n def event_hooks(self,event_hooks:dict[str,list[EventHook]])->None:\n  self._event_hooks={\n  \"request\":list(event_hooks.get(\"request\",[])),\n  \"response\":list(event_hooks.get(\"response\",[])),\n  }\n  \n @property\n def auth(self)->Auth |None:\n  ''\n\n\n\n\n\n  \n  return self._auth\n  \n @auth.setter\n def auth(self,auth:AuthTypes)->None:\n  self._auth=self._build_auth(auth)\n  \n @property\n def base_url(self)->URL:\n  ''\n\n  \n  return self._base_url\n  \n @base_url.setter\n def base_url(self,url:URL |str)->None:\n  self._base_url=self._enforce_trailing_slash(URL(url))\n  \n @property\n def headers(self)->Headers:\n  ''\n\n  \n  return self._headers\n  \n @headers.setter\n def headers(self,headers:HeaderTypes)->None:\n  client_headers=Headers(\n  {\n  b\"Accept\":b\"*/*\",\n  b\"Accept-Encoding\":ACCEPT_ENCODING.encode(\"ascii\"),\n  b\"Connection\":b\"keep-alive\",\n  b\"User-Agent\":USER_AGENT.encode(\"ascii\"),\n  }\n  )\n  client_headers.update(headers)\n  self._headers=client_headers\n  \n @property\n def cookies(self)->Cookies:\n  ''\n\n  \n  return self._cookies\n  \n @cookies.setter\n def cookies(self,cookies:CookieTypes)->None:\n  self._cookies=Cookies(cookies)\n  \n @property\n def params(self)->QueryParams:\n  ''\n\n  \n  return self._params\n  \n @params.setter\n def params(self,params:QueryParamTypes)->None:\n  self._params=QueryParams(params)\n  \n def build_request(\n self,\n method:str,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Request:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  url=self._merge_url(url)\n  headers=self._merge_headers(headers)\n  cookies=self._merge_cookies(cookies)\n  params=self._merge_queryparams(params)\n  extensions={}if extensions is None else extensions\n  if \"timeout\"not in extensions:\n   timeout=(\n   self.timeout\n   if isinstance(timeout,UseClientDefault)\n   else Timeout(timeout)\n   )\n   extensions=dict(**extensions,timeout=timeout.as_dict())\n  return Request(\n  method,\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  extensions=extensions,\n  )\n  \n def _merge_url(self,url:URL |str)->URL:\n  ''\n\n\n  \n  merge_url=URL(url)\n  if merge_url.is_relative_url:\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   merge_raw_path=self.base_url.raw_path+merge_url.raw_path.lstrip(b\"/\")\n   return self.base_url.copy_with(raw_path=merge_raw_path)\n  return merge_url\n  \n def _merge_cookies(self,cookies:CookieTypes |None=None)->CookieTypes |None:\n  ''\n\n\n  \n  if cookies or self.cookies:\n   merged_cookies=Cookies(self.cookies)\n   merged_cookies.update(cookies)\n   return merged_cookies\n  return cookies\n  \n def _merge_headers(self,headers:HeaderTypes |None=None)->HeaderTypes |None:\n  ''\n\n\n  \n  merged_headers=Headers(self.headers)\n  merged_headers.update(headers)\n  return merged_headers\n  \n def _merge_queryparams(\n self,params:QueryParamTypes |None=None\n )->QueryParamTypes |None:\n  ''\n\n\n  \n  if params or self.params:\n   merged_queryparams=QueryParams(self.params)\n   return merged_queryparams.merge(params)\n  return params\n  \n def _build_auth(self,auth:AuthTypes |None)->Auth |None:\n  if auth is None:\n   return None\n  elif isinstance(auth,tuple):\n   return BasicAuth(username=auth[0],password=auth[1])\n  elif isinstance(auth,Auth):\n   return auth\n  elif callable(auth):\n   return FunctionAuth(func=auth)\n  else:\n   raise TypeError(f'Invalid \"auth\" argument: {auth !r}')\n   \n def _build_request_auth(\n self,\n request:Request,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n )->Auth:\n  auth=(\n  self._auth if isinstance(auth,UseClientDefault)else self._build_auth(auth)\n  )\n  \n  if auth is not None:\n   return auth\n   \n  username,password=request.url.username,request.url.password\n  if username or password:\n   return BasicAuth(username=username,password=password)\n   \n  return Auth()\n  \n def _build_redirect_request(self,request:Request,response:Response)->Request:\n  ''\n\n\n  \n  method=self._redirect_method(request,response)\n  url=self._redirect_url(request,response)\n  headers=self._redirect_headers(request,url,method)\n  stream=self._redirect_stream(request,method)\n  cookies=Cookies(self.cookies)\n  return Request(\n  method=method,\n  url=url,\n  headers=headers,\n  cookies=cookies,\n  stream=stream,\n  extensions=request.extensions,\n  )\n  \n def _redirect_method(self,request:Request,response:Response)->str:\n  ''\n\n\n  \n  method=request.method\n  \n  \n  if response.status_code ==codes.SEE_OTHER and method !=\"HEAD\":\n   method=\"GET\"\n   \n   \n   \n  if response.status_code ==codes.FOUND and method !=\"HEAD\":\n   method=\"GET\"\n   \n   \n   \n  if response.status_code ==codes.MOVED_PERMANENTLY and method ==\"POST\":\n   method=\"GET\"\n   \n  return method\n  \n def _redirect_url(self,request:Request,response:Response)->URL:\n  ''\n\n  \n  location=response.headers[\"Location\"]\n  \n  try:\n   url=URL(location)\n  except InvalidURL as exc:\n   raise RemoteProtocolError(\n   f\"Invalid URL in location header: {exc}.\",request=request\n   )from None\n   \n   \n   \n  if url.scheme and not url.host:\n   url=url.copy_with(host=request.url.host)\n   \n   \n   \n  if url.is_relative_url:\n   url=request.url.join(url)\n   \n   \n  if request.url.fragment and not url.fragment:\n   url=url.copy_with(fragment=request.url.fragment)\n   \n  return url\n  \n def _redirect_headers(self,request:Request,url:URL,method:str)->Headers:\n  ''\n\n  \n  headers=Headers(request.headers)\n  \n  if not _same_origin(url,request.url):\n   if not _is_https_redirect(request.url,url):\n   \n   \n    headers.pop(\"Authorization\",None)\n    \n    \n   headers[\"Host\"]=url.netloc.decode(\"ascii\")\n   \n  if method !=request.method and method ==\"GET\":\n  \n  \n   headers.pop(\"Content-Length\",None)\n   headers.pop(\"Transfer-Encoding\",None)\n   \n   \n   \n  headers.pop(\"Cookie\",None)\n  \n  return headers\n  \n def _redirect_stream(\n self,request:Request,method:str\n )->SyncByteStream |AsyncByteStream |None:\n  ''\n\n  \n  if method !=request.method and method ==\"GET\":\n   return None\n   \n  return request.stream\n  \n def _set_timeout(self,request:Request)->None:\n  if \"timeout\"not in request.extensions:\n   timeout=(\n   self.timeout\n   if isinstance(self.timeout,UseClientDefault)\n   else Timeout(self.timeout)\n   )\n   request.extensions=dict(**request.extensions,timeout=timeout.as_dict())\n   \n   \nclass Client(BaseClient):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n *,\n auth:AuthTypes |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n proxy:ProxyTypes |None=None,\n mounts:None |(typing.Mapping[str,BaseTransport |None])=None,\n timeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\n follow_redirects:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n max_redirects:int=DEFAULT_MAX_REDIRECTS,\n event_hooks:None |(typing.Mapping[str,list[EventHook]])=None,\n base_url:URL |str=\"\",\n transport:BaseTransport |None=None,\n default_encoding:str |typing.Callable[[bytes],str]=\"utf-8\",\n )->None:\n  super().__init__(\n  auth=auth,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  timeout=timeout,\n  follow_redirects=follow_redirects,\n  max_redirects=max_redirects,\n  event_hooks=event_hooks,\n  base_url=base_url,\n  trust_env=trust_env,\n  default_encoding=default_encoding,\n  )\n  \n  if http2:\n   try:\n    import h2\n   except ImportError:\n    raise ImportError(\n    \"Using http2=True, but the 'h2' package is not installed. \"\n    \"Make sure to install httpx using `pip install httpx[http2]`.\"\n    )from None\n    \n  allow_env_proxies=trust_env and transport is None\n  proxy_map=self._get_proxy_map(proxy,allow_env_proxies)\n  \n  self._transport=self._init_transport(\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  transport=transport,\n  )\n  self._mounts:dict[URLPattern,BaseTransport |None]={\n  URLPattern(key):None\n  if proxy is None\n  else self._init_proxy_transport(\n  proxy,\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  )\n  for key,proxy in proxy_map.items()\n  }\n  if mounts is not None:\n   self._mounts.update(\n   {URLPattern(key):transport for key,transport in mounts.items()}\n   )\n   \n  self._mounts=dict(sorted(self._mounts.items()))\n  \n def _init_transport(\n self,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n transport:BaseTransport |None=None,\n )->BaseTransport:\n  if transport is not None:\n   return transport\n   \n  return HTTPTransport(\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  )\n  \n def _init_proxy_transport(\n self,\n proxy:Proxy,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n )->BaseTransport:\n  return HTTPTransport(\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  proxy=proxy,\n  )\n  \n def _transport_for_url(self,url:URL)->BaseTransport:\n  ''\n\n\n  \n  for pattern,transport in self._mounts.items():\n   if pattern.matches(url):\n    return self._transport if transport is None else transport\n    \n  return self._transport\n  \n def request(\n self,\n method:str,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if cookies is not None:\n   message=(\n   \"Setting per-request cookies=<...> is being deprecated, because \"\n   \"the expected behaviour on cookie persistence is ambiguous. Set \"\n   \"cookies directly on the client instance instead.\"\n   )\n   warnings.warn(message,DeprecationWarning,stacklevel=2)\n   \n  request=self.build_request(\n  method=method,\n  url=url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  return self.send(request,auth=auth,follow_redirects=follow_redirects)\n  \n @contextmanager\n def stream(\n self,\n method:str,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->typing.Iterator[Response]:\n  ''\n\n\n\n\n\n\n\n\n  \n  request=self.build_request(\n  method=method,\n  url=url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  response=self.send(\n  request=request,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  stream=True,\n  )\n  try:\n   yield response\n  finally:\n   response.close()\n   \n def send(\n self,\n request:Request,\n *,\n stream:bool=False,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n )->Response:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  if self._state ==ClientState.CLOSED:\n   raise RuntimeError(\"Cannot send a request, as the client has been closed.\")\n   \n  self._state=ClientState.OPENED\n  follow_redirects=(\n  self.follow_redirects\n  if isinstance(follow_redirects,UseClientDefault)\n  else follow_redirects\n  )\n  \n  self._set_timeout(request)\n  \n  auth=self._build_request_auth(request,auth)\n  \n  response=self._send_handling_auth(\n  request,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  history=[],\n  )\n  try:\n   if not stream:\n    response.read()\n    \n   return response\n   \n  except BaseException as exc:\n   response.close()\n   raise exc\n   \n def _send_handling_auth(\n self,\n request:Request,\n auth:Auth,\n follow_redirects:bool,\n history:list[Response],\n )->Response:\n  auth_flow=auth.sync_auth_flow(request)\n  try:\n   request=next(auth_flow)\n   \n   while True:\n    response=self._send_handling_redirects(\n    request,\n    follow_redirects=follow_redirects,\n    history=history,\n    )\n    try:\n     try:\n      next_request=auth_flow.send(response)\n     except StopIteration:\n      return response\n      \n     response.history=list(history)\n     response.read()\n     request=next_request\n     history.append(response)\n     \n    except BaseException as exc:\n     response.close()\n     raise exc\n  finally:\n   auth_flow.close()\n   \n def _send_handling_redirects(\n self,\n request:Request,\n follow_redirects:bool,\n history:list[Response],\n )->Response:\n  while True:\n   if len(history)>self.max_redirects:\n    raise TooManyRedirects(\n    \"Exceeded maximum allowed redirects.\",request=request\n    )\n    \n   for hook in self._event_hooks[\"request\"]:\n    hook(request)\n    \n   response=self._send_single_request(request)\n   try:\n    for hook in self._event_hooks[\"response\"]:\n     hook(response)\n    response.history=list(history)\n    \n    if not response.has_redirect_location:\n     return response\n     \n    request=self._build_redirect_request(request,response)\n    history=history+[response]\n    \n    if follow_redirects:\n     response.read()\n    else:\n     response.next_request=request\n     return response\n     \n   except BaseException as exc:\n    response.close()\n    raise exc\n    \n def _send_single_request(self,request:Request)->Response:\n  ''\n\n  \n  transport=self._transport_for_url(request.url)\n  start=time.perf_counter()\n  \n  if not isinstance(request.stream,SyncByteStream):\n   raise RuntimeError(\n   \"Attempted to send an async request with a sync Client instance.\"\n   )\n   \n  with request_context(request=request):\n   response=transport.handle_request(request)\n   \n  assert isinstance(response.stream,SyncByteStream)\n  \n  response.request=request\n  response.stream=BoundSyncStream(\n  response.stream,response=response,start=start\n  )\n  self.cookies.extract_cookies(response)\n  response.default_encoding=self._default_encoding\n  \n  logger.info(\n  'HTTP Request: %s %s \"%s %d %s\"',\n  request.method,\n  request.url,\n  response.http_version,\n  response.status_code,\n  response.reason_phrase,\n  )\n  \n  return response\n  \n def get(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"GET\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def options(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"OPTIONS\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def head(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"HEAD\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def post(\n self,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"POST\",\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def put(\n self,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"PUT\",\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def patch(\n self,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"PATCH\",\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def delete(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return self.request(\n  \"DELETE\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n def close(self)->None:\n  ''\n\n  \n  if self._state !=ClientState.CLOSED:\n   self._state=ClientState.CLOSED\n   \n   self._transport.close()\n   for transport in self._mounts.values():\n    if transport is not None:\n     transport.close()\n     \n def __enter__(self:T)->T:\n  if self._state !=ClientState.UNOPENED:\n   msg={\n   ClientState.OPENED:\"Cannot open a client instance more than once.\",\n   ClientState.CLOSED:(\n   \"Cannot reopen a client instance, once it has been closed.\"\n   ),\n   }[self._state]\n   raise RuntimeError(msg)\n   \n  self._state=ClientState.OPENED\n  \n  self._transport.__enter__()\n  for transport in self._mounts.values():\n   if transport is not None:\n    transport.__enter__()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:TracebackType |None=None,\n )->None:\n  self._state=ClientState.CLOSED\n  \n  self._transport.__exit__(exc_type,exc_value,traceback)\n  for transport in self._mounts.values():\n   if transport is not None:\n    transport.__exit__(exc_type,exc_value,traceback)\n    \n    \nclass AsyncClient(BaseClient):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n *,\n auth:AuthTypes |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n http1:bool=True,\n http2:bool=False,\n proxy:ProxyTypes |None=None,\n mounts:None |(typing.Mapping[str,AsyncBaseTransport |None])=None,\n timeout:TimeoutTypes=DEFAULT_TIMEOUT_CONFIG,\n follow_redirects:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n max_redirects:int=DEFAULT_MAX_REDIRECTS,\n event_hooks:None |(typing.Mapping[str,list[EventHook]])=None,\n base_url:URL |str=\"\",\n transport:AsyncBaseTransport |None=None,\n trust_env:bool=True,\n default_encoding:str |typing.Callable[[bytes],str]=\"utf-8\",\n )->None:\n  super().__init__(\n  auth=auth,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  timeout=timeout,\n  follow_redirects=follow_redirects,\n  max_redirects=max_redirects,\n  event_hooks=event_hooks,\n  base_url=base_url,\n  trust_env=trust_env,\n  default_encoding=default_encoding,\n  )\n  \n  if http2:\n   try:\n    import h2\n   except ImportError:\n    raise ImportError(\n    \"Using http2=True, but the 'h2' package is not installed. \"\n    \"Make sure to install httpx using `pip install httpx[http2]`.\"\n    )from None\n    \n  allow_env_proxies=trust_env and transport is None\n  proxy_map=self._get_proxy_map(proxy,allow_env_proxies)\n  \n  self._transport=self._init_transport(\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  transport=transport,\n  )\n  \n  self._mounts:dict[URLPattern,AsyncBaseTransport |None]={\n  URLPattern(key):None\n  if proxy is None\n  else self._init_proxy_transport(\n  proxy,\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  )\n  for key,proxy in proxy_map.items()\n  }\n  if mounts is not None:\n   self._mounts.update(\n   {URLPattern(key):transport for key,transport in mounts.items()}\n   )\n  self._mounts=dict(sorted(self._mounts.items()))\n  \n def _init_transport(\n self,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n transport:AsyncBaseTransport |None=None,\n )->AsyncBaseTransport:\n  if transport is not None:\n   return transport\n   \n  return AsyncHTTPTransport(\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  )\n  \n def _init_proxy_transport(\n self,\n proxy:Proxy,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n )->AsyncBaseTransport:\n  return AsyncHTTPTransport(\n  verify=verify,\n  cert=cert,\n  trust_env=trust_env,\n  http1=http1,\n  http2=http2,\n  limits=limits,\n  proxy=proxy,\n  )\n  \n def _transport_for_url(self,url:URL)->AsyncBaseTransport:\n  ''\n\n\n  \n  for pattern,transport in self._mounts.items():\n   if pattern.matches(url):\n    return self._transport if transport is None else transport\n    \n  return self._transport\n  \n async def request(\n self,\n method:str,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  if cookies is not None:\n   message=(\n   \"Setting per-request cookies=<...> is being deprecated, because \"\n   \"the expected behaviour on cookie persistence is ambiguous. Set \"\n   \"cookies directly on the client instance instead.\"\n   )\n   warnings.warn(message,DeprecationWarning,stacklevel=2)\n   \n  request=self.build_request(\n  method=method,\n  url=url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  return await self.send(request,auth=auth,follow_redirects=follow_redirects)\n  \n @asynccontextmanager\n async def stream(\n self,\n method:str,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->typing.AsyncIterator[Response]:\n  ''\n\n\n\n\n\n\n\n\n  \n  request=self.build_request(\n  method=method,\n  url=url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  response=await self.send(\n  request=request,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  stream=True,\n  )\n  try:\n   yield response\n  finally:\n   await response.aclose()\n   \n async def send(\n self,\n request:Request,\n *,\n stream:bool=False,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n )->Response:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n  \n  if self._state ==ClientState.CLOSED:\n   raise RuntimeError(\"Cannot send a request, as the client has been closed.\")\n   \n  self._state=ClientState.OPENED\n  follow_redirects=(\n  self.follow_redirects\n  if isinstance(follow_redirects,UseClientDefault)\n  else follow_redirects\n  )\n  \n  self._set_timeout(request)\n  \n  auth=self._build_request_auth(request,auth)\n  \n  response=await self._send_handling_auth(\n  request,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  history=[],\n  )\n  try:\n   if not stream:\n    await response.aread()\n    \n   return response\n   \n  except BaseException as exc:\n   await response.aclose()\n   raise exc\n   \n async def _send_handling_auth(\n self,\n request:Request,\n auth:Auth,\n follow_redirects:bool,\n history:list[Response],\n )->Response:\n  auth_flow=auth.async_auth_flow(request)\n  try:\n   request=await auth_flow.__anext__()\n   \n   while True:\n    response=await self._send_handling_redirects(\n    request,\n    follow_redirects=follow_redirects,\n    history=history,\n    )\n    try:\n     try:\n      next_request=await auth_flow.asend(response)\n     except StopAsyncIteration:\n      return response\n      \n     response.history=list(history)\n     await response.aread()\n     request=next_request\n     history.append(response)\n     \n    except BaseException as exc:\n     await response.aclose()\n     raise exc\n  finally:\n   await auth_flow.aclose()\n   \n async def _send_handling_redirects(\n self,\n request:Request,\n follow_redirects:bool,\n history:list[Response],\n )->Response:\n  while True:\n   if len(history)>self.max_redirects:\n    raise TooManyRedirects(\n    \"Exceeded maximum allowed redirects.\",request=request\n    )\n    \n   for hook in self._event_hooks[\"request\"]:\n    await hook(request)\n    \n   response=await self._send_single_request(request)\n   try:\n    for hook in self._event_hooks[\"response\"]:\n     await hook(response)\n     \n    response.history=list(history)\n    \n    if not response.has_redirect_location:\n     return response\n     \n    request=self._build_redirect_request(request,response)\n    history=history+[response]\n    \n    if follow_redirects:\n     await response.aread()\n    else:\n     response.next_request=request\n     return response\n     \n   except BaseException as exc:\n    await response.aclose()\n    raise exc\n    \n async def _send_single_request(self,request:Request)->Response:\n  ''\n\n  \n  transport=self._transport_for_url(request.url)\n  start=time.perf_counter()\n  \n  if not isinstance(request.stream,AsyncByteStream):\n   raise RuntimeError(\n   \"Attempted to send an sync request with an AsyncClient instance.\"\n   )\n   \n  with request_context(request=request):\n   response=await transport.handle_async_request(request)\n   \n  assert isinstance(response.stream,AsyncByteStream)\n  response.request=request\n  response.stream=BoundAsyncStream(\n  response.stream,response=response,start=start\n  )\n  self.cookies.extract_cookies(response)\n  response.default_encoding=self._default_encoding\n  \n  logger.info(\n  'HTTP Request: %s %s \"%s %d %s\"',\n  request.method,\n  request.url,\n  response.http_version,\n  response.status_code,\n  response.reason_phrase,\n  )\n  \n  return response\n  \n async def get(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault |None=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"GET\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def options(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"OPTIONS\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def head(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"HEAD\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def post(\n self,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"POST\",\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def put(\n self,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"PUT\",\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def patch(\n self,\n url:URL |str,\n *,\n content:RequestContent |None=None,\n data:RequestData |None=None,\n files:RequestFiles |None=None,\n json:typing.Any |None=None,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"PATCH\",\n  url,\n  content=content,\n  data=data,\n  files=files,\n  json=json,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def delete(\n self,\n url:URL |str,\n *,\n params:QueryParamTypes |None=None,\n headers:HeaderTypes |None=None,\n cookies:CookieTypes |None=None,\n auth:AuthTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n follow_redirects:bool |UseClientDefault=USE_CLIENT_DEFAULT,\n timeout:TimeoutTypes |UseClientDefault=USE_CLIENT_DEFAULT,\n extensions:RequestExtensions |None=None,\n )->Response:\n  ''\n\n\n\n  \n  return await self.request(\n  \"DELETE\",\n  url,\n  params=params,\n  headers=headers,\n  cookies=cookies,\n  auth=auth,\n  follow_redirects=follow_redirects,\n  timeout=timeout,\n  extensions=extensions,\n  )\n  \n async def aclose(self)->None:\n  ''\n\n  \n  if self._state !=ClientState.CLOSED:\n   self._state=ClientState.CLOSED\n   \n   await self._transport.aclose()\n   for proxy in self._mounts.values():\n    if proxy is not None:\n     await proxy.aclose()\n     \n async def __aenter__(self:U)->U:\n  if self._state !=ClientState.UNOPENED:\n   msg={\n   ClientState.OPENED:\"Cannot open a client instance more than once.\",\n   ClientState.CLOSED:(\n   \"Cannot reopen a client instance, once it has been closed.\"\n   ),\n   }[self._state]\n   raise RuntimeError(msg)\n   \n  self._state=ClientState.OPENED\n  \n  await self._transport.__aenter__()\n  for proxy in self._mounts.values():\n   if proxy is not None:\n    await proxy.__aenter__()\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:TracebackType |None=None,\n )->None:\n  self._state=ClientState.CLOSED\n  \n  await self._transport.__aexit__(exc_type,exc_value,traceback)\n  for proxy in self._mounts.values():\n   if proxy is not None:\n    await proxy.__aexit__(exc_type,exc_value,traceback)\n", ["__future__", "contextlib", "datetime", "enum", "h2", "httpx.__version__", "httpx._auth", "httpx._config", "httpx._decoders", "httpx._exceptions", "httpx._models", "httpx._status_codes", "httpx._transports.base", "httpx._transports.default", "httpx._types", "httpx._urls", "httpx._utils", "logging", "ssl", "time", "types", "typing", "warnings"]], "httpx._content": [".py", "from __future__ import annotations\n\nimport inspect\nimport warnings\nfrom json import dumps as json_dumps\nfrom typing import(\nAny,\nAsyncIterable,\nAsyncIterator,\nIterable,\nIterator,\nMapping,\n)\nfrom urllib.parse import urlencode\n\nfrom._exceptions import StreamClosed,StreamConsumed\nfrom._multipart import MultipartStream\nfrom._types import(\nAsyncByteStream,\nRequestContent,\nRequestData,\nRequestFiles,\nResponseContent,\nSyncByteStream,\n)\nfrom._utils import peek_filelike_length,primitive_value_to_str\n\n__all__=[\"ByteStream\"]\n\n\nclass ByteStream(AsyncByteStream,SyncByteStream):\n def __init__(self,stream:bytes)->None:\n  self._stream=stream\n  \n def __iter__(self)->Iterator[bytes]:\n  yield self._stream\n  \n async def __aiter__(self)->AsyncIterator[bytes]:\n  yield self._stream\n  \n  \nclass IteratorByteStream(SyncByteStream):\n CHUNK_SIZE=65_536\n \n def __init__(self,stream:Iterable[bytes])->None:\n  self._stream=stream\n  self._is_stream_consumed=False\n  self._is_generator=inspect.isgenerator(stream)\n  \n def __iter__(self)->Iterator[bytes]:\n  if self._is_stream_consumed and self._is_generator:\n   raise StreamConsumed()\n   \n  self._is_stream_consumed=True\n  if hasattr(self._stream,\"read\"):\n  \n   chunk=self._stream.read(self.CHUNK_SIZE)\n   while chunk:\n    yield chunk\n    chunk=self._stream.read(self.CHUNK_SIZE)\n  else:\n  \n   for part in self._stream:\n    yield part\n    \n    \nclass AsyncIteratorByteStream(AsyncByteStream):\n CHUNK_SIZE=65_536\n \n def __init__(self,stream:AsyncIterable[bytes])->None:\n  self._stream=stream\n  self._is_stream_consumed=False\n  self._is_generator=inspect.isasyncgen(stream)\n  \n async def __aiter__(self)->AsyncIterator[bytes]:\n  if self._is_stream_consumed and self._is_generator:\n   raise StreamConsumed()\n   \n  self._is_stream_consumed=True\n  if hasattr(self._stream,\"aread\"):\n  \n   chunk=await self._stream.aread(self.CHUNK_SIZE)\n   while chunk:\n    yield chunk\n    chunk=await self._stream.aread(self.CHUNK_SIZE)\n  else:\n  \n   async for part in self._stream:\n    yield part\n    \n    \nclass UnattachedStream(AsyncByteStream,SyncByteStream):\n ''\n\n\n\n \n \n def __iter__(self)->Iterator[bytes]:\n  raise StreamClosed()\n  \n async def __aiter__(self)->AsyncIterator[bytes]:\n  raise StreamClosed()\n  yield b\"\"\n  \n  \ndef encode_content(\ncontent:str |bytes |Iterable[bytes]|AsyncIterable[bytes],\n)->tuple[dict[str,str],SyncByteStream |AsyncByteStream]:\n if isinstance(content,(bytes,str)):\n  body=content.encode(\"utf-8\")if isinstance(content,str)else content\n  content_length=len(body)\n  headers={\"Content-Length\":str(content_length)}if body else{}\n  return headers,ByteStream(body)\n  \n elif isinstance(content,Iterable)and not isinstance(content,dict):\n \n \n \n \n  content_length_or_none=peek_filelike_length(content)\n  \n  if content_length_or_none is None:\n   headers={\"Transfer-Encoding\":\"chunked\"}\n  else:\n   headers={\"Content-Length\":str(content_length_or_none)}\n  return headers,IteratorByteStream(content)\n  \n elif isinstance(content,AsyncIterable):\n  headers={\"Transfer-Encoding\":\"chunked\"}\n  return headers,AsyncIteratorByteStream(content)\n  \n raise TypeError(f\"Unexpected type for 'content', {type(content)!r}\")\n \n \ndef encode_urlencoded_data(\ndata:RequestData,\n)->tuple[dict[str,str],ByteStream]:\n plain_data=[]\n for key,value in data.items():\n  if isinstance(value,(list,tuple)):\n   plain_data.extend([(key,primitive_value_to_str(item))for item in value])\n  else:\n   plain_data.append((key,primitive_value_to_str(value)))\n body=urlencode(plain_data,doseq=True).encode(\"utf-8\")\n content_length=str(len(body))\n content_type=\"application/x-www-form-urlencoded\"\n headers={\"Content-Length\":content_length,\"Content-Type\":content_type}\n return headers,ByteStream(body)\n \n \ndef encode_multipart_data(\ndata:RequestData,files:RequestFiles,boundary:bytes |None\n)->tuple[dict[str,str],MultipartStream]:\n multipart=MultipartStream(data=data,files=files,boundary=boundary)\n headers=multipart.get_headers()\n return headers,multipart\n \n \ndef encode_text(text:str)->tuple[dict[str,str],ByteStream]:\n body=text.encode(\"utf-8\")\n content_length=str(len(body))\n content_type=\"text/plain; charset=utf-8\"\n headers={\"Content-Length\":content_length,\"Content-Type\":content_type}\n return headers,ByteStream(body)\n \n \ndef encode_html(html:str)->tuple[dict[str,str],ByteStream]:\n body=html.encode(\"utf-8\")\n content_length=str(len(body))\n content_type=\"text/html; charset=utf-8\"\n headers={\"Content-Length\":content_length,\"Content-Type\":content_type}\n return headers,ByteStream(body)\n \n \ndef encode_json(json:Any)->tuple[dict[str,str],ByteStream]:\n body=json_dumps(\n json,ensure_ascii=False,separators=(\",\",\":\"),allow_nan=False\n ).encode(\"utf-8\")\n content_length=str(len(body))\n content_type=\"application/json\"\n headers={\"Content-Length\":content_length,\"Content-Type\":content_type}\n return headers,ByteStream(body)\n \n \ndef encode_request(\ncontent:RequestContent |None=None,\ndata:RequestData |None=None,\nfiles:RequestFiles |None=None,\njson:Any |None=None,\nboundary:bytes |None=None,\n)->tuple[dict[str,str],SyncByteStream |AsyncByteStream]:\n ''\n\n\n \n if data is not None and not isinstance(data,Mapping):\n \n \n \n \n \n \n \n  message=\"Use 'content=<...>' to upload raw bytes/text content.\"\n  warnings.warn(message,DeprecationWarning,stacklevel=2)\n  return encode_content(data)\n  \n if content is not None:\n  return encode_content(content)\n elif files:\n  return encode_multipart_data(data or{},files,boundary)\n elif data:\n  return encode_urlencoded_data(data)\n elif json is not None:\n  return encode_json(json)\n  \n return{},ByteStream(b\"\")\n \n \ndef encode_response(\ncontent:ResponseContent |None=None,\ntext:str |None=None,\nhtml:str |None=None,\njson:Any |None=None,\n)->tuple[dict[str,str],SyncByteStream |AsyncByteStream]:\n ''\n\n\n \n if content is not None:\n  return encode_content(content)\n elif text is not None:\n  return encode_text(text)\n elif html is not None:\n  return encode_html(html)\n elif json is not None:\n  return encode_json(json)\n  \n return{},ByteStream(b\"\")\n", ["__future__", "httpx._exceptions", "httpx._multipart", "httpx._types", "httpx._utils", "inspect", "json", "typing", "urllib.parse", "warnings"]], "httpx._transports": [".py", "from.asgi import *\nfrom.base import *\nfrom.default import *\nfrom.mock import *\nfrom.wsgi import *\n\n__all__=[\n\"ASGITransport\",\n\"AsyncBaseTransport\",\n\"BaseTransport\",\n\"AsyncHTTPTransport\",\n\"HTTPTransport\",\n\"MockTransport\",\n\"WSGITransport\",\n]\n", ["httpx._transports.asgi", "httpx._transports.base", "httpx._transports.default", "httpx._transports.mock", "httpx._transports.wsgi"], 1], "httpx._transports.base": [".py", "from __future__ import annotations\n\nimport typing\nfrom types import TracebackType\n\nfrom.._models import Request,Response\n\nT=typing.TypeVar(\"T\",bound=\"BaseTransport\")\nA=typing.TypeVar(\"A\",bound=\"AsyncBaseTransport\")\n\n__all__=[\"AsyncBaseTransport\",\"BaseTransport\"]\n\n\nclass BaseTransport:\n def __enter__(self:T)->T:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:TracebackType |None=None,\n )->None:\n  self.close()\n  \n def handle_request(self,request:Request)->Response:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError(\n  \"The 'handle_request' method must be implemented.\"\n  )\n  \n def close(self)->None:\n  pass\n  \n  \nclass AsyncBaseTransport:\n async def __aenter__(self:A)->A:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:TracebackType |None=None,\n )->None:\n  await self.aclose()\n  \n async def handle_async_request(\n self,\n request:Request,\n )->Response:\n  raise NotImplementedError(\n  \"The 'handle_async_request' method must be implemented.\"\n  )\n  \n async def aclose(self)->None:\n  pass\n", ["__future__", "httpx._models", "types", "typing"]], "httpx._transports.wsgi": [".py", "from __future__ import annotations\n\nimport io\nimport itertools\nimport sys\nimport typing\n\nfrom.._models import Request,Response\nfrom.._types import SyncByteStream\nfrom.base import BaseTransport\n\nif typing.TYPE_CHECKING:\n from _typeshed import OptExcInfo\n from _typeshed.wsgi import WSGIApplication\n \n_T=typing.TypeVar(\"_T\")\n\n\n__all__=[\"WSGITransport\"]\n\n\ndef _skip_leading_empty_chunks(body:typing.Iterable[_T])->typing.Iterable[_T]:\n body=iter(body)\n for chunk in body:\n  if chunk:\n   return itertools.chain([chunk],body)\n return[]\n \n \nclass WSGIByteStream(SyncByteStream):\n def __init__(self,result:typing.Iterable[bytes])->None:\n  self._close=getattr(result,\"close\",None)\n  self._result=_skip_leading_empty_chunks(result)\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  for part in self._result:\n   yield part\n   \n def close(self)->None:\n  if self._close is not None:\n   self._close()\n   \n   \nclass WSGITransport(BaseTransport):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n app:WSGIApplication,\n raise_app_exceptions:bool=True,\n script_name:str=\"\",\n remote_addr:str=\"127.0.0.1\",\n wsgi_errors:typing.TextIO |None=None,\n )->None:\n  self.app=app\n  self.raise_app_exceptions=raise_app_exceptions\n  self.script_name=script_name\n  self.remote_addr=remote_addr\n  self.wsgi_errors=wsgi_errors\n  \n def handle_request(self,request:Request)->Response:\n  request.read()\n  wsgi_input=io.BytesIO(request.content)\n  \n  port=request.url.port or{\"http\":80,\"https\":443}[request.url.scheme]\n  environ={\n  \"wsgi.version\":(1,0),\n  \"wsgi.url_scheme\":request.url.scheme,\n  \"wsgi.input\":wsgi_input,\n  \"wsgi.errors\":self.wsgi_errors or sys.stderr,\n  \"wsgi.multithread\":True,\n  \"wsgi.multiprocess\":False,\n  \"wsgi.run_once\":False,\n  \"REQUEST_METHOD\":request.method,\n  \"SCRIPT_NAME\":self.script_name,\n  \"PATH_INFO\":request.url.path,\n  \"QUERY_STRING\":request.url.query.decode(\"ascii\"),\n  \"SERVER_NAME\":request.url.host,\n  \"SERVER_PORT\":str(port),\n  \"SERVER_PROTOCOL\":\"HTTP/1.1\",\n  \"REMOTE_ADDR\":self.remote_addr,\n  }\n  for header_key,header_value in request.headers.raw:\n   key=header_key.decode(\"ascii\").upper().replace(\"-\",\"_\")\n   if key not in(\"CONTENT_TYPE\",\"CONTENT_LENGTH\"):\n    key=\"HTTP_\"+key\n   environ[key]=header_value.decode(\"ascii\")\n   \n  seen_status=None\n  seen_response_headers=None\n  seen_exc_info=None\n  \n  def start_response(\n  status:str,\n  response_headers:list[tuple[str,str]],\n  exc_info:OptExcInfo |None=None,\n  )->typing.Callable[[bytes],typing.Any]:\n   nonlocal seen_status,seen_response_headers,seen_exc_info\n   seen_status=status\n   seen_response_headers=response_headers\n   seen_exc_info=exc_info\n   return lambda _:None\n   \n  result=self.app(environ,start_response)\n  \n  stream=WSGIByteStream(result)\n  \n  assert seen_status is not None\n  assert seen_response_headers is not None\n  if seen_exc_info and seen_exc_info[0]and self.raise_app_exceptions:\n   raise seen_exc_info[1]\n   \n  status_code=int(seen_status.split()[0])\n  headers=[\n  (key.encode(\"ascii\"),value.encode(\"ascii\"))\n  for key,value in seen_response_headers\n  ]\n  \n  return Response(status_code,headers=headers,stream=stream)\n", ["__future__", "_typeshed", "_typeshed.wsgi", "httpx._models", "httpx._transports.base", "httpx._types", "io", "itertools", "sys", "typing"]], "httpx._transports.mock": [".py", "from __future__ import annotations\n\nimport typing\n\nfrom.._models import Request,Response\nfrom.base import AsyncBaseTransport,BaseTransport\n\nSyncHandler=typing.Callable[[Request],Response]\nAsyncHandler=typing.Callable[[Request],typing.Coroutine[None,None,Response]]\n\n\n__all__=[\"MockTransport\"]\n\n\nclass MockTransport(AsyncBaseTransport,BaseTransport):\n def __init__(self,handler:SyncHandler |AsyncHandler)->None:\n  self.handler=handler\n  \n def handle_request(\n self,\n request:Request,\n )->Response:\n  request.read()\n  response=self.handler(request)\n  if not isinstance(response,Response):\n   raise TypeError(\"Cannot use an async handler in a sync Client\")\n  return response\n  \n async def handle_async_request(\n self,\n request:Request,\n )->Response:\n  await request.aread()\n  response=self.handler(request)\n  \n  \n  \n  \n  \n  if not isinstance(response,Response):\n   response=await response\n   \n  return response\n", ["__future__", "httpx._models", "httpx._transports.base", "typing"]], "httpx._transports.default": [".py", "''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import annotations\n\nimport contextlib\nimport typing\nfrom types import TracebackType\n\nif typing.TYPE_CHECKING:\n import ssl\n \n import httpx\n \nfrom.._config import DEFAULT_LIMITS,Limits,Proxy,create_ssl_context\nfrom.._exceptions import(\nConnectError,\nConnectTimeout,\nLocalProtocolError,\nNetworkError,\nPoolTimeout,\nProtocolError,\nProxyError,\nReadError,\nReadTimeout,\nRemoteProtocolError,\nTimeoutException,\nUnsupportedProtocol,\nWriteError,\nWriteTimeout,\n)\nfrom.._models import Request,Response\nfrom.._types import AsyncByteStream,CertTypes,ProxyTypes,SyncByteStream\nfrom.._urls import URL\nfrom.base import AsyncBaseTransport,BaseTransport\n\nT=typing.TypeVar(\"T\",bound=\"HTTPTransport\")\nA=typing.TypeVar(\"A\",bound=\"AsyncHTTPTransport\")\n\nSOCKET_OPTION=typing.Union[\ntyping.Tuple[int,int,int],\ntyping.Tuple[int,int,typing.Union[bytes,bytearray]],\ntyping.Tuple[int,int,None,int],\n]\n\n__all__=[\"AsyncHTTPTransport\",\"HTTPTransport\"]\n\nHTTPCORE_EXC_MAP:dict[type[Exception],type[httpx.HTTPError]]={}\n\n\ndef _load_httpcore_exceptions()->dict[type[Exception],type[httpx.HTTPError]]:\n import httpcore\n \n return{\n httpcore.TimeoutException:TimeoutException,\n httpcore.ConnectTimeout:ConnectTimeout,\n httpcore.ReadTimeout:ReadTimeout,\n httpcore.WriteTimeout:WriteTimeout,\n httpcore.PoolTimeout:PoolTimeout,\n httpcore.NetworkError:NetworkError,\n httpcore.ConnectError:ConnectError,\n httpcore.ReadError:ReadError,\n httpcore.WriteError:WriteError,\n httpcore.ProxyError:ProxyError,\n httpcore.UnsupportedProtocol:UnsupportedProtocol,\n httpcore.ProtocolError:ProtocolError,\n httpcore.LocalProtocolError:LocalProtocolError,\n httpcore.RemoteProtocolError:RemoteProtocolError,\n }\n \n \n@contextlib.contextmanager\ndef map_httpcore_exceptions()->typing.Iterator[None]:\n global HTTPCORE_EXC_MAP\n if len(HTTPCORE_EXC_MAP)==0:\n  HTTPCORE_EXC_MAP=_load_httpcore_exceptions()\n try:\n  yield\n except Exception as exc:\n  mapped_exc=None\n  \n  for from_exc,to_exc in HTTPCORE_EXC_MAP.items():\n   if not isinstance(exc,from_exc):\n    continue\n    \n    \n    \n   if mapped_exc is None or issubclass(to_exc,mapped_exc):\n    mapped_exc=to_exc\n    \n  if mapped_exc is None:\n   raise\n   \n  message=str(exc)\n  raise mapped_exc(message)from exc\n  \n  \nclass ResponseStream(SyncByteStream):\n def __init__(self,httpcore_stream:typing.Iterable[bytes])->None:\n  self._httpcore_stream=httpcore_stream\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  with map_httpcore_exceptions():\n   for part in self._httpcore_stream:\n    yield part\n    \n def close(self)->None:\n  if hasattr(self._httpcore_stream,\"close\"):\n   self._httpcore_stream.close()\n   \n   \nclass HTTPTransport(BaseTransport):\n def __init__(\n self,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n proxy:ProxyTypes |None=None,\n uds:str |None=None,\n local_address:str |None=None,\n retries:int=0,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  import httpcore\n  \n  proxy=Proxy(url=proxy)if isinstance(proxy,(str,URL))else proxy\n  ssl_context=create_ssl_context(verify=verify,cert=cert,trust_env=trust_env)\n  \n  if proxy is None:\n   self._pool=httpcore.ConnectionPool(\n   ssl_context=ssl_context,\n   max_connections=limits.max_connections,\n   max_keepalive_connections=limits.max_keepalive_connections,\n   keepalive_expiry=limits.keepalive_expiry,\n   http1=http1,\n   http2=http2,\n   uds=uds,\n   local_address=local_address,\n   retries=retries,\n   socket_options=socket_options,\n   )\n  elif proxy.url.scheme in(\"http\",\"https\"):\n   self._pool=httpcore.HTTPProxy(\n   proxy_url=httpcore.URL(\n   scheme=proxy.url.raw_scheme,\n   host=proxy.url.raw_host,\n   port=proxy.url.port,\n   target=proxy.url.raw_path,\n   ),\n   proxy_auth=proxy.raw_auth,\n   proxy_headers=proxy.headers.raw,\n   ssl_context=ssl_context,\n   proxy_ssl_context=proxy.ssl_context,\n   max_connections=limits.max_connections,\n   max_keepalive_connections=limits.max_keepalive_connections,\n   keepalive_expiry=limits.keepalive_expiry,\n   http1=http1,\n   http2=http2,\n   socket_options=socket_options,\n   )\n  elif proxy.url.scheme in(\"socks5\",\"socks5h\"):\n   try:\n    import socksio\n   except ImportError:\n    raise ImportError(\n    \"Using SOCKS proxy, but the 'socksio' package is not installed. \"\n    \"Make sure to install httpx using `pip install httpx[socks]`.\"\n    )from None\n    \n   self._pool=httpcore.SOCKSProxy(\n   proxy_url=httpcore.URL(\n   scheme=proxy.url.raw_scheme,\n   host=proxy.url.raw_host,\n   port=proxy.url.port,\n   target=proxy.url.raw_path,\n   ),\n   proxy_auth=proxy.raw_auth,\n   ssl_context=ssl_context,\n   max_connections=limits.max_connections,\n   max_keepalive_connections=limits.max_keepalive_connections,\n   keepalive_expiry=limits.keepalive_expiry,\n   http1=http1,\n   http2=http2,\n   )\n  else:\n   raise ValueError(\n   \"Proxy protocol must be either 'http', 'https', 'socks5', or 'socks5h',\"\n   f\" but got {proxy.url.scheme !r}.\"\n   )\n   \n def __enter__(self:T)->T:\n  self._pool.__enter__()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:TracebackType |None=None,\n )->None:\n  with map_httpcore_exceptions():\n   self._pool.__exit__(exc_type,exc_value,traceback)\n   \n def handle_request(\n self,\n request:Request,\n )->Response:\n  assert isinstance(request.stream,SyncByteStream)\n  import httpcore\n  \n  req=httpcore.Request(\n  method=request.method,\n  url=httpcore.URL(\n  scheme=request.url.raw_scheme,\n  host=request.url.raw_host,\n  port=request.url.port,\n  target=request.url.raw_path,\n  ),\n  headers=request.headers.raw,\n  content=request.stream,\n  extensions=request.extensions,\n  )\n  with map_httpcore_exceptions():\n   resp=self._pool.handle_request(req)\n   \n  assert isinstance(resp.stream,typing.Iterable)\n  \n  return Response(\n  status_code=resp.status,\n  headers=resp.headers,\n  stream=ResponseStream(resp.stream),\n  extensions=resp.extensions,\n  )\n  \n def close(self)->None:\n  self._pool.close()\n  \n  \nclass AsyncResponseStream(AsyncByteStream):\n def __init__(self,httpcore_stream:typing.AsyncIterable[bytes])->None:\n  self._httpcore_stream=httpcore_stream\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  with map_httpcore_exceptions():\n   async for part in self._httpcore_stream:\n    yield part\n    \n async def aclose(self)->None:\n  if hasattr(self._httpcore_stream,\"aclose\"):\n   await self._httpcore_stream.aclose()\n   \n   \nclass AsyncHTTPTransport(AsyncBaseTransport):\n def __init__(\n self,\n verify:ssl.SSLContext |str |bool=True,\n cert:CertTypes |None=None,\n trust_env:bool=True,\n http1:bool=True,\n http2:bool=False,\n limits:Limits=DEFAULT_LIMITS,\n proxy:ProxyTypes |None=None,\n uds:str |None=None,\n local_address:str |None=None,\n retries:int=0,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  import httpcore\n  \n  proxy=Proxy(url=proxy)if isinstance(proxy,(str,URL))else proxy\n  ssl_context=create_ssl_context(verify=verify,cert=cert,trust_env=trust_env)\n  \n  if proxy is None:\n   self._pool=httpcore.AsyncConnectionPool(\n   ssl_context=ssl_context,\n   max_connections=limits.max_connections,\n   max_keepalive_connections=limits.max_keepalive_connections,\n   keepalive_expiry=limits.keepalive_expiry,\n   http1=http1,\n   http2=http2,\n   uds=uds,\n   local_address=local_address,\n   retries=retries,\n   socket_options=socket_options,\n   )\n  elif proxy.url.scheme in(\"http\",\"https\"):\n   self._pool=httpcore.AsyncHTTPProxy(\n   proxy_url=httpcore.URL(\n   scheme=proxy.url.raw_scheme,\n   host=proxy.url.raw_host,\n   port=proxy.url.port,\n   target=proxy.url.raw_path,\n   ),\n   proxy_auth=proxy.raw_auth,\n   proxy_headers=proxy.headers.raw,\n   proxy_ssl_context=proxy.ssl_context,\n   ssl_context=ssl_context,\n   max_connections=limits.max_connections,\n   max_keepalive_connections=limits.max_keepalive_connections,\n   keepalive_expiry=limits.keepalive_expiry,\n   http1=http1,\n   http2=http2,\n   socket_options=socket_options,\n   )\n  elif proxy.url.scheme in(\"socks5\",\"socks5h\"):\n   try:\n    import socksio\n   except ImportError:\n    raise ImportError(\n    \"Using SOCKS proxy, but the 'socksio' package is not installed. \"\n    \"Make sure to install httpx using `pip install httpx[socks]`.\"\n    )from None\n    \n   self._pool=httpcore.AsyncSOCKSProxy(\n   proxy_url=httpcore.URL(\n   scheme=proxy.url.raw_scheme,\n   host=proxy.url.raw_host,\n   port=proxy.url.port,\n   target=proxy.url.raw_path,\n   ),\n   proxy_auth=proxy.raw_auth,\n   ssl_context=ssl_context,\n   max_connections=limits.max_connections,\n   max_keepalive_connections=limits.max_keepalive_connections,\n   keepalive_expiry=limits.keepalive_expiry,\n   http1=http1,\n   http2=http2,\n   )\n  else:\n   raise ValueError(\n   \"Proxy protocol must be either 'http', 'https', 'socks5', or 'socks5h',\"\n   \" but got {proxy.url.scheme!r}.\"\n   )\n   \n async def __aenter__(self:A)->A:\n  await self._pool.__aenter__()\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:TracebackType |None=None,\n )->None:\n  with map_httpcore_exceptions():\n   await self._pool.__aexit__(exc_type,exc_value,traceback)\n   \n async def handle_async_request(\n self,\n request:Request,\n )->Response:\n  assert isinstance(request.stream,AsyncByteStream)\n  import httpcore\n  \n  req=httpcore.Request(\n  method=request.method,\n  url=httpcore.URL(\n  scheme=request.url.raw_scheme,\n  host=request.url.raw_host,\n  port=request.url.port,\n  target=request.url.raw_path,\n  ),\n  headers=request.headers.raw,\n  content=request.stream,\n  extensions=request.extensions,\n  )\n  with map_httpcore_exceptions():\n   resp=await self._pool.handle_async_request(req)\n   \n  assert isinstance(resp.stream,typing.AsyncIterable)\n  \n  return Response(\n  status_code=resp.status,\n  headers=resp.headers,\n  stream=AsyncResponseStream(resp.stream),\n  extensions=resp.extensions,\n  )\n  \n async def aclose(self)->None:\n  await self._pool.aclose()\n", ["__future__", "contextlib", "httpcore", "httpx", "httpx._config", "httpx._exceptions", "httpx._models", "httpx._transports.base", "httpx._types", "httpx._urls", "socksio", "ssl", "types", "typing"]], "httpx._transports.asgi": [".py", "from __future__ import annotations\n\nimport typing\n\nfrom.._models import Request,Response\nfrom.._types import AsyncByteStream\nfrom.base import AsyncBaseTransport\n\nif typing.TYPE_CHECKING:\n import asyncio\n \n import trio\n \n Event=typing.Union[asyncio.Event,trio.Event]\n \n \n_Message=typing.MutableMapping[str,typing.Any]\n_Receive=typing.Callable[[],typing.Awaitable[_Message]]\n_Send=typing.Callable[\n[typing.MutableMapping[str,typing.Any]],typing.Awaitable[None]\n]\n_ASGIApp=typing.Callable[\n[typing.MutableMapping[str,typing.Any],_Receive,_Send],typing.Awaitable[None]\n]\n\n__all__=[\"ASGITransport\"]\n\n\ndef is_running_trio()->bool:\n try:\n \n \n \n  import sniffio\n  \n  if sniffio.current_async_library()==\"trio\":\n   return True\n except ImportError:\n  pass\n  \n return False\n \n \ndef create_event()->Event:\n if is_running_trio():\n  import trio\n  \n  return trio.Event()\n  \n import asyncio\n \n return asyncio.Event()\n \n \nclass ASGIResponseStream(AsyncByteStream):\n def __init__(self,body:list[bytes])->None:\n  self._body=body\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  yield b\"\".join(self._body)\n  \n  \nclass ASGITransport(AsyncBaseTransport):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n app:_ASGIApp,\n raise_app_exceptions:bool=True,\n root_path:str=\"\",\n client:tuple[str,int]=(\"127.0.0.1\",123),\n )->None:\n  self.app=app\n  self.raise_app_exceptions=raise_app_exceptions\n  self.root_path=root_path\n  self.client=client\n  \n async def handle_async_request(\n self,\n request:Request,\n )->Response:\n  assert isinstance(request.stream,AsyncByteStream)\n  \n  \n  scope={\n  \"type\":\"http\",\n  \"asgi\":{\"version\":\"3.0\"},\n  \"http_version\":\"1.1\",\n  \"method\":request.method,\n  \"headers\":[(k.lower(),v)for(k,v)in request.headers.raw],\n  \"scheme\":request.url.scheme,\n  \"path\":request.url.path,\n  \"raw_path\":request.url.raw_path.split(b\"?\")[0],\n  \"query_string\":request.url.query,\n  \"server\":(request.url.host,request.url.port),\n  \"client\":self.client,\n  \"root_path\":self.root_path,\n  }\n  \n  \n  request_body_chunks=request.stream.__aiter__()\n  request_complete=False\n  \n  \n  status_code=None\n  response_headers=None\n  body_parts=[]\n  response_started=False\n  response_complete=create_event()\n  \n  \n  \n  async def receive()->dict[str,typing.Any]:\n   nonlocal request_complete\n   \n   if request_complete:\n    await response_complete.wait()\n    return{\"type\":\"http.disconnect\"}\n    \n   try:\n    body=await request_body_chunks.__anext__()\n   except StopAsyncIteration:\n    request_complete=True\n    return{\"type\":\"http.request\",\"body\":b\"\",\"more_body\":False}\n   return{\"type\":\"http.request\",\"body\":body,\"more_body\":True}\n   \n  async def send(message:typing.MutableMapping[str,typing.Any])->None:\n   nonlocal status_code,response_headers,response_started\n   \n   if message[\"type\"]==\"http.response.start\":\n    assert not response_started\n    \n    status_code=message[\"status\"]\n    response_headers=message.get(\"headers\",[])\n    response_started=True\n    \n   elif message[\"type\"]==\"http.response.body\":\n    assert not response_complete.is_set()\n    body=message.get(\"body\",b\"\")\n    more_body=message.get(\"more_body\",False)\n    \n    if body and request.method !=\"HEAD\":\n     body_parts.append(body)\n     \n    if not more_body:\n     response_complete.set()\n     \n  try:\n   await self.app(scope,receive,send)\n  except Exception:\n   if self.raise_app_exceptions:\n    raise\n    \n   response_complete.set()\n   if status_code is None:\n    status_code=500\n   if response_headers is None:\n    response_headers={}\n    \n  assert response_complete.is_set()\n  assert status_code is not None\n  assert response_headers is not None\n  \n  stream=ASGIResponseStream(body_parts)\n  \n  return Response(status_code,headers=response_headers,stream=stream)\n", ["__future__", "asyncio", "httpx._models", "httpx._transports.base", "httpx._types", "sniffio", "trio", "typing"]], "httpcore": [".py", "from._api import request,stream\nfrom._async import(\nAsyncConnectionInterface,\nAsyncConnectionPool,\nAsyncHTTP2Connection,\nAsyncHTTP11Connection,\nAsyncHTTPConnection,\nAsyncHTTPProxy,\nAsyncSOCKSProxy,\n)\nfrom._backends.base import(\nSOCKET_OPTION,\nAsyncNetworkBackend,\nAsyncNetworkStream,\nNetworkBackend,\nNetworkStream,\n)\nfrom._backends.mock import AsyncMockBackend,AsyncMockStream,MockBackend,MockStream\nfrom._backends.sync import SyncBackend\nfrom._exceptions import(\nConnectError,\nConnectionNotAvailable,\nConnectTimeout,\nLocalProtocolError,\nNetworkError,\nPoolTimeout,\nProtocolError,\nProxyError,\nReadError,\nReadTimeout,\nRemoteProtocolError,\nTimeoutException,\nUnsupportedProtocol,\nWriteError,\nWriteTimeout,\n)\nfrom._models import URL,Origin,Proxy,Request,Response\nfrom._ssl import default_ssl_context\nfrom._sync import(\nConnectionInterface,\nConnectionPool,\nHTTP2Connection,\nHTTP11Connection,\nHTTPConnection,\nHTTPProxy,\nSOCKSProxy,\n)\n\n\ntry:\n from._backends.anyio import AnyIOBackend\nexcept ImportError:\n\n class AnyIOBackend:\n  def __init__(self,*args,**kwargs):\n   msg=(\n   \"Attempted to use 'httpcore.AnyIOBackend' but 'anyio' is not installed.\"\n   )\n   raise RuntimeError(msg)\n   \n   \n   \ntry:\n from._backends.trio import TrioBackend\nexcept ImportError:\n\n class TrioBackend:\n  def __init__(self,*args,**kwargs):\n   msg=\"Attempted to use 'httpcore.TrioBackend' but 'trio' is not installed.\"\n   raise RuntimeError(msg)\n   \n   \n__all__=[\n\n\"request\",\n\"stream\",\n\n\"Origin\",\n\"URL\",\n\"Request\",\n\"Response\",\n\"Proxy\",\n\n\"AsyncHTTPConnection\",\n\"AsyncConnectionPool\",\n\"AsyncHTTPProxy\",\n\"AsyncHTTP11Connection\",\n\"AsyncHTTP2Connection\",\n\"AsyncConnectionInterface\",\n\"AsyncSOCKSProxy\",\n\n\"HTTPConnection\",\n\"ConnectionPool\",\n\"HTTPProxy\",\n\"HTTP11Connection\",\n\"HTTP2Connection\",\n\"ConnectionInterface\",\n\"SOCKSProxy\",\n\n\"SyncBackend\",\n\"AnyIOBackend\",\n\"TrioBackend\",\n\n\"AsyncMockBackend\",\n\"AsyncMockStream\",\n\"MockBackend\",\n\"MockStream\",\n\n\"AsyncNetworkStream\",\n\"AsyncNetworkBackend\",\n\"NetworkStream\",\n\"NetworkBackend\",\n\n\"default_ssl_context\",\n\"SOCKET_OPTION\",\n\n\"ConnectionNotAvailable\",\n\"ProxyError\",\n\"ProtocolError\",\n\"LocalProtocolError\",\n\"RemoteProtocolError\",\n\"UnsupportedProtocol\",\n\"TimeoutException\",\n\"PoolTimeout\",\n\"ConnectTimeout\",\n\"ReadTimeout\",\n\"WriteTimeout\",\n\"NetworkError\",\n\"ConnectError\",\n\"ReadError\",\n\"WriteError\",\n]\n\n__version__=\"1.0.7\"\n\n\n__locals=locals()\nfor __name in __all__:\n if not __name.startswith(\"__\"):\n  setattr(__locals[__name],\"__module__\",\"httpcore\")\n", ["httpcore._api", "httpcore._async", "httpcore._backends.anyio", "httpcore._backends.base", "httpcore._backends.mock", "httpcore._backends.sync", "httpcore._backends.trio", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._sync"], 1], "httpcore._utils": [".py", "from __future__ import annotations\n\nimport select\nimport socket\nimport sys\n\n\ndef is_socket_readable(sock:socket.socket |None)->bool:\n ''\n\n\n\n \n \n \n \n \n \n \n sock_fd=None if sock is None else sock.fileno()\n if sock_fd is None or sock_fd <0:\n  return True\n  \n  \n  \n  \n  \n  \n  \n if(\n sys.platform ==\"win32\"or getattr(select,\"poll\",None)is None\n ):\n  rready,_,_=select.select([sock_fd],[],[],0)\n  return bool(rready)\n p=select.poll()\n p.register(sock_fd,select.POLLIN)\n return bool(p.poll(0))\n", ["__future__", "select", "socket", "sys"]], "httpcore._exceptions": [".py", "import contextlib\nimport typing\n\nExceptionMapping=typing.Mapping[typing.Type[Exception],typing.Type[Exception]]\n\n\n@contextlib.contextmanager\ndef map_exceptions(map:ExceptionMapping)->typing.Iterator[None]:\n try:\n  yield\n except Exception as exc:\n  for from_exc,to_exc in map.items():\n   if isinstance(exc,from_exc):\n    raise to_exc(exc)from exc\n  raise\n  \n  \nclass ConnectionNotAvailable(Exception):\n pass\n \n \nclass ProxyError(Exception):\n pass\n \n \nclass UnsupportedProtocol(Exception):\n pass\n \n \nclass ProtocolError(Exception):\n pass\n \n \nclass RemoteProtocolError(ProtocolError):\n pass\n \n \nclass LocalProtocolError(ProtocolError):\n pass\n \n \n \n \n \nclass TimeoutException(Exception):\n pass\n \n \nclass PoolTimeout(TimeoutException):\n pass\n \n \nclass ConnectTimeout(TimeoutException):\n pass\n \n \nclass ReadTimeout(TimeoutException):\n pass\n \n \nclass WriteTimeout(TimeoutException):\n pass\n \n \n \n \n \nclass NetworkError(Exception):\n pass\n \n \nclass ConnectError(NetworkError):\n pass\n \n \nclass ReadError(NetworkError):\n pass\n \n \nclass WriteError(NetworkError):\n pass\n", ["contextlib", "typing"]], "httpcore._synchronization": [".py", "from __future__ import annotations\n\nimport threading\nimport types\n\nfrom._exceptions import ExceptionMapping,PoolTimeout,map_exceptions\n\n\n\n\ntry:\n import trio\nexcept(ImportError,NotImplementedError):\n trio=None\n \ntry:\n import anyio\nexcept ImportError:\n anyio=None\n \n \ndef current_async_library()->str:\n\n\n try:\n  import sniffio\n except ImportError:\n  environment=\"asyncio\"\n else:\n  environment=sniffio.current_async_library()\n  \n if environment not in(\"asyncio\",\"trio\"):\n  raise RuntimeError(\"Running under an unsupported async environment.\")\n  \n if environment ==\"asyncio\"and anyio is None:\n  raise RuntimeError(\n  \"Running with asyncio requires installation of 'httpcore[asyncio]'.\"\n  )\n  \n if environment ==\"trio\"and trio is None:\n  raise RuntimeError(\n  \"Running with trio requires installation of 'httpcore[trio]'.\"\n  )\n  \n return environment\n \n \nclass AsyncLock:\n ''\n\n\n\n\n \n \n def __init__(self)->None:\n  self._backend=\"\"\n  \n def setup(self)->None:\n  ''\n\n\n  \n  self._backend=current_async_library()\n  if self._backend ==\"trio\":\n   self._trio_lock=trio.Lock()\n  elif self._backend ==\"asyncio\":\n   self._anyio_lock=anyio.Lock()\n   \n async def __aenter__(self)->AsyncLock:\n  if not self._backend:\n   self.setup()\n   \n  if self._backend ==\"trio\":\n   await self._trio_lock.acquire()\n  elif self._backend ==\"asyncio\":\n   await self._anyio_lock.acquire()\n   \n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  if self._backend ==\"trio\":\n   self._trio_lock.release()\n  elif self._backend ==\"asyncio\":\n   self._anyio_lock.release()\n   \n   \nclass AsyncThreadLock:\n ''\n\n\n\n\n \n \n def __enter__(self)->AsyncThreadLock:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  pass\n  \n  \nclass AsyncEvent:\n def __init__(self)->None:\n  self._backend=\"\"\n  \n def setup(self)->None:\n  ''\n\n\n  \n  self._backend=current_async_library()\n  if self._backend ==\"trio\":\n   self._trio_event=trio.Event()\n  elif self._backend ==\"asyncio\":\n   self._anyio_event=anyio.Event()\n   \n def set(self)->None:\n  if not self._backend:\n   self.setup()\n   \n  if self._backend ==\"trio\":\n   self._trio_event.set()\n  elif self._backend ==\"asyncio\":\n   self._anyio_event.set()\n   \n async def wait(self,timeout:float |None=None)->None:\n  if not self._backend:\n   self.setup()\n   \n  if self._backend ==\"trio\":\n   trio_exc_map:ExceptionMapping={trio.TooSlowError:PoolTimeout}\n   timeout_or_inf=float(\"inf\")if timeout is None else timeout\n   with map_exceptions(trio_exc_map):\n    with trio.fail_after(timeout_or_inf):\n     await self._trio_event.wait()\n  elif self._backend ==\"asyncio\":\n   anyio_exc_map:ExceptionMapping={TimeoutError:PoolTimeout}\n   with map_exceptions(anyio_exc_map):\n    with anyio.fail_after(timeout):\n     await self._anyio_event.wait()\n     \n     \nclass AsyncSemaphore:\n def __init__(self,bound:int)->None:\n  self._bound=bound\n  self._backend=\"\"\n  \n def setup(self)->None:\n  ''\n\n\n  \n  self._backend=current_async_library()\n  if self._backend ==\"trio\":\n   self._trio_semaphore=trio.Semaphore(\n   initial_value=self._bound,max_value=self._bound\n   )\n  elif self._backend ==\"asyncio\":\n   self._anyio_semaphore=anyio.Semaphore(\n   initial_value=self._bound,max_value=self._bound\n   )\n   \n async def acquire(self)->None:\n  if not self._backend:\n   self.setup()\n   \n  if self._backend ==\"trio\":\n   await self._trio_semaphore.acquire()\n  elif self._backend ==\"asyncio\":\n   await self._anyio_semaphore.acquire()\n   \n async def release(self)->None:\n  if self._backend ==\"trio\":\n   self._trio_semaphore.release()\n  elif self._backend ==\"asyncio\":\n   self._anyio_semaphore.release()\n   \n   \nclass AsyncShieldCancellation:\n\n\n\n\n\n\n\n def __init__(self)->None:\n  ''\n\n\n  \n  self._backend=current_async_library()\n  \n  if self._backend ==\"trio\":\n   self._trio_shield=trio.CancelScope(shield=True)\n  elif self._backend ==\"asyncio\":\n   self._anyio_shield=anyio.CancelScope(shield=True)\n   \n def __enter__(self)->AsyncShieldCancellation:\n  if self._backend ==\"trio\":\n   self._trio_shield.__enter__()\n  elif self._backend ==\"asyncio\":\n   self._anyio_shield.__enter__()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  if self._backend ==\"trio\":\n   self._trio_shield.__exit__(exc_type,exc_value,traceback)\n  elif self._backend ==\"asyncio\":\n   self._anyio_shield.__exit__(exc_type,exc_value,traceback)\n   \n   \n   \n   \n   \nclass Lock:\n ''\n\n\n\n\n \n \n def __init__(self)->None:\n  self._lock=threading.Lock()\n  \n def __enter__(self)->Lock:\n  self._lock.acquire()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  self._lock.release()\n  \n  \nclass ThreadLock:\n ''\n\n\n\n\n \n \n def __init__(self)->None:\n  self._lock=threading.Lock()\n  \n def __enter__(self)->ThreadLock:\n  self._lock.acquire()\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  self._lock.release()\n  \n  \nclass Event:\n def __init__(self)->None:\n  self._event=threading.Event()\n  \n def set(self)->None:\n  self._event.set()\n  \n def wait(self,timeout:float |None=None)->None:\n  if timeout ==float(\"inf\"):\n   timeout=None\n  if not self._event.wait(timeout=timeout):\n   raise PoolTimeout()\n   \n   \nclass Semaphore:\n def __init__(self,bound:int)->None:\n  self._semaphore=threading.Semaphore(value=bound)\n  \n def acquire(self)->None:\n  self._semaphore.acquire()\n  \n def release(self)->None:\n  self._semaphore.release()\n  \n  \nclass ShieldCancellation:\n\n\n\n def __enter__(self)->ShieldCancellation:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  pass\n", ["__future__", "anyio", "httpcore._exceptions", "sniffio", "threading", "trio", "types"]], "httpcore._models": [".py", "from __future__ import annotations\n\nimport base64\nimport ssl\nimport typing\nimport urllib.parse\n\n\n\n\nByteOrStr=typing.Union[bytes,str]\nHeadersAsSequence=typing.Sequence[typing.Tuple[ByteOrStr,ByteOrStr]]\nHeadersAsMapping=typing.Mapping[ByteOrStr,ByteOrStr]\nHeaderTypes=typing.Union[HeadersAsSequence,HeadersAsMapping,None]\n\nExtensions=typing.MutableMapping[str,typing.Any]\n\n\ndef enforce_bytes(value:bytes |str,*,name:str)->bytes:\n ''\n\n\n\n\n\n\n \n if isinstance(value,str):\n  try:\n   return value.encode(\"ascii\")\n  except UnicodeEncodeError:\n   raise TypeError(f\"{name} strings may not include unicode characters.\")\n elif isinstance(value,bytes):\n  return value\n  \n seen_type=type(value).__name__\n raise TypeError(f\"{name} must be bytes or str, but got {seen_type}.\")\n \n \ndef enforce_url(value:URL |bytes |str,*,name:str)->URL:\n ''\n\n \n if isinstance(value,(bytes,str)):\n  return URL(value)\n elif isinstance(value,URL):\n  return value\n  \n seen_type=type(value).__name__\n raise TypeError(f\"{name} must be a URL, bytes, or str, but got {seen_type}.\")\n \n \ndef enforce_headers(\nvalue:HeadersAsMapping |HeadersAsSequence |None=None,*,name:str\n)->list[tuple[bytes,bytes]]:\n ''\n\n\n \n if value is None:\n  return[]\n elif isinstance(value,typing.Mapping):\n  return[\n  (\n  enforce_bytes(k,name=\"header name\"),\n  enforce_bytes(v,name=\"header value\"),\n  )\n  for k,v in value.items()\n  ]\n elif isinstance(value,typing.Sequence):\n  return[\n  (\n  enforce_bytes(k,name=\"header name\"),\n  enforce_bytes(v,name=\"header value\"),\n  )\n  for k,v in value\n  ]\n  \n seen_type=type(value).__name__\n raise TypeError(\n f\"{name} must be a mapping or sequence of two-tuples, but got {seen_type}.\"\n )\n \n \ndef enforce_stream(\nvalue:bytes |typing.Iterable[bytes]|typing.AsyncIterable[bytes]|None,\n*,\nname:str,\n)->typing.Iterable[bytes]|typing.AsyncIterable[bytes]:\n if value is None:\n  return ByteStream(b\"\")\n elif isinstance(value,bytes):\n  return ByteStream(value)\n return value\n \n \n \n \n \nDEFAULT_PORTS={\nb\"ftp\":21,\nb\"http\":80,\nb\"https\":443,\nb\"ws\":80,\nb\"wss\":443,\n}\n\n\ndef include_request_headers(\nheaders:list[tuple[bytes,bytes]],\n*,\nurl:\"URL\",\ncontent:None |bytes |typing.Iterable[bytes]|typing.AsyncIterable[bytes],\n)->list[tuple[bytes,bytes]]:\n headers_set=set(k.lower()for k,v in headers)\n \n if b\"host\"not in headers_set:\n  default_port=DEFAULT_PORTS.get(url.scheme)\n  if url.port is None or url.port ==default_port:\n   header_value=url.host\n  else:\n   header_value=b\"%b:%d\"%(url.host,url.port)\n  headers=[(b\"Host\",header_value)]+headers\n  \n if(\n content is not None\n and b\"content-length\"not in headers_set\n and b\"transfer-encoding\"not in headers_set\n ):\n  if isinstance(content,bytes):\n   content_length=str(len(content)).encode(\"ascii\")\n   headers +=[(b\"Content-Length\",content_length)]\n  else:\n   headers +=[(b\"Transfer-Encoding\",b\"chunked\")]\n   \n return headers\n \n \n \n \n \nclass ByteStream:\n ''\n\n\n \n \n def __init__(self,content:bytes)->None:\n  self._content=content\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  yield self._content\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  yield self._content\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{len(self._content)} bytes]>\"\n  \n  \nclass Origin:\n def __init__(self,scheme:bytes,host:bytes,port:int)->None:\n  self.scheme=scheme\n  self.host=host\n  self.port=port\n  \n def __eq__(self,other:typing.Any)->bool:\n  return(\n  isinstance(other,Origin)\n  and self.scheme ==other.scheme\n  and self.host ==other.host\n  and self.port ==other.port\n  )\n  \n def __str__(self)->str:\n  scheme=self.scheme.decode(\"ascii\")\n  host=self.host.decode(\"ascii\")\n  port=str(self.port)\n  return f\"{scheme}://{host}:{port}\"\n  \n  \nclass URL:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n url:bytes |str=\"\",\n *,\n scheme:bytes |str=b\"\",\n host:bytes |str=b\"\",\n port:int |None=None,\n target:bytes |str=b\"\",\n )->None:\n  ''\n\n\n\n\n\n\n\n  \n  if url:\n   parsed=urllib.parse.urlparse(enforce_bytes(url,name=\"url\"))\n   self.scheme=parsed.scheme\n   self.host=parsed.hostname or b\"\"\n   self.port=parsed.port\n   self.target=(parsed.path or b\"/\")+(\n   b\"?\"+parsed.query if parsed.query else b\"\"\n   )\n  else:\n   self.scheme=enforce_bytes(scheme,name=\"scheme\")\n   self.host=enforce_bytes(host,name=\"host\")\n   self.port=port\n   self.target=enforce_bytes(target,name=\"target\")\n   \n @property\n def origin(self)->Origin:\n  default_port={\n  b\"http\":80,\n  b\"https\":443,\n  b\"ws\":80,\n  b\"wss\":443,\n  b\"socks5\":1080,\n  b\"socks5h\":1080,\n  }[self.scheme]\n  return Origin(\n  scheme=self.scheme,host=self.host,port=self.port or default_port\n  )\n  \n def __eq__(self,other:typing.Any)->bool:\n  return(\n  isinstance(other,URL)\n  and other.scheme ==self.scheme\n  and other.host ==self.host\n  and other.port ==self.port\n  and other.target ==self.target\n  )\n  \n def __bytes__(self)->bytes:\n  if self.port is None:\n   return b\"%b://%b%b\"%(self.scheme,self.host,self.target)\n  return b\"%b://%b:%d%b\"%(self.scheme,self.host,self.port,self.target)\n  \n def __repr__(self)->str:\n  return(\n  f\"{self.__class__.__name__}(scheme={self.scheme !r}, \"\n  f\"host={self.host !r}, port={self.port !r}, target={self.target !r})\"\n  )\n  \n  \nclass Request:\n ''\n\n \n \n def __init__(\n self,\n method:bytes |str,\n url:URL |bytes |str,\n *,\n headers:HeaderTypes=None,\n content:bytes\n |typing.Iterable[bytes]\n |typing.AsyncIterable[bytes]\n |None=None,\n extensions:Extensions |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  self.method:bytes=enforce_bytes(method,name=\"method\")\n  self.url:URL=enforce_url(url,name=\"url\")\n  self.headers:list[tuple[bytes,bytes]]=enforce_headers(\n  headers,name=\"headers\"\n  )\n  self.stream:typing.Iterable[bytes]|typing.AsyncIterable[bytes]=(\n  enforce_stream(content,name=\"content\")\n  )\n  self.extensions={}if extensions is None else extensions\n  \n  if \"target\"in self.extensions:\n   self.url=URL(\n   scheme=self.url.scheme,\n   host=self.url.host,\n   port=self.url.port,\n   target=self.extensions[\"target\"],\n   )\n   \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.method !r}]>\"\n  \n  \nclass Response:\n ''\n\n \n \n def __init__(\n self,\n status:int,\n *,\n headers:HeaderTypes=None,\n content:bytes\n |typing.Iterable[bytes]\n |typing.AsyncIterable[bytes]\n |None=None,\n extensions:Extensions |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n  \n  self.status:int=status\n  self.headers:list[tuple[bytes,bytes]]=enforce_headers(\n  headers,name=\"headers\"\n  )\n  self.stream:typing.Iterable[bytes]|typing.AsyncIterable[bytes]=(\n  enforce_stream(content,name=\"content\")\n  )\n  self.extensions={}if extensions is None else extensions\n  \n  self._stream_consumed=False\n  \n @property\n def content(self)->bytes:\n  if not hasattr(self,\"_content\"):\n   if isinstance(self.stream,typing.Iterable):\n    raise RuntimeError(\n    \"Attempted to access 'response.content' on a streaming response. \"\n    \"Call 'response.read()' first.\"\n    )\n   else:\n    raise RuntimeError(\n    \"Attempted to access 'response.content' on a streaming response. \"\n    \"Call 'await response.aread()' first.\"\n    )\n  return self._content\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.status}]>\"\n  \n  \n  \n def read(self)->bytes:\n  if not isinstance(self.stream,typing.Iterable):\n   raise RuntimeError(\n   \"Attempted to read an asynchronous response using 'response.read()'. \"\n   \"You should use 'await response.aread()' instead.\"\n   )\n  if not hasattr(self,\"_content\"):\n   self._content=b\"\".join([part for part in self.iter_stream()])\n  return self._content\n  \n def iter_stream(self)->typing.Iterator[bytes]:\n  if not isinstance(self.stream,typing.Iterable):\n   raise RuntimeError(\n   \"Attempted to stream an asynchronous response using 'for ... in \"\n   \"response.iter_stream()'. \"\n   \"You should use 'async for ... in response.aiter_stream()' instead.\"\n   )\n  if self._stream_consumed:\n   raise RuntimeError(\n   \"Attempted to call 'for ... in response.iter_stream()' more than once.\"\n   )\n  self._stream_consumed=True\n  for chunk in self.stream:\n   yield chunk\n   \n def close(self)->None:\n  if not isinstance(self.stream,typing.Iterable):\n   raise RuntimeError(\n   \"Attempted to close an asynchronous response using 'response.close()'. \"\n   \"You should use 'await response.aclose()' instead.\"\n   )\n  if hasattr(self.stream,\"close\"):\n   self.stream.close()\n   \n   \n   \n async def aread(self)->bytes:\n  if not isinstance(self.stream,typing.AsyncIterable):\n   raise RuntimeError(\n   \"Attempted to read an synchronous response using \"\n   \"'await response.aread()'. \"\n   \"You should use 'response.read()' instead.\"\n   )\n  if not hasattr(self,\"_content\"):\n   self._content=b\"\".join([part async for part in self.aiter_stream()])\n  return self._content\n  \n async def aiter_stream(self)->typing.AsyncIterator[bytes]:\n  if not isinstance(self.stream,typing.AsyncIterable):\n   raise RuntimeError(\n   \"Attempted to stream an synchronous response using 'async for ... in \"\n   \"response.aiter_stream()'. \"\n   \"You should use 'for ... in response.iter_stream()' instead.\"\n   )\n  if self._stream_consumed:\n   raise RuntimeError(\n   \"Attempted to call 'async for ... in response.aiter_stream()' \"\n   \"more than once.\"\n   )\n  self._stream_consumed=True\n  async for chunk in self.stream:\n   yield chunk\n   \n async def aclose(self)->None:\n  if not isinstance(self.stream,typing.AsyncIterable):\n   raise RuntimeError(\n   \"Attempted to close a synchronous response using \"\n   \"'await response.aclose()'. \"\n   \"You should use 'response.close()' instead.\"\n   )\n  if hasattr(self.stream,\"aclose\"):\n   await self.stream.aclose()\n   \n   \nclass Proxy:\n def __init__(\n self,\n url:URL |bytes |str,\n auth:tuple[bytes |str,bytes |str]|None=None,\n headers:HeadersAsMapping |HeadersAsSequence |None=None,\n ssl_context:ssl.SSLContext |None=None,\n ):\n  self.url=enforce_url(url,name=\"url\")\n  self.headers=enforce_headers(headers,name=\"headers\")\n  self.ssl_context=ssl_context\n  \n  if auth is not None:\n   username=enforce_bytes(auth[0],name=\"auth\")\n   password=enforce_bytes(auth[1],name=\"auth\")\n   userpass=username+b\":\"+password\n   authorization=b\"Basic \"+base64.b64encode(userpass)\n   self.auth:tuple[bytes,bytes]|None=(username,password)\n   self.headers=[(b\"Proxy-Authorization\",authorization)]+self.headers\n  else:\n   self.auth=None\n", ["__future__", "base64", "ssl", "typing", "urllib.parse"]], "httpcore._api": [".py", "from __future__ import annotations\n\nimport contextlib\nimport typing\n\nfrom._models import URL,Extensions,HeaderTypes,Response\nfrom._sync.connection_pool import ConnectionPool\n\n\ndef request(\nmethod:bytes |str,\nurl:URL |bytes |str,\n*,\nheaders:HeaderTypes=None,\ncontent:bytes |typing.Iterator[bytes]|None=None,\nextensions:Extensions |None=None,\n)->Response:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n with ConnectionPool()as pool:\n  return pool.request(\n  method=method,\n  url=url,\n  headers=headers,\n  content=content,\n  extensions=extensions,\n  )\n  \n  \n@contextlib.contextmanager\ndef stream(\nmethod:bytes |str,\nurl:URL |bytes |str,\n*,\nheaders:HeaderTypes=None,\ncontent:bytes |typing.Iterator[bytes]|None=None,\nextensions:Extensions |None=None,\n)->typing.Iterator[Response]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n with ConnectionPool()as pool:\n  with pool.stream(\n  method=method,\n  url=url,\n  headers=headers,\n  content=content,\n  extensions=extensions,\n  )as response:\n   yield response\n", ["__future__", "contextlib", "httpcore._models", "httpcore._sync.connection_pool", "typing"]], "httpcore._trace": [".py", "from __future__ import annotations\n\nimport inspect\nimport logging\nimport types\nimport typing\n\nfrom._models import Request\n\n\nclass Trace:\n def __init__(\n self,\n name:str,\n logger:logging.Logger,\n request:Request |None=None,\n kwargs:dict[str,typing.Any]|None=None,\n )->None:\n  self.name=name\n  self.logger=logger\n  self.trace_extension=(\n  None if request is None else request.extensions.get(\"trace\")\n  )\n  self.debug=self.logger.isEnabledFor(logging.DEBUG)\n  self.kwargs=kwargs or{}\n  self.return_value:typing.Any=None\n  self.should_trace=self.debug or self.trace_extension is not None\n  self.prefix=self.logger.name.split(\".\")[-1]\n  \n def trace(self,name:str,info:dict[str,typing.Any])->None:\n  if self.trace_extension is not None:\n   prefix_and_name=f\"{self.prefix}.{name}\"\n   ret=self.trace_extension(prefix_and_name,info)\n   if inspect.iscoroutine(ret):\n    raise TypeError(\n    \"If you are using a synchronous interface, \"\n    \"the callback of the `trace` extension should \"\n    \"be a normal function instead of an asynchronous function.\"\n    )\n    \n  if self.debug:\n   if not info or \"return_value\"in info and info[\"return_value\"]is None:\n    message=name\n   else:\n    args=\" \".join([f\"{key}={value !r}\"for key,value in info.items()])\n    message=f\"{name} {args}\"\n   self.logger.debug(message)\n   \n def __enter__(self)->Trace:\n  if self.should_trace:\n   info=self.kwargs\n   self.trace(f\"{self.name}.started\",info)\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  if self.should_trace:\n   if exc_value is None:\n    info={\"return_value\":self.return_value}\n    self.trace(f\"{self.name}.complete\",info)\n   else:\n    info={\"exception\":exc_value}\n    self.trace(f\"{self.name}.failed\",info)\n    \n async def atrace(self,name:str,info:dict[str,typing.Any])->None:\n  if self.trace_extension is not None:\n   prefix_and_name=f\"{self.prefix}.{name}\"\n   coro=self.trace_extension(prefix_and_name,info)\n   if not inspect.iscoroutine(coro):\n    raise TypeError(\n    \"If you're using an asynchronous interface, \"\n    \"the callback of the `trace` extension should \"\n    \"be an asynchronous function rather than a normal function.\"\n    )\n   await coro\n   \n  if self.debug:\n   if not info or \"return_value\"in info and info[\"return_value\"]is None:\n    message=name\n   else:\n    args=\" \".join([f\"{key}={value !r}\"for key,value in info.items()])\n    message=f\"{name} {args}\"\n   self.logger.debug(message)\n   \n async def __aenter__(self)->Trace:\n  if self.should_trace:\n   info=self.kwargs\n   await self.atrace(f\"{self.name}.started\",info)\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  if self.should_trace:\n   if exc_value is None:\n    info={\"return_value\":self.return_value}\n    await self.atrace(f\"{self.name}.complete\",info)\n   else:\n    info={\"exception\":exc_value}\n    await self.atrace(f\"{self.name}.failed\",info)\n", ["__future__", "httpcore._models", "inspect", "logging", "types", "typing"]], "httpcore._ssl": [".py", "import ssl\n\nimport certifi\n\n\ndef default_ssl_context()->ssl.SSLContext:\n context=ssl.create_default_context()\n context.load_verify_locations(certifi.where())\n return context\n", ["certifi", "ssl"]], "httpcore._async": [".py", "from.connection import AsyncHTTPConnection\nfrom.connection_pool import AsyncConnectionPool\nfrom.http11 import AsyncHTTP11Connection\nfrom.http_proxy import AsyncHTTPProxy\nfrom.interfaces import AsyncConnectionInterface\n\ntry:\n from.http2 import AsyncHTTP2Connection\nexcept ImportError:\n\n class AsyncHTTP2Connection:\n  def __init__(self,*args,**kwargs)->None:\n   raise RuntimeError(\n   \"Attempted to use http2 support, but the `h2` package is not \"\n   \"installed. Use 'pip install httpcore[http2]'.\"\n   )\n   \n   \ntry:\n from.socks_proxy import AsyncSOCKSProxy\nexcept ImportError:\n\n class AsyncSOCKSProxy:\n  def __init__(self,*args,**kwargs)->None:\n   raise RuntimeError(\n   \"Attempted to use SOCKS support, but the `socksio` package is not \"\n   \"installed. Use 'pip install httpcore[socks]'.\"\n   )\n   \n   \n__all__=[\n\"AsyncHTTPConnection\",\n\"AsyncConnectionPool\",\n\"AsyncHTTPProxy\",\n\"AsyncHTTP11Connection\",\n\"AsyncHTTP2Connection\",\n\"AsyncConnectionInterface\",\n\"AsyncSOCKSProxy\",\n]\n", ["httpcore._async.connection", "httpcore._async.connection_pool", "httpcore._async.http11", "httpcore._async.http2", "httpcore._async.http_proxy", "httpcore._async.interfaces", "httpcore._async.socks_proxy"], 1], "httpcore._async.http2": [".py", "from __future__ import annotations\n\nimport enum\nimport logging\nimport time\nimport types\nimport typing\n\nimport h2.config\nimport h2.connection\nimport h2.events\nimport h2.exceptions\nimport h2.settings\n\nfrom.._backends.base import AsyncNetworkStream\nfrom.._exceptions import(\nConnectionNotAvailable,\nLocalProtocolError,\nRemoteProtocolError,\n)\nfrom.._models import Origin,Request,Response\nfrom.._synchronization import AsyncLock,AsyncSemaphore,AsyncShieldCancellation\nfrom.._trace import Trace\nfrom.interfaces import AsyncConnectionInterface\n\nlogger=logging.getLogger(\"httpcore.http2\")\n\n\ndef has_body_headers(request:Request)->bool:\n return any(\n k.lower()==b\"content-length\"or k.lower()==b\"transfer-encoding\"\n for k,v in request.headers\n )\n \n \nclass HTTPConnectionState(enum.IntEnum):\n ACTIVE=1\n IDLE=2\n CLOSED=3\n \n \nclass AsyncHTTP2Connection(AsyncConnectionInterface):\n READ_NUM_BYTES=64 *1024\n CONFIG=h2.config.H2Configuration(validate_inbound_headers=False)\n \n def __init__(\n self,\n origin:Origin,\n stream:AsyncNetworkStream,\n keepalive_expiry:float |None=None,\n ):\n  self._origin=origin\n  self._network_stream=stream\n  self._keepalive_expiry:float |None=keepalive_expiry\n  self._h2_state=h2.connection.H2Connection(config=self.CONFIG)\n  self._state=HTTPConnectionState.IDLE\n  self._expire_at:float |None=None\n  self._request_count=0\n  self._init_lock=AsyncLock()\n  self._state_lock=AsyncLock()\n  self._read_lock=AsyncLock()\n  self._write_lock=AsyncLock()\n  self._sent_connection_init=False\n  self._used_all_stream_ids=False\n  self._connection_error=False\n  \n  \n  self._events:dict[\n  int,\n  h2.events.ResponseReceived\n  |h2.events.DataReceived\n  |h2.events.StreamEnded\n  |h2.events.StreamReset,\n  ]={}\n  \n  \n  \n  self._connection_terminated:h2.events.ConnectionTerminated |None=None\n  \n  self._read_exception:Exception |None=None\n  self._write_exception:Exception |None=None\n  \n async def handle_async_request(self,request:Request)->Response:\n  if not self.can_handle_request(request.url.origin):\n  \n  \n  \n  \n   raise RuntimeError(\n   f\"Attempted to send request to {request.url.origin} on connection \"\n   f\"to {self._origin}\"\n   )\n   \n  async with self._state_lock:\n   if self._state in(HTTPConnectionState.ACTIVE,HTTPConnectionState.IDLE):\n    self._request_count +=1\n    self._expire_at=None\n    self._state=HTTPConnectionState.ACTIVE\n   else:\n    raise ConnectionNotAvailable()\n    \n  async with self._init_lock:\n   if not self._sent_connection_init:\n    try:\n     kwargs={\"request\":request}\n     async with Trace(\"send_connection_init\",logger,request,kwargs):\n      await self._send_connection_init(**kwargs)\n    except BaseException as exc:\n     with AsyncShieldCancellation():\n      await self.aclose()\n     raise exc\n     \n    self._sent_connection_init=True\n    \n    \n    \n    self._max_streams=1\n    \n    local_settings_max_streams=(\n    self._h2_state.local_settings.max_concurrent_streams\n    )\n    self._max_streams_semaphore=AsyncSemaphore(local_settings_max_streams)\n    \n    for _ in range(local_settings_max_streams -self._max_streams):\n     await self._max_streams_semaphore.acquire()\n     \n  await self._max_streams_semaphore.acquire()\n  \n  try:\n   stream_id=self._h2_state.get_next_available_stream_id()\n   self._events[stream_id]=[]\n  except h2.exceptions.NoAvailableStreamIDError:\n   self._used_all_stream_ids=True\n   self._request_count -=1\n   raise ConnectionNotAvailable()\n   \n  try:\n   kwargs={\"request\":request,\"stream_id\":stream_id}\n   async with Trace(\"send_request_headers\",logger,request,kwargs):\n    await self._send_request_headers(request=request,stream_id=stream_id)\n   async with Trace(\"send_request_body\",logger,request,kwargs):\n    await self._send_request_body(request=request,stream_id=stream_id)\n   async with Trace(\n   \"receive_response_headers\",logger,request,kwargs\n   )as trace:\n    status,headers=await self._receive_response(\n    request=request,stream_id=stream_id\n    )\n    trace.return_value=(status,headers)\n    \n   return Response(\n   status=status,\n   headers=headers,\n   content=HTTP2ConnectionByteStream(self,request,stream_id=stream_id),\n   extensions={\n   \"http_version\":b\"HTTP/2\",\n   \"network_stream\":self._network_stream,\n   \"stream_id\":stream_id,\n   },\n   )\n  except BaseException as exc:\n   with AsyncShieldCancellation():\n    kwargs={\"stream_id\":stream_id}\n    async with Trace(\"response_closed\",logger,request,kwargs):\n     await self._response_closed(stream_id=stream_id)\n     \n   if isinstance(exc,h2.exceptions.ProtocolError):\n   \n   \n   \n   \n   \n   \n   \n   \n   \n    if self._connection_terminated:\n     raise RemoteProtocolError(self._connection_terminated)\n     \n     \n    raise LocalProtocolError(exc)\n    \n   raise exc\n   \n async def _send_connection_init(self,request:Request)->None:\n  ''\n\n\n  \n  \n  \n  \n  self._h2_state.local_settings=h2.settings.Settings(\n  client=True,\n  initial_values={\n  \n  \n  h2.settings.SettingCodes.ENABLE_PUSH:0,\n  \n  h2.settings.SettingCodes.MAX_CONCURRENT_STREAMS:100,\n  h2.settings.SettingCodes.MAX_HEADER_LIST_SIZE:65536,\n  },\n  )\n  \n  \n  \n  \n  del self._h2_state.local_settings[\n  h2.settings.SettingCodes.ENABLE_CONNECT_PROTOCOL\n  ]\n  \n  self._h2_state.initiate_connection()\n  self._h2_state.increment_flow_control_window(2 **24)\n  await self._write_outgoing_data(request)\n  \n  \n  \n async def _send_request_headers(self,request:Request,stream_id:int)->None:\n  ''\n\n  \n  end_stream=not has_body_headers(request)\n  \n  \n  \n  \n  \n  authority=[v for k,v in request.headers if k.lower()==b\"host\"][0]\n  \n  headers=[\n  (b\":method\",request.method),\n  (b\":authority\",authority),\n  (b\":scheme\",request.url.scheme),\n  (b\":path\",request.url.target),\n  ]+[\n  (k.lower(),v)\n  for k,v in request.headers\n  if k.lower()\n  not in(\n  b\"host\",\n  b\"transfer-encoding\",\n  )\n  ]\n  \n  self._h2_state.send_headers(stream_id,headers,end_stream=end_stream)\n  self._h2_state.increment_flow_control_window(2 **24,stream_id=stream_id)\n  await self._write_outgoing_data(request)\n  \n async def _send_request_body(self,request:Request,stream_id:int)->None:\n  ''\n\n  \n  if not has_body_headers(request):\n   return\n   \n  assert isinstance(request.stream,typing.AsyncIterable)\n  async for data in request.stream:\n   await self._send_stream_data(request,stream_id,data)\n  await self._send_end_stream(request,stream_id)\n  \n async def _send_stream_data(\n self,request:Request,stream_id:int,data:bytes\n )->None:\n  ''\n\n  \n  while data:\n   max_flow=await self._wait_for_outgoing_flow(request,stream_id)\n   chunk_size=min(len(data),max_flow)\n   chunk,data=data[:chunk_size],data[chunk_size:]\n   self._h2_state.send_data(stream_id,chunk)\n   await self._write_outgoing_data(request)\n   \n async def _send_end_stream(self,request:Request,stream_id:int)->None:\n  ''\n\n  \n  self._h2_state.end_stream(stream_id)\n  await self._write_outgoing_data(request)\n  \n  \n  \n async def _receive_response(\n self,request:Request,stream_id:int\n )->tuple[int,list[tuple[bytes,bytes]]]:\n  ''\n\n  \n  while True:\n   event=await self._receive_stream_event(request,stream_id)\n   if isinstance(event,h2.events.ResponseReceived):\n    break\n    \n  status_code=200\n  headers=[]\n  for k,v in event.headers:\n   if k ==b\":status\":\n    status_code=int(v.decode(\"ascii\",errors=\"ignore\"))\n   elif not k.startswith(b\":\"):\n    headers.append((k,v))\n    \n  return(status_code,headers)\n  \n async def _receive_response_body(\n self,request:Request,stream_id:int\n )->typing.AsyncIterator[bytes]:\n  ''\n\n  \n  while True:\n   event=await self._receive_stream_event(request,stream_id)\n   if isinstance(event,h2.events.DataReceived):\n    amount=event.flow_controlled_length\n    self._h2_state.acknowledge_received_data(amount,stream_id)\n    await self._write_outgoing_data(request)\n    yield event.data\n   elif isinstance(event,h2.events.StreamEnded):\n    break\n    \n async def _receive_stream_event(\n self,request:Request,stream_id:int\n )->h2.events.ResponseReceived |h2.events.DataReceived |h2.events.StreamEnded:\n  ''\n\n\n\n  \n  while not self._events.get(stream_id):\n   await self._receive_events(request,stream_id)\n  event=self._events[stream_id].pop(0)\n  if isinstance(event,h2.events.StreamReset):\n   raise RemoteProtocolError(event)\n  return event\n  \n async def _receive_events(\n self,request:Request,stream_id:int |None=None\n )->None:\n  ''\n\n\n  \n  async with self._read_lock:\n   if self._connection_terminated is not None:\n    last_stream_id=self._connection_terminated.last_stream_id\n    if stream_id and last_stream_id and stream_id >last_stream_id:\n     self._request_count -=1\n     raise ConnectionNotAvailable()\n    raise RemoteProtocolError(self._connection_terminated)\n    \n    \n    \n    \n    \n    \n    \n   if stream_id is None or not self._events.get(stream_id):\n    events=await self._read_incoming_data(request)\n    for event in events:\n     if isinstance(event,h2.events.RemoteSettingsChanged):\n      async with Trace(\n      \"receive_remote_settings\",logger,request\n      )as trace:\n       await self._receive_remote_settings_change(event)\n       trace.return_value=event\n       \n     elif isinstance(\n     event,\n     (\n     h2.events.ResponseReceived,\n     h2.events.DataReceived,\n     h2.events.StreamEnded,\n     h2.events.StreamReset,\n     ),\n     ):\n      if event.stream_id in self._events:\n       self._events[event.stream_id].append(event)\n       \n     elif isinstance(event,h2.events.ConnectionTerminated):\n      self._connection_terminated=event\n      \n  await self._write_outgoing_data(request)\n  \n async def _receive_remote_settings_change(self,event:h2.events.Event)->None:\n  max_concurrent_streams=event.changed_settings.get(\n  h2.settings.SettingCodes.MAX_CONCURRENT_STREAMS\n  )\n  if max_concurrent_streams:\n   new_max_streams=min(\n   max_concurrent_streams.new_value,\n   self._h2_state.local_settings.max_concurrent_streams,\n   )\n   if new_max_streams and new_max_streams !=self._max_streams:\n    while new_max_streams >self._max_streams:\n     await self._max_streams_semaphore.release()\n     self._max_streams +=1\n    while new_max_streams <self._max_streams:\n     await self._max_streams_semaphore.acquire()\n     self._max_streams -=1\n     \n async def _response_closed(self,stream_id:int)->None:\n  await self._max_streams_semaphore.release()\n  del self._events[stream_id]\n  async with self._state_lock:\n   if self._connection_terminated and not self._events:\n    await self.aclose()\n    \n   elif self._state ==HTTPConnectionState.ACTIVE and not self._events:\n    self._state=HTTPConnectionState.IDLE\n    if self._keepalive_expiry is not None:\n     now=time.monotonic()\n     self._expire_at=now+self._keepalive_expiry\n    if self._used_all_stream_ids:\n     await self.aclose()\n     \n async def aclose(self)->None:\n \n \n  self._h2_state.close_connection()\n  self._state=HTTPConnectionState.CLOSED\n  await self._network_stream.aclose()\n  \n  \n  \n async def _read_incoming_data(self,request:Request)->list[h2.events.Event]:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"read\",None)\n  \n  if self._read_exception is not None:\n   raise self._read_exception\n   \n  try:\n   data=await self._network_stream.read(self.READ_NUM_BYTES,timeout)\n   if data ==b\"\":\n    raise RemoteProtocolError(\"Server disconnected\")\n  except Exception as exc:\n  \n  \n  \n  \n  \n  \n  \n  \n   self._read_exception=exc\n   self._connection_error=True\n   raise exc\n   \n  events:list[h2.events.Event]=self._h2_state.receive_data(data)\n  \n  return events\n  \n async def _write_outgoing_data(self,request:Request)->None:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"write\",None)\n  \n  async with self._write_lock:\n   data_to_send=self._h2_state.data_to_send()\n   \n   if self._write_exception is not None:\n    raise self._write_exception\n    \n   try:\n    await self._network_stream.write(data_to_send,timeout)\n   except Exception as exc:\n   \n   \n   \n   \n   \n   \n   \n   \n    self._write_exception=exc\n    self._connection_error=True\n    raise exc\n    \n    \n    \n async def _wait_for_outgoing_flow(self,request:Request,stream_id:int)->int:\n  ''\n\n\n\n\n\n  \n  local_flow:int=self._h2_state.local_flow_control_window(stream_id)\n  max_frame_size:int=self._h2_state.max_outbound_frame_size\n  flow=min(local_flow,max_frame_size)\n  while flow ==0:\n   await self._receive_events(request)\n   local_flow=self._h2_state.local_flow_control_window(stream_id)\n   max_frame_size=self._h2_state.max_outbound_frame_size\n   flow=min(local_flow,max_frame_size)\n  return flow\n  \n  \n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._origin\n  \n def is_available(self)->bool:\n  return(\n  self._state !=HTTPConnectionState.CLOSED\n  and not self._connection_error\n  and not self._used_all_stream_ids\n  and not(\n  self._h2_state.state_machine.state\n  ==h2.connection.ConnectionState.CLOSED\n  )\n  )\n  \n def has_expired(self)->bool:\n  now=time.monotonic()\n  return self._expire_at is not None and now >self._expire_at\n  \n def is_idle(self)->bool:\n  return self._state ==HTTPConnectionState.IDLE\n  \n def is_closed(self)->bool:\n  return self._state ==HTTPConnectionState.CLOSED\n  \n def info(self)->str:\n  origin=str(self._origin)\n  return(\n  f\"{origin !r}, HTTP/2, {self._state.name}, \"\n  f\"Request Count: {self._request_count}\"\n  )\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  origin=str(self._origin)\n  return(\n  f\"<{class_name} [{origin !r}, {self._state.name}, \"\n  f\"Request Count: {self._request_count}]>\"\n  )\n  \n  \n  \n  \n async def __aenter__(self)->AsyncHTTP2Connection:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  await self.aclose()\n  \n  \nclass HTTP2ConnectionByteStream:\n def __init__(\n self,connection:AsyncHTTP2Connection,request:Request,stream_id:int\n )->None:\n  self._connection=connection\n  self._request=request\n  self._stream_id=stream_id\n  self._closed=False\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  kwargs={\"request\":self._request,\"stream_id\":self._stream_id}\n  try:\n   async with Trace(\"receive_response_body\",logger,self._request,kwargs):\n    async for chunk in self._connection._receive_response_body(\n    request=self._request,stream_id=self._stream_id\n    ):\n     yield chunk\n  except BaseException as exc:\n  \n  \n  \n   with AsyncShieldCancellation():\n    await self.aclose()\n   raise exc\n   \n async def aclose(self)->None:\n  if not self._closed:\n   self._closed=True\n   kwargs={\"stream_id\":self._stream_id}\n   async with Trace(\"response_closed\",logger,self._request,kwargs):\n    await self._connection._response_closed(stream_id=self._stream_id)\n", ["__future__", "enum", "h2.config", "h2.connection", "h2.events", "h2.exceptions", "h2.settings", "httpcore._async.interfaces", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._synchronization", "httpcore._trace", "logging", "time", "types", "typing"]], "httpcore._async.connection": [".py", "from __future__ import annotations\n\nimport itertools\nimport logging\nimport ssl\nimport types\nimport typing\n\nfrom.._backends.auto import AutoBackend\nfrom.._backends.base import SOCKET_OPTION,AsyncNetworkBackend,AsyncNetworkStream\nfrom.._exceptions import ConnectError,ConnectTimeout\nfrom.._models import Origin,Request,Response\nfrom.._ssl import default_ssl_context\nfrom.._synchronization import AsyncLock\nfrom.._trace import Trace\nfrom.http11 import AsyncHTTP11Connection\nfrom.interfaces import AsyncConnectionInterface\n\nRETRIES_BACKOFF_FACTOR=0.5\n\n\nlogger=logging.getLogger(\"httpcore.connection\")\n\n\ndef exponential_backoff(factor:float)->typing.Iterator[float]:\n ''\n\n\n\n\n\n \n yield 0\n for n in itertools.count():\n  yield factor *2 **n\n  \n  \nclass AsyncHTTPConnection(AsyncConnectionInterface):\n def __init__(\n self,\n origin:Origin,\n ssl_context:ssl.SSLContext |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n local_address:str |None=None,\n uds:str |None=None,\n network_backend:AsyncNetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  self._origin=origin\n  self._ssl_context=ssl_context\n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  self._retries=retries\n  self._local_address=local_address\n  self._uds=uds\n  \n  self._network_backend:AsyncNetworkBackend=(\n  AutoBackend()if network_backend is None else network_backend\n  )\n  self._connection:AsyncConnectionInterface |None=None\n  self._connect_failed:bool=False\n  self._request_lock=AsyncLock()\n  self._socket_options=socket_options\n  \n async def handle_async_request(self,request:Request)->Response:\n  if not self.can_handle_request(request.url.origin):\n   raise RuntimeError(\n   f\"Attempted to send request to {request.url.origin} on connection to {self._origin}\"\n   )\n   \n  try:\n   async with self._request_lock:\n    if self._connection is None:\n     stream=await self._connect(request)\n     \n     ssl_object=stream.get_extra_info(\"ssl_object\")\n     http2_negotiated=(\n     ssl_object is not None\n     and ssl_object.selected_alpn_protocol()==\"h2\"\n     )\n     if http2_negotiated or(self._http2 and not self._http1):\n      from.http2 import AsyncHTTP2Connection\n      \n      self._connection=AsyncHTTP2Connection(\n      origin=self._origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n     else:\n      self._connection=AsyncHTTP11Connection(\n      origin=self._origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n  except BaseException as exc:\n   self._connect_failed=True\n   raise exc\n   \n  return await self._connection.handle_async_request(request)\n  \n async def _connect(self,request:Request)->AsyncNetworkStream:\n  timeouts=request.extensions.get(\"timeout\",{})\n  sni_hostname=request.extensions.get(\"sni_hostname\",None)\n  timeout=timeouts.get(\"connect\",None)\n  \n  retries_left=self._retries\n  delays=exponential_backoff(factor=RETRIES_BACKOFF_FACTOR)\n  \n  while True:\n   try:\n    if self._uds is None:\n     kwargs={\n     \"host\":self._origin.host.decode(\"ascii\"),\n     \"port\":self._origin.port,\n     \"local_address\":self._local_address,\n     \"timeout\":timeout,\n     \"socket_options\":self._socket_options,\n     }\n     async with Trace(\"connect_tcp\",logger,request,kwargs)as trace:\n      stream=await self._network_backend.connect_tcp(**kwargs)\n      trace.return_value=stream\n    else:\n     kwargs={\n     \"path\":self._uds,\n     \"timeout\":timeout,\n     \"socket_options\":self._socket_options,\n     }\n     async with Trace(\n     \"connect_unix_socket\",logger,request,kwargs\n     )as trace:\n      stream=await self._network_backend.connect_unix_socket(\n      **kwargs\n      )\n      trace.return_value=stream\n      \n    if self._origin.scheme in(b\"https\",b\"wss\"):\n     ssl_context=(\n     default_ssl_context()\n     if self._ssl_context is None\n     else self._ssl_context\n     )\n     alpn_protocols=[\"http/1.1\",\"h2\"]if self._http2 else[\"http/1.1\"]\n     ssl_context.set_alpn_protocols(alpn_protocols)\n     \n     kwargs={\n     \"ssl_context\":ssl_context,\n     \"server_hostname\":sni_hostname\n     or self._origin.host.decode(\"ascii\"),\n     \"timeout\":timeout,\n     }\n     async with Trace(\"start_tls\",logger,request,kwargs)as trace:\n      stream=await stream.start_tls(**kwargs)\n      trace.return_value=stream\n    return stream\n   except(ConnectError,ConnectTimeout):\n    if retries_left <=0:\n     raise\n    retries_left -=1\n    delay=next(delays)\n    async with Trace(\"retry\",logger,request,kwargs)as trace:\n     await self._network_backend.sleep(delay)\n     \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._origin\n  \n async def aclose(self)->None:\n  if self._connection is not None:\n   async with Trace(\"close\",logger,None,{}):\n    await self._connection.aclose()\n    \n def is_available(self)->bool:\n  if self._connection is None:\n  \n  \n  \n   return(\n   self._http2\n   and(self._origin.scheme ==b\"https\"or not self._http1)\n   and not self._connect_failed\n   )\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_closed()\n  \n def info(self)->str:\n  if self._connection is None:\n   return \"CONNECTION FAILED\"if self._connect_failed else \"CONNECTING\"\n  return self._connection.info()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n  \n  \n  \n  \n async def __aenter__(self)->AsyncHTTPConnection:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  await self.aclose()\n", ["__future__", "httpcore._async.http11", "httpcore._async.http2", "httpcore._async.interfaces", "httpcore._backends.auto", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._synchronization", "httpcore._trace", "itertools", "logging", "ssl", "types", "typing"]], "httpcore._async.http_proxy": [".py", "from __future__ import annotations\n\nimport base64\nimport logging\nimport ssl\nimport typing\n\nfrom.._backends.base import SOCKET_OPTION,AsyncNetworkBackend\nfrom.._exceptions import ProxyError\nfrom.._models import(\nURL,\nOrigin,\nRequest,\nResponse,\nenforce_bytes,\nenforce_headers,\nenforce_url,\n)\nfrom.._ssl import default_ssl_context\nfrom.._synchronization import AsyncLock\nfrom.._trace import Trace\nfrom.connection import AsyncHTTPConnection\nfrom.connection_pool import AsyncConnectionPool\nfrom.http11 import AsyncHTTP11Connection\nfrom.interfaces import AsyncConnectionInterface\n\nByteOrStr=typing.Union[bytes,str]\nHeadersAsSequence=typing.Sequence[typing.Tuple[ByteOrStr,ByteOrStr]]\nHeadersAsMapping=typing.Mapping[ByteOrStr,ByteOrStr]\n\n\nlogger=logging.getLogger(\"httpcore.proxy\")\n\n\ndef merge_headers(\ndefault_headers:typing.Sequence[tuple[bytes,bytes]]|None=None,\noverride_headers:typing.Sequence[tuple[bytes,bytes]]|None=None,\n)->list[tuple[bytes,bytes]]:\n ''\n\n\n \n default_headers=[]if default_headers is None else list(default_headers)\n override_headers=[]if override_headers is None else list(override_headers)\n has_override=set(key.lower()for key,value in override_headers)\n default_headers=[\n (key,value)\n for key,value in default_headers\n if key.lower()not in has_override\n ]\n return default_headers+override_headers\n \n \nclass AsyncHTTPProxy(AsyncConnectionPool):\n ''\n\n \n \n def __init__(\n self,\n proxy_url:URL |bytes |str,\n proxy_auth:tuple[bytes |str,bytes |str]|None=None,\n proxy_headers:HeadersAsMapping |HeadersAsSequence |None=None,\n ssl_context:ssl.SSLContext |None=None,\n proxy_ssl_context:ssl.SSLContext |None=None,\n max_connections:int |None=10,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n local_address:str |None=None,\n uds:str |None=None,\n network_backend:AsyncNetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  super().__init__(\n  ssl_context=ssl_context,\n  max_connections=max_connections,\n  max_keepalive_connections=max_keepalive_connections,\n  keepalive_expiry=keepalive_expiry,\n  http1=http1,\n  http2=http2,\n  network_backend=network_backend,\n  retries=retries,\n  local_address=local_address,\n  uds=uds,\n  socket_options=socket_options,\n  )\n  \n  self._proxy_url=enforce_url(proxy_url,name=\"proxy_url\")\n  if(\n  self._proxy_url.scheme ==b\"http\"and proxy_ssl_context is not None\n  ):\n   raise RuntimeError(\n   \"The `proxy_ssl_context` argument is not allowed for the http scheme\"\n   )\n   \n  self._ssl_context=ssl_context\n  self._proxy_ssl_context=proxy_ssl_context\n  self._proxy_headers=enforce_headers(proxy_headers,name=\"proxy_headers\")\n  if proxy_auth is not None:\n   username=enforce_bytes(proxy_auth[0],name=\"proxy_auth\")\n   password=enforce_bytes(proxy_auth[1],name=\"proxy_auth\")\n   userpass=username+b\":\"+password\n   authorization=b\"Basic \"+base64.b64encode(userpass)\n   self._proxy_headers=[\n   (b\"Proxy-Authorization\",authorization)\n   ]+self._proxy_headers\n   \n def create_connection(self,origin:Origin)->AsyncConnectionInterface:\n  if origin.scheme ==b\"http\":\n   return AsyncForwardHTTPConnection(\n   proxy_origin=self._proxy_url.origin,\n   proxy_headers=self._proxy_headers,\n   remote_origin=origin,\n   keepalive_expiry=self._keepalive_expiry,\n   network_backend=self._network_backend,\n   proxy_ssl_context=self._proxy_ssl_context,\n   )\n  return AsyncTunnelHTTPConnection(\n  proxy_origin=self._proxy_url.origin,\n  proxy_headers=self._proxy_headers,\n  remote_origin=origin,\n  ssl_context=self._ssl_context,\n  proxy_ssl_context=self._proxy_ssl_context,\n  keepalive_expiry=self._keepalive_expiry,\n  http1=self._http1,\n  http2=self._http2,\n  network_backend=self._network_backend,\n  )\n  \n  \nclass AsyncForwardHTTPConnection(AsyncConnectionInterface):\n def __init__(\n self,\n proxy_origin:Origin,\n remote_origin:Origin,\n proxy_headers:HeadersAsMapping |HeadersAsSequence |None=None,\n keepalive_expiry:float |None=None,\n network_backend:AsyncNetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n proxy_ssl_context:ssl.SSLContext |None=None,\n )->None:\n  self._connection=AsyncHTTPConnection(\n  origin=proxy_origin,\n  keepalive_expiry=keepalive_expiry,\n  network_backend=network_backend,\n  socket_options=socket_options,\n  ssl_context=proxy_ssl_context,\n  )\n  self._proxy_origin=proxy_origin\n  self._proxy_headers=enforce_headers(proxy_headers,name=\"proxy_headers\")\n  self._remote_origin=remote_origin\n  \n async def handle_async_request(self,request:Request)->Response:\n  headers=merge_headers(self._proxy_headers,request.headers)\n  url=URL(\n  scheme=self._proxy_origin.scheme,\n  host=self._proxy_origin.host,\n  port=self._proxy_origin.port,\n  target=bytes(request.url),\n  )\n  proxy_request=Request(\n  method=request.method,\n  url=url,\n  headers=headers,\n  content=request.stream,\n  extensions=request.extensions,\n  )\n  return await self._connection.handle_async_request(proxy_request)\n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._remote_origin\n  \n async def aclose(self)->None:\n  await self._connection.aclose()\n  \n def info(self)->str:\n  return self._connection.info()\n  \n def is_available(self)->bool:\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  return self._connection.is_closed()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n  \n  \nclass AsyncTunnelHTTPConnection(AsyncConnectionInterface):\n def __init__(\n self,\n proxy_origin:Origin,\n remote_origin:Origin,\n ssl_context:ssl.SSLContext |None=None,\n proxy_ssl_context:ssl.SSLContext |None=None,\n proxy_headers:typing.Sequence[tuple[bytes,bytes]]|None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n network_backend:AsyncNetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  self._connection:AsyncConnectionInterface=AsyncHTTPConnection(\n  origin=proxy_origin,\n  keepalive_expiry=keepalive_expiry,\n  network_backend=network_backend,\n  socket_options=socket_options,\n  ssl_context=proxy_ssl_context,\n  )\n  self._proxy_origin=proxy_origin\n  self._remote_origin=remote_origin\n  self._ssl_context=ssl_context\n  self._proxy_ssl_context=proxy_ssl_context\n  self._proxy_headers=enforce_headers(proxy_headers,name=\"proxy_headers\")\n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  self._connect_lock=AsyncLock()\n  self._connected=False\n  \n async def handle_async_request(self,request:Request)->Response:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"connect\",None)\n  \n  async with self._connect_lock:\n   if not self._connected:\n    target=b\"%b:%d\"%(self._remote_origin.host,self._remote_origin.port)\n    \n    connect_url=URL(\n    scheme=self._proxy_origin.scheme,\n    host=self._proxy_origin.host,\n    port=self._proxy_origin.port,\n    target=target,\n    )\n    connect_headers=merge_headers(\n    [(b\"Host\",target),(b\"Accept\",b\"*/*\")],self._proxy_headers\n    )\n    connect_request=Request(\n    method=b\"CONNECT\",\n    url=connect_url,\n    headers=connect_headers,\n    extensions=request.extensions,\n    )\n    connect_response=await self._connection.handle_async_request(\n    connect_request\n    )\n    \n    if connect_response.status <200 or connect_response.status >299:\n     reason_bytes=connect_response.extensions.get(\"reason_phrase\",b\"\")\n     reason_str=reason_bytes.decode(\"ascii\",errors=\"ignore\")\n     msg=\"%d %s\"%(connect_response.status,reason_str)\n     await self._connection.aclose()\n     raise ProxyError(msg)\n     \n    stream=connect_response.extensions[\"network_stream\"]\n    \n    \n    ssl_context=(\n    default_ssl_context()\n    if self._ssl_context is None\n    else self._ssl_context\n    )\n    alpn_protocols=[\"http/1.1\",\"h2\"]if self._http2 else[\"http/1.1\"]\n    ssl_context.set_alpn_protocols(alpn_protocols)\n    \n    kwargs={\n    \"ssl_context\":ssl_context,\n    \"server_hostname\":self._remote_origin.host.decode(\"ascii\"),\n    \"timeout\":timeout,\n    }\n    async with Trace(\"start_tls\",logger,request,kwargs)as trace:\n     stream=await stream.start_tls(**kwargs)\n     trace.return_value=stream\n     \n     \n    ssl_object=stream.get_extra_info(\"ssl_object\")\n    http2_negotiated=(\n    ssl_object is not None\n    and ssl_object.selected_alpn_protocol()==\"h2\"\n    )\n    \n    \n    if http2_negotiated or(self._http2 and not self._http1):\n     from.http2 import AsyncHTTP2Connection\n     \n     self._connection=AsyncHTTP2Connection(\n     origin=self._remote_origin,\n     stream=stream,\n     keepalive_expiry=self._keepalive_expiry,\n     )\n    else:\n     self._connection=AsyncHTTP11Connection(\n     origin=self._remote_origin,\n     stream=stream,\n     keepalive_expiry=self._keepalive_expiry,\n     )\n     \n    self._connected=True\n  return await self._connection.handle_async_request(request)\n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._remote_origin\n  \n async def aclose(self)->None:\n  await self._connection.aclose()\n  \n def info(self)->str:\n  return self._connection.info()\n  \n def is_available(self)->bool:\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  return self._connection.is_closed()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n", ["__future__", "base64", "httpcore._async.connection", "httpcore._async.connection_pool", "httpcore._async.http11", "httpcore._async.http2", "httpcore._async.interfaces", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._synchronization", "httpcore._trace", "logging", "ssl", "typing"]], "httpcore._async.interfaces": [".py", "from __future__ import annotations\n\nimport contextlib\nimport typing\n\nfrom.._models import(\nURL,\nExtensions,\nHeaderTypes,\nOrigin,\nRequest,\nResponse,\nenforce_bytes,\nenforce_headers,\nenforce_url,\ninclude_request_headers,\n)\n\n\nclass AsyncRequestInterface:\n async def request(\n self,\n method:bytes |str,\n url:URL |bytes |str,\n *,\n headers:HeaderTypes=None,\n content:bytes |typing.AsyncIterator[bytes]|None=None,\n extensions:Extensions |None=None,\n )->Response:\n \n  method=enforce_bytes(method,name=\"method\")\n  url=enforce_url(url,name=\"url\")\n  headers=enforce_headers(headers,name=\"headers\")\n  \n  \n  headers=include_request_headers(headers,url=url,content=content)\n  \n  request=Request(\n  method=method,\n  url=url,\n  headers=headers,\n  content=content,\n  extensions=extensions,\n  )\n  response=await self.handle_async_request(request)\n  try:\n   await response.aread()\n  finally:\n   await response.aclose()\n  return response\n  \n @contextlib.asynccontextmanager\n async def stream(\n self,\n method:bytes |str,\n url:URL |bytes |str,\n *,\n headers:HeaderTypes=None,\n content:bytes |typing.AsyncIterator[bytes]|None=None,\n extensions:Extensions |None=None,\n )->typing.AsyncIterator[Response]:\n \n  method=enforce_bytes(method,name=\"method\")\n  url=enforce_url(url,name=\"url\")\n  headers=enforce_headers(headers,name=\"headers\")\n  \n  \n  headers=include_request_headers(headers,url=url,content=content)\n  \n  request=Request(\n  method=method,\n  url=url,\n  headers=headers,\n  content=content,\n  extensions=extensions,\n  )\n  response=await self.handle_async_request(request)\n  try:\n   yield response\n  finally:\n   await response.aclose()\n   \n async def handle_async_request(self,request:Request)->Response:\n  raise NotImplementedError()\n  \n  \nclass AsyncConnectionInterface(AsyncRequestInterface):\n async def aclose(self)->None:\n  raise NotImplementedError()\n  \n def info(self)->str:\n  raise NotImplementedError()\n  \n def can_handle_request(self,origin:Origin)->bool:\n  raise NotImplementedError()\n  \n def is_available(self)->bool:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError()\n  \n def has_expired(self)->bool:\n  ''\n\n\n\n\n  \n  raise NotImplementedError()\n  \n def is_idle(self)->bool:\n  ''\n\n  \n  raise NotImplementedError()\n  \n def is_closed(self)->bool:\n  ''\n\n\n\n\n  \n  raise NotImplementedError()\n", ["__future__", "contextlib", "httpcore._models", "typing"]], "httpcore._async.http11": [".py", "from __future__ import annotations\n\nimport enum\nimport logging\nimport ssl\nimport time\nimport types\nimport typing\n\nimport h11\n\nfrom.._backends.base import AsyncNetworkStream\nfrom.._exceptions import(\nConnectionNotAvailable,\nLocalProtocolError,\nRemoteProtocolError,\nWriteError,\nmap_exceptions,\n)\nfrom.._models import Origin,Request,Response\nfrom.._synchronization import AsyncLock,AsyncShieldCancellation\nfrom.._trace import Trace\nfrom.interfaces import AsyncConnectionInterface\n\nlogger=logging.getLogger(\"httpcore.http11\")\n\n\n\nH11SendEvent=typing.Union[\nh11.Request,\nh11.Data,\nh11.EndOfMessage,\n]\n\n\nclass HTTPConnectionState(enum.IntEnum):\n NEW=0\n ACTIVE=1\n IDLE=2\n CLOSED=3\n \n \nclass AsyncHTTP11Connection(AsyncConnectionInterface):\n READ_NUM_BYTES=64 *1024\n MAX_INCOMPLETE_EVENT_SIZE=100 *1024\n \n def __init__(\n self,\n origin:Origin,\n stream:AsyncNetworkStream,\n keepalive_expiry:float |None=None,\n )->None:\n  self._origin=origin\n  self._network_stream=stream\n  self._keepalive_expiry:float |None=keepalive_expiry\n  self._expire_at:float |None=None\n  self._state=HTTPConnectionState.NEW\n  self._state_lock=AsyncLock()\n  self._request_count=0\n  self._h11_state=h11.Connection(\n  our_role=h11.CLIENT,\n  max_incomplete_event_size=self.MAX_INCOMPLETE_EVENT_SIZE,\n  )\n  \n async def handle_async_request(self,request:Request)->Response:\n  if not self.can_handle_request(request.url.origin):\n   raise RuntimeError(\n   f\"Attempted to send request to {request.url.origin} on connection \"\n   f\"to {self._origin}\"\n   )\n   \n  async with self._state_lock:\n   if self._state in(HTTPConnectionState.NEW,HTTPConnectionState.IDLE):\n    self._request_count +=1\n    self._state=HTTPConnectionState.ACTIVE\n    self._expire_at=None\n   else:\n    raise ConnectionNotAvailable()\n    \n  try:\n   kwargs={\"request\":request}\n   try:\n    async with Trace(\n    \"send_request_headers\",logger,request,kwargs\n    )as trace:\n     await self._send_request_headers(**kwargs)\n    async with Trace(\"send_request_body\",logger,request,kwargs)as trace:\n     await self._send_request_body(**kwargs)\n   except WriteError:\n   \n   \n   \n   \n   \n    pass\n    \n   async with Trace(\n   \"receive_response_headers\",logger,request,kwargs\n   )as trace:\n    (\n    http_version,\n    status,\n    reason_phrase,\n    headers,\n    trailing_data,\n    )=await self._receive_response_headers(**kwargs)\n    trace.return_value=(\n    http_version,\n    status,\n    reason_phrase,\n    headers,\n    )\n    \n   network_stream=self._network_stream\n   \n   \n   if(status ==101)or(\n   (request.method ==b\"CONNECT\")and(200 <=status <300)\n   ):\n    network_stream=AsyncHTTP11UpgradeStream(network_stream,trailing_data)\n    \n   return Response(\n   status=status,\n   headers=headers,\n   content=HTTP11ConnectionByteStream(self,request),\n   extensions={\n   \"http_version\":http_version,\n   \"reason_phrase\":reason_phrase,\n   \"network_stream\":network_stream,\n   },\n   )\n  except BaseException as exc:\n   with AsyncShieldCancellation():\n    async with Trace(\"response_closed\",logger,request)as trace:\n     await self._response_closed()\n   raise exc\n   \n   \n   \n async def _send_request_headers(self,request:Request)->None:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"write\",None)\n  \n  with map_exceptions({h11.LocalProtocolError:LocalProtocolError}):\n   event=h11.Request(\n   method=request.method,\n   target=request.url.target,\n   headers=request.headers,\n   )\n  await self._send_event(event,timeout=timeout)\n  \n async def _send_request_body(self,request:Request)->None:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"write\",None)\n  \n  assert isinstance(request.stream,typing.AsyncIterable)\n  async for chunk in request.stream:\n   event=h11.Data(data=chunk)\n   await self._send_event(event,timeout=timeout)\n   \n  await self._send_event(h11.EndOfMessage(),timeout=timeout)\n  \n async def _send_event(self,event:h11.Event,timeout:float |None=None)->None:\n  bytes_to_send=self._h11_state.send(event)\n  if bytes_to_send is not None:\n   await self._network_stream.write(bytes_to_send,timeout=timeout)\n   \n   \n   \n async def _receive_response_headers(\n self,request:Request\n )->tuple[bytes,int,bytes,list[tuple[bytes,bytes]],bytes]:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"read\",None)\n  \n  while True:\n   event=await self._receive_event(timeout=timeout)\n   if isinstance(event,h11.Response):\n    break\n   if(\n   isinstance(event,h11.InformationalResponse)\n   and event.status_code ==101\n   ):\n    break\n    \n  http_version=b\"HTTP/\"+event.http_version\n  \n  \n  \n  headers=event.headers.raw_items()\n  \n  trailing_data,_=self._h11_state.trailing_data\n  \n  return http_version,event.status_code,event.reason,headers,trailing_data\n  \n async def _receive_response_body(\n self,request:Request\n )->typing.AsyncIterator[bytes]:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"read\",None)\n  \n  while True:\n   event=await self._receive_event(timeout=timeout)\n   if isinstance(event,h11.Data):\n    yield bytes(event.data)\n   elif isinstance(event,(h11.EndOfMessage,h11.PAUSED)):\n    break\n    \n async def _receive_event(\n self,timeout:float |None=None\n )->h11.Event |type[h11.PAUSED]:\n  while True:\n   with map_exceptions({h11.RemoteProtocolError:RemoteProtocolError}):\n    event=self._h11_state.next_event()\n    \n   if event is h11.NEED_DATA:\n    data=await self._network_stream.read(\n    self.READ_NUM_BYTES,timeout=timeout\n    )\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    if data ==b\"\"and self._h11_state.their_state ==h11.SEND_RESPONSE:\n     msg=\"Server disconnected without sending a response.\"\n     raise RemoteProtocolError(msg)\n     \n    self._h11_state.receive_data(data)\n   else:\n   \n    return event\n    \n async def _response_closed(self)->None:\n  async with self._state_lock:\n   if(\n   self._h11_state.our_state is h11.DONE\n   and self._h11_state.their_state is h11.DONE\n   ):\n    self._state=HTTPConnectionState.IDLE\n    self._h11_state.start_next_cycle()\n    if self._keepalive_expiry is not None:\n     now=time.monotonic()\n     self._expire_at=now+self._keepalive_expiry\n   else:\n    await self.aclose()\n    \n    \n    \n async def aclose(self)->None:\n \n \n  self._state=HTTPConnectionState.CLOSED\n  await self._network_stream.aclose()\n  \n  \n  \n  \n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._origin\n  \n def is_available(self)->bool:\n \n \n \n \n  return self._state ==HTTPConnectionState.IDLE\n  \n def has_expired(self)->bool:\n  now=time.monotonic()\n  keepalive_expired=self._expire_at is not None and now >self._expire_at\n  \n  \n  \n  \n  server_disconnected=(\n  self._state ==HTTPConnectionState.IDLE\n  and self._network_stream.get_extra_info(\"is_readable\")\n  )\n  \n  return keepalive_expired or server_disconnected\n  \n def is_idle(self)->bool:\n  return self._state ==HTTPConnectionState.IDLE\n  \n def is_closed(self)->bool:\n  return self._state ==HTTPConnectionState.CLOSED\n  \n def info(self)->str:\n  origin=str(self._origin)\n  return(\n  f\"{origin !r}, HTTP/1.1, {self._state.name}, \"\n  f\"Request Count: {self._request_count}\"\n  )\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  origin=str(self._origin)\n  return(\n  f\"<{class_name} [{origin !r}, {self._state.name}, \"\n  f\"Request Count: {self._request_count}]>\"\n  )\n  \n  \n  \n  \n async def __aenter__(self)->AsyncHTTP11Connection:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  await self.aclose()\n  \n  \nclass HTTP11ConnectionByteStream:\n def __init__(self,connection:AsyncHTTP11Connection,request:Request)->None:\n  self._connection=connection\n  self._request=request\n  self._closed=False\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  kwargs={\"request\":self._request}\n  try:\n   async with Trace(\"receive_response_body\",logger,self._request,kwargs):\n    async for chunk in self._connection._receive_response_body(**kwargs):\n     yield chunk\n  except BaseException as exc:\n  \n  \n  \n   with AsyncShieldCancellation():\n    await self.aclose()\n   raise exc\n   \n async def aclose(self)->None:\n  if not self._closed:\n   self._closed=True\n   async with Trace(\"response_closed\",logger,self._request):\n    await self._connection._response_closed()\n    \n    \nclass AsyncHTTP11UpgradeStream(AsyncNetworkStream):\n def __init__(self,stream:AsyncNetworkStream,leading_data:bytes)->None:\n  self._stream=stream\n  self._leading_data=leading_data\n  \n async def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  if self._leading_data:\n   buffer=self._leading_data[:max_bytes]\n   self._leading_data=self._leading_data[max_bytes:]\n   return buffer\n  else:\n   return await self._stream.read(max_bytes,timeout)\n   \n async def write(self,buffer:bytes,timeout:float |None=None)->None:\n  await self._stream.write(buffer,timeout)\n  \n async def aclose(self)->None:\n  await self._stream.aclose()\n  \n async def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->AsyncNetworkStream:\n  return await self._stream.start_tls(ssl_context,server_hostname,timeout)\n  \n def get_extra_info(self,info:str)->typing.Any:\n  return self._stream.get_extra_info(info)\n", ["__future__", "enum", "h11", "httpcore._async.interfaces", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._synchronization", "httpcore._trace", "logging", "ssl", "time", "types", "typing"]], "httpcore._async.socks_proxy": [".py", "from __future__ import annotations\n\nimport logging\nimport ssl\n\nimport socksio\n\nfrom.._backends.auto import AutoBackend\nfrom.._backends.base import AsyncNetworkBackend,AsyncNetworkStream\nfrom.._exceptions import ConnectionNotAvailable,ProxyError\nfrom.._models import URL,Origin,Request,Response,enforce_bytes,enforce_url\nfrom.._ssl import default_ssl_context\nfrom.._synchronization import AsyncLock\nfrom.._trace import Trace\nfrom.connection_pool import AsyncConnectionPool\nfrom.http11 import AsyncHTTP11Connection\nfrom.interfaces import AsyncConnectionInterface\n\nlogger=logging.getLogger(\"httpcore.socks\")\n\n\nAUTH_METHODS={\nb\"\\x00\":\"NO AUTHENTICATION REQUIRED\",\nb\"\\x01\":\"GSSAPI\",\nb\"\\x02\":\"USERNAME/PASSWORD\",\nb\"\\xff\":\"NO ACCEPTABLE METHODS\",\n}\n\nREPLY_CODES={\nb\"\\x00\":\"Succeeded\",\nb\"\\x01\":\"General SOCKS server failure\",\nb\"\\x02\":\"Connection not allowed by ruleset\",\nb\"\\x03\":\"Network unreachable\",\nb\"\\x04\":\"Host unreachable\",\nb\"\\x05\":\"Connection refused\",\nb\"\\x06\":\"TTL expired\",\nb\"\\x07\":\"Command not supported\",\nb\"\\x08\":\"Address type not supported\",\n}\n\n\nasync def _init_socks5_connection(\nstream:AsyncNetworkStream,\n*,\nhost:bytes,\nport:int,\nauth:tuple[bytes,bytes]|None=None,\n)->None:\n conn=socksio.socks5.SOCKS5Connection()\n \n \n auth_method=(\n socksio.socks5.SOCKS5AuthMethod.NO_AUTH_REQUIRED\n if auth is None\n else socksio.socks5.SOCKS5AuthMethod.USERNAME_PASSWORD\n )\n conn.send(socksio.socks5.SOCKS5AuthMethodsRequest([auth_method]))\n outgoing_bytes=conn.data_to_send()\n await stream.write(outgoing_bytes)\n \n \n incoming_bytes=await stream.read(max_bytes=4096)\n response=conn.receive_data(incoming_bytes)\n assert isinstance(response,socksio.socks5.SOCKS5AuthReply)\n if response.method !=auth_method:\n  requested=AUTH_METHODS.get(auth_method,\"UNKNOWN\")\n  responded=AUTH_METHODS.get(response.method,\"UNKNOWN\")\n  raise ProxyError(\n  f\"Requested {requested} from proxy server, but got {responded}.\"\n  )\n  \n if response.method ==socksio.socks5.SOCKS5AuthMethod.USERNAME_PASSWORD:\n \n  assert auth is not None\n  username,password=auth\n  conn.send(socksio.socks5.SOCKS5UsernamePasswordRequest(username,password))\n  outgoing_bytes=conn.data_to_send()\n  await stream.write(outgoing_bytes)\n  \n  \n  incoming_bytes=await stream.read(max_bytes=4096)\n  response=conn.receive_data(incoming_bytes)\n  assert isinstance(response,socksio.socks5.SOCKS5UsernamePasswordReply)\n  if not response.success:\n   raise ProxyError(\"Invalid username/password\")\n   \n   \n conn.send(\n socksio.socks5.SOCKS5CommandRequest.from_address(\n socksio.socks5.SOCKS5Command.CONNECT,(host,port)\n )\n )\n outgoing_bytes=conn.data_to_send()\n await stream.write(outgoing_bytes)\n \n \n incoming_bytes=await stream.read(max_bytes=4096)\n response=conn.receive_data(incoming_bytes)\n assert isinstance(response,socksio.socks5.SOCKS5Reply)\n if response.reply_code !=socksio.socks5.SOCKS5ReplyCode.SUCCEEDED:\n  reply_code=REPLY_CODES.get(response.reply_code,\"UNKOWN\")\n  raise ProxyError(f\"Proxy Server could not connect: {reply_code}.\")\n  \n  \nclass AsyncSOCKSProxy(AsyncConnectionPool):\n ''\n\n \n \n def __init__(\n self,\n proxy_url:URL |bytes |str,\n proxy_auth:tuple[bytes |str,bytes |str]|None=None,\n ssl_context:ssl.SSLContext |None=None,\n max_connections:int |None=10,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n network_backend:AsyncNetworkBackend |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  super().__init__(\n  ssl_context=ssl_context,\n  max_connections=max_connections,\n  max_keepalive_connections=max_keepalive_connections,\n  keepalive_expiry=keepalive_expiry,\n  http1=http1,\n  http2=http2,\n  network_backend=network_backend,\n  retries=retries,\n  )\n  self._ssl_context=ssl_context\n  self._proxy_url=enforce_url(proxy_url,name=\"proxy_url\")\n  if proxy_auth is not None:\n   username,password=proxy_auth\n   username_bytes=enforce_bytes(username,name=\"proxy_auth\")\n   password_bytes=enforce_bytes(password,name=\"proxy_auth\")\n   self._proxy_auth:tuple[bytes,bytes]|None=(\n   username_bytes,\n   password_bytes,\n   )\n  else:\n   self._proxy_auth=None\n   \n def create_connection(self,origin:Origin)->AsyncConnectionInterface:\n  return AsyncSocks5Connection(\n  proxy_origin=self._proxy_url.origin,\n  remote_origin=origin,\n  proxy_auth=self._proxy_auth,\n  ssl_context=self._ssl_context,\n  keepalive_expiry=self._keepalive_expiry,\n  http1=self._http1,\n  http2=self._http2,\n  network_backend=self._network_backend,\n  )\n  \n  \nclass AsyncSocks5Connection(AsyncConnectionInterface):\n def __init__(\n self,\n proxy_origin:Origin,\n remote_origin:Origin,\n proxy_auth:tuple[bytes,bytes]|None=None,\n ssl_context:ssl.SSLContext |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n network_backend:AsyncNetworkBackend |None=None,\n )->None:\n  self._proxy_origin=proxy_origin\n  self._remote_origin=remote_origin\n  self._proxy_auth=proxy_auth\n  self._ssl_context=ssl_context\n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  \n  self._network_backend:AsyncNetworkBackend=(\n  AutoBackend()if network_backend is None else network_backend\n  )\n  self._connect_lock=AsyncLock()\n  self._connection:AsyncConnectionInterface |None=None\n  self._connect_failed=False\n  \n async def handle_async_request(self,request:Request)->Response:\n  timeouts=request.extensions.get(\"timeout\",{})\n  sni_hostname=request.extensions.get(\"sni_hostname\",None)\n  timeout=timeouts.get(\"connect\",None)\n  \n  async with self._connect_lock:\n   if self._connection is None:\n    try:\n    \n     kwargs={\n     \"host\":self._proxy_origin.host.decode(\"ascii\"),\n     \"port\":self._proxy_origin.port,\n     \"timeout\":timeout,\n     }\n     async with Trace(\"connect_tcp\",logger,request,kwargs)as trace:\n      stream=await self._network_backend.connect_tcp(**kwargs)\n      trace.return_value=stream\n      \n      \n     kwargs={\n     \"stream\":stream,\n     \"host\":self._remote_origin.host.decode(\"ascii\"),\n     \"port\":self._remote_origin.port,\n     \"auth\":self._proxy_auth,\n     }\n     async with Trace(\n     \"setup_socks5_connection\",logger,request,kwargs\n     )as trace:\n      await _init_socks5_connection(**kwargs)\n      trace.return_value=stream\n      \n      \n     if self._remote_origin.scheme ==b\"https\":\n      ssl_context=(\n      default_ssl_context()\n      if self._ssl_context is None\n      else self._ssl_context\n      )\n      alpn_protocols=(\n      [\"http/1.1\",\"h2\"]if self._http2 else[\"http/1.1\"]\n      )\n      ssl_context.set_alpn_protocols(alpn_protocols)\n      \n      kwargs={\n      \"ssl_context\":ssl_context,\n      \"server_hostname\":sni_hostname\n      or self._remote_origin.host.decode(\"ascii\"),\n      \"timeout\":timeout,\n      }\n      async with Trace(\"start_tls\",logger,request,kwargs)as trace:\n       stream=await stream.start_tls(**kwargs)\n       trace.return_value=stream\n       \n       \n     ssl_object=stream.get_extra_info(\"ssl_object\")\n     http2_negotiated=(\n     ssl_object is not None\n     and ssl_object.selected_alpn_protocol()==\"h2\"\n     )\n     \n     \n     if http2_negotiated or(\n     self._http2 and not self._http1\n     ):\n      from.http2 import AsyncHTTP2Connection\n      \n      self._connection=AsyncHTTP2Connection(\n      origin=self._remote_origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n     else:\n      self._connection=AsyncHTTP11Connection(\n      origin=self._remote_origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n    except Exception as exc:\n     self._connect_failed=True\n     raise exc\n   elif not self._connection.is_available():\n    raise ConnectionNotAvailable()\n    \n  return await self._connection.handle_async_request(request)\n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._remote_origin\n  \n async def aclose(self)->None:\n  if self._connection is not None:\n   await self._connection.aclose()\n   \n def is_available(self)->bool:\n  if self._connection is None:\n  \n  \n  \n   return(\n   self._http2\n   and(self._remote_origin.scheme ==b\"https\"or not self._http1)\n   and not self._connect_failed\n   )\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_closed()\n  \n def info(self)->str:\n  if self._connection is None:\n   return \"CONNECTION FAILED\"if self._connect_failed else \"CONNECTING\"\n  return self._connection.info()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n", ["__future__", "httpcore._async.connection_pool", "httpcore._async.http11", "httpcore._async.http2", "httpcore._async.interfaces", "httpcore._backends.auto", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._synchronization", "httpcore._trace", "logging", "socksio", "ssl"]], "httpcore._async.connection_pool": [".py", "from __future__ import annotations\n\nimport ssl\nimport sys\nimport types\nimport typing\n\nfrom.._backends.auto import AutoBackend\nfrom.._backends.base import SOCKET_OPTION,AsyncNetworkBackend\nfrom.._exceptions import ConnectionNotAvailable,UnsupportedProtocol\nfrom.._models import Origin,Proxy,Request,Response\nfrom.._synchronization import AsyncEvent,AsyncShieldCancellation,AsyncThreadLock\nfrom.connection import AsyncHTTPConnection\nfrom.interfaces import AsyncConnectionInterface,AsyncRequestInterface\n\n\nclass AsyncPoolRequest:\n def __init__(self,request:Request)->None:\n  self.request=request\n  self.connection:AsyncConnectionInterface |None=None\n  self._connection_acquired=AsyncEvent()\n  \n def assign_to_connection(self,connection:AsyncConnectionInterface |None)->None:\n  self.connection=connection\n  self._connection_acquired.set()\n  \n def clear_connection(self)->None:\n  self.connection=None\n  self._connection_acquired=AsyncEvent()\n  \n async def wait_for_connection(\n self,timeout:float |None=None\n )->AsyncConnectionInterface:\n  if self.connection is None:\n   await self._connection_acquired.wait(timeout=timeout)\n  assert self.connection is not None\n  return self.connection\n  \n def is_queued(self)->bool:\n  return self.connection is None\n  \n  \nclass AsyncConnectionPool(AsyncRequestInterface):\n ''\n\n \n \n def __init__(\n self,\n ssl_context:ssl.SSLContext |None=None,\n proxy:Proxy |None=None,\n max_connections:int |None=10,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n local_address:str |None=None,\n uds:str |None=None,\n network_backend:AsyncNetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  self._ssl_context=ssl_context\n  self._proxy=proxy\n  self._max_connections=(\n  sys.maxsize if max_connections is None else max_connections\n  )\n  self._max_keepalive_connections=(\n  sys.maxsize\n  if max_keepalive_connections is None\n  else max_keepalive_connections\n  )\n  self._max_keepalive_connections=min(\n  self._max_connections,self._max_keepalive_connections\n  )\n  \n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  self._retries=retries\n  self._local_address=local_address\n  self._uds=uds\n  \n  self._network_backend=(\n  AutoBackend()if network_backend is None else network_backend\n  )\n  self._socket_options=socket_options\n  \n  \n  \n  self._connections:list[AsyncConnectionInterface]=[]\n  self._requests:list[AsyncPoolRequest]=[]\n  \n  \n  \n  \n  self._optional_thread_lock=AsyncThreadLock()\n  \n def create_connection(self,origin:Origin)->AsyncConnectionInterface:\n  if self._proxy is not None:\n   if self._proxy.url.scheme in(b\"socks5\",b\"socks5h\"):\n    from.socks_proxy import AsyncSocks5Connection\n    \n    return AsyncSocks5Connection(\n    proxy_origin=self._proxy.url.origin,\n    proxy_auth=self._proxy.auth,\n    remote_origin=origin,\n    ssl_context=self._ssl_context,\n    keepalive_expiry=self._keepalive_expiry,\n    http1=self._http1,\n    http2=self._http2,\n    network_backend=self._network_backend,\n    )\n   elif origin.scheme ==b\"http\":\n    from.http_proxy import AsyncForwardHTTPConnection\n    \n    return AsyncForwardHTTPConnection(\n    proxy_origin=self._proxy.url.origin,\n    proxy_headers=self._proxy.headers,\n    proxy_ssl_context=self._proxy.ssl_context,\n    remote_origin=origin,\n    keepalive_expiry=self._keepalive_expiry,\n    network_backend=self._network_backend,\n    )\n   from.http_proxy import AsyncTunnelHTTPConnection\n   \n   return AsyncTunnelHTTPConnection(\n   proxy_origin=self._proxy.url.origin,\n   proxy_headers=self._proxy.headers,\n   proxy_ssl_context=self._proxy.ssl_context,\n   remote_origin=origin,\n   ssl_context=self._ssl_context,\n   keepalive_expiry=self._keepalive_expiry,\n   http1=self._http1,\n   http2=self._http2,\n   network_backend=self._network_backend,\n   )\n   \n  return AsyncHTTPConnection(\n  origin=origin,\n  ssl_context=self._ssl_context,\n  keepalive_expiry=self._keepalive_expiry,\n  http1=self._http1,\n  http2=self._http2,\n  retries=self._retries,\n  local_address=self._local_address,\n  uds=self._uds,\n  network_backend=self._network_backend,\n  socket_options=self._socket_options,\n  )\n  \n @property\n def connections(self)->list[AsyncConnectionInterface]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return list(self._connections)\n  \n async def handle_async_request(self,request:Request)->Response:\n  ''\n\n\n\n  \n  scheme=request.url.scheme.decode()\n  if scheme ==\"\":\n   raise UnsupportedProtocol(\n   \"Request URL is missing an 'http://' or 'https://' protocol.\"\n   )\n  if scheme not in(\"http\",\"https\",\"ws\",\"wss\"):\n   raise UnsupportedProtocol(\n   f\"Request URL has an unsupported protocol '{scheme}://'.\"\n   )\n   \n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"pool\",None)\n  \n  with self._optional_thread_lock:\n  \n   pool_request=AsyncPoolRequest(request)\n   self._requests.append(pool_request)\n   \n  try:\n   while True:\n    with self._optional_thread_lock:\n    \n    \n     closing=self._assign_requests_to_connections()\n    await self._close_connections(closing)\n    \n    \n    connection=await pool_request.wait_for_connection(timeout=timeout)\n    \n    try:\n    \n     response=await connection.handle_async_request(\n     pool_request.request\n     )\n    except ConnectionNotAvailable:\n    \n    \n    \n    \n     pool_request.clear_connection()\n    else:\n     break\n     \n  except BaseException as exc:\n   with self._optional_thread_lock:\n   \n   \n    self._requests.remove(pool_request)\n    closing=self._assign_requests_to_connections()\n    \n   await self._close_connections(closing)\n   raise exc from None\n   \n   \n   \n  assert isinstance(response.stream,typing.AsyncIterable)\n  return Response(\n  status=response.status,\n  headers=response.headers,\n  content=PoolByteStream(\n  stream=response.stream,pool_request=pool_request,pool=self\n  ),\n  extensions=response.extensions,\n  )\n  \n def _assign_requests_to_connections(self)->list[AsyncConnectionInterface]:\n  ''\n\n\n\n\n\n\n\n  \n  closing_connections=[]\n  \n  \n  \n  for connection in list(self._connections):\n   if connection.is_closed():\n   \n    self._connections.remove(connection)\n   elif connection.has_expired():\n   \n    self._connections.remove(connection)\n    closing_connections.append(connection)\n   elif(\n   connection.is_idle()\n   and len([connection.is_idle()for connection in self._connections])\n   >self._max_keepalive_connections\n   ):\n   \n    self._connections.remove(connection)\n    closing_connections.append(connection)\n    \n    \n  queued_requests=[request for request in self._requests if request.is_queued()]\n  for pool_request in queued_requests:\n   origin=pool_request.request.url.origin\n   available_connections=[\n   connection\n   for connection in self._connections\n   if connection.can_handle_request(origin)and connection.is_available()\n   ]\n   idle_connections=[\n   connection for connection in self._connections if connection.is_idle()\n   ]\n   \n   \n   \n   \n   \n   \n   \n   if available_connections:\n   \n    connection=available_connections[0]\n    pool_request.assign_to_connection(connection)\n   elif len(self._connections)<self._max_connections:\n   \n    connection=self.create_connection(origin)\n    self._connections.append(connection)\n    pool_request.assign_to_connection(connection)\n   elif idle_connections:\n   \n    connection=idle_connections[0]\n    self._connections.remove(connection)\n    closing_connections.append(connection)\n    \n    connection=self.create_connection(origin)\n    self._connections.append(connection)\n    pool_request.assign_to_connection(connection)\n    \n  return closing_connections\n  \n async def _close_connections(self,closing:list[AsyncConnectionInterface])->None:\n \n  with AsyncShieldCancellation():\n   for connection in closing:\n    await connection.aclose()\n    \n async def aclose(self)->None:\n \n \n  with self._optional_thread_lock:\n   closing_connections=list(self._connections)\n   self._connections=[]\n  await self._close_connections(closing_connections)\n  \n async def __aenter__(self)->AsyncConnectionPool:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  await self.aclose()\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  with self._optional_thread_lock:\n   request_is_queued=[request.is_queued()for request in self._requests]\n   connection_is_idle=[\n   connection.is_idle()for connection in self._connections\n   ]\n   \n   num_active_requests=request_is_queued.count(False)\n   num_queued_requests=request_is_queued.count(True)\n   num_active_connections=connection_is_idle.count(False)\n   num_idle_connections=connection_is_idle.count(True)\n   \n  requests_info=(\n  f\"Requests: {num_active_requests} active, {num_queued_requests} queued\"\n  )\n  connection_info=(\n  f\"Connections: {num_active_connections} active, {num_idle_connections} idle\"\n  )\n  \n  return f\"<{class_name} [{requests_info} | {connection_info}]>\"\n  \n  \nclass PoolByteStream:\n def __init__(\n self,\n stream:typing.AsyncIterable[bytes],\n pool_request:AsyncPoolRequest,\n pool:AsyncConnectionPool,\n )->None:\n  self._stream=stream\n  self._pool_request=pool_request\n  self._pool=pool\n  self._closed=False\n  \n async def __aiter__(self)->typing.AsyncIterator[bytes]:\n  try:\n   async for part in self._stream:\n    yield part\n  except BaseException as exc:\n   await self.aclose()\n   raise exc from None\n   \n async def aclose(self)->None:\n  if not self._closed:\n   self._closed=True\n   with AsyncShieldCancellation():\n    if hasattr(self._stream,\"aclose\"):\n     await self._stream.aclose()\n     \n   with self._pool._optional_thread_lock:\n    self._pool._requests.remove(self._pool_request)\n    closing=self._pool._assign_requests_to_connections()\n    \n   await self._pool._close_connections(closing)\n", ["__future__", "httpcore._async.connection", "httpcore._async.http_proxy", "httpcore._async.interfaces", "httpcore._async.socks_proxy", "httpcore._backends.auto", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._synchronization", "ssl", "sys", "types", "typing"]], "httpcore._backends": [".py", "", [], 1], "httpcore._backends.base": [".py", "from __future__ import annotations\n\nimport ssl\nimport time\nimport typing\n\nSOCKET_OPTION=typing.Union[\ntyping.Tuple[int,int,int],\ntyping.Tuple[int,int,typing.Union[bytes,bytearray]],\ntyping.Tuple[int,int,None,int],\n]\n\n\nclass NetworkStream:\n def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  raise NotImplementedError()\n  \n def write(self,buffer:bytes,timeout:float |None=None)->None:\n  raise NotImplementedError()\n  \n def close(self)->None:\n  raise NotImplementedError()\n  \n def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->NetworkStream:\n  raise NotImplementedError()\n  \n def get_extra_info(self,info:str)->typing.Any:\n  return None\n  \n  \nclass NetworkBackend:\n def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->NetworkStream:\n  raise NotImplementedError()\n  \n def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->NetworkStream:\n  raise NotImplementedError()\n  \n def sleep(self,seconds:float)->None:\n  time.sleep(seconds)\n  \n  \nclass AsyncNetworkStream:\n async def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  raise NotImplementedError()\n  \n async def write(self,buffer:bytes,timeout:float |None=None)->None:\n  raise NotImplementedError()\n  \n async def aclose(self)->None:\n  raise NotImplementedError()\n  \n async def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->AsyncNetworkStream:\n  raise NotImplementedError()\n  \n def get_extra_info(self,info:str)->typing.Any:\n  return None\n  \n  \nclass AsyncNetworkBackend:\n async def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  raise NotImplementedError()\n  \n async def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  raise NotImplementedError()\n  \n async def sleep(self,seconds:float)->None:\n  raise NotImplementedError()\n", ["__future__", "ssl", "time", "typing"]], "httpcore._backends.anyio": [".py", "from __future__ import annotations\n\nimport ssl\nimport typing\n\nimport anyio\n\nfrom.._exceptions import(\nConnectError,\nConnectTimeout,\nReadError,\nReadTimeout,\nWriteError,\nWriteTimeout,\nmap_exceptions,\n)\nfrom.._utils import is_socket_readable\nfrom.base import SOCKET_OPTION,AsyncNetworkBackend,AsyncNetworkStream\n\n\nclass AnyIOStream(AsyncNetworkStream):\n def __init__(self,stream:anyio.abc.ByteStream)->None:\n  self._stream=stream\n  \n async def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  exc_map={\n  TimeoutError:ReadTimeout,\n  anyio.BrokenResourceError:ReadError,\n  anyio.ClosedResourceError:ReadError,\n  anyio.EndOfStream:ReadError,\n  }\n  with map_exceptions(exc_map):\n   with anyio.fail_after(timeout):\n    try:\n     return await self._stream.receive(max_bytes=max_bytes)\n    except anyio.EndOfStream:\n     return b\"\"\n     \n async def write(self,buffer:bytes,timeout:float |None=None)->None:\n  if not buffer:\n   return\n   \n  exc_map={\n  TimeoutError:WriteTimeout,\n  anyio.BrokenResourceError:WriteError,\n  anyio.ClosedResourceError:WriteError,\n  }\n  with map_exceptions(exc_map):\n   with anyio.fail_after(timeout):\n    await self._stream.send(item=buffer)\n    \n async def aclose(self)->None:\n  await self._stream.aclose()\n  \n async def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->AsyncNetworkStream:\n  exc_map={\n  TimeoutError:ConnectTimeout,\n  anyio.BrokenResourceError:ConnectError,\n  anyio.EndOfStream:ConnectError,\n  ssl.SSLError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   try:\n    with anyio.fail_after(timeout):\n     ssl_stream=await anyio.streams.tls.TLSStream.wrap(\n     self._stream,\n     ssl_context=ssl_context,\n     hostname=server_hostname,\n     standard_compatible=False,\n     server_side=False,\n     )\n   except Exception as exc:\n    await self.aclose()\n    raise exc\n  return AnyIOStream(ssl_stream)\n  \n def get_extra_info(self,info:str)->typing.Any:\n  if info ==\"ssl_object\":\n   return self._stream.extra(anyio.streams.tls.TLSAttribute.ssl_object,None)\n  if info ==\"client_addr\":\n   return self._stream.extra(anyio.abc.SocketAttribute.local_address,None)\n  if info ==\"server_addr\":\n   return self._stream.extra(anyio.abc.SocketAttribute.remote_address,None)\n  if info ==\"socket\":\n   return self._stream.extra(anyio.abc.SocketAttribute.raw_socket,None)\n  if info ==\"is_readable\":\n   sock=self._stream.extra(anyio.abc.SocketAttribute.raw_socket,None)\n   return is_socket_readable(sock)\n  return None\n  \n  \nclass AnyIOBackend(AsyncNetworkBackend):\n async def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  if socket_options is None:\n   socket_options=[]\n  exc_map={\n  TimeoutError:ConnectTimeout,\n  OSError:ConnectError,\n  anyio.BrokenResourceError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   with anyio.fail_after(timeout):\n    stream:anyio.abc.ByteStream=await anyio.connect_tcp(\n    remote_host=host,\n    remote_port=port,\n    local_host=local_address,\n    )\n    \n    for option in socket_options:\n     stream._raw_socket.setsockopt(*option)\n  return AnyIOStream(stream)\n  \n async def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  if socket_options is None:\n   socket_options=[]\n  exc_map={\n  TimeoutError:ConnectTimeout,\n  OSError:ConnectError,\n  anyio.BrokenResourceError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   with anyio.fail_after(timeout):\n    stream:anyio.abc.ByteStream=await anyio.connect_unix(path)\n    for option in socket_options:\n     stream._raw_socket.setsockopt(*option)\n  return AnyIOStream(stream)\n  \n async def sleep(self,seconds:float)->None:\n  await anyio.sleep(seconds)\n", ["__future__", "anyio", "httpcore._backends.base", "httpcore._exceptions", "httpcore._utils", "ssl", "typing"]], "httpcore._backends.auto": [".py", "from __future__ import annotations\n\nimport typing\n\nfrom.._synchronization import current_async_library\nfrom.base import SOCKET_OPTION,AsyncNetworkBackend,AsyncNetworkStream\n\n\nclass AutoBackend(AsyncNetworkBackend):\n async def _init_backend(self)->None:\n  if not(hasattr(self,\"_backend\")):\n   backend=current_async_library()\n   if backend ==\"trio\":\n    from.trio import TrioBackend\n    \n    self._backend:AsyncNetworkBackend=TrioBackend()\n   else:\n    from.anyio import AnyIOBackend\n    \n    self._backend=AnyIOBackend()\n    \n async def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  await self._init_backend()\n  return await self._backend.connect_tcp(\n  host,\n  port,\n  timeout=timeout,\n  local_address=local_address,\n  socket_options=socket_options,\n  )\n  \n async def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  await self._init_backend()\n  return await self._backend.connect_unix_socket(\n  path,timeout=timeout,socket_options=socket_options\n  )\n  \n async def sleep(self,seconds:float)->None:\n  await self._init_backend()\n  return await self._backend.sleep(seconds)\n", ["__future__", "httpcore._backends.anyio", "httpcore._backends.base", "httpcore._backends.trio", "httpcore._synchronization", "typing"]], "httpcore._backends.sync": [".py", "from __future__ import annotations\n\nimport functools\nimport socket\nimport ssl\nimport sys\nimport typing\n\nfrom.._exceptions import(\nConnectError,\nConnectTimeout,\nExceptionMapping,\nReadError,\nReadTimeout,\nWriteError,\nWriteTimeout,\nmap_exceptions,\n)\nfrom.._utils import is_socket_readable\nfrom.base import SOCKET_OPTION,NetworkBackend,NetworkStream\n\n\nclass TLSinTLSStream(NetworkStream):\n ''\n\n\n\n\n \n \n \n TLS_RECORD_SIZE=16384\n \n def __init__(\n self,\n sock:socket.socket,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n ):\n  self._sock=sock\n  self._incoming=ssl.MemoryBIO()\n  self._outgoing=ssl.MemoryBIO()\n  \n  self.ssl_obj=ssl_context.wrap_bio(\n  incoming=self._incoming,\n  outgoing=self._outgoing,\n  server_hostname=server_hostname,\n  )\n  \n  self._sock.settimeout(timeout)\n  self._perform_io(self.ssl_obj.do_handshake)\n  \n def _perform_io(\n self,\n func:typing.Callable[...,typing.Any],\n )->typing.Any:\n  ret=None\n  \n  while True:\n   errno=None\n   try:\n    ret=func()\n   except(ssl.SSLWantReadError,ssl.SSLWantWriteError)as e:\n    errno=e.errno\n    \n   self._sock.sendall(self._outgoing.read())\n   \n   if errno ==ssl.SSL_ERROR_WANT_READ:\n    buf=self._sock.recv(self.TLS_RECORD_SIZE)\n    \n    if buf:\n     self._incoming.write(buf)\n    else:\n     self._incoming.write_eof()\n   if errno is None:\n    return ret\n    \n def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  exc_map:ExceptionMapping={socket.timeout:ReadTimeout,OSError:ReadError}\n  with map_exceptions(exc_map):\n   self._sock.settimeout(timeout)\n   return typing.cast(\n   bytes,self._perform_io(functools.partial(self.ssl_obj.read,max_bytes))\n   )\n   \n def write(self,buffer:bytes,timeout:float |None=None)->None:\n  exc_map:ExceptionMapping={socket.timeout:WriteTimeout,OSError:WriteError}\n  with map_exceptions(exc_map):\n   self._sock.settimeout(timeout)\n   while buffer:\n    nsent=self._perform_io(functools.partial(self.ssl_obj.write,buffer))\n    buffer=buffer[nsent:]\n    \n def close(self)->None:\n  self._sock.close()\n  \n def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->NetworkStream:\n  raise NotImplementedError()\n  \n def get_extra_info(self,info:str)->typing.Any:\n  if info ==\"ssl_object\":\n   return self.ssl_obj\n  if info ==\"client_addr\":\n   return self._sock.getsockname()\n  if info ==\"server_addr\":\n   return self._sock.getpeername()\n  if info ==\"socket\":\n   return self._sock\n  if info ==\"is_readable\":\n   return is_socket_readable(self._sock)\n  return None\n  \n  \nclass SyncStream(NetworkStream):\n def __init__(self,sock:socket.socket)->None:\n  self._sock=sock\n  \n def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  exc_map:ExceptionMapping={socket.timeout:ReadTimeout,OSError:ReadError}\n  with map_exceptions(exc_map):\n   self._sock.settimeout(timeout)\n   return self._sock.recv(max_bytes)\n   \n def write(self,buffer:bytes,timeout:float |None=None)->None:\n  if not buffer:\n   return\n   \n  exc_map:ExceptionMapping={socket.timeout:WriteTimeout,OSError:WriteError}\n  with map_exceptions(exc_map):\n   while buffer:\n    self._sock.settimeout(timeout)\n    n=self._sock.send(buffer)\n    buffer=buffer[n:]\n    \n def close(self)->None:\n  self._sock.close()\n  \n def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->NetworkStream:\n  exc_map:ExceptionMapping={\n  socket.timeout:ConnectTimeout,\n  OSError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   try:\n    if isinstance(self._sock,ssl.SSLSocket):\n    \n    \n    \n     return TLSinTLSStream(\n     self._sock,ssl_context,server_hostname,timeout\n     )\n    else:\n     self._sock.settimeout(timeout)\n     sock=ssl_context.wrap_socket(\n     self._sock,server_hostname=server_hostname\n     )\n   except Exception as exc:\n    self.close()\n    raise exc\n  return SyncStream(sock)\n  \n def get_extra_info(self,info:str)->typing.Any:\n  if info ==\"ssl_object\"and isinstance(self._sock,ssl.SSLSocket):\n   return self._sock._sslobj\n  if info ==\"client_addr\":\n   return self._sock.getsockname()\n  if info ==\"server_addr\":\n   return self._sock.getpeername()\n  if info ==\"socket\":\n   return self._sock\n  if info ==\"is_readable\":\n   return is_socket_readable(self._sock)\n  return None\n  \n  \nclass SyncBackend(NetworkBackend):\n def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->NetworkStream:\n \n \n  if socket_options is None:\n   socket_options=[]\n  address=(host,port)\n  source_address=None if local_address is None else(local_address,0)\n  exc_map:ExceptionMapping={\n  socket.timeout:ConnectTimeout,\n  OSError:ConnectError,\n  }\n  \n  with map_exceptions(exc_map):\n   sock=socket.create_connection(\n   address,\n   timeout,\n   source_address=source_address,\n   )\n   for option in socket_options:\n    sock.setsockopt(*option)\n   sock.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)\n  return SyncStream(sock)\n  \n def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->NetworkStream:\n  if sys.platform ==\"win32\":\n   raise RuntimeError(\n   \"Attempted to connect to a UNIX socket on a Windows system.\"\n   )\n  if socket_options is None:\n   socket_options=[]\n   \n  exc_map:ExceptionMapping={\n  socket.timeout:ConnectTimeout,\n  OSError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   sock=socket.socket(socket.AF_UNIX,socket.SOCK_STREAM)\n   for option in socket_options:\n    sock.setsockopt(*option)\n   sock.settimeout(timeout)\n   sock.connect(path)\n  return SyncStream(sock)\n", ["__future__", "functools", "httpcore._backends.base", "httpcore._exceptions", "httpcore._utils", "socket", "ssl", "sys", "typing"]], "httpcore._backends.mock": [".py", "from __future__ import annotations\n\nimport ssl\nimport typing\n\nfrom.._exceptions import ReadError\nfrom.base import(\nSOCKET_OPTION,\nAsyncNetworkBackend,\nAsyncNetworkStream,\nNetworkBackend,\nNetworkStream,\n)\n\n\nclass MockSSLObject:\n def __init__(self,http2:bool):\n  self._http2=http2\n  \n def selected_alpn_protocol(self)->str:\n  return \"h2\"if self._http2 else \"http/1.1\"\n  \n  \nclass MockStream(NetworkStream):\n def __init__(self,buffer:list[bytes],http2:bool=False)->None:\n  self._buffer=buffer\n  self._http2=http2\n  self._closed=False\n  \n def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  if self._closed:\n   raise ReadError(\"Connection closed\")\n  if not self._buffer:\n   return b\"\"\n  return self._buffer.pop(0)\n  \n def write(self,buffer:bytes,timeout:float |None=None)->None:\n  pass\n  \n def close(self)->None:\n  self._closed=True\n  \n def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->NetworkStream:\n  return self\n  \n def get_extra_info(self,info:str)->typing.Any:\n  return MockSSLObject(http2=self._http2)if info ==\"ssl_object\"else None\n  \n def __repr__(self)->str:\n  return \"<httpcore.MockStream>\"\n  \n  \nclass MockBackend(NetworkBackend):\n def __init__(self,buffer:list[bytes],http2:bool=False)->None:\n  self._buffer=buffer\n  self._http2=http2\n  \n def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->NetworkStream:\n  return MockStream(list(self._buffer),http2=self._http2)\n  \n def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->NetworkStream:\n  return MockStream(list(self._buffer),http2=self._http2)\n  \n def sleep(self,seconds:float)->None:\n  pass\n  \n  \nclass AsyncMockStream(AsyncNetworkStream):\n def __init__(self,buffer:list[bytes],http2:bool=False)->None:\n  self._buffer=buffer\n  self._http2=http2\n  self._closed=False\n  \n async def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  if self._closed:\n   raise ReadError(\"Connection closed\")\n  if not self._buffer:\n   return b\"\"\n  return self._buffer.pop(0)\n  \n async def write(self,buffer:bytes,timeout:float |None=None)->None:\n  pass\n  \n async def aclose(self)->None:\n  self._closed=True\n  \n async def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->AsyncNetworkStream:\n  return self\n  \n def get_extra_info(self,info:str)->typing.Any:\n  return MockSSLObject(http2=self._http2)if info ==\"ssl_object\"else None\n  \n def __repr__(self)->str:\n  return \"<httpcore.AsyncMockStream>\"\n  \n  \nclass AsyncMockBackend(AsyncNetworkBackend):\n def __init__(self,buffer:list[bytes],http2:bool=False)->None:\n  self._buffer=buffer\n  self._http2=http2\n  \n async def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  return AsyncMockStream(list(self._buffer),http2=self._http2)\n  \n async def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  return AsyncMockStream(list(self._buffer),http2=self._http2)\n  \n async def sleep(self,seconds:float)->None:\n  pass\n", ["__future__", "httpcore._backends.base", "httpcore._exceptions", "ssl", "typing"]], "httpcore._backends.trio": [".py", "from __future__ import annotations\n\nimport ssl\nimport typing\n\nimport trio\n\nfrom.._exceptions import(\nConnectError,\nConnectTimeout,\nExceptionMapping,\nReadError,\nReadTimeout,\nWriteError,\nWriteTimeout,\nmap_exceptions,\n)\nfrom.base import SOCKET_OPTION,AsyncNetworkBackend,AsyncNetworkStream\n\n\nclass TrioStream(AsyncNetworkStream):\n def __init__(self,stream:trio.abc.Stream)->None:\n  self._stream=stream\n  \n async def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  timeout_or_inf=float(\"inf\")if timeout is None else timeout\n  exc_map:ExceptionMapping={\n  trio.TooSlowError:ReadTimeout,\n  trio.BrokenResourceError:ReadError,\n  trio.ClosedResourceError:ReadError,\n  }\n  with map_exceptions(exc_map):\n   with trio.fail_after(timeout_or_inf):\n    data:bytes=await self._stream.receive_some(max_bytes=max_bytes)\n    return data\n    \n async def write(self,buffer:bytes,timeout:float |None=None)->None:\n  if not buffer:\n   return\n   \n  timeout_or_inf=float(\"inf\")if timeout is None else timeout\n  exc_map:ExceptionMapping={\n  trio.TooSlowError:WriteTimeout,\n  trio.BrokenResourceError:WriteError,\n  trio.ClosedResourceError:WriteError,\n  }\n  with map_exceptions(exc_map):\n   with trio.fail_after(timeout_or_inf):\n    await self._stream.send_all(data=buffer)\n    \n async def aclose(self)->None:\n  await self._stream.aclose()\n  \n async def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->AsyncNetworkStream:\n  timeout_or_inf=float(\"inf\")if timeout is None else timeout\n  exc_map:ExceptionMapping={\n  trio.TooSlowError:ConnectTimeout,\n  trio.BrokenResourceError:ConnectError,\n  }\n  ssl_stream=trio.SSLStream(\n  self._stream,\n  ssl_context=ssl_context,\n  server_hostname=server_hostname,\n  https_compatible=True,\n  server_side=False,\n  )\n  with map_exceptions(exc_map):\n   try:\n    with trio.fail_after(timeout_or_inf):\n     await ssl_stream.do_handshake()\n   except Exception as exc:\n    await self.aclose()\n    raise exc\n  return TrioStream(ssl_stream)\n  \n def get_extra_info(self,info:str)->typing.Any:\n  if info ==\"ssl_object\"and isinstance(self._stream,trio.SSLStream):\n  \n  \n   return self._stream._ssl_object\n  if info ==\"client_addr\":\n   return self._get_socket_stream().socket.getsockname()\n  if info ==\"server_addr\":\n   return self._get_socket_stream().socket.getpeername()\n  if info ==\"socket\":\n   stream=self._stream\n   while isinstance(stream,trio.SSLStream):\n    stream=stream.transport_stream\n   assert isinstance(stream,trio.SocketStream)\n   return stream.socket\n  if info ==\"is_readable\":\n   socket=self.get_extra_info(\"socket\")\n   return socket.is_readable()\n  return None\n  \n def _get_socket_stream(self)->trio.SocketStream:\n  stream=self._stream\n  while isinstance(stream,trio.SSLStream):\n   stream=stream.transport_stream\n  assert isinstance(stream,trio.SocketStream)\n  return stream\n  \n  \nclass TrioBackend(AsyncNetworkBackend):\n async def connect_tcp(\n self,\n host:str,\n port:int,\n timeout:float |None=None,\n local_address:str |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n \n \n  if socket_options is None:\n   socket_options=[]\n  timeout_or_inf=float(\"inf\")if timeout is None else timeout\n  exc_map:ExceptionMapping={\n  trio.TooSlowError:ConnectTimeout,\n  trio.BrokenResourceError:ConnectError,\n  OSError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   with trio.fail_after(timeout_or_inf):\n    stream:trio.abc.Stream=await trio.open_tcp_stream(\n    host=host,port=port,local_address=local_address\n    )\n    for option in socket_options:\n     stream.setsockopt(*option)\n  return TrioStream(stream)\n  \n async def connect_unix_socket(\n self,\n path:str,\n timeout:float |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->AsyncNetworkStream:\n  if socket_options is None:\n   socket_options=[]\n  timeout_or_inf=float(\"inf\")if timeout is None else timeout\n  exc_map:ExceptionMapping={\n  trio.TooSlowError:ConnectTimeout,\n  trio.BrokenResourceError:ConnectError,\n  OSError:ConnectError,\n  }\n  with map_exceptions(exc_map):\n   with trio.fail_after(timeout_or_inf):\n    stream:trio.abc.Stream=await trio.open_unix_socket(path)\n    for option in socket_options:\n     stream.setsockopt(*option)\n  return TrioStream(stream)\n  \n async def sleep(self,seconds:float)->None:\n  await trio.sleep(seconds)\n", ["__future__", "httpcore._backends.base", "httpcore._exceptions", "ssl", "trio", "typing"]], "httpcore._sync": [".py", "from.connection import HTTPConnection\nfrom.connection_pool import ConnectionPool\nfrom.http11 import HTTP11Connection\nfrom.http_proxy import HTTPProxy\nfrom.interfaces import ConnectionInterface\n\ntry:\n from.http2 import HTTP2Connection\nexcept ImportError:\n\n class HTTP2Connection:\n  def __init__(self,*args,**kwargs)->None:\n   raise RuntimeError(\n   \"Attempted to use http2 support, but the `h2` package is not \"\n   \"installed. Use 'pip install httpcore[http2]'.\"\n   )\n   \n   \ntry:\n from.socks_proxy import SOCKSProxy\nexcept ImportError:\n\n class SOCKSProxy:\n  def __init__(self,*args,**kwargs)->None:\n   raise RuntimeError(\n   \"Attempted to use SOCKS support, but the `socksio` package is not \"\n   \"installed. Use 'pip install httpcore[socks]'.\"\n   )\n   \n   \n__all__=[\n\"HTTPConnection\",\n\"ConnectionPool\",\n\"HTTPProxy\",\n\"HTTP11Connection\",\n\"HTTP2Connection\",\n\"ConnectionInterface\",\n\"SOCKSProxy\",\n]\n", ["httpcore._sync.connection", "httpcore._sync.connection_pool", "httpcore._sync.http11", "httpcore._sync.http2", "httpcore._sync.http_proxy", "httpcore._sync.interfaces", "httpcore._sync.socks_proxy"], 1], "httpcore._sync.http2": [".py", "from __future__ import annotations\n\nimport enum\nimport logging\nimport time\nimport types\nimport typing\n\nimport h2.config\nimport h2.connection\nimport h2.events\nimport h2.exceptions\nimport h2.settings\n\nfrom.._backends.base import NetworkStream\nfrom.._exceptions import(\nConnectionNotAvailable,\nLocalProtocolError,\nRemoteProtocolError,\n)\nfrom.._models import Origin,Request,Response\nfrom.._synchronization import Lock,Semaphore,ShieldCancellation\nfrom.._trace import Trace\nfrom.interfaces import ConnectionInterface\n\nlogger=logging.getLogger(\"httpcore.http2\")\n\n\ndef has_body_headers(request:Request)->bool:\n return any(\n k.lower()==b\"content-length\"or k.lower()==b\"transfer-encoding\"\n for k,v in request.headers\n )\n \n \nclass HTTPConnectionState(enum.IntEnum):\n ACTIVE=1\n IDLE=2\n CLOSED=3\n \n \nclass HTTP2Connection(ConnectionInterface):\n READ_NUM_BYTES=64 *1024\n CONFIG=h2.config.H2Configuration(validate_inbound_headers=False)\n \n def __init__(\n self,\n origin:Origin,\n stream:NetworkStream,\n keepalive_expiry:float |None=None,\n ):\n  self._origin=origin\n  self._network_stream=stream\n  self._keepalive_expiry:float |None=keepalive_expiry\n  self._h2_state=h2.connection.H2Connection(config=self.CONFIG)\n  self._state=HTTPConnectionState.IDLE\n  self._expire_at:float |None=None\n  self._request_count=0\n  self._init_lock=Lock()\n  self._state_lock=Lock()\n  self._read_lock=Lock()\n  self._write_lock=Lock()\n  self._sent_connection_init=False\n  self._used_all_stream_ids=False\n  self._connection_error=False\n  \n  \n  self._events:dict[\n  int,\n  h2.events.ResponseReceived\n  |h2.events.DataReceived\n  |h2.events.StreamEnded\n  |h2.events.StreamReset,\n  ]={}\n  \n  \n  \n  self._connection_terminated:h2.events.ConnectionTerminated |None=None\n  \n  self._read_exception:Exception |None=None\n  self._write_exception:Exception |None=None\n  \n def handle_request(self,request:Request)->Response:\n  if not self.can_handle_request(request.url.origin):\n  \n  \n  \n  \n   raise RuntimeError(\n   f\"Attempted to send request to {request.url.origin} on connection \"\n   f\"to {self._origin}\"\n   )\n   \n  with self._state_lock:\n   if self._state in(HTTPConnectionState.ACTIVE,HTTPConnectionState.IDLE):\n    self._request_count +=1\n    self._expire_at=None\n    self._state=HTTPConnectionState.ACTIVE\n   else:\n    raise ConnectionNotAvailable()\n    \n  with self._init_lock:\n   if not self._sent_connection_init:\n    try:\n     kwargs={\"request\":request}\n     with Trace(\"send_connection_init\",logger,request,kwargs):\n      self._send_connection_init(**kwargs)\n    except BaseException as exc:\n     with ShieldCancellation():\n      self.close()\n     raise exc\n     \n    self._sent_connection_init=True\n    \n    \n    \n    self._max_streams=1\n    \n    local_settings_max_streams=(\n    self._h2_state.local_settings.max_concurrent_streams\n    )\n    self._max_streams_semaphore=Semaphore(local_settings_max_streams)\n    \n    for _ in range(local_settings_max_streams -self._max_streams):\n     self._max_streams_semaphore.acquire()\n     \n  self._max_streams_semaphore.acquire()\n  \n  try:\n   stream_id=self._h2_state.get_next_available_stream_id()\n   self._events[stream_id]=[]\n  except h2.exceptions.NoAvailableStreamIDError:\n   self._used_all_stream_ids=True\n   self._request_count -=1\n   raise ConnectionNotAvailable()\n   \n  try:\n   kwargs={\"request\":request,\"stream_id\":stream_id}\n   with Trace(\"send_request_headers\",logger,request,kwargs):\n    self._send_request_headers(request=request,stream_id=stream_id)\n   with Trace(\"send_request_body\",logger,request,kwargs):\n    self._send_request_body(request=request,stream_id=stream_id)\n   with Trace(\n   \"receive_response_headers\",logger,request,kwargs\n   )as trace:\n    status,headers=self._receive_response(\n    request=request,stream_id=stream_id\n    )\n    trace.return_value=(status,headers)\n    \n   return Response(\n   status=status,\n   headers=headers,\n   content=HTTP2ConnectionByteStream(self,request,stream_id=stream_id),\n   extensions={\n   \"http_version\":b\"HTTP/2\",\n   \"network_stream\":self._network_stream,\n   \"stream_id\":stream_id,\n   },\n   )\n  except BaseException as exc:\n   with ShieldCancellation():\n    kwargs={\"stream_id\":stream_id}\n    with Trace(\"response_closed\",logger,request,kwargs):\n     self._response_closed(stream_id=stream_id)\n     \n   if isinstance(exc,h2.exceptions.ProtocolError):\n   \n   \n   \n   \n   \n   \n   \n   \n   \n    if self._connection_terminated:\n     raise RemoteProtocolError(self._connection_terminated)\n     \n     \n    raise LocalProtocolError(exc)\n    \n   raise exc\n   \n def _send_connection_init(self,request:Request)->None:\n  ''\n\n\n  \n  \n  \n  \n  self._h2_state.local_settings=h2.settings.Settings(\n  client=True,\n  initial_values={\n  \n  \n  h2.settings.SettingCodes.ENABLE_PUSH:0,\n  \n  h2.settings.SettingCodes.MAX_CONCURRENT_STREAMS:100,\n  h2.settings.SettingCodes.MAX_HEADER_LIST_SIZE:65536,\n  },\n  )\n  \n  \n  \n  \n  del self._h2_state.local_settings[\n  h2.settings.SettingCodes.ENABLE_CONNECT_PROTOCOL\n  ]\n  \n  self._h2_state.initiate_connection()\n  self._h2_state.increment_flow_control_window(2 **24)\n  self._write_outgoing_data(request)\n  \n  \n  \n def _send_request_headers(self,request:Request,stream_id:int)->None:\n  ''\n\n  \n  end_stream=not has_body_headers(request)\n  \n  \n  \n  \n  \n  authority=[v for k,v in request.headers if k.lower()==b\"host\"][0]\n  \n  headers=[\n  (b\":method\",request.method),\n  (b\":authority\",authority),\n  (b\":scheme\",request.url.scheme),\n  (b\":path\",request.url.target),\n  ]+[\n  (k.lower(),v)\n  for k,v in request.headers\n  if k.lower()\n  not in(\n  b\"host\",\n  b\"transfer-encoding\",\n  )\n  ]\n  \n  self._h2_state.send_headers(stream_id,headers,end_stream=end_stream)\n  self._h2_state.increment_flow_control_window(2 **24,stream_id=stream_id)\n  self._write_outgoing_data(request)\n  \n def _send_request_body(self,request:Request,stream_id:int)->None:\n  ''\n\n  \n  if not has_body_headers(request):\n   return\n   \n  assert isinstance(request.stream,typing.Iterable)\n  for data in request.stream:\n   self._send_stream_data(request,stream_id,data)\n  self._send_end_stream(request,stream_id)\n  \n def _send_stream_data(\n self,request:Request,stream_id:int,data:bytes\n )->None:\n  ''\n\n  \n  while data:\n   max_flow=self._wait_for_outgoing_flow(request,stream_id)\n   chunk_size=min(len(data),max_flow)\n   chunk,data=data[:chunk_size],data[chunk_size:]\n   self._h2_state.send_data(stream_id,chunk)\n   self._write_outgoing_data(request)\n   \n def _send_end_stream(self,request:Request,stream_id:int)->None:\n  ''\n\n  \n  self._h2_state.end_stream(stream_id)\n  self._write_outgoing_data(request)\n  \n  \n  \n def _receive_response(\n self,request:Request,stream_id:int\n )->tuple[int,list[tuple[bytes,bytes]]]:\n  ''\n\n  \n  while True:\n   event=self._receive_stream_event(request,stream_id)\n   if isinstance(event,h2.events.ResponseReceived):\n    break\n    \n  status_code=200\n  headers=[]\n  for k,v in event.headers:\n   if k ==b\":status\":\n    status_code=int(v.decode(\"ascii\",errors=\"ignore\"))\n   elif not k.startswith(b\":\"):\n    headers.append((k,v))\n    \n  return(status_code,headers)\n  \n def _receive_response_body(\n self,request:Request,stream_id:int\n )->typing.Iterator[bytes]:\n  ''\n\n  \n  while True:\n   event=self._receive_stream_event(request,stream_id)\n   if isinstance(event,h2.events.DataReceived):\n    amount=event.flow_controlled_length\n    self._h2_state.acknowledge_received_data(amount,stream_id)\n    self._write_outgoing_data(request)\n    yield event.data\n   elif isinstance(event,h2.events.StreamEnded):\n    break\n    \n def _receive_stream_event(\n self,request:Request,stream_id:int\n )->h2.events.ResponseReceived |h2.events.DataReceived |h2.events.StreamEnded:\n  ''\n\n\n\n  \n  while not self._events.get(stream_id):\n   self._receive_events(request,stream_id)\n  event=self._events[stream_id].pop(0)\n  if isinstance(event,h2.events.StreamReset):\n   raise RemoteProtocolError(event)\n  return event\n  \n def _receive_events(\n self,request:Request,stream_id:int |None=None\n )->None:\n  ''\n\n\n  \n  with self._read_lock:\n   if self._connection_terminated is not None:\n    last_stream_id=self._connection_terminated.last_stream_id\n    if stream_id and last_stream_id and stream_id >last_stream_id:\n     self._request_count -=1\n     raise ConnectionNotAvailable()\n    raise RemoteProtocolError(self._connection_terminated)\n    \n    \n    \n    \n    \n    \n    \n   if stream_id is None or not self._events.get(stream_id):\n    events=self._read_incoming_data(request)\n    for event in events:\n     if isinstance(event,h2.events.RemoteSettingsChanged):\n      with Trace(\n      \"receive_remote_settings\",logger,request\n      )as trace:\n       self._receive_remote_settings_change(event)\n       trace.return_value=event\n       \n     elif isinstance(\n     event,\n     (\n     h2.events.ResponseReceived,\n     h2.events.DataReceived,\n     h2.events.StreamEnded,\n     h2.events.StreamReset,\n     ),\n     ):\n      if event.stream_id in self._events:\n       self._events[event.stream_id].append(event)\n       \n     elif isinstance(event,h2.events.ConnectionTerminated):\n      self._connection_terminated=event\n      \n  self._write_outgoing_data(request)\n  \n def _receive_remote_settings_change(self,event:h2.events.Event)->None:\n  max_concurrent_streams=event.changed_settings.get(\n  h2.settings.SettingCodes.MAX_CONCURRENT_STREAMS\n  )\n  if max_concurrent_streams:\n   new_max_streams=min(\n   max_concurrent_streams.new_value,\n   self._h2_state.local_settings.max_concurrent_streams,\n   )\n   if new_max_streams and new_max_streams !=self._max_streams:\n    while new_max_streams >self._max_streams:\n     self._max_streams_semaphore.release()\n     self._max_streams +=1\n    while new_max_streams <self._max_streams:\n     self._max_streams_semaphore.acquire()\n     self._max_streams -=1\n     \n def _response_closed(self,stream_id:int)->None:\n  self._max_streams_semaphore.release()\n  del self._events[stream_id]\n  with self._state_lock:\n   if self._connection_terminated and not self._events:\n    self.close()\n    \n   elif self._state ==HTTPConnectionState.ACTIVE and not self._events:\n    self._state=HTTPConnectionState.IDLE\n    if self._keepalive_expiry is not None:\n     now=time.monotonic()\n     self._expire_at=now+self._keepalive_expiry\n    if self._used_all_stream_ids:\n     self.close()\n     \n def close(self)->None:\n \n \n  self._h2_state.close_connection()\n  self._state=HTTPConnectionState.CLOSED\n  self._network_stream.close()\n  \n  \n  \n def _read_incoming_data(self,request:Request)->list[h2.events.Event]:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"read\",None)\n  \n  if self._read_exception is not None:\n   raise self._read_exception\n   \n  try:\n   data=self._network_stream.read(self.READ_NUM_BYTES,timeout)\n   if data ==b\"\":\n    raise RemoteProtocolError(\"Server disconnected\")\n  except Exception as exc:\n  \n  \n  \n  \n  \n  \n  \n  \n   self._read_exception=exc\n   self._connection_error=True\n   raise exc\n   \n  events:list[h2.events.Event]=self._h2_state.receive_data(data)\n  \n  return events\n  \n def _write_outgoing_data(self,request:Request)->None:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"write\",None)\n  \n  with self._write_lock:\n   data_to_send=self._h2_state.data_to_send()\n   \n   if self._write_exception is not None:\n    raise self._write_exception\n    \n   try:\n    self._network_stream.write(data_to_send,timeout)\n   except Exception as exc:\n   \n   \n   \n   \n   \n   \n   \n   \n    self._write_exception=exc\n    self._connection_error=True\n    raise exc\n    \n    \n    \n def _wait_for_outgoing_flow(self,request:Request,stream_id:int)->int:\n  ''\n\n\n\n\n\n  \n  local_flow:int=self._h2_state.local_flow_control_window(stream_id)\n  max_frame_size:int=self._h2_state.max_outbound_frame_size\n  flow=min(local_flow,max_frame_size)\n  while flow ==0:\n   self._receive_events(request)\n   local_flow=self._h2_state.local_flow_control_window(stream_id)\n   max_frame_size=self._h2_state.max_outbound_frame_size\n   flow=min(local_flow,max_frame_size)\n  return flow\n  \n  \n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._origin\n  \n def is_available(self)->bool:\n  return(\n  self._state !=HTTPConnectionState.CLOSED\n  and not self._connection_error\n  and not self._used_all_stream_ids\n  and not(\n  self._h2_state.state_machine.state\n  ==h2.connection.ConnectionState.CLOSED\n  )\n  )\n  \n def has_expired(self)->bool:\n  now=time.monotonic()\n  return self._expire_at is not None and now >self._expire_at\n  \n def is_idle(self)->bool:\n  return self._state ==HTTPConnectionState.IDLE\n  \n def is_closed(self)->bool:\n  return self._state ==HTTPConnectionState.CLOSED\n  \n def info(self)->str:\n  origin=str(self._origin)\n  return(\n  f\"{origin !r}, HTTP/2, {self._state.name}, \"\n  f\"Request Count: {self._request_count}\"\n  )\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  origin=str(self._origin)\n  return(\n  f\"<{class_name} [{origin !r}, {self._state.name}, \"\n  f\"Request Count: {self._request_count}]>\"\n  )\n  \n  \n  \n  \n def __enter__(self)->HTTP2Connection:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  self.close()\n  \n  \nclass HTTP2ConnectionByteStream:\n def __init__(\n self,connection:HTTP2Connection,request:Request,stream_id:int\n )->None:\n  self._connection=connection\n  self._request=request\n  self._stream_id=stream_id\n  self._closed=False\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  kwargs={\"request\":self._request,\"stream_id\":self._stream_id}\n  try:\n   with Trace(\"receive_response_body\",logger,self._request,kwargs):\n    for chunk in self._connection._receive_response_body(\n    request=self._request,stream_id=self._stream_id\n    ):\n     yield chunk\n  except BaseException as exc:\n  \n  \n  \n   with ShieldCancellation():\n    self.close()\n   raise exc\n   \n def close(self)->None:\n  if not self._closed:\n   self._closed=True\n   kwargs={\"stream_id\":self._stream_id}\n   with Trace(\"response_closed\",logger,self._request,kwargs):\n    self._connection._response_closed(stream_id=self._stream_id)\n", ["__future__", "enum", "h2.config", "h2.connection", "h2.events", "h2.exceptions", "h2.settings", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._sync.interfaces", "httpcore._synchronization", "httpcore._trace", "logging", "time", "types", "typing"]], "httpcore._sync.connection": [".py", "from __future__ import annotations\n\nimport itertools\nimport logging\nimport ssl\nimport types\nimport typing\n\nfrom.._backends.sync import SyncBackend\nfrom.._backends.base import SOCKET_OPTION,NetworkBackend,NetworkStream\nfrom.._exceptions import ConnectError,ConnectTimeout\nfrom.._models import Origin,Request,Response\nfrom.._ssl import default_ssl_context\nfrom.._synchronization import Lock\nfrom.._trace import Trace\nfrom.http11 import HTTP11Connection\nfrom.interfaces import ConnectionInterface\n\nRETRIES_BACKOFF_FACTOR=0.5\n\n\nlogger=logging.getLogger(\"httpcore.connection\")\n\n\ndef exponential_backoff(factor:float)->typing.Iterator[float]:\n ''\n\n\n\n\n\n \n yield 0\n for n in itertools.count():\n  yield factor *2 **n\n  \n  \nclass HTTPConnection(ConnectionInterface):\n def __init__(\n self,\n origin:Origin,\n ssl_context:ssl.SSLContext |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n local_address:str |None=None,\n uds:str |None=None,\n network_backend:NetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  self._origin=origin\n  self._ssl_context=ssl_context\n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  self._retries=retries\n  self._local_address=local_address\n  self._uds=uds\n  \n  self._network_backend:NetworkBackend=(\n  SyncBackend()if network_backend is None else network_backend\n  )\n  self._connection:ConnectionInterface |None=None\n  self._connect_failed:bool=False\n  self._request_lock=Lock()\n  self._socket_options=socket_options\n  \n def handle_request(self,request:Request)->Response:\n  if not self.can_handle_request(request.url.origin):\n   raise RuntimeError(\n   f\"Attempted to send request to {request.url.origin} on connection to {self._origin}\"\n   )\n   \n  try:\n   with self._request_lock:\n    if self._connection is None:\n     stream=self._connect(request)\n     \n     ssl_object=stream.get_extra_info(\"ssl_object\")\n     http2_negotiated=(\n     ssl_object is not None\n     and ssl_object.selected_alpn_protocol()==\"h2\"\n     )\n     if http2_negotiated or(self._http2 and not self._http1):\n      from.http2 import HTTP2Connection\n      \n      self._connection=HTTP2Connection(\n      origin=self._origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n     else:\n      self._connection=HTTP11Connection(\n      origin=self._origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n  except BaseException as exc:\n   self._connect_failed=True\n   raise exc\n   \n  return self._connection.handle_request(request)\n  \n def _connect(self,request:Request)->NetworkStream:\n  timeouts=request.extensions.get(\"timeout\",{})\n  sni_hostname=request.extensions.get(\"sni_hostname\",None)\n  timeout=timeouts.get(\"connect\",None)\n  \n  retries_left=self._retries\n  delays=exponential_backoff(factor=RETRIES_BACKOFF_FACTOR)\n  \n  while True:\n   try:\n    if self._uds is None:\n     kwargs={\n     \"host\":self._origin.host.decode(\"ascii\"),\n     \"port\":self._origin.port,\n     \"local_address\":self._local_address,\n     \"timeout\":timeout,\n     \"socket_options\":self._socket_options,\n     }\n     with Trace(\"connect_tcp\",logger,request,kwargs)as trace:\n      stream=self._network_backend.connect_tcp(**kwargs)\n      trace.return_value=stream\n    else:\n     kwargs={\n     \"path\":self._uds,\n     \"timeout\":timeout,\n     \"socket_options\":self._socket_options,\n     }\n     with Trace(\n     \"connect_unix_socket\",logger,request,kwargs\n     )as trace:\n      stream=self._network_backend.connect_unix_socket(\n      **kwargs\n      )\n      trace.return_value=stream\n      \n    if self._origin.scheme in(b\"https\",b\"wss\"):\n     ssl_context=(\n     default_ssl_context()\n     if self._ssl_context is None\n     else self._ssl_context\n     )\n     alpn_protocols=[\"http/1.1\",\"h2\"]if self._http2 else[\"http/1.1\"]\n     ssl_context.set_alpn_protocols(alpn_protocols)\n     \n     kwargs={\n     \"ssl_context\":ssl_context,\n     \"server_hostname\":sni_hostname\n     or self._origin.host.decode(\"ascii\"),\n     \"timeout\":timeout,\n     }\n     with Trace(\"start_tls\",logger,request,kwargs)as trace:\n      stream=stream.start_tls(**kwargs)\n      trace.return_value=stream\n    return stream\n   except(ConnectError,ConnectTimeout):\n    if retries_left <=0:\n     raise\n    retries_left -=1\n    delay=next(delays)\n    with Trace(\"retry\",logger,request,kwargs)as trace:\n     self._network_backend.sleep(delay)\n     \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._origin\n  \n def close(self)->None:\n  if self._connection is not None:\n   with Trace(\"close\",logger,None,{}):\n    self._connection.close()\n    \n def is_available(self)->bool:\n  if self._connection is None:\n  \n  \n  \n   return(\n   self._http2\n   and(self._origin.scheme ==b\"https\"or not self._http1)\n   and not self._connect_failed\n   )\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_closed()\n  \n def info(self)->str:\n  if self._connection is None:\n   return \"CONNECTION FAILED\"if self._connect_failed else \"CONNECTING\"\n  return self._connection.info()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n  \n  \n  \n  \n def __enter__(self)->HTTPConnection:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  self.close()\n", ["__future__", "httpcore._backends.base", "httpcore._backends.sync", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._sync.http11", "httpcore._sync.http2", "httpcore._sync.interfaces", "httpcore._synchronization", "httpcore._trace", "itertools", "logging", "ssl", "types", "typing"]], "httpcore._sync.http_proxy": [".py", "from __future__ import annotations\n\nimport base64\nimport logging\nimport ssl\nimport typing\n\nfrom.._backends.base import SOCKET_OPTION,NetworkBackend\nfrom.._exceptions import ProxyError\nfrom.._models import(\nURL,\nOrigin,\nRequest,\nResponse,\nenforce_bytes,\nenforce_headers,\nenforce_url,\n)\nfrom.._ssl import default_ssl_context\nfrom.._synchronization import Lock\nfrom.._trace import Trace\nfrom.connection import HTTPConnection\nfrom.connection_pool import ConnectionPool\nfrom.http11 import HTTP11Connection\nfrom.interfaces import ConnectionInterface\n\nByteOrStr=typing.Union[bytes,str]\nHeadersAsSequence=typing.Sequence[typing.Tuple[ByteOrStr,ByteOrStr]]\nHeadersAsMapping=typing.Mapping[ByteOrStr,ByteOrStr]\n\n\nlogger=logging.getLogger(\"httpcore.proxy\")\n\n\ndef merge_headers(\ndefault_headers:typing.Sequence[tuple[bytes,bytes]]|None=None,\noverride_headers:typing.Sequence[tuple[bytes,bytes]]|None=None,\n)->list[tuple[bytes,bytes]]:\n ''\n\n\n \n default_headers=[]if default_headers is None else list(default_headers)\n override_headers=[]if override_headers is None else list(override_headers)\n has_override=set(key.lower()for key,value in override_headers)\n default_headers=[\n (key,value)\n for key,value in default_headers\n if key.lower()not in has_override\n ]\n return default_headers+override_headers\n \n \nclass HTTPProxy(ConnectionPool):\n ''\n\n \n \n def __init__(\n self,\n proxy_url:URL |bytes |str,\n proxy_auth:tuple[bytes |str,bytes |str]|None=None,\n proxy_headers:HeadersAsMapping |HeadersAsSequence |None=None,\n ssl_context:ssl.SSLContext |None=None,\n proxy_ssl_context:ssl.SSLContext |None=None,\n max_connections:int |None=10,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n local_address:str |None=None,\n uds:str |None=None,\n network_backend:NetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  super().__init__(\n  ssl_context=ssl_context,\n  max_connections=max_connections,\n  max_keepalive_connections=max_keepalive_connections,\n  keepalive_expiry=keepalive_expiry,\n  http1=http1,\n  http2=http2,\n  network_backend=network_backend,\n  retries=retries,\n  local_address=local_address,\n  uds=uds,\n  socket_options=socket_options,\n  )\n  \n  self._proxy_url=enforce_url(proxy_url,name=\"proxy_url\")\n  if(\n  self._proxy_url.scheme ==b\"http\"and proxy_ssl_context is not None\n  ):\n   raise RuntimeError(\n   \"The `proxy_ssl_context` argument is not allowed for the http scheme\"\n   )\n   \n  self._ssl_context=ssl_context\n  self._proxy_ssl_context=proxy_ssl_context\n  self._proxy_headers=enforce_headers(proxy_headers,name=\"proxy_headers\")\n  if proxy_auth is not None:\n   username=enforce_bytes(proxy_auth[0],name=\"proxy_auth\")\n   password=enforce_bytes(proxy_auth[1],name=\"proxy_auth\")\n   userpass=username+b\":\"+password\n   authorization=b\"Basic \"+base64.b64encode(userpass)\n   self._proxy_headers=[\n   (b\"Proxy-Authorization\",authorization)\n   ]+self._proxy_headers\n   \n def create_connection(self,origin:Origin)->ConnectionInterface:\n  if origin.scheme ==b\"http\":\n   return ForwardHTTPConnection(\n   proxy_origin=self._proxy_url.origin,\n   proxy_headers=self._proxy_headers,\n   remote_origin=origin,\n   keepalive_expiry=self._keepalive_expiry,\n   network_backend=self._network_backend,\n   proxy_ssl_context=self._proxy_ssl_context,\n   )\n  return TunnelHTTPConnection(\n  proxy_origin=self._proxy_url.origin,\n  proxy_headers=self._proxy_headers,\n  remote_origin=origin,\n  ssl_context=self._ssl_context,\n  proxy_ssl_context=self._proxy_ssl_context,\n  keepalive_expiry=self._keepalive_expiry,\n  http1=self._http1,\n  http2=self._http2,\n  network_backend=self._network_backend,\n  )\n  \n  \nclass ForwardHTTPConnection(ConnectionInterface):\n def __init__(\n self,\n proxy_origin:Origin,\n remote_origin:Origin,\n proxy_headers:HeadersAsMapping |HeadersAsSequence |None=None,\n keepalive_expiry:float |None=None,\n network_backend:NetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n proxy_ssl_context:ssl.SSLContext |None=None,\n )->None:\n  self._connection=HTTPConnection(\n  origin=proxy_origin,\n  keepalive_expiry=keepalive_expiry,\n  network_backend=network_backend,\n  socket_options=socket_options,\n  ssl_context=proxy_ssl_context,\n  )\n  self._proxy_origin=proxy_origin\n  self._proxy_headers=enforce_headers(proxy_headers,name=\"proxy_headers\")\n  self._remote_origin=remote_origin\n  \n def handle_request(self,request:Request)->Response:\n  headers=merge_headers(self._proxy_headers,request.headers)\n  url=URL(\n  scheme=self._proxy_origin.scheme,\n  host=self._proxy_origin.host,\n  port=self._proxy_origin.port,\n  target=bytes(request.url),\n  )\n  proxy_request=Request(\n  method=request.method,\n  url=url,\n  headers=headers,\n  content=request.stream,\n  extensions=request.extensions,\n  )\n  return self._connection.handle_request(proxy_request)\n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._remote_origin\n  \n def close(self)->None:\n  self._connection.close()\n  \n def info(self)->str:\n  return self._connection.info()\n  \n def is_available(self)->bool:\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  return self._connection.is_closed()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n  \n  \nclass TunnelHTTPConnection(ConnectionInterface):\n def __init__(\n self,\n proxy_origin:Origin,\n remote_origin:Origin,\n ssl_context:ssl.SSLContext |None=None,\n proxy_ssl_context:ssl.SSLContext |None=None,\n proxy_headers:typing.Sequence[tuple[bytes,bytes]]|None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n network_backend:NetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  self._connection:ConnectionInterface=HTTPConnection(\n  origin=proxy_origin,\n  keepalive_expiry=keepalive_expiry,\n  network_backend=network_backend,\n  socket_options=socket_options,\n  ssl_context=proxy_ssl_context,\n  )\n  self._proxy_origin=proxy_origin\n  self._remote_origin=remote_origin\n  self._ssl_context=ssl_context\n  self._proxy_ssl_context=proxy_ssl_context\n  self._proxy_headers=enforce_headers(proxy_headers,name=\"proxy_headers\")\n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  self._connect_lock=Lock()\n  self._connected=False\n  \n def handle_request(self,request:Request)->Response:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"connect\",None)\n  \n  with self._connect_lock:\n   if not self._connected:\n    target=b\"%b:%d\"%(self._remote_origin.host,self._remote_origin.port)\n    \n    connect_url=URL(\n    scheme=self._proxy_origin.scheme,\n    host=self._proxy_origin.host,\n    port=self._proxy_origin.port,\n    target=target,\n    )\n    connect_headers=merge_headers(\n    [(b\"Host\",target),(b\"Accept\",b\"*/*\")],self._proxy_headers\n    )\n    connect_request=Request(\n    method=b\"CONNECT\",\n    url=connect_url,\n    headers=connect_headers,\n    extensions=request.extensions,\n    )\n    connect_response=self._connection.handle_request(\n    connect_request\n    )\n    \n    if connect_response.status <200 or connect_response.status >299:\n     reason_bytes=connect_response.extensions.get(\"reason_phrase\",b\"\")\n     reason_str=reason_bytes.decode(\"ascii\",errors=\"ignore\")\n     msg=\"%d %s\"%(connect_response.status,reason_str)\n     self._connection.close()\n     raise ProxyError(msg)\n     \n    stream=connect_response.extensions[\"network_stream\"]\n    \n    \n    ssl_context=(\n    default_ssl_context()\n    if self._ssl_context is None\n    else self._ssl_context\n    )\n    alpn_protocols=[\"http/1.1\",\"h2\"]if self._http2 else[\"http/1.1\"]\n    ssl_context.set_alpn_protocols(alpn_protocols)\n    \n    kwargs={\n    \"ssl_context\":ssl_context,\n    \"server_hostname\":self._remote_origin.host.decode(\"ascii\"),\n    \"timeout\":timeout,\n    }\n    with Trace(\"start_tls\",logger,request,kwargs)as trace:\n     stream=stream.start_tls(**kwargs)\n     trace.return_value=stream\n     \n     \n    ssl_object=stream.get_extra_info(\"ssl_object\")\n    http2_negotiated=(\n    ssl_object is not None\n    and ssl_object.selected_alpn_protocol()==\"h2\"\n    )\n    \n    \n    if http2_negotiated or(self._http2 and not self._http1):\n     from.http2 import HTTP2Connection\n     \n     self._connection=HTTP2Connection(\n     origin=self._remote_origin,\n     stream=stream,\n     keepalive_expiry=self._keepalive_expiry,\n     )\n    else:\n     self._connection=HTTP11Connection(\n     origin=self._remote_origin,\n     stream=stream,\n     keepalive_expiry=self._keepalive_expiry,\n     )\n     \n    self._connected=True\n  return self._connection.handle_request(request)\n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._remote_origin\n  \n def close(self)->None:\n  self._connection.close()\n  \n def info(self)->str:\n  return self._connection.info()\n  \n def is_available(self)->bool:\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  return self._connection.is_closed()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n", ["__future__", "base64", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._sync.connection", "httpcore._sync.connection_pool", "httpcore._sync.http11", "httpcore._sync.http2", "httpcore._sync.interfaces", "httpcore._synchronization", "httpcore._trace", "logging", "ssl", "typing"]], "httpcore._sync.interfaces": [".py", "from __future__ import annotations\n\nimport contextlib\nimport typing\n\nfrom.._models import(\nURL,\nExtensions,\nHeaderTypes,\nOrigin,\nRequest,\nResponse,\nenforce_bytes,\nenforce_headers,\nenforce_url,\ninclude_request_headers,\n)\n\n\nclass RequestInterface:\n def request(\n self,\n method:bytes |str,\n url:URL |bytes |str,\n *,\n headers:HeaderTypes=None,\n content:bytes |typing.Iterator[bytes]|None=None,\n extensions:Extensions |None=None,\n )->Response:\n \n  method=enforce_bytes(method,name=\"method\")\n  url=enforce_url(url,name=\"url\")\n  headers=enforce_headers(headers,name=\"headers\")\n  \n  \n  headers=include_request_headers(headers,url=url,content=content)\n  \n  request=Request(\n  method=method,\n  url=url,\n  headers=headers,\n  content=content,\n  extensions=extensions,\n  )\n  response=self.handle_request(request)\n  try:\n   response.read()\n  finally:\n   response.close()\n  return response\n  \n @contextlib.contextmanager\n def stream(\n self,\n method:bytes |str,\n url:URL |bytes |str,\n *,\n headers:HeaderTypes=None,\n content:bytes |typing.Iterator[bytes]|None=None,\n extensions:Extensions |None=None,\n )->typing.Iterator[Response]:\n \n  method=enforce_bytes(method,name=\"method\")\n  url=enforce_url(url,name=\"url\")\n  headers=enforce_headers(headers,name=\"headers\")\n  \n  \n  headers=include_request_headers(headers,url=url,content=content)\n  \n  request=Request(\n  method=method,\n  url=url,\n  headers=headers,\n  content=content,\n  extensions=extensions,\n  )\n  response=self.handle_request(request)\n  try:\n   yield response\n  finally:\n   response.close()\n   \n def handle_request(self,request:Request)->Response:\n  raise NotImplementedError()\n  \n  \nclass ConnectionInterface(RequestInterface):\n def close(self)->None:\n  raise NotImplementedError()\n  \n def info(self)->str:\n  raise NotImplementedError()\n  \n def can_handle_request(self,origin:Origin)->bool:\n  raise NotImplementedError()\n  \n def is_available(self)->bool:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  raise NotImplementedError()\n  \n def has_expired(self)->bool:\n  ''\n\n\n\n\n  \n  raise NotImplementedError()\n  \n def is_idle(self)->bool:\n  ''\n\n  \n  raise NotImplementedError()\n  \n def is_closed(self)->bool:\n  ''\n\n\n\n\n  \n  raise NotImplementedError()\n", ["__future__", "contextlib", "httpcore._models", "typing"]], "httpcore._sync.http11": [".py", "from __future__ import annotations\n\nimport enum\nimport logging\nimport ssl\nimport time\nimport types\nimport typing\n\nimport h11\n\nfrom.._backends.base import NetworkStream\nfrom.._exceptions import(\nConnectionNotAvailable,\nLocalProtocolError,\nRemoteProtocolError,\nWriteError,\nmap_exceptions,\n)\nfrom.._models import Origin,Request,Response\nfrom.._synchronization import Lock,ShieldCancellation\nfrom.._trace import Trace\nfrom.interfaces import ConnectionInterface\n\nlogger=logging.getLogger(\"httpcore.http11\")\n\n\n\nH11SendEvent=typing.Union[\nh11.Request,\nh11.Data,\nh11.EndOfMessage,\n]\n\n\nclass HTTPConnectionState(enum.IntEnum):\n NEW=0\n ACTIVE=1\n IDLE=2\n CLOSED=3\n \n \nclass HTTP11Connection(ConnectionInterface):\n READ_NUM_BYTES=64 *1024\n MAX_INCOMPLETE_EVENT_SIZE=100 *1024\n \n def __init__(\n self,\n origin:Origin,\n stream:NetworkStream,\n keepalive_expiry:float |None=None,\n )->None:\n  self._origin=origin\n  self._network_stream=stream\n  self._keepalive_expiry:float |None=keepalive_expiry\n  self._expire_at:float |None=None\n  self._state=HTTPConnectionState.NEW\n  self._state_lock=Lock()\n  self._request_count=0\n  self._h11_state=h11.Connection(\n  our_role=h11.CLIENT,\n  max_incomplete_event_size=self.MAX_INCOMPLETE_EVENT_SIZE,\n  )\n  \n def handle_request(self,request:Request)->Response:\n  if not self.can_handle_request(request.url.origin):\n   raise RuntimeError(\n   f\"Attempted to send request to {request.url.origin} on connection \"\n   f\"to {self._origin}\"\n   )\n   \n  with self._state_lock:\n   if self._state in(HTTPConnectionState.NEW,HTTPConnectionState.IDLE):\n    self._request_count +=1\n    self._state=HTTPConnectionState.ACTIVE\n    self._expire_at=None\n   else:\n    raise ConnectionNotAvailable()\n    \n  try:\n   kwargs={\"request\":request}\n   try:\n    with Trace(\n    \"send_request_headers\",logger,request,kwargs\n    )as trace:\n     self._send_request_headers(**kwargs)\n    with Trace(\"send_request_body\",logger,request,kwargs)as trace:\n     self._send_request_body(**kwargs)\n   except WriteError:\n   \n   \n   \n   \n   \n    pass\n    \n   with Trace(\n   \"receive_response_headers\",logger,request,kwargs\n   )as trace:\n    (\n    http_version,\n    status,\n    reason_phrase,\n    headers,\n    trailing_data,\n    )=self._receive_response_headers(**kwargs)\n    trace.return_value=(\n    http_version,\n    status,\n    reason_phrase,\n    headers,\n    )\n    \n   network_stream=self._network_stream\n   \n   \n   if(status ==101)or(\n   (request.method ==b\"CONNECT\")and(200 <=status <300)\n   ):\n    network_stream=HTTP11UpgradeStream(network_stream,trailing_data)\n    \n   return Response(\n   status=status,\n   headers=headers,\n   content=HTTP11ConnectionByteStream(self,request),\n   extensions={\n   \"http_version\":http_version,\n   \"reason_phrase\":reason_phrase,\n   \"network_stream\":network_stream,\n   },\n   )\n  except BaseException as exc:\n   with ShieldCancellation():\n    with Trace(\"response_closed\",logger,request)as trace:\n     self._response_closed()\n   raise exc\n   \n   \n   \n def _send_request_headers(self,request:Request)->None:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"write\",None)\n  \n  with map_exceptions({h11.LocalProtocolError:LocalProtocolError}):\n   event=h11.Request(\n   method=request.method,\n   target=request.url.target,\n   headers=request.headers,\n   )\n  self._send_event(event,timeout=timeout)\n  \n def _send_request_body(self,request:Request)->None:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"write\",None)\n  \n  assert isinstance(request.stream,typing.Iterable)\n  for chunk in request.stream:\n   event=h11.Data(data=chunk)\n   self._send_event(event,timeout=timeout)\n   \n  self._send_event(h11.EndOfMessage(),timeout=timeout)\n  \n def _send_event(self,event:h11.Event,timeout:float |None=None)->None:\n  bytes_to_send=self._h11_state.send(event)\n  if bytes_to_send is not None:\n   self._network_stream.write(bytes_to_send,timeout=timeout)\n   \n   \n   \n def _receive_response_headers(\n self,request:Request\n )->tuple[bytes,int,bytes,list[tuple[bytes,bytes]],bytes]:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"read\",None)\n  \n  while True:\n   event=self._receive_event(timeout=timeout)\n   if isinstance(event,h11.Response):\n    break\n   if(\n   isinstance(event,h11.InformationalResponse)\n   and event.status_code ==101\n   ):\n    break\n    \n  http_version=b\"HTTP/\"+event.http_version\n  \n  \n  \n  headers=event.headers.raw_items()\n  \n  trailing_data,_=self._h11_state.trailing_data\n  \n  return http_version,event.status_code,event.reason,headers,trailing_data\n  \n def _receive_response_body(\n self,request:Request\n )->typing.Iterator[bytes]:\n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"read\",None)\n  \n  while True:\n   event=self._receive_event(timeout=timeout)\n   if isinstance(event,h11.Data):\n    yield bytes(event.data)\n   elif isinstance(event,(h11.EndOfMessage,h11.PAUSED)):\n    break\n    \n def _receive_event(\n self,timeout:float |None=None\n )->h11.Event |type[h11.PAUSED]:\n  while True:\n   with map_exceptions({h11.RemoteProtocolError:RemoteProtocolError}):\n    event=self._h11_state.next_event()\n    \n   if event is h11.NEED_DATA:\n    data=self._network_stream.read(\n    self.READ_NUM_BYTES,timeout=timeout\n    )\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    if data ==b\"\"and self._h11_state.their_state ==h11.SEND_RESPONSE:\n     msg=\"Server disconnected without sending a response.\"\n     raise RemoteProtocolError(msg)\n     \n    self._h11_state.receive_data(data)\n   else:\n   \n    return event\n    \n def _response_closed(self)->None:\n  with self._state_lock:\n   if(\n   self._h11_state.our_state is h11.DONE\n   and self._h11_state.their_state is h11.DONE\n   ):\n    self._state=HTTPConnectionState.IDLE\n    self._h11_state.start_next_cycle()\n    if self._keepalive_expiry is not None:\n     now=time.monotonic()\n     self._expire_at=now+self._keepalive_expiry\n   else:\n    self.close()\n    \n    \n    \n def close(self)->None:\n \n \n  self._state=HTTPConnectionState.CLOSED\n  self._network_stream.close()\n  \n  \n  \n  \n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._origin\n  \n def is_available(self)->bool:\n \n \n \n \n  return self._state ==HTTPConnectionState.IDLE\n  \n def has_expired(self)->bool:\n  now=time.monotonic()\n  keepalive_expired=self._expire_at is not None and now >self._expire_at\n  \n  \n  \n  \n  server_disconnected=(\n  self._state ==HTTPConnectionState.IDLE\n  and self._network_stream.get_extra_info(\"is_readable\")\n  )\n  \n  return keepalive_expired or server_disconnected\n  \n def is_idle(self)->bool:\n  return self._state ==HTTPConnectionState.IDLE\n  \n def is_closed(self)->bool:\n  return self._state ==HTTPConnectionState.CLOSED\n  \n def info(self)->str:\n  origin=str(self._origin)\n  return(\n  f\"{origin !r}, HTTP/1.1, {self._state.name}, \"\n  f\"Request Count: {self._request_count}\"\n  )\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  origin=str(self._origin)\n  return(\n  f\"<{class_name} [{origin !r}, {self._state.name}, \"\n  f\"Request Count: {self._request_count}]>\"\n  )\n  \n  \n  \n  \n def __enter__(self)->HTTP11Connection:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  self.close()\n  \n  \nclass HTTP11ConnectionByteStream:\n def __init__(self,connection:HTTP11Connection,request:Request)->None:\n  self._connection=connection\n  self._request=request\n  self._closed=False\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  kwargs={\"request\":self._request}\n  try:\n   with Trace(\"receive_response_body\",logger,self._request,kwargs):\n    for chunk in self._connection._receive_response_body(**kwargs):\n     yield chunk\n  except BaseException as exc:\n  \n  \n  \n   with ShieldCancellation():\n    self.close()\n   raise exc\n   \n def close(self)->None:\n  if not self._closed:\n   self._closed=True\n   with Trace(\"response_closed\",logger,self._request):\n    self._connection._response_closed()\n    \n    \nclass HTTP11UpgradeStream(NetworkStream):\n def __init__(self,stream:NetworkStream,leading_data:bytes)->None:\n  self._stream=stream\n  self._leading_data=leading_data\n  \n def read(self,max_bytes:int,timeout:float |None=None)->bytes:\n  if self._leading_data:\n   buffer=self._leading_data[:max_bytes]\n   self._leading_data=self._leading_data[max_bytes:]\n   return buffer\n  else:\n   return self._stream.read(max_bytes,timeout)\n   \n def write(self,buffer:bytes,timeout:float |None=None)->None:\n  self._stream.write(buffer,timeout)\n  \n def close(self)->None:\n  self._stream.close()\n  \n def start_tls(\n self,\n ssl_context:ssl.SSLContext,\n server_hostname:str |None=None,\n timeout:float |None=None,\n )->NetworkStream:\n  return self._stream.start_tls(ssl_context,server_hostname,timeout)\n  \n def get_extra_info(self,info:str)->typing.Any:\n  return self._stream.get_extra_info(info)\n", ["__future__", "enum", "h11", "httpcore._backends.base", "httpcore._exceptions", "httpcore._models", "httpcore._sync.interfaces", "httpcore._synchronization", "httpcore._trace", "logging", "ssl", "time", "types", "typing"]], "httpcore._sync.socks_proxy": [".py", "from __future__ import annotations\n\nimport logging\nimport ssl\n\nimport socksio\n\nfrom.._backends.sync import SyncBackend\nfrom.._backends.base import NetworkBackend,NetworkStream\nfrom.._exceptions import ConnectionNotAvailable,ProxyError\nfrom.._models import URL,Origin,Request,Response,enforce_bytes,enforce_url\nfrom.._ssl import default_ssl_context\nfrom.._synchronization import Lock\nfrom.._trace import Trace\nfrom.connection_pool import ConnectionPool\nfrom.http11 import HTTP11Connection\nfrom.interfaces import ConnectionInterface\n\nlogger=logging.getLogger(\"httpcore.socks\")\n\n\nAUTH_METHODS={\nb\"\\x00\":\"NO AUTHENTICATION REQUIRED\",\nb\"\\x01\":\"GSSAPI\",\nb\"\\x02\":\"USERNAME/PASSWORD\",\nb\"\\xff\":\"NO ACCEPTABLE METHODS\",\n}\n\nREPLY_CODES={\nb\"\\x00\":\"Succeeded\",\nb\"\\x01\":\"General SOCKS server failure\",\nb\"\\x02\":\"Connection not allowed by ruleset\",\nb\"\\x03\":\"Network unreachable\",\nb\"\\x04\":\"Host unreachable\",\nb\"\\x05\":\"Connection refused\",\nb\"\\x06\":\"TTL expired\",\nb\"\\x07\":\"Command not supported\",\nb\"\\x08\":\"Address type not supported\",\n}\n\n\ndef _init_socks5_connection(\nstream:NetworkStream,\n*,\nhost:bytes,\nport:int,\nauth:tuple[bytes,bytes]|None=None,\n)->None:\n conn=socksio.socks5.SOCKS5Connection()\n \n \n auth_method=(\n socksio.socks5.SOCKS5AuthMethod.NO_AUTH_REQUIRED\n if auth is None\n else socksio.socks5.SOCKS5AuthMethod.USERNAME_PASSWORD\n )\n conn.send(socksio.socks5.SOCKS5AuthMethodsRequest([auth_method]))\n outgoing_bytes=conn.data_to_send()\n stream.write(outgoing_bytes)\n \n \n incoming_bytes=stream.read(max_bytes=4096)\n response=conn.receive_data(incoming_bytes)\n assert isinstance(response,socksio.socks5.SOCKS5AuthReply)\n if response.method !=auth_method:\n  requested=AUTH_METHODS.get(auth_method,\"UNKNOWN\")\n  responded=AUTH_METHODS.get(response.method,\"UNKNOWN\")\n  raise ProxyError(\n  f\"Requested {requested} from proxy server, but got {responded}.\"\n  )\n  \n if response.method ==socksio.socks5.SOCKS5AuthMethod.USERNAME_PASSWORD:\n \n  assert auth is not None\n  username,password=auth\n  conn.send(socksio.socks5.SOCKS5UsernamePasswordRequest(username,password))\n  outgoing_bytes=conn.data_to_send()\n  stream.write(outgoing_bytes)\n  \n  \n  incoming_bytes=stream.read(max_bytes=4096)\n  response=conn.receive_data(incoming_bytes)\n  assert isinstance(response,socksio.socks5.SOCKS5UsernamePasswordReply)\n  if not response.success:\n   raise ProxyError(\"Invalid username/password\")\n   \n   \n conn.send(\n socksio.socks5.SOCKS5CommandRequest.from_address(\n socksio.socks5.SOCKS5Command.CONNECT,(host,port)\n )\n )\n outgoing_bytes=conn.data_to_send()\n stream.write(outgoing_bytes)\n \n \n incoming_bytes=stream.read(max_bytes=4096)\n response=conn.receive_data(incoming_bytes)\n assert isinstance(response,socksio.socks5.SOCKS5Reply)\n if response.reply_code !=socksio.socks5.SOCKS5ReplyCode.SUCCEEDED:\n  reply_code=REPLY_CODES.get(response.reply_code,\"UNKOWN\")\n  raise ProxyError(f\"Proxy Server could not connect: {reply_code}.\")\n  \n  \nclass SOCKSProxy(ConnectionPool):\n ''\n\n \n \n def __init__(\n self,\n proxy_url:URL |bytes |str,\n proxy_auth:tuple[bytes |str,bytes |str]|None=None,\n ssl_context:ssl.SSLContext |None=None,\n max_connections:int |None=10,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n network_backend:NetworkBackend |None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  super().__init__(\n  ssl_context=ssl_context,\n  max_connections=max_connections,\n  max_keepalive_connections=max_keepalive_connections,\n  keepalive_expiry=keepalive_expiry,\n  http1=http1,\n  http2=http2,\n  network_backend=network_backend,\n  retries=retries,\n  )\n  self._ssl_context=ssl_context\n  self._proxy_url=enforce_url(proxy_url,name=\"proxy_url\")\n  if proxy_auth is not None:\n   username,password=proxy_auth\n   username_bytes=enforce_bytes(username,name=\"proxy_auth\")\n   password_bytes=enforce_bytes(password,name=\"proxy_auth\")\n   self._proxy_auth:tuple[bytes,bytes]|None=(\n   username_bytes,\n   password_bytes,\n   )\n  else:\n   self._proxy_auth=None\n   \n def create_connection(self,origin:Origin)->ConnectionInterface:\n  return Socks5Connection(\n  proxy_origin=self._proxy_url.origin,\n  remote_origin=origin,\n  proxy_auth=self._proxy_auth,\n  ssl_context=self._ssl_context,\n  keepalive_expiry=self._keepalive_expiry,\n  http1=self._http1,\n  http2=self._http2,\n  network_backend=self._network_backend,\n  )\n  \n  \nclass Socks5Connection(ConnectionInterface):\n def __init__(\n self,\n proxy_origin:Origin,\n remote_origin:Origin,\n proxy_auth:tuple[bytes,bytes]|None=None,\n ssl_context:ssl.SSLContext |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n network_backend:NetworkBackend |None=None,\n )->None:\n  self._proxy_origin=proxy_origin\n  self._remote_origin=remote_origin\n  self._proxy_auth=proxy_auth\n  self._ssl_context=ssl_context\n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  \n  self._network_backend:NetworkBackend=(\n  SyncBackend()if network_backend is None else network_backend\n  )\n  self._connect_lock=Lock()\n  self._connection:ConnectionInterface |None=None\n  self._connect_failed=False\n  \n def handle_request(self,request:Request)->Response:\n  timeouts=request.extensions.get(\"timeout\",{})\n  sni_hostname=request.extensions.get(\"sni_hostname\",None)\n  timeout=timeouts.get(\"connect\",None)\n  \n  with self._connect_lock:\n   if self._connection is None:\n    try:\n    \n     kwargs={\n     \"host\":self._proxy_origin.host.decode(\"ascii\"),\n     \"port\":self._proxy_origin.port,\n     \"timeout\":timeout,\n     }\n     with Trace(\"connect_tcp\",logger,request,kwargs)as trace:\n      stream=self._network_backend.connect_tcp(**kwargs)\n      trace.return_value=stream\n      \n      \n     kwargs={\n     \"stream\":stream,\n     \"host\":self._remote_origin.host.decode(\"ascii\"),\n     \"port\":self._remote_origin.port,\n     \"auth\":self._proxy_auth,\n     }\n     with Trace(\n     \"setup_socks5_connection\",logger,request,kwargs\n     )as trace:\n      _init_socks5_connection(**kwargs)\n      trace.return_value=stream\n      \n      \n     if self._remote_origin.scheme ==b\"https\":\n      ssl_context=(\n      default_ssl_context()\n      if self._ssl_context is None\n      else self._ssl_context\n      )\n      alpn_protocols=(\n      [\"http/1.1\",\"h2\"]if self._http2 else[\"http/1.1\"]\n      )\n      ssl_context.set_alpn_protocols(alpn_protocols)\n      \n      kwargs={\n      \"ssl_context\":ssl_context,\n      \"server_hostname\":sni_hostname\n      or self._remote_origin.host.decode(\"ascii\"),\n      \"timeout\":timeout,\n      }\n      with Trace(\"start_tls\",logger,request,kwargs)as trace:\n       stream=stream.start_tls(**kwargs)\n       trace.return_value=stream\n       \n       \n     ssl_object=stream.get_extra_info(\"ssl_object\")\n     http2_negotiated=(\n     ssl_object is not None\n     and ssl_object.selected_alpn_protocol()==\"h2\"\n     )\n     \n     \n     if http2_negotiated or(\n     self._http2 and not self._http1\n     ):\n      from.http2 import HTTP2Connection\n      \n      self._connection=HTTP2Connection(\n      origin=self._remote_origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n     else:\n      self._connection=HTTP11Connection(\n      origin=self._remote_origin,\n      stream=stream,\n      keepalive_expiry=self._keepalive_expiry,\n      )\n    except Exception as exc:\n     self._connect_failed=True\n     raise exc\n   elif not self._connection.is_available():\n    raise ConnectionNotAvailable()\n    \n  return self._connection.handle_request(request)\n  \n def can_handle_request(self,origin:Origin)->bool:\n  return origin ==self._remote_origin\n  \n def close(self)->None:\n  if self._connection is not None:\n   self._connection.close()\n   \n def is_available(self)->bool:\n  if self._connection is None:\n  \n  \n  \n   return(\n   self._http2\n   and(self._remote_origin.scheme ==b\"https\"or not self._http1)\n   and not self._connect_failed\n   )\n  return self._connection.is_available()\n  \n def has_expired(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.has_expired()\n  \n def is_idle(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_idle()\n  \n def is_closed(self)->bool:\n  if self._connection is None:\n   return self._connect_failed\n  return self._connection.is_closed()\n  \n def info(self)->str:\n  if self._connection is None:\n   return \"CONNECTION FAILED\"if self._connect_failed else \"CONNECTING\"\n  return self._connection.info()\n  \n def __repr__(self)->str:\n  return f\"<{self.__class__.__name__} [{self.info()}]>\"\n", ["__future__", "httpcore._backends.base", "httpcore._backends.sync", "httpcore._exceptions", "httpcore._models", "httpcore._ssl", "httpcore._sync.connection_pool", "httpcore._sync.http11", "httpcore._sync.http2", "httpcore._sync.interfaces", "httpcore._synchronization", "httpcore._trace", "logging", "socksio", "ssl"]], "httpcore._sync.connection_pool": [".py", "from __future__ import annotations\n\nimport ssl\nimport sys\nimport types\nimport typing\n\nfrom.._backends.sync import SyncBackend\nfrom.._backends.base import SOCKET_OPTION,NetworkBackend\nfrom.._exceptions import ConnectionNotAvailable,UnsupportedProtocol\nfrom.._models import Origin,Proxy,Request,Response\nfrom.._synchronization import Event,ShieldCancellation,ThreadLock\nfrom.connection import HTTPConnection\nfrom.interfaces import ConnectionInterface,RequestInterface\n\n\nclass PoolRequest:\n def __init__(self,request:Request)->None:\n  self.request=request\n  self.connection:ConnectionInterface |None=None\n  self._connection_acquired=Event()\n  \n def assign_to_connection(self,connection:ConnectionInterface |None)->None:\n  self.connection=connection\n  self._connection_acquired.set()\n  \n def clear_connection(self)->None:\n  self.connection=None\n  self._connection_acquired=Event()\n  \n def wait_for_connection(\n self,timeout:float |None=None\n )->ConnectionInterface:\n  if self.connection is None:\n   self._connection_acquired.wait(timeout=timeout)\n  assert self.connection is not None\n  return self.connection\n  \n def is_queued(self)->bool:\n  return self.connection is None\n  \n  \nclass ConnectionPool(RequestInterface):\n ''\n\n \n \n def __init__(\n self,\n ssl_context:ssl.SSLContext |None=None,\n proxy:Proxy |None=None,\n max_connections:int |None=10,\n max_keepalive_connections:int |None=None,\n keepalive_expiry:float |None=None,\n http1:bool=True,\n http2:bool=False,\n retries:int=0,\n local_address:str |None=None,\n uds:str |None=None,\n network_backend:NetworkBackend |None=None,\n socket_options:typing.Iterable[SOCKET_OPTION]|None=None,\n )->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  self._ssl_context=ssl_context\n  self._proxy=proxy\n  self._max_connections=(\n  sys.maxsize if max_connections is None else max_connections\n  )\n  self._max_keepalive_connections=(\n  sys.maxsize\n  if max_keepalive_connections is None\n  else max_keepalive_connections\n  )\n  self._max_keepalive_connections=min(\n  self._max_connections,self._max_keepalive_connections\n  )\n  \n  self._keepalive_expiry=keepalive_expiry\n  self._http1=http1\n  self._http2=http2\n  self._retries=retries\n  self._local_address=local_address\n  self._uds=uds\n  \n  self._network_backend=(\n  SyncBackend()if network_backend is None else network_backend\n  )\n  self._socket_options=socket_options\n  \n  \n  \n  self._connections:list[ConnectionInterface]=[]\n  self._requests:list[PoolRequest]=[]\n  \n  \n  \n  \n  self._optional_thread_lock=ThreadLock()\n  \n def create_connection(self,origin:Origin)->ConnectionInterface:\n  if self._proxy is not None:\n   if self._proxy.url.scheme in(b\"socks5\",b\"socks5h\"):\n    from.socks_proxy import Socks5Connection\n    \n    return Socks5Connection(\n    proxy_origin=self._proxy.url.origin,\n    proxy_auth=self._proxy.auth,\n    remote_origin=origin,\n    ssl_context=self._ssl_context,\n    keepalive_expiry=self._keepalive_expiry,\n    http1=self._http1,\n    http2=self._http2,\n    network_backend=self._network_backend,\n    )\n   elif origin.scheme ==b\"http\":\n    from.http_proxy import ForwardHTTPConnection\n    \n    return ForwardHTTPConnection(\n    proxy_origin=self._proxy.url.origin,\n    proxy_headers=self._proxy.headers,\n    proxy_ssl_context=self._proxy.ssl_context,\n    remote_origin=origin,\n    keepalive_expiry=self._keepalive_expiry,\n    network_backend=self._network_backend,\n    )\n   from.http_proxy import TunnelHTTPConnection\n   \n   return TunnelHTTPConnection(\n   proxy_origin=self._proxy.url.origin,\n   proxy_headers=self._proxy.headers,\n   proxy_ssl_context=self._proxy.ssl_context,\n   remote_origin=origin,\n   ssl_context=self._ssl_context,\n   keepalive_expiry=self._keepalive_expiry,\n   http1=self._http1,\n   http2=self._http2,\n   network_backend=self._network_backend,\n   )\n   \n  return HTTPConnection(\n  origin=origin,\n  ssl_context=self._ssl_context,\n  keepalive_expiry=self._keepalive_expiry,\n  http1=self._http1,\n  http2=self._http2,\n  retries=self._retries,\n  local_address=self._local_address,\n  uds=self._uds,\n  network_backend=self._network_backend,\n  socket_options=self._socket_options,\n  )\n  \n @property\n def connections(self)->list[ConnectionInterface]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return list(self._connections)\n  \n def handle_request(self,request:Request)->Response:\n  ''\n\n\n\n  \n  scheme=request.url.scheme.decode()\n  if scheme ==\"\":\n   raise UnsupportedProtocol(\n   \"Request URL is missing an 'http://' or 'https://' protocol.\"\n   )\n  if scheme not in(\"http\",\"https\",\"ws\",\"wss\"):\n   raise UnsupportedProtocol(\n   f\"Request URL has an unsupported protocol '{scheme}://'.\"\n   )\n   \n  timeouts=request.extensions.get(\"timeout\",{})\n  timeout=timeouts.get(\"pool\",None)\n  \n  with self._optional_thread_lock:\n  \n   pool_request=PoolRequest(request)\n   self._requests.append(pool_request)\n   \n  try:\n   while True:\n    with self._optional_thread_lock:\n    \n    \n     closing=self._assign_requests_to_connections()\n    self._close_connections(closing)\n    \n    \n    connection=pool_request.wait_for_connection(timeout=timeout)\n    \n    try:\n    \n     response=connection.handle_request(\n     pool_request.request\n     )\n    except ConnectionNotAvailable:\n    \n    \n    \n    \n     pool_request.clear_connection()\n    else:\n     break\n     \n  except BaseException as exc:\n   with self._optional_thread_lock:\n   \n   \n    self._requests.remove(pool_request)\n    closing=self._assign_requests_to_connections()\n    \n   self._close_connections(closing)\n   raise exc from None\n   \n   \n   \n  assert isinstance(response.stream,typing.Iterable)\n  return Response(\n  status=response.status,\n  headers=response.headers,\n  content=PoolByteStream(\n  stream=response.stream,pool_request=pool_request,pool=self\n  ),\n  extensions=response.extensions,\n  )\n  \n def _assign_requests_to_connections(self)->list[ConnectionInterface]:\n  ''\n\n\n\n\n\n\n\n  \n  closing_connections=[]\n  \n  \n  \n  for connection in list(self._connections):\n   if connection.is_closed():\n   \n    self._connections.remove(connection)\n   elif connection.has_expired():\n   \n    self._connections.remove(connection)\n    closing_connections.append(connection)\n   elif(\n   connection.is_idle()\n   and len([connection.is_idle()for connection in self._connections])\n   >self._max_keepalive_connections\n   ):\n   \n    self._connections.remove(connection)\n    closing_connections.append(connection)\n    \n    \n  queued_requests=[request for request in self._requests if request.is_queued()]\n  for pool_request in queued_requests:\n   origin=pool_request.request.url.origin\n   available_connections=[\n   connection\n   for connection in self._connections\n   if connection.can_handle_request(origin)and connection.is_available()\n   ]\n   idle_connections=[\n   connection for connection in self._connections if connection.is_idle()\n   ]\n   \n   \n   \n   \n   \n   \n   \n   if available_connections:\n   \n    connection=available_connections[0]\n    pool_request.assign_to_connection(connection)\n   elif len(self._connections)<self._max_connections:\n   \n    connection=self.create_connection(origin)\n    self._connections.append(connection)\n    pool_request.assign_to_connection(connection)\n   elif idle_connections:\n   \n    connection=idle_connections[0]\n    self._connections.remove(connection)\n    closing_connections.append(connection)\n    \n    connection=self.create_connection(origin)\n    self._connections.append(connection)\n    pool_request.assign_to_connection(connection)\n    \n  return closing_connections\n  \n def _close_connections(self,closing:list[ConnectionInterface])->None:\n \n  with ShieldCancellation():\n   for connection in closing:\n    connection.close()\n    \n def close(self)->None:\n \n \n  with self._optional_thread_lock:\n   closing_connections=list(self._connections)\n   self._connections=[]\n  self._close_connections(closing_connections)\n  \n def __enter__(self)->ConnectionPool:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None=None,\n exc_value:BaseException |None=None,\n traceback:types.TracebackType |None=None,\n )->None:\n  self.close()\n  \n def __repr__(self)->str:\n  class_name=self.__class__.__name__\n  with self._optional_thread_lock:\n   request_is_queued=[request.is_queued()for request in self._requests]\n   connection_is_idle=[\n   connection.is_idle()for connection in self._connections\n   ]\n   \n   num_active_requests=request_is_queued.count(False)\n   num_queued_requests=request_is_queued.count(True)\n   num_active_connections=connection_is_idle.count(False)\n   num_idle_connections=connection_is_idle.count(True)\n   \n  requests_info=(\n  f\"Requests: {num_active_requests} active, {num_queued_requests} queued\"\n  )\n  connection_info=(\n  f\"Connections: {num_active_connections} active, {num_idle_connections} idle\"\n  )\n  \n  return f\"<{class_name} [{requests_info} | {connection_info}]>\"\n  \n  \nclass PoolByteStream:\n def __init__(\n self,\n stream:typing.Iterable[bytes],\n pool_request:PoolRequest,\n pool:ConnectionPool,\n )->None:\n  self._stream=stream\n  self._pool_request=pool_request\n  self._pool=pool\n  self._closed=False\n  \n def __iter__(self)->typing.Iterator[bytes]:\n  try:\n   for part in self._stream:\n    yield part\n  except BaseException as exc:\n   self.close()\n   raise exc from None\n   \n def close(self)->None:\n  if not self._closed:\n   self._closed=True\n   with ShieldCancellation():\n    if hasattr(self._stream,\"close\"):\n     self._stream.close()\n     \n   with self._pool._optional_thread_lock:\n    self._pool._requests.remove(self._pool_request)\n    closing=self._pool._assign_requests_to_connections()\n    \n   self._pool._close_connections(closing)\n", ["__future__", "httpcore._backends.base", "httpcore._backends.sync", "httpcore._exceptions", "httpcore._models", "httpcore._sync.connection", "httpcore._sync.http_proxy", "httpcore._sync.interfaces", "httpcore._sync.socks_proxy", "httpcore._synchronization", "ssl", "sys", "types", "typing"]], "groq": [".py", "\n\nfrom. import types\nfrom._types import NOT_GIVEN,Omit,NoneType,NotGiven,Transport,ProxiesTypes\nfrom._utils import file_from_path\nfrom._client import Groq,Client,Stream,Timeout,AsyncGroq,Transport,AsyncClient,AsyncStream,RequestOptions\nfrom._models import BaseModel\nfrom._version import __title__,__version__\nfrom._response import APIResponse as APIResponse,AsyncAPIResponse as AsyncAPIResponse\nfrom._constants import DEFAULT_TIMEOUT,DEFAULT_MAX_RETRIES,DEFAULT_CONNECTION_LIMITS\nfrom._exceptions import(\nAPIError,\nGroqError,\nConflictError,\nNotFoundError,\nAPIStatusError,\nRateLimitError,\nAPITimeoutError,\nBadRequestError,\nAPIConnectionError,\nAuthenticationError,\nInternalServerError,\nPermissionDeniedError,\nUnprocessableEntityError,\nAPIResponseValidationError,\n)\nfrom._base_client import DefaultHttpxClient,DefaultAsyncHttpxClient\nfrom._utils._logs import setup_logging as _setup_logging\n\n__all__=[\n\"types\",\n\"__version__\",\n\"__title__\",\n\"NoneType\",\n\"Transport\",\n\"ProxiesTypes\",\n\"NotGiven\",\n\"NOT_GIVEN\",\n\"Omit\",\n\"GroqError\",\n\"APIError\",\n\"APIStatusError\",\n\"APITimeoutError\",\n\"APIConnectionError\",\n\"APIResponseValidationError\",\n\"BadRequestError\",\n\"AuthenticationError\",\n\"PermissionDeniedError\",\n\"NotFoundError\",\n\"ConflictError\",\n\"UnprocessableEntityError\",\n\"RateLimitError\",\n\"InternalServerError\",\n\"Timeout\",\n\"RequestOptions\",\n\"Client\",\n\"AsyncClient\",\n\"Stream\",\n\"AsyncStream\",\n\"Groq\",\n\"AsyncGroq\",\n\"file_from_path\",\n\"BaseModel\",\n\"DEFAULT_TIMEOUT\",\n\"DEFAULT_MAX_RETRIES\",\n\"DEFAULT_CONNECTION_LIMITS\",\n\"DefaultHttpxClient\",\n\"DefaultAsyncHttpxClient\",\n]\n\n_setup_logging()\n\n\n\n\n\n__locals=locals()\nfor __name in __all__:\n if not __name.startswith(\"__\"):\n  try:\n   __locals[__name].__module__=\"groq\"\n  except(TypeError,AttributeError):\n  \n   pass\n", ["groq", "groq._base_client", "groq._client", "groq._constants", "groq._exceptions", "groq._models", "groq._response", "groq._types", "groq._utils", "groq._utils._logs", "groq._version"], 1], "groq._resource": [".py", "\n\nfrom __future__ import annotations\n\nimport time\nfrom typing import TYPE_CHECKING\n\nimport anyio\n\nif TYPE_CHECKING:\n from._client import Groq,AsyncGroq\n \n \nclass SyncAPIResource:\n _client:Groq\n \n def __init__(self,client:Groq)->None:\n  self._client=client\n  self._get=client.get\n  self._post=client.post\n  self._patch=client.patch\n  self._put=client.put\n  self._delete=client.delete\n  self._get_api_list=client.get_api_list\n  \n def _sleep(self,seconds:float)->None:\n  time.sleep(seconds)\n  \n  \nclass AsyncAPIResource:\n _client:AsyncGroq\n \n def __init__(self,client:AsyncGroq)->None:\n  self._client=client\n  self._get=client.get\n  self._post=client.post\n  self._patch=client.patch\n  self._put=client.put\n  self._delete=client.delete\n  self._get_api_list=client.get_api_list\n  \n async def _sleep(self,seconds:float)->None:\n  await anyio.sleep(seconds)\n", ["__future__", "anyio", "groq._client", "time", "typing"]], "groq._compat": [".py", "from __future__ import annotations\n\nfrom typing import TYPE_CHECKING,Any,Union,Generic,TypeVar,Callable,cast,overload\nfrom datetime import date,datetime\nfrom typing_extensions import Self,Literal\n\nimport pydantic\nfrom pydantic.fields import FieldInfo\n\nfrom._types import IncEx,StrBytesIntFloat\n\n_T=TypeVar(\"_T\")\n_ModelT=TypeVar(\"_ModelT\",bound=pydantic.BaseModel)\n\n\n\n\n\n\nPYDANTIC_V2=pydantic.VERSION.startswith(\"2.\")\n\n\nif TYPE_CHECKING:\n\n def parse_date(value:date |StrBytesIntFloat)->date:\n  ...\n  \n def parse_datetime(value:Union[datetime,StrBytesIntFloat])->datetime:\n  ...\n  \n def get_args(t:type[Any])->tuple[Any,...]:\n  ...\n  \n def is_union(tp:type[Any]|None)->bool:\n  ...\n  \n def get_origin(t:type[Any])->type[Any]|None:\n  ...\n  \n def is_literal_type(type_:type[Any])->bool:\n  ...\n  \n def is_typeddict(type_:type[Any])->bool:\n  ...\n  \nelse:\n if PYDANTIC_V2:\n  from pydantic.v1.typing import(\n  get_args as get_args,\n  is_union as is_union,\n  get_origin as get_origin,\n  is_typeddict as is_typeddict,\n  is_literal_type as is_literal_type,\n  )\n  from pydantic.v1.datetime_parse import parse_date as parse_date,parse_datetime as parse_datetime\n else:\n  from pydantic.typing import(\n  get_args as get_args,\n  is_union as is_union,\n  get_origin as get_origin,\n  is_typeddict as is_typeddict,\n  is_literal_type as is_literal_type,\n  )\n  from pydantic.datetime_parse import parse_date as parse_date,parse_datetime as parse_datetime\n  \n  \n  \nif TYPE_CHECKING:\n from pydantic import ConfigDict as ConfigDict\nelse:\n if PYDANTIC_V2:\n  from pydantic import ConfigDict\n else:\n \n  ConfigDict=None\n  \n  \n  \ndef parse_obj(model:type[_ModelT],value:object)->_ModelT:\n if PYDANTIC_V2:\n  return model.model_validate(value)\n else:\n  return cast(_ModelT,model.parse_obj(value))\n  \n  \ndef field_is_required(field:FieldInfo)->bool:\n if PYDANTIC_V2:\n  return field.is_required()\n return field.required\n \n \ndef field_get_default(field:FieldInfo)->Any:\n value=field.get_default()\n if PYDANTIC_V2:\n  from pydantic_core import PydanticUndefined\n  \n  if value ==PydanticUndefined:\n   return None\n  return value\n return value\n \n \ndef field_outer_type(field:FieldInfo)->Any:\n if PYDANTIC_V2:\n  return field.annotation\n return field.outer_type_\n \n \ndef get_model_config(model:type[pydantic.BaseModel])->Any:\n if PYDANTIC_V2:\n  return model.model_config\n return model.__config__\n \n \ndef get_model_fields(model:type[pydantic.BaseModel])->dict[str,FieldInfo]:\n if PYDANTIC_V2:\n  return model.model_fields\n return model.__fields__\n \n \ndef model_copy(model:_ModelT,*,deep:bool=False)->_ModelT:\n if PYDANTIC_V2:\n  return model.model_copy(deep=deep)\n return model.copy(deep=deep)\n \n \ndef model_json(model:pydantic.BaseModel,*,indent:int |None=None)->str:\n if PYDANTIC_V2:\n  return model.model_dump_json(indent=indent)\n return model.json(indent=indent)\n \n \ndef model_dump(\nmodel:pydantic.BaseModel,\n*,\nexclude:IncEx |None=None,\nexclude_unset:bool=False,\nexclude_defaults:bool=False,\nwarnings:bool=True,\nmode:Literal[\"json\",\"python\"]=\"python\",\n)->dict[str,Any]:\n if PYDANTIC_V2 or hasattr(model,\"model_dump\"):\n  return model.model_dump(\n  mode=mode,\n  exclude=exclude,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  \n  warnings=warnings if PYDANTIC_V2 else True,\n  )\n return cast(\n \"dict[str, Any]\",\n model.dict(\n exclude=exclude,\n exclude_unset=exclude_unset,\n exclude_defaults=exclude_defaults,\n ),\n )\n \n \ndef model_parse(model:type[_ModelT],data:Any)->_ModelT:\n if PYDANTIC_V2:\n  return model.model_validate(data)\n return model.parse_obj(data)\n \n \n \nif TYPE_CHECKING:\n\n class GenericModel(pydantic.BaseModel):...\n \nelse:\n if PYDANTIC_V2:\n \n \n \n  class GenericModel(pydantic.BaseModel):...\n  \n else:\n  import pydantic.generics\n  \n  class GenericModel(pydantic.generics.GenericModel,pydantic.BaseModel):...\n  \n  \n  \nif TYPE_CHECKING:\n cached_property=property\n \n \n \n \n \n \n \n \n \n class typed_cached_property(Generic[_T]):\n  func:Callable[[Any],_T]\n  attrname:str |None\n  \n  def __init__(self,func:Callable[[Any],_T])->None:...\n  \n  @overload\n  def __get__(self,instance:None,owner:type[Any]|None=None)->Self:...\n  \n  @overload\n  def __get__(self,instance:object,owner:type[Any]|None=None)->_T:...\n  \n  def __get__(self,instance:object,owner:type[Any]|None=None)->_T |Self:\n   raise NotImplementedError()\n   \n  def __set_name__(self,owner:type[Any],name:str)->None:...\n  \n  \n  def __set__(self,instance:object,value:_T)->None:...\nelse:\n from functools import cached_property as cached_property\n \n typed_cached_property=cached_property\n", ["__future__", "datetime", "functools", "groq._types", "pydantic", "pydantic.datetime_parse", "pydantic.fields", "pydantic.generics", "pydantic.typing", "pydantic.v1.datetime_parse", "pydantic.v1.typing", "pydantic_core", "typing", "typing_extensions"]], "groq._qs": [".py", "from __future__ import annotations\n\nfrom typing import Any,List,Tuple,Union,Mapping,TypeVar\nfrom urllib.parse import parse_qs,urlencode\nfrom typing_extensions import Literal,get_args\n\nfrom._types import NOT_GIVEN,NotGiven,NotGivenOr\nfrom._utils import flatten\n\n_T=TypeVar(\"_T\")\n\n\nArrayFormat=Literal[\"comma\",\"repeat\",\"indices\",\"brackets\"]\nNestedFormat=Literal[\"dots\",\"brackets\"]\n\nPrimitiveData=Union[str,int,float,bool,None]\n\n\nData=Union[PrimitiveData,List[Any],Tuple[Any],\"Mapping[str, Any]\"]\nParams=Mapping[str,Data]\n\n\nclass Querystring:\n array_format:ArrayFormat\n nested_format:NestedFormat\n \n def __init__(\n self,\n *,\n array_format:ArrayFormat=\"repeat\",\n nested_format:NestedFormat=\"brackets\",\n )->None:\n  self.array_format=array_format\n  self.nested_format=nested_format\n  \n def parse(self,query:str)->Mapping[str,object]:\n \n  return parse_qs(query)\n  \n def stringify(\n self,\n params:Params,\n *,\n array_format:NotGivenOr[ArrayFormat]=NOT_GIVEN,\n nested_format:NotGivenOr[NestedFormat]=NOT_GIVEN,\n )->str:\n  return urlencode(\n  self.stringify_items(\n  params,\n  array_format=array_format,\n  nested_format=nested_format,\n  )\n  )\n  \n def stringify_items(\n self,\n params:Params,\n *,\n array_format:NotGivenOr[ArrayFormat]=NOT_GIVEN,\n nested_format:NotGivenOr[NestedFormat]=NOT_GIVEN,\n )->list[tuple[str,str]]:\n  opts=Options(\n  qs=self,\n  array_format=array_format,\n  nested_format=nested_format,\n  )\n  return flatten([self._stringify_item(key,value,opts)for key,value in params.items()])\n  \n def _stringify_item(\n self,\n key:str,\n value:Data,\n opts:Options,\n )->list[tuple[str,str]]:\n  if isinstance(value,Mapping):\n   items:list[tuple[str,str]]=[]\n   nested_format=opts.nested_format\n   for subkey,subvalue in value.items():\n    items.extend(\n    self._stringify_item(\n    \n    f\"{key}.{subkey}\"if nested_format ==\"dots\"else f\"{key}[{subkey}]\",\n    subvalue,\n    opts,\n    )\n    )\n   return items\n   \n  if isinstance(value,(list,tuple)):\n   array_format=opts.array_format\n   if array_format ==\"comma\":\n    return[\n    (\n    key,\n    \",\".join(self._primitive_value_to_str(item)for item in value if item is not None),\n    ),\n    ]\n   elif array_format ==\"repeat\":\n    items=[]\n    for item in value:\n     items.extend(self._stringify_item(key,item,opts))\n    return items\n   elif array_format ==\"indices\":\n    raise NotImplementedError(\"The array indices format is not supported yet\")\n   elif array_format ==\"brackets\":\n    items=[]\n    key=key+\"[]\"\n    for item in value:\n     items.extend(self._stringify_item(key,item,opts))\n    return items\n   else:\n    raise NotImplementedError(\n    f\"Unknown array_format value: {array_format}, choose from {', '.join(get_args(ArrayFormat))}\"\n    )\n    \n  serialised=self._primitive_value_to_str(value)\n  if not serialised:\n   return[]\n  return[(key,serialised)]\n  \n def _primitive_value_to_str(self,value:PrimitiveData)->str:\n \n  if value is True:\n   return \"true\"\n  elif value is False:\n   return \"false\"\n  elif value is None:\n   return \"\"\n  return str(value)\n  \n  \n_qs=Querystring()\nparse=_qs.parse\nstringify=_qs.stringify\nstringify_items=_qs.stringify_items\n\n\nclass Options:\n array_format:ArrayFormat\n nested_format:NestedFormat\n \n def __init__(\n self,\n qs:Querystring=_qs,\n *,\n array_format:NotGivenOr[ArrayFormat]=NOT_GIVEN,\n nested_format:NotGivenOr[NestedFormat]=NOT_GIVEN,\n )->None:\n  self.array_format=qs.array_format if isinstance(array_format,NotGiven)else array_format\n  self.nested_format=qs.nested_format if isinstance(nested_format,NotGiven)else nested_format\n", ["__future__", "groq._types", "groq._utils", "typing", "typing_extensions", "urllib.parse"]], "groq._response": [".py", "from __future__ import annotations\n\nimport os\nimport inspect\nimport logging\nimport datetime\nimport functools\nfrom types import TracebackType\nfrom typing import(\nTYPE_CHECKING,\nAny,\nUnion,\nGeneric,\nTypeVar,\nCallable,\nIterator,\nAsyncIterator,\ncast,\noverload,\n)\nfrom typing_extensions import Awaitable,ParamSpec,override,get_origin\n\nimport anyio\nimport httpx\nimport pydantic\n\nfrom._types import NoneType\nfrom._utils import is_given,extract_type_arg,is_annotated_type,is_type_alias_type,extract_type_var_from_base\nfrom._models import BaseModel,is_basemodel\nfrom._constants import RAW_RESPONSE_HEADER,OVERRIDE_CAST_TO_HEADER\nfrom._streaming import Stream,AsyncStream,is_stream_class_type,extract_stream_chunk_type\nfrom._exceptions import GroqError,APIResponseValidationError\n\nif TYPE_CHECKING:\n from._models import FinalRequestOptions\n from._base_client import BaseClient\n \n \nP=ParamSpec(\"P\")\nR=TypeVar(\"R\")\n_T=TypeVar(\"_T\")\n_APIResponseT=TypeVar(\"_APIResponseT\",bound=\"APIResponse[Any]\")\n_AsyncAPIResponseT=TypeVar(\"_AsyncAPIResponseT\",bound=\"AsyncAPIResponse[Any]\")\n\nlog:logging.Logger=logging.getLogger(__name__)\n\n\nclass BaseAPIResponse(Generic[R]):\n _cast_to:type[R]\n _client:BaseClient[Any,Any]\n _parsed_by_type:dict[type[Any],Any]\n _is_sse_stream:bool\n _stream_cls:type[Stream[Any]]|type[AsyncStream[Any]]|None\n _options:FinalRequestOptions\n \n http_response:httpx.Response\n \n retries_taken:int\n '' \n \n def __init__(\n self,\n *,\n raw:httpx.Response,\n cast_to:type[R],\n client:BaseClient[Any,Any],\n stream:bool,\n stream_cls:type[Stream[Any]]|type[AsyncStream[Any]]|None,\n options:FinalRequestOptions,\n retries_taken:int=0,\n )->None:\n  self._cast_to=cast_to\n  self._client=client\n  self._parsed_by_type={}\n  self._is_sse_stream=stream\n  self._stream_cls=stream_cls\n  self._options=options\n  self.http_response=raw\n  self.retries_taken=retries_taken\n  \n @property\n def headers(self)->httpx.Headers:\n  return self.http_response.headers\n  \n @property\n def http_request(self)->httpx.Request:\n  ''\n  return self.http_response.request\n  \n @property\n def status_code(self)->int:\n  return self.http_response.status_code\n  \n @property\n def url(self)->httpx.URL:\n  ''\n  return self.http_response.url\n  \n @property\n def method(self)->str:\n  return self.http_request.method\n  \n @property\n def http_version(self)->str:\n  return self.http_response.http_version\n  \n @property\n def elapsed(self)->datetime.timedelta:\n  ''\n  return self.http_response.elapsed\n  \n @property\n def is_closed(self)->bool:\n  ''\n\n\n\n\n  \n  return self.http_response.is_closed\n  \n @override\n def __repr__(self)->str:\n  return(\n  f\"<{self.__class__.__name__} [{self.status_code} {self.http_response.reason_phrase}] type={self._cast_to}>\"\n  )\n  \n def _parse(self,*,to:type[_T]|None=None)->R |_T:\n  cast_to=to if to is not None else self._cast_to\n  \n  \n  if is_type_alias_type(cast_to):\n   cast_to=cast_to.__value__\n   \n   \n  if cast_to and is_annotated_type(cast_to):\n   cast_to=extract_type_arg(cast_to,0)\n   \n  if self._is_sse_stream:\n   if to:\n    if not is_stream_class_type(to):\n     raise TypeError(f\"Expected custom parse type to be a subclass of {Stream} or {AsyncStream}\")\n     \n    return cast(\n    _T,\n    to(\n    cast_to=extract_stream_chunk_type(\n    to,\n    failure_message=\"Expected custom stream type to be passed with a type argument, e.g. Stream[ChunkType]\",\n    ),\n    response=self.http_response,\n    client=cast(Any,self._client),\n    ),\n    )\n    \n   if self._stream_cls:\n    return cast(\n    R,\n    self._stream_cls(\n    cast_to=extract_stream_chunk_type(self._stream_cls),\n    response=self.http_response,\n    client=cast(Any,self._client),\n    ),\n    )\n    \n   stream_cls=cast(\"type[Stream[Any]] | type[AsyncStream[Any]] | None\",self._client._default_stream_cls)\n   if stream_cls is None:\n    raise MissingStreamClassError()\n    \n   return cast(\n   R,\n   stream_cls(\n   cast_to=cast_to,\n   response=self.http_response,\n   client=cast(Any,self._client),\n   ),\n   )\n   \n  if cast_to is NoneType:\n   return cast(R,None)\n   \n  response=self.http_response\n  if cast_to ==str:\n   return cast(R,response.text)\n   \n  if cast_to ==bytes:\n   return cast(R,response.content)\n   \n  if cast_to ==int:\n   return cast(R,int(response.text))\n   \n  if cast_to ==float:\n   return cast(R,float(response.text))\n   \n  if cast_to ==bool:\n   return cast(R,response.text.lower()==\"true\")\n   \n  origin=get_origin(cast_to)or cast_to\n  \n  if origin ==APIResponse:\n   raise RuntimeError(\"Unexpected state - cast_to is `APIResponse`\")\n   \n  if inspect.isclass(origin)and issubclass(origin,httpx.Response):\n  \n  \n  \n  \n  \n   if cast_to !=httpx.Response:\n    raise ValueError(f\"Subclasses of httpx.Response cannot be passed to `cast_to`\")\n   return cast(R,response)\n   \n  if inspect.isclass(origin)and not issubclass(origin,BaseModel)and issubclass(origin,pydantic.BaseModel):\n   raise TypeError(\"Pydantic models must subclass our base model type, e.g. `from groq import BaseModel`\")\n   \n  if(\n  cast_to is not object\n  and not origin is list\n  and not origin is dict\n  and not origin is Union\n  and not issubclass(origin,BaseModel)\n  ):\n   raise RuntimeError(\n   f\"Unsupported type, expected {cast_to} to be a subclass of {BaseModel}, {dict}, {list}, {Union}, {NoneType}, {str} or {httpx.Response}.\"\n   )\n   \n   \n   \n  content_type,*_=response.headers.get(\"content-type\",\"*\").split(\";\")\n  if content_type !=\"application/json\":\n   if is_basemodel(cast_to):\n    try:\n     data=response.json()\n    except Exception as exc:\n     log.debug(\"Could not read JSON from response data due to %s - %s\",type(exc),exc)\n    else:\n     return self._client._process_response_data(\n     data=data,\n     cast_to=cast_to,\n     response=response,\n     )\n     \n   if self._client._strict_response_validation:\n    raise APIResponseValidationError(\n    response=response,\n    message=f\"Expected Content-Type response header to be `application/json` but received `{content_type}` instead.\",\n    body=response.text,\n    )\n    \n    \n    \n    \n   return response.text\n   \n  data=response.json()\n  \n  return self._client._process_response_data(\n  data=data,\n  cast_to=cast_to,\n  response=response,\n  )\n  \n  \nclass APIResponse(BaseAPIResponse[R]):\n @overload\n def parse(self,*,to:type[_T])->_T:...\n \n @overload\n def parse(self)->R:...\n \n def parse(self,*,to:type[_T]|None=None)->R |_T:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  cache_key=to if to is not None else self._cast_to\n  cached=self._parsed_by_type.get(cache_key)\n  if cached is not None:\n   return cached\n   \n  if not self._is_sse_stream:\n   self.read()\n   \n  parsed=self._parse(to=to)\n  if is_given(self._options.post_parser):\n   parsed=self._options.post_parser(parsed)\n   \n  self._parsed_by_type[cache_key]=parsed\n  return parsed\n  \n def read(self)->bytes:\n  ''\n  try:\n   return self.http_response.read()\n  except httpx.StreamConsumed as exc:\n  \n  \n  \n   raise StreamAlreadyConsumed()from exc\n   \n def text(self)->str:\n  ''\n  self.read()\n  return self.http_response.text\n  \n def json(self)->object:\n  ''\n  self.read()\n  return self.http_response.json()\n  \n def close(self)->None:\n  ''\n\n\n  \n  self.http_response.close()\n  \n def iter_bytes(self,chunk_size:int |None=None)->Iterator[bytes]:\n  ''\n\n\n\n  \n  for chunk in self.http_response.iter_bytes(chunk_size):\n   yield chunk\n   \n def iter_text(self,chunk_size:int |None=None)->Iterator[str]:\n  ''\n\n\n  \n  for chunk in self.http_response.iter_text(chunk_size):\n   yield chunk\n   \n def iter_lines(self)->Iterator[str]:\n  ''\n  for chunk in self.http_response.iter_lines():\n   yield chunk\n   \n   \nclass AsyncAPIResponse(BaseAPIResponse[R]):\n @overload\n async def parse(self,*,to:type[_T])->_T:...\n \n @overload\n async def parse(self)->R:...\n \n async def parse(self,*,to:type[_T]|None=None)->R |_T:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  cache_key=to if to is not None else self._cast_to\n  cached=self._parsed_by_type.get(cache_key)\n  if cached is not None:\n   return cached\n   \n  if not self._is_sse_stream:\n   await self.read()\n   \n  parsed=self._parse(to=to)\n  if is_given(self._options.post_parser):\n   parsed=self._options.post_parser(parsed)\n   \n  self._parsed_by_type[cache_key]=parsed\n  return parsed\n  \n async def read(self)->bytes:\n  ''\n  try:\n   return await self.http_response.aread()\n  except httpx.StreamConsumed as exc:\n  \n  \n  \n   raise StreamAlreadyConsumed()from exc\n   \n async def text(self)->str:\n  ''\n  await self.read()\n  return self.http_response.text\n  \n async def json(self)->object:\n  ''\n  await self.read()\n  return self.http_response.json()\n  \n async def close(self)->None:\n  ''\n\n\n  \n  await self.http_response.aclose()\n  \n async def iter_bytes(self,chunk_size:int |None=None)->AsyncIterator[bytes]:\n  ''\n\n\n\n  \n  async for chunk in self.http_response.aiter_bytes(chunk_size):\n   yield chunk\n   \n async def iter_text(self,chunk_size:int |None=None)->AsyncIterator[str]:\n  ''\n\n\n  \n  async for chunk in self.http_response.aiter_text(chunk_size):\n   yield chunk\n   \n async def iter_lines(self)->AsyncIterator[str]:\n  ''\n  async for chunk in self.http_response.aiter_lines():\n   yield chunk\n   \n   \nclass BinaryAPIResponse(APIResponse[bytes]):\n ''\n\n\n\n\n \n \n def write_to_file(\n self,\n file:str |os.PathLike[str],\n )->None:\n  ''\n\n\n\n\n\n\n  \n  with open(file,mode=\"wb\")as f:\n   for data in self.iter_bytes():\n    f.write(data)\n    \n    \nclass AsyncBinaryAPIResponse(AsyncAPIResponse[bytes]):\n ''\n\n\n\n\n \n \n async def write_to_file(\n self,\n file:str |os.PathLike[str],\n )->None:\n  ''\n\n\n\n\n\n\n  \n  path=anyio.Path(file)\n  async with await path.open(mode=\"wb\")as f:\n   async for data in self.iter_bytes():\n    await f.write(data)\n    \n    \nclass StreamedBinaryAPIResponse(APIResponse[bytes]):\n def stream_to_file(\n self,\n file:str |os.PathLike[str],\n *,\n chunk_size:int |None=None,\n )->None:\n  ''\n\n\n  \n  with open(file,mode=\"wb\")as f:\n   for data in self.iter_bytes(chunk_size):\n    f.write(data)\n    \n    \nclass AsyncStreamedBinaryAPIResponse(AsyncAPIResponse[bytes]):\n async def stream_to_file(\n self,\n file:str |os.PathLike[str],\n *,\n chunk_size:int |None=None,\n )->None:\n  ''\n\n\n  \n  path=anyio.Path(file)\n  async with await path.open(mode=\"wb\")as f:\n   async for data in self.iter_bytes(chunk_size):\n    await f.write(data)\n    \n    \nclass MissingStreamClassError(TypeError):\n def __init__(self)->None:\n  super().__init__(\n  \"The `stream` argument was set to `True` but the `stream_cls` argument was not given. See `groq._streaming` for reference\",\n  )\n  \n  \nclass StreamAlreadyConsumed(GroqError):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self)->None:\n  message=(\n  \"Attempted to read or stream some content, but the content has \"\n  \"already been streamed. \"\n  \"This could be due to attempting to stream the response \"\n  \"content more than once.\"\n  \"\\n\\n\"\n  \"You can fix this by manually accumulating the response content while streaming \"\n  \"or by calling `.read()` before starting to stream.\"\n  )\n  super().__init__(message)\n  \n  \nclass ResponseContextManager(Generic[_APIResponseT]):\n ''\n\n\n \n \n def __init__(self,request_func:Callable[[],_APIResponseT])->None:\n  self._request_func=request_func\n  self.__response:_APIResponseT |None=None\n  \n def __enter__(self)->_APIResponseT:\n  self.__response=self._request_func()\n  return self.__response\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  if self.__response is not None:\n   self.__response.close()\n   \n   \nclass AsyncResponseContextManager(Generic[_AsyncAPIResponseT]):\n ''\n\n\n \n \n def __init__(self,api_request:Awaitable[_AsyncAPIResponseT])->None:\n  self._api_request=api_request\n  self.__response:_AsyncAPIResponseT |None=None\n  \n async def __aenter__(self)->_AsyncAPIResponseT:\n  self.__response=await self._api_request\n  return self.__response\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  if self.__response is not None:\n   await self.__response.close()\n   \n   \ndef to_streamed_response_wrapper(func:Callable[P,R])->Callable[P,ResponseContextManager[APIResponse[R]]]:\n ''\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->ResponseContextManager[APIResponse[R]]:\n  extra_headers:dict[str,str]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"stream\"\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  make_request=functools.partial(func,*args,**kwargs)\n  \n  return ResponseContextManager(cast(Callable[[],APIResponse[R]],make_request))\n  \n return wrapped\n \n \ndef async_to_streamed_response_wrapper(\nfunc:Callable[P,Awaitable[R]],\n)->Callable[P,AsyncResponseContextManager[AsyncAPIResponse[R]]]:\n ''\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->AsyncResponseContextManager[AsyncAPIResponse[R]]:\n  extra_headers:dict[str,str]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"stream\"\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  make_request=func(*args,**kwargs)\n  \n  return AsyncResponseContextManager(cast(Awaitable[AsyncAPIResponse[R]],make_request))\n  \n return wrapped\n \n \ndef to_custom_streamed_response_wrapper(\nfunc:Callable[P,object],\nresponse_cls:type[_APIResponseT],\n)->Callable[P,ResponseContextManager[_APIResponseT]]:\n ''\n\n\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->ResponseContextManager[_APIResponseT]:\n  extra_headers:dict[str,Any]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"stream\"\n  extra_headers[OVERRIDE_CAST_TO_HEADER]=response_cls\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  make_request=functools.partial(func,*args,**kwargs)\n  \n  return ResponseContextManager(cast(Callable[[],_APIResponseT],make_request))\n  \n return wrapped\n \n \ndef async_to_custom_streamed_response_wrapper(\nfunc:Callable[P,Awaitable[object]],\nresponse_cls:type[_AsyncAPIResponseT],\n)->Callable[P,AsyncResponseContextManager[_AsyncAPIResponseT]]:\n ''\n\n\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->AsyncResponseContextManager[_AsyncAPIResponseT]:\n  extra_headers:dict[str,Any]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"stream\"\n  extra_headers[OVERRIDE_CAST_TO_HEADER]=response_cls\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  make_request=func(*args,**kwargs)\n  \n  return AsyncResponseContextManager(cast(Awaitable[_AsyncAPIResponseT],make_request))\n  \n return wrapped\n \n \ndef to_raw_response_wrapper(func:Callable[P,R])->Callable[P,APIResponse[R]]:\n ''\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->APIResponse[R]:\n  extra_headers:dict[str,str]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"raw\"\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  return cast(APIResponse[R],func(*args,**kwargs))\n  \n return wrapped\n \n \ndef async_to_raw_response_wrapper(func:Callable[P,Awaitable[R]])->Callable[P,Awaitable[AsyncAPIResponse[R]]]:\n ''\n\n \n \n @functools.wraps(func)\n async def wrapped(*args:P.args,**kwargs:P.kwargs)->AsyncAPIResponse[R]:\n  extra_headers:dict[str,str]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"raw\"\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  return cast(AsyncAPIResponse[R],await func(*args,**kwargs))\n  \n return wrapped\n \n \ndef to_custom_raw_response_wrapper(\nfunc:Callable[P,object],\nresponse_cls:type[_APIResponseT],\n)->Callable[P,_APIResponseT]:\n ''\n\n\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->_APIResponseT:\n  extra_headers:dict[str,Any]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"raw\"\n  extra_headers[OVERRIDE_CAST_TO_HEADER]=response_cls\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  return cast(_APIResponseT,func(*args,**kwargs))\n  \n return wrapped\n \n \ndef async_to_custom_raw_response_wrapper(\nfunc:Callable[P,Awaitable[object]],\nresponse_cls:type[_AsyncAPIResponseT],\n)->Callable[P,Awaitable[_AsyncAPIResponseT]]:\n ''\n\n\n\n \n \n @functools.wraps(func)\n def wrapped(*args:P.args,**kwargs:P.kwargs)->Awaitable[_AsyncAPIResponseT]:\n  extra_headers:dict[str,Any]={**(cast(Any,kwargs.get(\"extra_headers\"))or{})}\n  extra_headers[RAW_RESPONSE_HEADER]=\"raw\"\n  extra_headers[OVERRIDE_CAST_TO_HEADER]=response_cls\n  \n  kwargs[\"extra_headers\"]=extra_headers\n  \n  return cast(Awaitable[_AsyncAPIResponseT],func(*args,**kwargs))\n  \n return wrapped\n \n \ndef extract_response_type(typ:type[BaseAPIResponse[Any]])->type:\n ''\n\n\n\n\n\n\n\n\n \n return extract_type_var_from_base(\n typ,\n generic_bases=cast(\"tuple[type, ...]\",(BaseAPIResponse,APIResponse,AsyncAPIResponse)),\n index=0,\n )\n", ["__future__", "anyio", "datetime", "functools", "groq._base_client", "groq._constants", "groq._exceptions", "groq._models", "groq._streaming", "groq._types", "groq._utils", "httpx", "inspect", "logging", "os", "pydantic", "types", "typing", "typing_extensions"]], "groq._exceptions": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal\n\nimport httpx\n\n__all__=[\n\"BadRequestError\",\n\"AuthenticationError\",\n\"PermissionDeniedError\",\n\"NotFoundError\",\n\"ConflictError\",\n\"UnprocessableEntityError\",\n\"RateLimitError\",\n\"InternalServerError\",\n]\n\n\nclass GroqError(Exception):\n pass\n \n \nclass APIError(GroqError):\n message:str\n request:httpx.Request\n \n body:object |None\n ''\n\n\n\n\n\n\n\n \n \n def __init__(self,message:str,request:httpx.Request,*,body:object |None)->None:\n  super().__init__(message)\n  self.request=request\n  self.message=message\n  self.body=body\n  \n  \nclass APIResponseValidationError(APIError):\n response:httpx.Response\n status_code:int\n \n def __init__(self,response:httpx.Response,body:object |None,*,message:str |None=None)->None:\n  super().__init__(message or \"Data returned by API invalid for expected schema.\",response.request,body=body)\n  self.response=response\n  self.status_code=response.status_code\n  \n  \nclass APIStatusError(APIError):\n ''\n \n response:httpx.Response\n status_code:int\n \n def __init__(self,message:str,*,response:httpx.Response,body:object |None)->None:\n  super().__init__(message,response.request,body=body)\n  self.response=response\n  self.status_code=response.status_code\n  \n  \nclass APIConnectionError(APIError):\n def __init__(self,*,message:str=\"Connection error.\",request:httpx.Request)->None:\n  super().__init__(message,request,body=None)\n  \n  \nclass APITimeoutError(APIConnectionError):\n def __init__(self,request:httpx.Request)->None:\n  super().__init__(message=\"Request timed out.\",request=request)\n  \n  \nclass BadRequestError(APIStatusError):\n status_code:Literal[400]=400\n \n \nclass AuthenticationError(APIStatusError):\n status_code:Literal[401]=401\n \n \nclass PermissionDeniedError(APIStatusError):\n status_code:Literal[403]=403\n \n \nclass NotFoundError(APIStatusError):\n status_code:Literal[404]=404\n \n \nclass ConflictError(APIStatusError):\n status_code:Literal[409]=409\n \n \nclass UnprocessableEntityError(APIStatusError):\n status_code:Literal[422]=422\n \n \nclass RateLimitError(APIStatusError):\n status_code:Literal[429]=429\n \n \nclass InternalServerError(APIStatusError):\n pass\n", ["__future__", "httpx", "typing_extensions"]], "groq._files": [".py", "from __future__ import annotations\n\nimport io\nimport os\nimport pathlib\nfrom typing import overload\nfrom typing_extensions import TypeGuard\n\nimport anyio\n\nfrom._types import(\nFileTypes,\nFileContent,\nRequestFiles,\nHttpxFileTypes,\nBase64FileInput,\nHttpxFileContent,\nHttpxRequestFiles,\n)\nfrom._utils import is_tuple_t,is_mapping_t,is_sequence_t\n\n\ndef is_base64_file_input(obj:object)->TypeGuard[Base64FileInput]:\n return isinstance(obj,io.IOBase)or isinstance(obj,os.PathLike)\n \n \ndef is_file_content(obj:object)->TypeGuard[FileContent]:\n return(\n isinstance(obj,bytes)or isinstance(obj,tuple)or isinstance(obj,io.IOBase)or isinstance(obj,os.PathLike)\n )\n \n \ndef assert_is_file_content(obj:object,*,key:str |None=None)->None:\n if not is_file_content(obj):\n  prefix=f\"Expected entry at `{key}`\"if key is not None else f\"Expected file input `{obj !r}`\"\n  raise RuntimeError(\n  f\"{prefix} to be bytes, an io.IOBase instance, PathLike or a tuple but received {type(obj)} instead.\"\n  )from None\n  \n  \n@overload\ndef to_httpx_files(files:None)->None:...\n\n\n@overload\ndef to_httpx_files(files:RequestFiles)->HttpxRequestFiles:...\n\n\ndef to_httpx_files(files:RequestFiles |None)->HttpxRequestFiles |None:\n if files is None:\n  return None\n  \n if is_mapping_t(files):\n  files={key:_transform_file(file)for key,file in files.items()}\n elif is_sequence_t(files):\n  files=[(key,_transform_file(file))for key,file in files]\n else:\n  raise TypeError(f\"Unexpected file type input {type(files)}, expected mapping or sequence\")\n  \n return files\n \n \ndef _transform_file(file:FileTypes)->HttpxFileTypes:\n if is_file_content(file):\n  if isinstance(file,os.PathLike):\n   path=pathlib.Path(file)\n   return(path.name,path.read_bytes())\n   \n  return file\n  \n if is_tuple_t(file):\n  return(file[0],_read_file_content(file[1]),*file[2:])\n  \n raise TypeError(f\"Expected file types input to be a FileContent type or to be a tuple\")\n \n \ndef _read_file_content(file:FileContent)->HttpxFileContent:\n if isinstance(file,os.PathLike):\n  return pathlib.Path(file).read_bytes()\n return file\n \n \n@overload\nasync def async_to_httpx_files(files:None)->None:...\n\n\n@overload\nasync def async_to_httpx_files(files:RequestFiles)->HttpxRequestFiles:...\n\n\nasync def async_to_httpx_files(files:RequestFiles |None)->HttpxRequestFiles |None:\n if files is None:\n  return None\n  \n if is_mapping_t(files):\n  files={key:await _async_transform_file(file)for key,file in files.items()}\n elif is_sequence_t(files):\n  files=[(key,await _async_transform_file(file))for key,file in files]\n else:\n  raise TypeError(\"Unexpected file type input {type(files)}, expected mapping or sequence\")\n  \n return files\n \n \nasync def _async_transform_file(file:FileTypes)->HttpxFileTypes:\n if is_file_content(file):\n  if isinstance(file,os.PathLike):\n   path=anyio.Path(file)\n   return(path.name,await path.read_bytes())\n   \n  return file\n  \n if is_tuple_t(file):\n  return(file[0],await _async_read_file_content(file[1]),*file[2:])\n  \n raise TypeError(f\"Expected file types input to be a FileContent type or to be a tuple\")\n \n \nasync def _async_read_file_content(file:FileContent)->HttpxFileContent:\n if isinstance(file,os.PathLike):\n  return await anyio.Path(file).read_bytes()\n  \n return file\n", ["__future__", "anyio", "groq._types", "groq._utils", "io", "os", "pathlib", "typing", "typing_extensions"]], "groq._types": [".py", "from __future__ import annotations\n\nfrom os import PathLike\nfrom typing import(\nIO,\nTYPE_CHECKING,\nAny,\nDict,\nList,\nType,\nTuple,\nUnion,\nMapping,\nTypeVar,\nCallable,\nOptional,\nSequence,\n)\nfrom typing_extensions import Set,Literal,Protocol,TypeAlias,TypedDict,override,runtime_checkable\n\nimport httpx\nimport pydantic\nfrom httpx import URL,Proxy,Timeout,Response,BaseTransport,AsyncBaseTransport\n\nif TYPE_CHECKING:\n from._models import BaseModel\n from._response import APIResponse,AsyncAPIResponse\n \nTransport=BaseTransport\nAsyncTransport=AsyncBaseTransport\nQuery=Mapping[str,object]\nBody=object\nAnyMapping=Mapping[str,object]\nModelT=TypeVar(\"ModelT\",bound=pydantic.BaseModel)\n_T=TypeVar(\"_T\")\n\n\n\n\nProxiesDict=Dict[\"str | URL\",Union[None,str,URL,Proxy]]\nProxiesTypes=Union[str,Proxy,ProxiesDict]\nif TYPE_CHECKING:\n Base64FileInput=Union[IO[bytes],PathLike[str]]\n FileContent=Union[IO[bytes],bytes,PathLike[str]]\nelse:\n Base64FileInput=Union[IO[bytes],PathLike]\n FileContent=Union[IO[bytes],bytes,PathLike]\nFileTypes=Union[\n\nFileContent,\n\nTuple[Optional[str],FileContent],\n\nTuple[Optional[str],FileContent,Optional[str]],\n\nTuple[Optional[str],FileContent,Optional[str],Mapping[str,str]],\n]\nRequestFiles=Union[Mapping[str,FileTypes],Sequence[Tuple[str,FileTypes]]]\n\n\nHttpxFileContent=Union[IO[bytes],bytes]\nHttpxFileTypes=Union[\n\nHttpxFileContent,\n\nTuple[Optional[str],HttpxFileContent],\n\nTuple[Optional[str],HttpxFileContent,Optional[str]],\n\nTuple[Optional[str],HttpxFileContent,Optional[str],Mapping[str,str]],\n]\nHttpxRequestFiles=Union[Mapping[str,HttpxFileTypes],Sequence[Tuple[str,HttpxFileTypes]]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif TYPE_CHECKING:\n NoneType:Type[None]\nelse:\n NoneType=type(None)\n \n \nclass RequestOptions(TypedDict,total=False):\n headers:Headers\n max_retries:int\n timeout:float |Timeout |None\n params:Query\n extra_json:AnyMapping\n idempotency_key:str\n \n \n \nclass NotGiven:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __bool__(self)->Literal[False]:\n  return False\n  \n @override\n def __repr__(self)->str:\n  return \"NOT_GIVEN\"\n  \n  \nNotGivenOr=Union[_T,NotGiven]\nNOT_GIVEN=NotGiven()\n\n\nclass Omit:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __bool__(self)->Literal[False]:\n  return False\n  \n  \n@runtime_checkable\nclass ModelBuilderProtocol(Protocol):\n @classmethod\n def build(\n cls:type[_T],\n *,\n response:Response,\n data:object,\n )->_T:...\n \n \nHeaders=Mapping[str,Union[str,Omit]]\n\n\nclass HeadersLikeProtocol(Protocol):\n def get(self,__key:str)->str |None:...\n \n \nHeadersLike=Union[Headers,HeadersLikeProtocol]\n\nResponseT=TypeVar(\n\"ResponseT\",\nbound=Union[\nobject,\nstr,\nNone,\n\"BaseModel\",\nList[Any],\nDict[str,Any],\nResponse,\nModelBuilderProtocol,\n\"APIResponse[Any]\",\n\"AsyncAPIResponse[Any]\",\n],\n)\n\nStrBytesIntFloat=Union[str,bytes,int,float]\n\n\n\nIncEx:TypeAlias=Union[Set[int],Set[str],Mapping[int,Union[\"IncEx\",bool]],Mapping[str,Union[\"IncEx\",bool]]]\n\nPostParser=Callable[[Any],Any]\n\n\n@runtime_checkable\nclass InheritsGeneric(Protocol):\n ''\n\n\n\n \n \n __orig_bases__:tuple[_GenericAlias]\n \n \nclass _GenericAlias(Protocol):\n __origin__:type[object]\n \n \nclass HttpxSendArgs(TypedDict,total=False):\n auth:httpx.Auth\n", ["__future__", "groq._models", "groq._response", "httpx", "os", "pydantic", "typing", "typing_extensions"]], "groq._streaming": [".py", "\nfrom __future__ import annotations\n\nimport json\nimport inspect\nfrom types import TracebackType\nfrom typing import TYPE_CHECKING,Any,Generic,TypeVar,Iterator,AsyncIterator,cast\nfrom typing_extensions import Self,Protocol,TypeGuard,override,get_origin,runtime_checkable\n\nimport httpx\n\nfrom._utils import is_mapping,extract_type_var_from_base\nfrom._exceptions import APIError\n\nif TYPE_CHECKING:\n from._client import Groq,AsyncGroq\n \n \n_T=TypeVar(\"_T\")\n\n\nclass Stream(Generic[_T]):\n ''\n \n response:httpx.Response\n \n _decoder:SSEBytesDecoder\n \n def __init__(\n self,\n *,\n cast_to:type[_T],\n response:httpx.Response,\n client:Groq,\n )->None:\n  self.response=response\n  self._cast_to=cast_to\n  self._client=client\n  self._decoder=client._make_sse_decoder()\n  self._iterator=self.__stream__()\n  \n def __next__(self)->_T:\n  return self._iterator.__next__()\n  \n def __iter__(self)->Iterator[_T]:\n  for item in self._iterator:\n   yield item\n   \n def _iter_events(self)->Iterator[ServerSentEvent]:\n  yield from self._decoder.iter_bytes(self.response.iter_bytes())\n  \n def __stream__(self)->Iterator[_T]:\n  cast_to=cast(Any,self._cast_to)\n  response=self.response\n  process_data=self._client._process_response_data\n  iterator=self._iter_events()\n  \n  for sse in iterator:\n   if sse.data.startswith(\"[DONE]\"):\n    break\n    \n   if sse.event is None:\n    data=sse.json()\n    if is_mapping(data)and data.get(\"error\"):\n     message=None\n     error=data.get(\"error\")\n     if is_mapping(error):\n      message=error.get(\"message\")\n     if not message or not isinstance(message,str):\n      message=\"An error occurred during streaming\"\n      \n     raise APIError(\n     message=message,\n     request=self.response.request,\n     body=data[\"error\"],\n     )\n     \n    yield process_data(data=data,cast_to=cast_to,response=response)\n    \n   else:\n    data=sse.json()\n    \n    if sse.event ==\"error\"and is_mapping(data)and data.get(\"error\"):\n     message=None\n     error=data.get(\"error\")\n     if is_mapping(error):\n      message=error.get(\"message\")\n     if not message or not isinstance(message,str):\n      message=\"An error occurred during streaming\"\n      \n     raise APIError(\n     message=message,\n     request=self.response.request,\n     body=data[\"error\"],\n     )\n     \n    yield process_data(data={\"data\":data,\"event\":sse.event},cast_to=cast_to,response=response)\n    \n    \n  for _sse in iterator:\n   ...\n   \n def __enter__(self)->Self:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.close()\n  \n def close(self)->None:\n  ''\n\n\n\n  \n  self.response.close()\n  \n  \nclass AsyncStream(Generic[_T]):\n ''\n \n response:httpx.Response\n \n _decoder:SSEDecoder |SSEBytesDecoder\n \n def __init__(\n self,\n *,\n cast_to:type[_T],\n response:httpx.Response,\n client:AsyncGroq,\n )->None:\n  self.response=response\n  self._cast_to=cast_to\n  self._client=client\n  self._decoder=client._make_sse_decoder()\n  self._iterator=self.__stream__()\n  \n async def __anext__(self)->_T:\n  return await self._iterator.__anext__()\n  \n async def __aiter__(self)->AsyncIterator[_T]:\n  async for item in self._iterator:\n   yield item\n   \n async def _iter_events(self)->AsyncIterator[ServerSentEvent]:\n  async for sse in self._decoder.aiter_bytes(self.response.aiter_bytes()):\n   yield sse\n   \n async def __stream__(self)->AsyncIterator[_T]:\n  cast_to=cast(Any,self._cast_to)\n  response=self.response\n  process_data=self._client._process_response_data\n  iterator=self._iter_events()\n  \n  async for sse in iterator:\n   if sse.data.startswith(\"[DONE]\"):\n    break\n    \n   if sse.event is None:\n    data=sse.json()\n    if is_mapping(data)and data.get(\"error\"):\n     message=None\n     error=data.get(\"error\")\n     if is_mapping(error):\n      message=error.get(\"message\")\n     if not message or not isinstance(message,str):\n      message=\"An error occurred during streaming\"\n      \n     raise APIError(\n     message=message,\n     request=self.response.request,\n     body=data[\"error\"],\n     )\n     \n    yield process_data(data=data,cast_to=cast_to,response=response)\n    \n   else:\n    data=sse.json()\n    \n    if sse.event ==\"error\"and is_mapping(data)and data.get(\"error\"):\n     message=None\n     error=data.get(\"error\")\n     if is_mapping(error):\n      message=error.get(\"message\")\n     if not message or not isinstance(message,str):\n      message=\"An error occurred during streaming\"\n      \n     raise APIError(\n     message=message,\n     request=self.response.request,\n     body=data[\"error\"],\n     )\n     \n    yield process_data(data={\"data\":data,\"event\":sse.event},cast_to=cast_to,response=response)\n    \n    \n  async for _sse in iterator:\n   ...\n   \n async def __aenter__(self)->Self:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  await self.close()\n  \n async def close(self)->None:\n  ''\n\n\n\n  \n  await self.response.aclose()\n  \n  \nclass ServerSentEvent:\n def __init__(\n self,\n *,\n event:str |None=None,\n data:str |None=None,\n id:str |None=None,\n retry:int |None=None,\n )->None:\n  if data is None:\n   data=\"\"\n   \n  self._id=id\n  self._data=data\n  self._event=event or None\n  self._retry=retry\n  \n @property\n def event(self)->str |None:\n  return self._event\n  \n @property\n def id(self)->str |None:\n  return self._id\n  \n @property\n def retry(self)->int |None:\n  return self._retry\n  \n @property\n def data(self)->str:\n  return self._data\n  \n def json(self)->Any:\n  return json.loads(self.data)\n  \n @override\n def __repr__(self)->str:\n  return f\"ServerSentEvent(event={self.event}, data={self.data}, id={self.id}, retry={self.retry})\"\n  \n  \nclass SSEDecoder:\n _data:list[str]\n _event:str |None\n _retry:int |None\n _last_event_id:str |None\n \n def __init__(self)->None:\n  self._event=None\n  self._data=[]\n  self._last_event_id=None\n  self._retry=None\n  \n def iter_bytes(self,iterator:Iterator[bytes])->Iterator[ServerSentEvent]:\n  ''\n  for chunk in self._iter_chunks(iterator):\n  \n   for raw_line in chunk.splitlines():\n    line=raw_line.decode(\"utf-8\")\n    sse=self.decode(line)\n    if sse:\n     yield sse\n     \n def _iter_chunks(self,iterator:Iterator[bytes])->Iterator[bytes]:\n  ''\n  data=b\"\"\n  for chunk in iterator:\n   for line in chunk.splitlines(keepends=True):\n    data +=line\n    if data.endswith((b\"\\r\\r\",b\"\\n\\n\",b\"\\r\\n\\r\\n\")):\n     yield data\n     data=b\"\"\n  if data:\n   yield data\n   \n async def aiter_bytes(self,iterator:AsyncIterator[bytes])->AsyncIterator[ServerSentEvent]:\n  ''\n  async for chunk in self._aiter_chunks(iterator):\n  \n   for raw_line in chunk.splitlines():\n    line=raw_line.decode(\"utf-8\")\n    sse=self.decode(line)\n    if sse:\n     yield sse\n     \n async def _aiter_chunks(self,iterator:AsyncIterator[bytes])->AsyncIterator[bytes]:\n  ''\n  data=b\"\"\n  async for chunk in iterator:\n   for line in chunk.splitlines(keepends=True):\n    data +=line\n    if data.endswith((b\"\\r\\r\",b\"\\n\\n\",b\"\\r\\n\\r\\n\")):\n     yield data\n     data=b\"\"\n  if data:\n   yield data\n   \n def decode(self,line:str)->ServerSentEvent |None:\n \n \n  if not line:\n   if not self._event and not self._data and not self._last_event_id and self._retry is None:\n    return None\n    \n   sse=ServerSentEvent(\n   event=self._event,\n   data=\"\\n\".join(self._data),\n   id=self._last_event_id,\n   retry=self._retry,\n   )\n   \n   \n   self._event=None\n   self._data=[]\n   self._retry=None\n   \n   return sse\n   \n  if line.startswith(\":\"):\n   return None\n   \n  fieldname,_,value=line.partition(\":\")\n  \n  if value.startswith(\" \"):\n   value=value[1:]\n   \n  if fieldname ==\"event\":\n   self._event=value\n  elif fieldname ==\"data\":\n   self._data.append(value)\n  elif fieldname ==\"id\":\n   if \"\\0\"in value:\n    pass\n   else:\n    self._last_event_id=value\n  elif fieldname ==\"retry\":\n   try:\n    self._retry=int(value)\n   except(TypeError,ValueError):\n    pass\n  else:\n   pass\n   \n  return None\n  \n  \n@runtime_checkable\nclass SSEBytesDecoder(Protocol):\n def iter_bytes(self,iterator:Iterator[bytes])->Iterator[ServerSentEvent]:\n  ''\n  ...\n  \n def aiter_bytes(self,iterator:AsyncIterator[bytes])->AsyncIterator[ServerSentEvent]:\n  ''\n  ...\n  \n  \ndef is_stream_class_type(typ:type)->TypeGuard[type[Stream[object]]|type[AsyncStream[object]]]:\n ''\n origin=get_origin(typ)or typ\n return inspect.isclass(origin)and issubclass(origin,(Stream,AsyncStream))\n \n \ndef extract_stream_chunk_type(\nstream_cls:type,\n*,\nfailure_message:str |None=None,\n)->type:\n ''\n\n\n\n\n\n\n\n\n \n from._base_client import Stream,AsyncStream\n \n return extract_type_var_from_base(\n stream_cls,\n index=0,\n generic_bases=cast(\"tuple[type, ...]\",(Stream,AsyncStream)),\n failure_message=failure_message,\n )\n", ["__future__", "groq._base_client", "groq._client", "groq._exceptions", "groq._utils", "httpx", "inspect", "json", "types", "typing", "typing_extensions"]], "groq._constants": [".py", "\n\nimport httpx\n\nRAW_RESPONSE_HEADER=\"X-Stainless-Raw-Response\"\nOVERRIDE_CAST_TO_HEADER=\"____stainless_override_cast_to\"\n\n\nDEFAULT_TIMEOUT=httpx.Timeout(timeout=60.0,connect=5.0)\nDEFAULT_MAX_RETRIES=2\nDEFAULT_CONNECTION_LIMITS=httpx.Limits(max_connections=100,max_keepalive_connections=20)\n\nINITIAL_RETRY_DELAY=0.5\nMAX_RETRY_DELAY=8.0\n", ["httpx"]], "groq._models": [".py", "from __future__ import annotations\n\nimport os\nimport inspect\nfrom typing import TYPE_CHECKING,Any,Type,Union,Generic,TypeVar,Callable,cast\nfrom datetime import date,datetime\nfrom typing_extensions import(\nUnpack,\nLiteral,\nClassVar,\nProtocol,\nRequired,\nParamSpec,\nTypedDict,\nTypeGuard,\nfinal,\noverride,\nruntime_checkable,\n)\n\nimport pydantic\nimport pydantic.generics\nfrom pydantic.fields import FieldInfo\n\nfrom._types import(\nBody,\nIncEx,\nQuery,\nModelT,\nHeaders,\nTimeout,\nNotGiven,\nAnyMapping,\nHttpxRequestFiles,\n)\nfrom._utils import(\nPropertyInfo,\nis_list,\nis_given,\njson_safe,\nlru_cache,\nis_mapping,\nparse_date,\ncoerce_boolean,\nparse_datetime,\nstrip_not_given,\nextract_type_arg,\nis_annotated_type,\nis_type_alias_type,\nstrip_annotated_type,\n)\nfrom._compat import(\nPYDANTIC_V2,\nConfigDict,\nGenericModel as BaseGenericModel,\nget_args,\nis_union,\nparse_obj,\nget_origin,\nis_literal_type,\nget_model_config,\nget_model_fields,\nfield_get_default,\n)\nfrom._constants import RAW_RESPONSE_HEADER\n\nif TYPE_CHECKING:\n from pydantic_core.core_schema import ModelField,LiteralSchema,ModelFieldsSchema\n \n__all__=[\"BaseModel\",\"GenericModel\"]\n\n_T=TypeVar(\"_T\")\n_BaseModelT=TypeVar(\"_BaseModelT\",bound=\"BaseModel\")\n\nP=ParamSpec(\"P\")\n\n\n@runtime_checkable\nclass _ConfigProtocol(Protocol):\n allow_population_by_field_name:bool\n \n \nclass BaseModel(pydantic.BaseModel):\n if PYDANTIC_V2:\n  model_config:ClassVar[ConfigDict]=ConfigDict(\n  extra=\"allow\",defer_build=coerce_boolean(os.environ.get(\"DEFER_PYDANTIC_BUILD\",\"true\"))\n  )\n else:\n \n  @property\n  @override\n  def model_fields_set(self)->set[str]:\n  \n   return self.__fields_set__\n   \n  class Config(pydantic.BaseConfig):\n   extra:Any=pydantic.Extra.allow\n   \n def to_dict(\n self,\n *,\n mode:Literal[\"json\",\"python\"]=\"python\",\n use_api_names:bool=True,\n exclude_unset:bool=True,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n warnings:bool=True,\n )->dict[str,object]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.model_dump(\n  mode=mode,\n  by_alias=use_api_names,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  warnings=warnings,\n  )\n  \n def to_json(\n self,\n *,\n indent:int |None=2,\n use_api_names:bool=True,\n exclude_unset:bool=True,\n exclude_defaults:bool=False,\n exclude_none:bool=False,\n warnings:bool=True,\n )->str:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self.model_dump_json(\n  indent=indent,\n  by_alias=use_api_names,\n  exclude_unset=exclude_unset,\n  exclude_defaults=exclude_defaults,\n  exclude_none=exclude_none,\n  warnings=warnings,\n  )\n  \n @override\n def __str__(self)->str:\n \n  return f'{self.__repr_name__()}({self.__repr_str__(\", \")})'\n  \n  \n  \n @classmethod\n @override\n def construct(\n cls:Type[ModelT],\n _fields_set:set[str]|None=None,\n **values:object,\n )->ModelT:\n  m=cls.__new__(cls)\n  fields_values:dict[str,object]={}\n  \n  config=get_model_config(cls)\n  populate_by_name=(\n  config.allow_population_by_field_name\n  if isinstance(config,_ConfigProtocol)\n  else config.get(\"populate_by_name\")\n  )\n  \n  if _fields_set is None:\n   _fields_set=set()\n   \n  model_fields=get_model_fields(cls)\n  for name,field in model_fields.items():\n   key=field.alias\n   if key is None or(key not in values and populate_by_name):\n    key=name\n    \n   if key in values:\n    fields_values[name]=_construct_field(value=values[key],field=field,key=key)\n    _fields_set.add(name)\n   else:\n    fields_values[name]=field_get_default(field)\n    \n  _extra={}\n  for key,value in values.items():\n   if key not in model_fields:\n    if PYDANTIC_V2:\n     _extra[key]=value\n    else:\n     _fields_set.add(key)\n     fields_values[key]=value\n     \n  object.__setattr__(m,\"__dict__\",fields_values)\n  \n  if PYDANTIC_V2:\n  \n   object.__setattr__(m,\"__pydantic_private__\",None)\n   object.__setattr__(m,\"__pydantic_extra__\",_extra)\n   object.__setattr__(m,\"__pydantic_fields_set__\",_fields_set)\n  else:\n  \n   m._init_private_attributes()\n   \n   \n   object.__setattr__(m,\"__fields_set__\",_fields_set)\n   \n  return m\n  \n if not TYPE_CHECKING:\n \n \n \n  model_construct=construct\n  \n if not PYDANTIC_V2:\n \n \n \n \n \n  @override\n  def model_dump(\n  self,\n  *,\n  mode:Literal[\"json\",\"python\"]|str=\"python\",\n  include:IncEx |None=None,\n  exclude:IncEx |None=None,\n  by_alias:bool=False,\n  exclude_unset:bool=False,\n  exclude_defaults:bool=False,\n  exclude_none:bool=False,\n  round_trip:bool=False,\n  warnings:bool |Literal[\"none\",\"warn\",\"error\"]=True,\n  context:dict[str,Any]|None=None,\n  serialize_as_any:bool=False,\n  )->dict[str,Any]:\n   ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n   if mode not in{\"json\",\"python\"}:\n    raise ValueError(\"mode must be either 'json' or 'python'\")\n   if round_trip !=False:\n    raise ValueError(\"round_trip is only supported in Pydantic v2\")\n   if warnings !=True:\n    raise ValueError(\"warnings is only supported in Pydantic v2\")\n   if context is not None:\n    raise ValueError(\"context is only supported in Pydantic v2\")\n   if serialize_as_any !=False:\n    raise ValueError(\"serialize_as_any is only supported in Pydantic v2\")\n   dumped=super().dict(\n   include=include,\n   exclude=exclude,\n   by_alias=by_alias,\n   exclude_unset=exclude_unset,\n   exclude_defaults=exclude_defaults,\n   exclude_none=exclude_none,\n   )\n   \n   return cast(dict[str,Any],json_safe(dumped))if mode ==\"json\"else dumped\n   \n  @override\n  def model_dump_json(\n  self,\n  *,\n  indent:int |None=None,\n  include:IncEx |None=None,\n  exclude:IncEx |None=None,\n  by_alias:bool=False,\n  exclude_unset:bool=False,\n  exclude_defaults:bool=False,\n  exclude_none:bool=False,\n  round_trip:bool=False,\n  warnings:bool |Literal[\"none\",\"warn\",\"error\"]=True,\n  context:dict[str,Any]|None=None,\n  serialize_as_any:bool=False,\n  )->str:\n   ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n   if round_trip !=False:\n    raise ValueError(\"round_trip is only supported in Pydantic v2\")\n   if warnings !=True:\n    raise ValueError(\"warnings is only supported in Pydantic v2\")\n   if context is not None:\n    raise ValueError(\"context is only supported in Pydantic v2\")\n   if serialize_as_any !=False:\n    raise ValueError(\"serialize_as_any is only supported in Pydantic v2\")\n   return super().json(\n   indent=indent,\n   include=include,\n   exclude=exclude,\n   by_alias=by_alias,\n   exclude_unset=exclude_unset,\n   exclude_defaults=exclude_defaults,\n   exclude_none=exclude_none,\n   )\n   \n   \ndef _construct_field(value:object,field:FieldInfo,key:str)->object:\n if value is None:\n  return field_get_default(field)\n  \n if PYDANTIC_V2:\n  type_=field.annotation\n else:\n  type_=cast(type,field.outer_type_)\n  \n if type_ is None:\n  raise RuntimeError(f\"Unexpected field type is None for {key}\")\n  \n return construct_type(value=value,type_=type_)\n \n \ndef is_basemodel(type_:type)->bool:\n ''\n if is_union(type_):\n  for variant in get_args(type_):\n   if is_basemodel(variant):\n    return True\n    \n  return False\n  \n return is_basemodel_type(type_)\n \n \ndef is_basemodel_type(type_:type)->TypeGuard[type[BaseModel]|type[GenericModel]]:\n origin=get_origin(type_)or type_\n if not inspect.isclass(origin):\n  return False\n return issubclass(origin,BaseModel)or issubclass(origin,GenericModel)\n \n \ndef build(\nbase_model_cls:Callable[P,_BaseModelT],\n*args:P.args,\n**kwargs:P.kwargs,\n)->_BaseModelT:\n ''\n\n\n\n\n\n\n\n\n \n if args:\n  raise TypeError(\n  \"Received positional arguments which are not supported; Keyword arguments must be used instead\",\n  )\n  \n return cast(_BaseModelT,construct_type(type_=base_model_cls,value=kwargs))\n \n \ndef construct_type_unchecked(*,value:object,type_:type[_T])->_T:\n ''\n\n\n\n \n return cast(_T,construct_type(value=value,type_=type_))\n \n \ndef construct_type(*,value:object,type_:object)->object:\n ''\n\n\n \n \n \n type_=cast(\"type[object]\",type_)\n if is_type_alias_type(type_):\n  type_=type_.__value__\n  \n  \n if is_annotated_type(type_):\n  meta:tuple[Any,...]=get_args(type_)[1:]\n  type_=extract_type_arg(type_,0)\n else:\n  meta=tuple()\n  \n  \n  \n origin=get_origin(type_)or type_\n args=get_args(type_)\n \n if is_union(origin):\n  try:\n   return validate_type(type_=cast(\"type[object]\",type_),value=value)\n  except Exception:\n   pass\n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n  discriminator=_build_discriminated_union_meta(union=type_,meta_annotations=meta)\n  if discriminator and is_mapping(value):\n   variant_value=value.get(discriminator.field_alias_from or discriminator.field_name)\n   if variant_value and isinstance(variant_value,str):\n    variant_type=discriminator.mapping.get(variant_value)\n    if variant_type:\n     return construct_type(type_=variant_type,value=value)\n     \n     \n  for variant in args:\n   try:\n    return construct_type(value=value,type_=variant)\n   except Exception:\n    continue\n    \n  raise RuntimeError(f\"Could not convert data into a valid instance of {type_}\")\n  \n if origin ==dict:\n  if not is_mapping(value):\n   return value\n   \n  _,items_type=get_args(type_)\n  return{key:construct_type(value=item,type_=items_type)for key,item in value.items()}\n  \n if not is_literal_type(type_)and(issubclass(origin,BaseModel)or issubclass(origin,GenericModel)):\n  if is_list(value):\n   return[cast(Any,type_).construct(**entry)if is_mapping(entry)else entry for entry in value]\n   \n  if is_mapping(value):\n   if issubclass(type_,BaseModel):\n    return type_.construct(**value)\n    \n   return cast(Any,type_).construct(**value)\n   \n if origin ==list:\n  if not is_list(value):\n   return value\n   \n  inner_type=args[0]\n  return[construct_type(value=entry,type_=inner_type)for entry in value]\n  \n if origin ==float:\n  if isinstance(value,int):\n   coerced=float(value)\n   if coerced !=value:\n    return value\n   return coerced\n   \n  return value\n  \n if type_ ==datetime:\n  try:\n   return parse_datetime(value)\n  except Exception:\n   return value\n   \n if type_ ==date:\n  try:\n   return parse_date(value)\n  except Exception:\n   return value\n   \n return value\n \n \n@runtime_checkable\nclass CachedDiscriminatorType(Protocol):\n __discriminator__:DiscriminatorDetails\n \n \nclass DiscriminatorDetails:\n field_name:str\n ''\n\n\n\n\n\n\n\n \n \n field_alias_from:str |None\n ''\n\n\n\n\n\n\n\n \n \n mapping:dict[str,type]\n ''\n\n\n \n \n def __init__(\n self,\n *,\n mapping:dict[str,type],\n discriminator_field:str,\n discriminator_alias:str |None,\n )->None:\n  self.mapping=mapping\n  self.field_name=discriminator_field\n  self.field_alias_from=discriminator_alias\n  \n  \ndef _build_discriminated_union_meta(*,union:type,meta_annotations:tuple[Any,...])->DiscriminatorDetails |None:\n if isinstance(union,CachedDiscriminatorType):\n  return union.__discriminator__\n  \n discriminator_field_name:str |None=None\n \n for annotation in meta_annotations:\n  if isinstance(annotation,PropertyInfo)and annotation.discriminator is not None:\n   discriminator_field_name=annotation.discriminator\n   break\n   \n if not discriminator_field_name:\n  return None\n  \n mapping:dict[str,type]={}\n discriminator_alias:str |None=None\n \n for variant in get_args(union):\n  variant=strip_annotated_type(variant)\n  if is_basemodel_type(variant):\n   if PYDANTIC_V2:\n    field=_extract_field_schema_pv2(variant,discriminator_field_name)\n    if not field:\n     continue\n     \n     \n    discriminator_alias=field.get(\"serialization_alias\")\n    \n    field_schema=field[\"schema\"]\n    \n    if field_schema[\"type\"]==\"literal\":\n     for entry in cast(\"LiteralSchema\",field_schema)[\"expected\"]:\n      if isinstance(entry,str):\n       mapping[entry]=variant\n   else:\n    field_info=cast(\"dict[str, FieldInfo]\",variant.__fields__).get(discriminator_field_name)\n    if not field_info:\n     continue\n     \n     \n    discriminator_alias=field_info.alias\n    \n    if field_info.annotation and is_literal_type(field_info.annotation):\n     for entry in get_args(field_info.annotation):\n      if isinstance(entry,str):\n       mapping[entry]=variant\n       \n if not mapping:\n  return None\n  \n details=DiscriminatorDetails(\n mapping=mapping,\n discriminator_field=discriminator_field_name,\n discriminator_alias=discriminator_alias,\n )\n cast(CachedDiscriminatorType,union).__discriminator__=details\n return details\n \n \ndef _extract_field_schema_pv2(model:type[BaseModel],field_name:str)->ModelField |None:\n schema=model.__pydantic_core_schema__\n if schema[\"type\"]!=\"model\":\n  return None\n  \n fields_schema=schema[\"schema\"]\n if fields_schema[\"type\"]!=\"model-fields\":\n  return None\n  \n fields_schema=cast(\"ModelFieldsSchema\",fields_schema)\n \n field=fields_schema[\"fields\"].get(field_name)\n if not field:\n  return None\n  \n return cast(\"ModelField\",field)\n \n \ndef validate_type(*,type_:type[_T],value:object)->_T:\n ''\n if inspect.isclass(type_)and issubclass(type_,pydantic.BaseModel):\n  return cast(_T,parse_obj(type_,value))\n  \n return cast(_T,_validate_non_model_type(type_=type_,value=value))\n \n \ndef set_pydantic_config(typ:Any,config:pydantic.ConfigDict)->None:\n ''\n\n\n \n setattr(typ,\"__pydantic_config__\",config)\n \n \n \n \nif TYPE_CHECKING:\n GenericModel=BaseModel\nelse:\n\n class GenericModel(BaseGenericModel,BaseModel):\n  pass\n  \n  \nif PYDANTIC_V2:\n from pydantic import TypeAdapter as _TypeAdapter\n \n _CachedTypeAdapter=cast(\"TypeAdapter[object]\",lru_cache(maxsize=None)(_TypeAdapter))\n \n if TYPE_CHECKING:\n  from pydantic import TypeAdapter\n else:\n  TypeAdapter=_CachedTypeAdapter\n  \n def _validate_non_model_type(*,type_:type[_T],value:object)->_T:\n  return TypeAdapter(type_).validate_python(value)\n  \nelif not TYPE_CHECKING:\n\n class RootModel(GenericModel,Generic[_T]):\n  ''\n\n\n\n\n\n\n\n  \n  \n  __root__:_T\n  \n def _validate_non_model_type(*,type_:type[_T],value:object)->_T:\n  model=_create_pydantic_model(type_).validate(value)\n  return cast(_T,model.__root__)\n  \n def _create_pydantic_model(type_:_T)->Type[RootModel[_T]]:\n  return RootModel[type_]\n  \n  \nclass FinalRequestOptionsInput(TypedDict,total=False):\n method:Required[str]\n url:Required[str]\n params:Query\n headers:Headers\n max_retries:int\n timeout:float |Timeout |None\n files:HttpxRequestFiles |None\n idempotency_key:str\n json_data:Body\n extra_json:AnyMapping\n \n \n@final\nclass FinalRequestOptions(pydantic.BaseModel):\n method:str\n url:str\n params:Query={}\n headers:Union[Headers,NotGiven]=NotGiven()\n max_retries:Union[int,NotGiven]=NotGiven()\n timeout:Union[float,Timeout,None,NotGiven]=NotGiven()\n files:Union[HttpxRequestFiles,None]=None\n idempotency_key:Union[str,None]=None\n post_parser:Union[Callable[[Any],Any],NotGiven]=NotGiven()\n \n \n \n json_data:Union[Body,None]=None\n extra_json:Union[AnyMapping,None]=None\n \n if PYDANTIC_V2:\n  model_config:ClassVar[ConfigDict]=ConfigDict(arbitrary_types_allowed=True)\n else:\n \n  class Config(pydantic.BaseConfig):\n   arbitrary_types_allowed:bool=True\n   \n def get_max_retries(self,max_retries:int)->int:\n  if isinstance(self.max_retries,NotGiven):\n   return max_retries\n  return self.max_retries\n  \n def _strip_raw_response_header(self)->None:\n  if not is_given(self.headers):\n   return\n   \n  if self.headers.get(RAW_RESPONSE_HEADER):\n   self.headers={**self.headers}\n   self.headers.pop(RAW_RESPONSE_HEADER)\n   \n   \n   \n   \n   \n   \n   \n @classmethod\n def construct(\n cls,\n _fields_set:set[str]|None=None,\n **values:Unpack[FinalRequestOptionsInput],\n )->FinalRequestOptions:\n  kwargs:dict[str,Any]={\n  \n  \n  key:strip_not_given(value)\n  for key,value in values.items()\n  }\n  if PYDANTIC_V2:\n   return super().model_construct(_fields_set,**kwargs)\n  return cast(FinalRequestOptions,super().construct(_fields_set,**kwargs))\n  \n if not TYPE_CHECKING:\n \n  model_construct=construct\n", ["__future__", "datetime", "groq._compat", "groq._constants", "groq._types", "groq._utils", "inspect", "os", "pydantic", "pydantic.fields", "pydantic.generics", "pydantic_core.core_schema", "typing", "typing_extensions"]], "groq._base_client": [".py", "from __future__ import annotations\n\nimport sys\nimport json\nimport time\nimport uuid\nimport email\nimport asyncio\nimport inspect\nimport logging\nimport platform\nimport warnings\nimport email.utils\nfrom types import TracebackType\nfrom random import random\nfrom typing import(\nTYPE_CHECKING,\nAny,\nDict,\nType,\nUnion,\nGeneric,\nMapping,\nTypeVar,\nIterable,\nIterator,\nOptional,\nGenerator,\nAsyncIterator,\ncast,\noverload,\n)\nfrom typing_extensions import Literal,override,get_origin\n\nimport anyio\nimport httpx\nimport distro\nimport pydantic\nfrom httpx import URL,Limits\nfrom pydantic import PrivateAttr\n\nfrom. import _exceptions\nfrom._qs import Querystring\nfrom._files import to_httpx_files,async_to_httpx_files\nfrom._types import(\nNOT_GIVEN,\nBody,\nOmit,\nQuery,\nHeaders,\nTimeout,\nNotGiven,\nResponseT,\nTransport,\nAnyMapping,\nPostParser,\nProxiesTypes,\nRequestFiles,\nHttpxSendArgs,\nAsyncTransport,\nRequestOptions,\nHttpxRequestFiles,\nModelBuilderProtocol,\n)\nfrom._utils import is_dict,is_list,asyncify,is_given,lru_cache,is_mapping\nfrom._compat import model_copy,model_dump\nfrom._models import GenericModel,FinalRequestOptions,validate_type,construct_type\nfrom._response import(\nAPIResponse,\nBaseAPIResponse,\nAsyncAPIResponse,\nextract_response_type,\n)\nfrom._constants import(\nDEFAULT_TIMEOUT,\nMAX_RETRY_DELAY,\nDEFAULT_MAX_RETRIES,\nINITIAL_RETRY_DELAY,\nRAW_RESPONSE_HEADER,\nOVERRIDE_CAST_TO_HEADER,\nDEFAULT_CONNECTION_LIMITS,\n)\nfrom._streaming import Stream,SSEDecoder,AsyncStream,SSEBytesDecoder\nfrom._exceptions import(\nAPIStatusError,\nAPITimeoutError,\nAPIConnectionError,\nAPIResponseValidationError,\n)\n\nlog:logging.Logger=logging.getLogger(__name__)\n\n\nSyncPageT=TypeVar(\"SyncPageT\",bound=\"BaseSyncPage[Any]\")\nAsyncPageT=TypeVar(\"AsyncPageT\",bound=\"BaseAsyncPage[Any]\")\n\n\n_T=TypeVar(\"_T\")\n_T_co=TypeVar(\"_T_co\",covariant=True)\n\n_StreamT=TypeVar(\"_StreamT\",bound=Stream[Any])\n_AsyncStreamT=TypeVar(\"_AsyncStreamT\",bound=AsyncStream[Any])\n\nif TYPE_CHECKING:\n from httpx._config import DEFAULT_TIMEOUT_CONFIG as HTTPX_DEFAULT_TIMEOUT\nelse:\n try:\n  from httpx._config import DEFAULT_TIMEOUT_CONFIG as HTTPX_DEFAULT_TIMEOUT\n except ImportError:\n \n  HTTPX_DEFAULT_TIMEOUT=Timeout(5.0)\n  \n  \nclass PageInfo:\n ''\n\n\n \n \n url:URL |NotGiven\n params:Query |NotGiven\n \n @overload\n def __init__(\n self,\n *,\n url:URL,\n )->None:...\n \n @overload\n def __init__(\n self,\n *,\n params:Query,\n )->None:...\n \n def __init__(\n self,\n *,\n url:URL |NotGiven=NOT_GIVEN,\n params:Query |NotGiven=NOT_GIVEN,\n )->None:\n  self.url=url\n  self.params=params\n  \n @override\n def __repr__(self)->str:\n  if self.url:\n   return f\"{self.__class__.__name__}(url={self.url})\"\n  return f\"{self.__class__.__name__}(params={self.params})\"\n  \n  \nclass BasePage(GenericModel,Generic[_T]):\n ''\n\n\n\n\n\n\n\n\n \n \n _options:FinalRequestOptions=PrivateAttr()\n _model:Type[_T]=PrivateAttr()\n \n def has_next_page(self)->bool:\n  items=self._get_page_items()\n  if not items:\n   return False\n  return self.next_page_info()is not None\n  \n def next_page_info(self)->Optional[PageInfo]:...\n \n def _get_page_items(self)->Iterable[_T]:\n  ...\n  \n def _params_from_url(self,url:URL)->httpx.QueryParams:\n \n  return httpx.QueryParams(cast(Any,self._options.params)).merge(url.params)\n  \n def _info_to_options(self,info:PageInfo)->FinalRequestOptions:\n  options=model_copy(self._options)\n  options._strip_raw_response_header()\n  \n  if not isinstance(info.params,NotGiven):\n   options.params={**options.params,**info.params}\n   return options\n   \n  if not isinstance(info.url,NotGiven):\n   params=self._params_from_url(info.url)\n   url=info.url.copy_with(params=params)\n   options.params=dict(url.params)\n   options.url=str(url)\n   return options\n   \n  raise ValueError(\"Unexpected PageInfo state\")\n  \n  \nclass BaseSyncPage(BasePage[_T],Generic[_T]):\n _client:SyncAPIClient=pydantic.PrivateAttr()\n \n def _set_private_attributes(\n self,\n client:SyncAPIClient,\n model:Type[_T],\n options:FinalRequestOptions,\n )->None:\n  self._model=model\n  self._client=client\n  self._options=options\n  \n  \n  \n  \n  \n  \n  \n  \n  \n def __iter__(self)->Iterator[_T]:\n  for page in self.iter_pages():\n   for item in page._get_page_items():\n    yield item\n    \n def iter_pages(self:SyncPageT)->Iterator[SyncPageT]:\n  page=self\n  while True:\n   yield page\n   if page.has_next_page():\n    page=page.get_next_page()\n   else:\n    return\n    \n def get_next_page(self:SyncPageT)->SyncPageT:\n  info=self.next_page_info()\n  if not info:\n   raise RuntimeError(\n   \"No next page expected; please check `.has_next_page()` before calling `.get_next_page()`.\"\n   )\n   \n  options=self._info_to_options(info)\n  return self._client._request_api_list(self._model,page=self.__class__,options=options)\n  \n  \nclass AsyncPaginator(Generic[_T,AsyncPageT]):\n def __init__(\n self,\n client:AsyncAPIClient,\n options:FinalRequestOptions,\n page_cls:Type[AsyncPageT],\n model:Type[_T],\n )->None:\n  self._model=model\n  self._client=client\n  self._options=options\n  self._page_cls=page_cls\n  \n def __await__(self)->Generator[Any,None,AsyncPageT]:\n  return self._get_page().__await__()\n  \n async def _get_page(self)->AsyncPageT:\n  def _parser(resp:AsyncPageT)->AsyncPageT:\n   resp._set_private_attributes(\n   model=self._model,\n   options=self._options,\n   client=self._client,\n   )\n   return resp\n   \n  self._options.post_parser=_parser\n  \n  return await self._client.request(self._page_cls,self._options)\n  \n async def __aiter__(self)->AsyncIterator[_T]:\n \n  page=cast(\n  AsyncPageT,\n  await self,\n  )\n  async for item in page:\n   yield item\n   \n   \nclass BaseAsyncPage(BasePage[_T],Generic[_T]):\n _client:AsyncAPIClient=pydantic.PrivateAttr()\n \n def _set_private_attributes(\n self,\n model:Type[_T],\n client:AsyncAPIClient,\n options:FinalRequestOptions,\n )->None:\n  self._model=model\n  self._client=client\n  self._options=options\n  \n async def __aiter__(self)->AsyncIterator[_T]:\n  async for page in self.iter_pages():\n   for item in page._get_page_items():\n    yield item\n    \n async def iter_pages(self:AsyncPageT)->AsyncIterator[AsyncPageT]:\n  page=self\n  while True:\n   yield page\n   if page.has_next_page():\n    page=await page.get_next_page()\n   else:\n    return\n    \n async def get_next_page(self:AsyncPageT)->AsyncPageT:\n  info=self.next_page_info()\n  if not info:\n   raise RuntimeError(\n   \"No next page expected; please check `.has_next_page()` before calling `.get_next_page()`.\"\n   )\n   \n  options=self._info_to_options(info)\n  return await self._client._request_api_list(self._model,page=self.__class__,options=options)\n  \n  \n_HttpxClientT=TypeVar(\"_HttpxClientT\",bound=Union[httpx.Client,httpx.AsyncClient])\n_DefaultStreamT=TypeVar(\"_DefaultStreamT\",bound=Union[Stream[Any],AsyncStream[Any]])\n\n\nclass BaseClient(Generic[_HttpxClientT,_DefaultStreamT]):\n _client:_HttpxClientT\n _version:str\n _base_url:URL\n max_retries:int\n timeout:Union[float,Timeout,None]\n _limits:httpx.Limits\n _proxies:ProxiesTypes |None\n _transport:Transport |AsyncTransport |None\n _strict_response_validation:bool\n _idempotency_header:str |None\n _default_stream_cls:type[_DefaultStreamT]|None=None\n \n def __init__(\n self,\n *,\n version:str,\n base_url:str |URL,\n _strict_response_validation:bool,\n max_retries:int=DEFAULT_MAX_RETRIES,\n timeout:float |Timeout |None=DEFAULT_TIMEOUT,\n limits:httpx.Limits,\n transport:Transport |AsyncTransport |None,\n proxies:ProxiesTypes |None,\n custom_headers:Mapping[str,str]|None=None,\n custom_query:Mapping[str,object]|None=None,\n )->None:\n  self._version=version\n  self._base_url=self._enforce_trailing_slash(URL(base_url))\n  self.max_retries=max_retries\n  self.timeout=timeout\n  self._limits=limits\n  self._proxies=proxies\n  self._transport=transport\n  self._custom_headers=custom_headers or{}\n  self._custom_query=custom_query or{}\n  self._strict_response_validation=_strict_response_validation\n  self._idempotency_header=None\n  self._platform:Platform |None=None\n  \n  if max_retries is None:\n   raise TypeError(\n   \"max_retries cannot be None. If you want to disable retries, pass `0`; if you want unlimited retries, pass `math.inf` or a very high number; if you want the default behavior, pass `groq.DEFAULT_MAX_RETRIES`\"\n   )\n   \n def _enforce_trailing_slash(self,url:URL)->URL:\n  if url.raw_path.endswith(b\"/\"):\n   return url\n  return url.copy_with(raw_path=url.raw_path+b\"/\")\n  \n def _make_status_error_from_response(\n self,\n response:httpx.Response,\n )->APIStatusError:\n  if response.is_closed and not response.is_stream_consumed:\n  \n  \n  \n   body=None\n   err_msg=f\"Error code: {response.status_code}\"\n  else:\n   err_text=response.text.strip()\n   body=err_text\n   \n   try:\n    body=json.loads(err_text)\n    err_msg=f\"Error code: {response.status_code} - {body}\"\n   except Exception:\n    err_msg=err_text or f\"Error code: {response.status_code}\"\n    \n  return self._make_status_error(err_msg,body=body,response=response)\n  \n def _make_status_error(\n self,\n err_msg:str,\n *,\n body:object,\n response:httpx.Response,\n )->_exceptions.APIStatusError:\n  raise NotImplementedError()\n  \n def _build_headers(self,options:FinalRequestOptions,*,retries_taken:int=0)->httpx.Headers:\n  custom_headers=options.headers or{}\n  headers_dict=_merge_mappings(self.default_headers,custom_headers)\n  self._validate_headers(headers_dict,custom_headers)\n  \n  \n  headers=httpx.Headers(headers_dict)\n  \n  idempotency_header=self._idempotency_header\n  if idempotency_header and options.method.lower()!=\"get\"and idempotency_header not in headers:\n   headers[idempotency_header]=options.idempotency_key or self._idempotency_key()\n   \n   \n   \n  if \"x-stainless-retry-count\"not in(header.lower()for header in custom_headers):\n   headers[\"x-stainless-retry-count\"]=str(retries_taken)\n   \n  return headers\n  \n def _prepare_url(self,url:str)->URL:\n  ''\n\n\n  \n  \n  merge_url=URL(url)\n  if merge_url.is_relative_url:\n   merge_raw_path=self.base_url.raw_path+merge_url.raw_path.lstrip(b\"/\")\n   return self.base_url.copy_with(raw_path=merge_raw_path)\n   \n  return merge_url\n  \n def _make_sse_decoder(self)->SSEDecoder |SSEBytesDecoder:\n  return SSEDecoder()\n  \n def _build_request(\n self,\n options:FinalRequestOptions,\n *,\n retries_taken:int=0,\n )->httpx.Request:\n  if log.isEnabledFor(logging.DEBUG):\n   log.debug(\"Request options: %s\",model_dump(options,exclude_unset=True))\n   \n  kwargs:dict[str,Any]={}\n  \n  json_data=options.json_data\n  if options.extra_json is not None:\n   if json_data is None:\n    json_data=cast(Body,options.extra_json)\n   elif is_mapping(json_data):\n    json_data=_merge_mappings(json_data,options.extra_json)\n   else:\n    raise RuntimeError(f\"Unexpected JSON data type, {type(json_data)}, cannot merge with `extra_body`\")\n    \n  headers=self._build_headers(options,retries_taken=retries_taken)\n  params=_merge_mappings(self.default_query,options.params)\n  content_type=headers.get(\"Content-Type\")\n  files=options.files\n  \n  \n  \n  \n  \n  \n  if content_type is not None and content_type.startswith(\"multipart/form-data\"):\n   if \"boundary\"not in content_type:\n   \n   \n    headers.pop(\"Content-Type\")\n    \n    \n    \n   if json_data:\n    if not is_dict(json_data):\n     raise TypeError(\n     f\"Expected query input to be a dictionary for multipart requests but got {type(json_data)} instead.\"\n     )\n    kwargs[\"data\"]=self._serialize_multipartform(json_data)\n    \n    \n    \n    \n    \n    \n    \n   if not files:\n    files=cast(HttpxRequestFiles,ForceMultipartDict())\n    \n  prepared_url=self._prepare_url(options.url)\n  if \"_\"in prepared_url.host:\n  \n   kwargs[\"extensions\"]={\"sni_hostname\":prepared_url.host.replace(\"_\",\"-\")}\n   \n   \n  return self._client.build_request(\n  headers=headers,\n  timeout=self.timeout if isinstance(options.timeout,NotGiven)else options.timeout,\n  method=options.method,\n  url=prepared_url,\n  \n  \n  \n  \n  params=self.qs.stringify(cast(Mapping[str,Any],params))if params else None,\n  json=json_data,\n  files=files,\n  **kwargs,\n  )\n  \n def _serialize_multipartform(self,data:Mapping[object,object])->dict[str,object]:\n  items=self.qs.stringify_items(\n  \n  \n  data,\n  array_format=\"brackets\",\n  )\n  serialized:dict[str,object]={}\n  for key,value in items:\n   existing=serialized.get(key)\n   \n   if not existing:\n    serialized[key]=value\n    continue\n    \n    \n    \n    \n    \n    \n    \n    \n    \n   if is_list(existing):\n    existing.append(value)\n   else:\n    serialized[key]=[existing,value]\n    \n  return serialized\n  \n def _maybe_override_cast_to(self,cast_to:type[ResponseT],options:FinalRequestOptions)->type[ResponseT]:\n  if not is_given(options.headers):\n   return cast_to\n   \n   \n  headers=dict(options.headers)\n  \n  \n  \n  \n  override_cast_to=headers.pop(OVERRIDE_CAST_TO_HEADER,NOT_GIVEN)\n  if is_given(override_cast_to):\n   options.headers=headers\n   return cast(Type[ResponseT],override_cast_to)\n   \n  return cast_to\n  \n def _should_stream_response_body(self,request:httpx.Request)->bool:\n  return request.headers.get(RAW_RESPONSE_HEADER)==\"stream\"\n  \n def _process_response_data(\n self,\n *,\n data:object,\n cast_to:type[ResponseT],\n response:httpx.Response,\n )->ResponseT:\n  if data is None:\n   return cast(ResponseT,None)\n   \n  if cast_to is object:\n   return cast(ResponseT,data)\n   \n  try:\n   if inspect.isclass(cast_to)and issubclass(cast_to,ModelBuilderProtocol):\n    return cast(ResponseT,cast_to.build(response=response,data=data))\n    \n   if self._strict_response_validation:\n    return cast(ResponseT,validate_type(type_=cast_to,value=data))\n    \n   return cast(ResponseT,construct_type(type_=cast_to,value=data))\n  except pydantic.ValidationError as err:\n   raise APIResponseValidationError(response=response,body=data)from err\n   \n @property\n def qs(self)->Querystring:\n  return Querystring()\n  \n @property\n def custom_auth(self)->httpx.Auth |None:\n  return None\n  \n @property\n def auth_headers(self)->dict[str,str]:\n  return{}\n  \n @property\n def default_headers(self)->dict[str,str |Omit]:\n  return{\n  \"Accept\":\"application/json\",\n  \"Content-Type\":\"application/json\",\n  \"User-Agent\":self.user_agent,\n  **self.platform_headers(),\n  **self.auth_headers,\n  **self._custom_headers,\n  }\n  \n @property\n def default_query(self)->dict[str,object]:\n  return{\n  **self._custom_query,\n  }\n  \n def _validate_headers(\n self,\n headers:Headers,\n custom_headers:Headers,\n )->None:\n  ''\n\n\n  \n  return\n  \n @property\n def user_agent(self)->str:\n  return f\"{self.__class__.__name__}/Python {self._version}\"\n  \n @property\n def base_url(self)->URL:\n  return self._base_url\n  \n @base_url.setter\n def base_url(self,url:URL |str)->None:\n  self._base_url=self._enforce_trailing_slash(url if isinstance(url,URL)else URL(url))\n  \n def platform_headers(self)->Dict[str,str]:\n \n \n \n  return platform_headers(self._version,platform=self._platform)\n  \n def _parse_retry_after_header(self,response_headers:Optional[httpx.Headers]=None)->float |None:\n  ''\n\n\n\n  \n  if response_headers is None:\n   return None\n   \n   \n   \n  try:\n   retry_ms_header=response_headers.get(\"retry-after-ms\",None)\n   return float(retry_ms_header)/1000\n  except(TypeError,ValueError):\n   pass\n   \n   \n  retry_header=response_headers.get(\"retry-after\")\n  try:\n  \n  \n   return float(retry_header)\n  except(TypeError,ValueError):\n   pass\n   \n   \n  retry_date_tuple=email.utils.parsedate_tz(retry_header)\n  if retry_date_tuple is None:\n   return None\n   \n  retry_date=email.utils.mktime_tz(retry_date_tuple)\n  return float(retry_date -time.time())\n  \n def _calculate_retry_timeout(\n self,\n remaining_retries:int,\n options:FinalRequestOptions,\n response_headers:Optional[httpx.Headers]=None,\n )->float:\n  max_retries=options.get_max_retries(self.max_retries)\n  \n  \n  retry_after=self._parse_retry_after_header(response_headers)\n  if retry_after is not None and 0 <retry_after <=60:\n   return retry_after\n   \n   \n  nb_retries=min(max_retries -remaining_retries,1000)\n  \n  \n  sleep_seconds=min(INITIAL_RETRY_DELAY *pow(2.0,nb_retries),MAX_RETRY_DELAY)\n  \n  \n  jitter=1 -0.25 *random()\n  timeout=sleep_seconds *jitter\n  return timeout if timeout >=0 else 0\n  \n def _should_retry(self,response:httpx.Response)->bool:\n \n  should_retry_header=response.headers.get(\"x-should-retry\")\n  \n  \n  if should_retry_header ==\"true\":\n   log.debug(\"Retrying as header `x-should-retry` is set to `true`\")\n   return True\n  if should_retry_header ==\"false\":\n   log.debug(\"Not retrying as header `x-should-retry` is set to `false`\")\n   return False\n   \n   \n  if response.status_code ==408:\n   log.debug(\"Retrying due to status code %i\",response.status_code)\n   return True\n   \n   \n  if response.status_code ==409:\n   log.debug(\"Retrying due to status code %i\",response.status_code)\n   return True\n   \n   \n  if response.status_code ==429:\n   log.debug(\"Retrying due to status code %i\",response.status_code)\n   return True\n   \n   \n  if response.status_code >=500:\n   log.debug(\"Retrying due to status code %i\",response.status_code)\n   return True\n   \n  log.debug(\"Not retrying\")\n  return False\n  \n def _idempotency_key(self)->str:\n  return f\"stainless-python-retry-{uuid.uuid4()}\"\n  \n  \nclass _DefaultHttpxClient(httpx.Client):\n def __init__(self,**kwargs:Any)->None:\n  kwargs.setdefault(\"timeout\",DEFAULT_TIMEOUT)\n  kwargs.setdefault(\"limits\",DEFAULT_CONNECTION_LIMITS)\n  kwargs.setdefault(\"follow_redirects\",True)\n  super().__init__(**kwargs)\n  \n  \nif TYPE_CHECKING:\n DefaultHttpxClient=httpx.Client\n ''\n\n\n\n\n \nelse:\n DefaultHttpxClient=_DefaultHttpxClient\n \n \nclass SyncHttpxClientWrapper(DefaultHttpxClient):\n def __del__(self)->None:\n  try:\n   self.close()\n  except Exception:\n   pass\n   \n   \nclass SyncAPIClient(BaseClient[httpx.Client,Stream[Any]]):\n _client:httpx.Client\n _default_stream_cls:type[Stream[Any]]|None=None\n \n def __init__(\n self,\n *,\n version:str,\n base_url:str |URL,\n max_retries:int=DEFAULT_MAX_RETRIES,\n timeout:float |Timeout |None |NotGiven=NOT_GIVEN,\n transport:Transport |None=None,\n proxies:ProxiesTypes |None=None,\n limits:Limits |None=None,\n http_client:httpx.Client |None=None,\n custom_headers:Mapping[str,str]|None=None,\n custom_query:Mapping[str,object]|None=None,\n _strict_response_validation:bool,\n )->None:\n  kwargs:dict[str,Any]={}\n  if limits is not None:\n   warnings.warn(\n   \"The `connection_pool_limits` argument is deprecated. The `http_client` argument should be passed instead\",\n   category=DeprecationWarning,\n   stacklevel=3,\n   )\n   if http_client is not None:\n    raise ValueError(\"The `http_client` argument is mutually exclusive with `connection_pool_limits`\")\n  else:\n   limits=DEFAULT_CONNECTION_LIMITS\n   \n  if transport is not None:\n   kwargs[\"transport\"]=transport\n   warnings.warn(\n   \"The `transport` argument is deprecated. The `http_client` argument should be passed instead\",\n   category=DeprecationWarning,\n   stacklevel=3,\n   )\n   if http_client is not None:\n    raise ValueError(\"The `http_client` argument is mutually exclusive with `transport`\")\n    \n  if proxies is not None:\n   kwargs[\"proxies\"]=proxies\n   warnings.warn(\n   \"The `proxies` argument is deprecated. The `http_client` argument should be passed instead\",\n   category=DeprecationWarning,\n   stacklevel=3,\n   )\n   if http_client is not None:\n    raise ValueError(\"The `http_client` argument is mutually exclusive with `proxies`\")\n    \n  if not is_given(timeout):\n  \n  \n  \n  \n  \n  \n  \n   if http_client and http_client.timeout !=HTTPX_DEFAULT_TIMEOUT:\n    timeout=http_client.timeout\n   else:\n    timeout=DEFAULT_TIMEOUT\n    \n  if http_client is not None and not isinstance(http_client,httpx.Client):\n   raise TypeError(\n   f\"Invalid `http_client` argument; Expected an instance of `httpx.Client` but got {type(http_client)}\"\n   )\n   \n  super().__init__(\n  version=version,\n  limits=limits,\n  \n  timeout=cast(Timeout,timeout),\n  proxies=proxies,\n  base_url=base_url,\n  transport=transport,\n  max_retries=max_retries,\n  custom_query=custom_query,\n  custom_headers=custom_headers,\n  _strict_response_validation=_strict_response_validation,\n  )\n  self._client=http_client or SyncHttpxClientWrapper(\n  base_url=base_url,\n  \n  timeout=cast(Timeout,timeout),\n  limits=limits,\n  follow_redirects=True,\n  **kwargs,\n  )\n  \n def is_closed(self)->bool:\n  return self._client.is_closed\n  \n def close(self)->None:\n  ''\n\n\n  \n  \n  \n  if hasattr(self,\"_client\"):\n   self._client.close()\n   \n def __enter__(self:_T)->_T:\n  return self\n  \n def __exit__(\n self,\n exc_type:type[BaseException]|None,\n exc:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  self.close()\n  \n def _prepare_options(\n self,\n options:FinalRequestOptions,\n )->FinalRequestOptions:\n  ''\n  return options\n  \n def _prepare_request(\n self,\n request:httpx.Request,\n )->None:\n  ''\n\n\n\n  \n  return None\n  \n @overload\n def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n remaining_retries:Optional[int]=None,\n *,\n stream:Literal[True],\n stream_cls:Type[_StreamT],\n )->_StreamT:...\n \n @overload\n def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n remaining_retries:Optional[int]=None,\n *,\n stream:Literal[False]=False,\n )->ResponseT:...\n \n @overload\n def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n remaining_retries:Optional[int]=None,\n *,\n stream:bool=False,\n stream_cls:Type[_StreamT]|None=None,\n )->ResponseT |_StreamT:...\n \n def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n remaining_retries:Optional[int]=None,\n *,\n stream:bool=False,\n stream_cls:type[_StreamT]|None=None,\n )->ResponseT |_StreamT:\n  if remaining_retries is not None:\n   retries_taken=options.get_max_retries(self.max_retries)-remaining_retries\n  else:\n   retries_taken=0\n   \n  return self._request(\n  cast_to=cast_to,\n  options=options,\n  stream=stream,\n  stream_cls=stream_cls,\n  retries_taken=retries_taken,\n  )\n  \n def _request(\n self,\n *,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n retries_taken:int,\n stream:bool,\n stream_cls:type[_StreamT]|None,\n )->ResponseT |_StreamT:\n \n \n \n  input_options=model_copy(options)\n  \n  cast_to=self._maybe_override_cast_to(cast_to,options)\n  options=self._prepare_options(options)\n  \n  remaining_retries=options.get_max_retries(self.max_retries)-retries_taken\n  request=self._build_request(options,retries_taken=retries_taken)\n  self._prepare_request(request)\n  \n  kwargs:HttpxSendArgs={}\n  if self.custom_auth is not None:\n   kwargs[\"auth\"]=self.custom_auth\n   \n  log.debug(\"Sending HTTP Request: %s %s\",request.method,request.url)\n  \n  try:\n   response=self._client.send(\n   request,\n   stream=stream or self._should_stream_response_body(request=request),\n   **kwargs,\n   )\n  except httpx.TimeoutException as err:\n   log.debug(\"Encountered httpx.TimeoutException\",exc_info=True)\n   \n   if remaining_retries >0:\n    return self._retry_request(\n    input_options,\n    cast_to,\n    retries_taken=retries_taken,\n    stream=stream,\n    stream_cls=stream_cls,\n    response_headers=None,\n    )\n    \n   log.debug(\"Raising timeout error\")\n   raise APITimeoutError(request=request)from err\n  except Exception as err:\n   log.debug(\"Encountered Exception\",exc_info=True)\n   \n   if remaining_retries >0:\n    return self._retry_request(\n    input_options,\n    cast_to,\n    retries_taken=retries_taken,\n    stream=stream,\n    stream_cls=stream_cls,\n    response_headers=None,\n    )\n    \n   log.debug(\"Raising connection error\")\n   raise APIConnectionError(request=request)from err\n   \n  log.debug(\n  'HTTP Response: %s %s \"%i %s\" %s',\n  request.method,\n  request.url,\n  response.status_code,\n  response.reason_phrase,\n  response.headers,\n  )\n  \n  try:\n   response.raise_for_status()\n  except httpx.HTTPStatusError as err:\n   log.debug(\"Encountered httpx.HTTPStatusError\",exc_info=True)\n   \n   if remaining_retries >0 and self._should_retry(err.response):\n    err.response.close()\n    return self._retry_request(\n    input_options,\n    cast_to,\n    retries_taken=retries_taken,\n    response_headers=err.response.headers,\n    stream=stream,\n    stream_cls=stream_cls,\n    )\n    \n    \n    \n   if not err.response.is_closed:\n    err.response.read()\n    \n   log.debug(\"Re-raising status error\")\n   raise self._make_status_error_from_response(err.response)from None\n   \n  return self._process_response(\n  cast_to=cast_to,\n  options=options,\n  response=response,\n  stream=stream,\n  stream_cls=stream_cls,\n  retries_taken=retries_taken,\n  )\n  \n def _retry_request(\n self,\n options:FinalRequestOptions,\n cast_to:Type[ResponseT],\n *,\n retries_taken:int,\n response_headers:httpx.Headers |None,\n stream:bool,\n stream_cls:type[_StreamT]|None,\n )->ResponseT |_StreamT:\n  remaining_retries=options.get_max_retries(self.max_retries)-retries_taken\n  if remaining_retries ==1:\n   log.debug(\"1 retry left\")\n  else:\n   log.debug(\"%i retries left\",remaining_retries)\n   \n  timeout=self._calculate_retry_timeout(remaining_retries,options,response_headers)\n  log.info(\"Retrying request to %s in %f seconds\",options.url,timeout)\n  \n  \n  \n  time.sleep(timeout)\n  \n  return self._request(\n  options=options,\n  cast_to=cast_to,\n  retries_taken=retries_taken+1,\n  stream=stream,\n  stream_cls=stream_cls,\n  )\n  \n def _process_response(\n self,\n *,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n response:httpx.Response,\n stream:bool,\n stream_cls:type[Stream[Any]]|type[AsyncStream[Any]]|None,\n retries_taken:int=0,\n )->ResponseT:\n  origin=get_origin(cast_to)or cast_to\n  \n  if inspect.isclass(origin)and issubclass(origin,BaseAPIResponse):\n   if not issubclass(origin,APIResponse):\n    raise TypeError(f\"API Response types must subclass {APIResponse}; Received {origin}\")\n    \n   response_cls=cast(\"type[BaseAPIResponse[Any]]\",cast_to)\n   return cast(\n   ResponseT,\n   response_cls(\n   raw=response,\n   client=self,\n   cast_to=extract_response_type(response_cls),\n   stream=stream,\n   stream_cls=stream_cls,\n   options=options,\n   retries_taken=retries_taken,\n   ),\n   )\n   \n  if cast_to ==httpx.Response:\n   return cast(ResponseT,response)\n   \n  api_response=APIResponse(\n  raw=response,\n  client=self,\n  cast_to=cast(\"type[ResponseT]\",cast_to),\n  stream=stream,\n  stream_cls=stream_cls,\n  options=options,\n  retries_taken=retries_taken,\n  )\n  if bool(response.request.headers.get(RAW_RESPONSE_HEADER)):\n   return cast(ResponseT,api_response)\n   \n  return api_response.parse()\n  \n def _request_api_list(\n self,\n model:Type[object],\n page:Type[SyncPageT],\n options:FinalRequestOptions,\n )->SyncPageT:\n  def _parser(resp:SyncPageT)->SyncPageT:\n   resp._set_private_attributes(\n   client=self,\n   model=model,\n   options=options,\n   )\n   return resp\n   \n  options.post_parser=_parser\n  \n  return self.request(page,options,stream=False)\n  \n @overload\n def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:Literal[False]=False,\n )->ResponseT:...\n \n @overload\n def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:Literal[True],\n stream_cls:type[_StreamT],\n )->_StreamT:...\n \n @overload\n def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:bool,\n stream_cls:type[_StreamT]|None=None,\n )->ResponseT |_StreamT:...\n \n def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:bool=False,\n stream_cls:type[_StreamT]|None=None,\n )->ResponseT |_StreamT:\n  opts=FinalRequestOptions.construct(method=\"get\",url=path,**options)\n  \n  \n  return cast(ResponseT,self.request(cast_to,opts,stream=stream,stream_cls=stream_cls))\n  \n @overload\n def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n files:RequestFiles |None=None,\n stream:Literal[False]=False,\n )->ResponseT:...\n \n @overload\n def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n files:RequestFiles |None=None,\n stream:Literal[True],\n stream_cls:type[_StreamT],\n )->_StreamT:...\n \n @overload\n def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n files:RequestFiles |None=None,\n stream:bool,\n stream_cls:type[_StreamT]|None=None,\n )->ResponseT |_StreamT:...\n \n def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n files:RequestFiles |None=None,\n stream:bool=False,\n stream_cls:type[_StreamT]|None=None,\n )->ResponseT |_StreamT:\n  opts=FinalRequestOptions.construct(\n  method=\"post\",url=path,json_data=body,files=to_httpx_files(files),**options\n  )\n  return cast(ResponseT,self.request(cast_to,opts,stream=stream,stream_cls=stream_cls))\n  \n def patch(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n )->ResponseT:\n  opts=FinalRequestOptions.construct(method=\"patch\",url=path,json_data=body,**options)\n  return self.request(cast_to,opts)\n  \n def put(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n files:RequestFiles |None=None,\n options:RequestOptions={},\n )->ResponseT:\n  opts=FinalRequestOptions.construct(\n  method=\"put\",url=path,json_data=body,files=to_httpx_files(files),**options\n  )\n  return self.request(cast_to,opts)\n  \n def delete(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n )->ResponseT:\n  opts=FinalRequestOptions.construct(method=\"delete\",url=path,json_data=body,**options)\n  return self.request(cast_to,opts)\n  \n def get_api_list(\n self,\n path:str,\n *,\n model:Type[object],\n page:Type[SyncPageT],\n body:Body |None=None,\n options:RequestOptions={},\n method:str=\"get\",\n )->SyncPageT:\n  opts=FinalRequestOptions.construct(method=method,url=path,json_data=body,**options)\n  return self._request_api_list(model,page,opts)\n  \n  \nclass _DefaultAsyncHttpxClient(httpx.AsyncClient):\n def __init__(self,**kwargs:Any)->None:\n  kwargs.setdefault(\"timeout\",DEFAULT_TIMEOUT)\n  kwargs.setdefault(\"limits\",DEFAULT_CONNECTION_LIMITS)\n  kwargs.setdefault(\"follow_redirects\",True)\n  super().__init__(**kwargs)\n  \n  \nif TYPE_CHECKING:\n DefaultAsyncHttpxClient=httpx.AsyncClient\n ''\n\n\n\n\n \nelse:\n DefaultAsyncHttpxClient=_DefaultAsyncHttpxClient\n \n \nclass AsyncHttpxClientWrapper(DefaultAsyncHttpxClient):\n def __del__(self)->None:\n  try:\n  \n   asyncio.get_running_loop().create_task(self.aclose())\n  except Exception:\n   pass\n   \n   \nclass AsyncAPIClient(BaseClient[httpx.AsyncClient,AsyncStream[Any]]):\n _client:httpx.AsyncClient\n _default_stream_cls:type[AsyncStream[Any]]|None=None\n \n def __init__(\n self,\n *,\n version:str,\n base_url:str |URL,\n _strict_response_validation:bool,\n max_retries:int=DEFAULT_MAX_RETRIES,\n timeout:float |Timeout |None |NotGiven=NOT_GIVEN,\n transport:AsyncTransport |None=None,\n proxies:ProxiesTypes |None=None,\n limits:Limits |None=None,\n http_client:httpx.AsyncClient |None=None,\n custom_headers:Mapping[str,str]|None=None,\n custom_query:Mapping[str,object]|None=None,\n )->None:\n  kwargs:dict[str,Any]={}\n  if limits is not None:\n   warnings.warn(\n   \"The `connection_pool_limits` argument is deprecated. The `http_client` argument should be passed instead\",\n   category=DeprecationWarning,\n   stacklevel=3,\n   )\n   if http_client is not None:\n    raise ValueError(\"The `http_client` argument is mutually exclusive with `connection_pool_limits`\")\n  else:\n   limits=DEFAULT_CONNECTION_LIMITS\n   \n  if transport is not None:\n   kwargs[\"transport\"]=transport\n   warnings.warn(\n   \"The `transport` argument is deprecated. The `http_client` argument should be passed instead\",\n   category=DeprecationWarning,\n   stacklevel=3,\n   )\n   if http_client is not None:\n    raise ValueError(\"The `http_client` argument is mutually exclusive with `transport`\")\n    \n  if proxies is not None:\n   kwargs[\"proxies\"]=proxies\n   warnings.warn(\n   \"The `proxies` argument is deprecated. The `http_client` argument should be passed instead\",\n   category=DeprecationWarning,\n   stacklevel=3,\n   )\n   if http_client is not None:\n    raise ValueError(\"The `http_client` argument is mutually exclusive with `proxies`\")\n    \n  if not is_given(timeout):\n  \n  \n  \n  \n  \n  \n  \n   if http_client and http_client.timeout !=HTTPX_DEFAULT_TIMEOUT:\n    timeout=http_client.timeout\n   else:\n    timeout=DEFAULT_TIMEOUT\n    \n  if http_client is not None and not isinstance(http_client,httpx.AsyncClient):\n   raise TypeError(\n   f\"Invalid `http_client` argument; Expected an instance of `httpx.AsyncClient` but got {type(http_client)}\"\n   )\n   \n  super().__init__(\n  version=version,\n  base_url=base_url,\n  limits=limits,\n  \n  timeout=cast(Timeout,timeout),\n  proxies=proxies,\n  transport=transport,\n  max_retries=max_retries,\n  custom_query=custom_query,\n  custom_headers=custom_headers,\n  _strict_response_validation=_strict_response_validation,\n  )\n  self._client=http_client or AsyncHttpxClientWrapper(\n  base_url=base_url,\n  \n  timeout=cast(Timeout,timeout),\n  limits=limits,\n  follow_redirects=True,\n  **kwargs,\n  )\n  \n def is_closed(self)->bool:\n  return self._client.is_closed\n  \n async def close(self)->None:\n  ''\n\n\n  \n  await self._client.aclose()\n  \n async def __aenter__(self:_T)->_T:\n  return self\n  \n async def __aexit__(\n self,\n exc_type:type[BaseException]|None,\n exc:BaseException |None,\n exc_tb:TracebackType |None,\n )->None:\n  await self.close()\n  \n async def _prepare_options(\n self,\n options:FinalRequestOptions,\n )->FinalRequestOptions:\n  ''\n  return options\n  \n async def _prepare_request(\n self,\n request:httpx.Request,\n )->None:\n  ''\n\n\n\n  \n  return None\n  \n @overload\n async def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n *,\n stream:Literal[False]=False,\n remaining_retries:Optional[int]=None,\n )->ResponseT:...\n \n @overload\n async def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n *,\n stream:Literal[True],\n stream_cls:type[_AsyncStreamT],\n remaining_retries:Optional[int]=None,\n )->_AsyncStreamT:...\n \n @overload\n async def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n *,\n stream:bool,\n stream_cls:type[_AsyncStreamT]|None=None,\n remaining_retries:Optional[int]=None,\n )->ResponseT |_AsyncStreamT:...\n \n async def request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n *,\n stream:bool=False,\n stream_cls:type[_AsyncStreamT]|None=None,\n remaining_retries:Optional[int]=None,\n )->ResponseT |_AsyncStreamT:\n  if remaining_retries is not None:\n   retries_taken=options.get_max_retries(self.max_retries)-remaining_retries\n  else:\n   retries_taken=0\n   \n  return await self._request(\n  cast_to=cast_to,\n  options=options,\n  stream=stream,\n  stream_cls=stream_cls,\n  retries_taken=retries_taken,\n  )\n  \n async def _request(\n self,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n *,\n stream:bool,\n stream_cls:type[_AsyncStreamT]|None,\n retries_taken:int,\n )->ResponseT |_AsyncStreamT:\n  if self._platform is None:\n  \n  \n   self._platform=await asyncify(get_platform)()\n   \n   \n   \n   \n  input_options=model_copy(options)\n  \n  cast_to=self._maybe_override_cast_to(cast_to,options)\n  options=await self._prepare_options(options)\n  \n  remaining_retries=options.get_max_retries(self.max_retries)-retries_taken\n  request=self._build_request(options,retries_taken=retries_taken)\n  await self._prepare_request(request)\n  \n  kwargs:HttpxSendArgs={}\n  if self.custom_auth is not None:\n   kwargs[\"auth\"]=self.custom_auth\n   \n  try:\n   response=await self._client.send(\n   request,\n   stream=stream or self._should_stream_response_body(request=request),\n   **kwargs,\n   )\n  except httpx.TimeoutException as err:\n   log.debug(\"Encountered httpx.TimeoutException\",exc_info=True)\n   \n   if remaining_retries >0:\n    return await self._retry_request(\n    input_options,\n    cast_to,\n    retries_taken=retries_taken,\n    stream=stream,\n    stream_cls=stream_cls,\n    response_headers=None,\n    )\n    \n   log.debug(\"Raising timeout error\")\n   raise APITimeoutError(request=request)from err\n  except Exception as err:\n   log.debug(\"Encountered Exception\",exc_info=True)\n   \n   if remaining_retries >0:\n    return await self._retry_request(\n    input_options,\n    cast_to,\n    retries_taken=retries_taken,\n    stream=stream,\n    stream_cls=stream_cls,\n    response_headers=None,\n    )\n    \n   log.debug(\"Raising connection error\")\n   raise APIConnectionError(request=request)from err\n   \n  log.debug(\n  'HTTP Request: %s %s \"%i %s\"',request.method,request.url,response.status_code,response.reason_phrase\n  )\n  \n  try:\n   response.raise_for_status()\n  except httpx.HTTPStatusError as err:\n   log.debug(\"Encountered httpx.HTTPStatusError\",exc_info=True)\n   \n   if remaining_retries >0 and self._should_retry(err.response):\n    await err.response.aclose()\n    return await self._retry_request(\n    input_options,\n    cast_to,\n    retries_taken=retries_taken,\n    response_headers=err.response.headers,\n    stream=stream,\n    stream_cls=stream_cls,\n    )\n    \n    \n    \n   if not err.response.is_closed:\n    await err.response.aread()\n    \n   log.debug(\"Re-raising status error\")\n   raise self._make_status_error_from_response(err.response)from None\n   \n  return await self._process_response(\n  cast_to=cast_to,\n  options=options,\n  response=response,\n  stream=stream,\n  stream_cls=stream_cls,\n  retries_taken=retries_taken,\n  )\n  \n async def _retry_request(\n self,\n options:FinalRequestOptions,\n cast_to:Type[ResponseT],\n *,\n retries_taken:int,\n response_headers:httpx.Headers |None,\n stream:bool,\n stream_cls:type[_AsyncStreamT]|None,\n )->ResponseT |_AsyncStreamT:\n  remaining_retries=options.get_max_retries(self.max_retries)-retries_taken\n  if remaining_retries ==1:\n   log.debug(\"1 retry left\")\n  else:\n   log.debug(\"%i retries left\",remaining_retries)\n   \n  timeout=self._calculate_retry_timeout(remaining_retries,options,response_headers)\n  log.info(\"Retrying request to %s in %f seconds\",options.url,timeout)\n  \n  await anyio.sleep(timeout)\n  \n  return await self._request(\n  options=options,\n  cast_to=cast_to,\n  retries_taken=retries_taken+1,\n  stream=stream,\n  stream_cls=stream_cls,\n  )\n  \n async def _process_response(\n self,\n *,\n cast_to:Type[ResponseT],\n options:FinalRequestOptions,\n response:httpx.Response,\n stream:bool,\n stream_cls:type[Stream[Any]]|type[AsyncStream[Any]]|None,\n retries_taken:int=0,\n )->ResponseT:\n  origin=get_origin(cast_to)or cast_to\n  \n  if inspect.isclass(origin)and issubclass(origin,BaseAPIResponse):\n   if not issubclass(origin,AsyncAPIResponse):\n    raise TypeError(f\"API Response types must subclass {AsyncAPIResponse}; Received {origin}\")\n    \n   response_cls=cast(\"type[BaseAPIResponse[Any]]\",cast_to)\n   return cast(\n   \"ResponseT\",\n   response_cls(\n   raw=response,\n   client=self,\n   cast_to=extract_response_type(response_cls),\n   stream=stream,\n   stream_cls=stream_cls,\n   options=options,\n   retries_taken=retries_taken,\n   ),\n   )\n   \n  if cast_to ==httpx.Response:\n   return cast(ResponseT,response)\n   \n  api_response=AsyncAPIResponse(\n  raw=response,\n  client=self,\n  cast_to=cast(\"type[ResponseT]\",cast_to),\n  stream=stream,\n  stream_cls=stream_cls,\n  options=options,\n  retries_taken=retries_taken,\n  )\n  if bool(response.request.headers.get(RAW_RESPONSE_HEADER)):\n   return cast(ResponseT,api_response)\n   \n  return await api_response.parse()\n  \n def _request_api_list(\n self,\n model:Type[_T],\n page:Type[AsyncPageT],\n options:FinalRequestOptions,\n )->AsyncPaginator[_T,AsyncPageT]:\n  return AsyncPaginator(client=self,options=options,page_cls=page,model=model)\n  \n @overload\n async def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:Literal[False]=False,\n )->ResponseT:...\n \n @overload\n async def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:Literal[True],\n stream_cls:type[_AsyncStreamT],\n )->_AsyncStreamT:...\n \n @overload\n async def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:bool,\n stream_cls:type[_AsyncStreamT]|None=None,\n )->ResponseT |_AsyncStreamT:...\n \n async def get(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n options:RequestOptions={},\n stream:bool=False,\n stream_cls:type[_AsyncStreamT]|None=None,\n )->ResponseT |_AsyncStreamT:\n  opts=FinalRequestOptions.construct(method=\"get\",url=path,**options)\n  return await self.request(cast_to,opts,stream=stream,stream_cls=stream_cls)\n  \n @overload\n async def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n files:RequestFiles |None=None,\n options:RequestOptions={},\n stream:Literal[False]=False,\n )->ResponseT:...\n \n @overload\n async def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n files:RequestFiles |None=None,\n options:RequestOptions={},\n stream:Literal[True],\n stream_cls:type[_AsyncStreamT],\n )->_AsyncStreamT:...\n \n @overload\n async def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n files:RequestFiles |None=None,\n options:RequestOptions={},\n stream:bool,\n stream_cls:type[_AsyncStreamT]|None=None,\n )->ResponseT |_AsyncStreamT:...\n \n async def post(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n files:RequestFiles |None=None,\n options:RequestOptions={},\n stream:bool=False,\n stream_cls:type[_AsyncStreamT]|None=None,\n )->ResponseT |_AsyncStreamT:\n  opts=FinalRequestOptions.construct(\n  method=\"post\",url=path,json_data=body,files=await async_to_httpx_files(files),**options\n  )\n  return await self.request(cast_to,opts,stream=stream,stream_cls=stream_cls)\n  \n async def patch(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n )->ResponseT:\n  opts=FinalRequestOptions.construct(method=\"patch\",url=path,json_data=body,**options)\n  return await self.request(cast_to,opts)\n  \n async def put(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n files:RequestFiles |None=None,\n options:RequestOptions={},\n )->ResponseT:\n  opts=FinalRequestOptions.construct(\n  method=\"put\",url=path,json_data=body,files=await async_to_httpx_files(files),**options\n  )\n  return await self.request(cast_to,opts)\n  \n async def delete(\n self,\n path:str,\n *,\n cast_to:Type[ResponseT],\n body:Body |None=None,\n options:RequestOptions={},\n )->ResponseT:\n  opts=FinalRequestOptions.construct(method=\"delete\",url=path,json_data=body,**options)\n  return await self.request(cast_to,opts)\n  \n def get_api_list(\n self,\n path:str,\n *,\n model:Type[_T],\n page:Type[AsyncPageT],\n body:Body |None=None,\n options:RequestOptions={},\n method:str=\"get\",\n )->AsyncPaginator[_T,AsyncPageT]:\n  opts=FinalRequestOptions.construct(method=method,url=path,json_data=body,**options)\n  return self._request_api_list(model,page,opts)\n  \n  \ndef make_request_options(\n*,\nquery:Query |None=None,\nextra_headers:Headers |None=None,\nextra_query:Query |None=None,\nextra_body:Body |None=None,\nidempotency_key:str |None=None,\ntimeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\npost_parser:PostParser |NotGiven=NOT_GIVEN,\n)->RequestOptions:\n ''\n options:RequestOptions={}\n if extra_headers is not None:\n  options[\"headers\"]=extra_headers\n  \n if extra_body is not None:\n  options[\"extra_json\"]=cast(AnyMapping,extra_body)\n  \n if query is not None:\n  options[\"params\"]=query\n  \n if extra_query is not None:\n  options[\"params\"]={**options.get(\"params\",{}),**extra_query}\n  \n if not isinstance(timeout,NotGiven):\n  options[\"timeout\"]=timeout\n  \n if idempotency_key is not None:\n  options[\"idempotency_key\"]=idempotency_key\n  \n if is_given(post_parser):\n \n  options[\"post_parser\"]=post_parser\n  \n return options\n \n \nclass ForceMultipartDict(Dict[str,None]):\n def __bool__(self)->bool:\n  return True\n  \n  \nclass OtherPlatform:\n def __init__(self,name:str)->None:\n  self.name=name\n  \n @override\n def __str__(self)->str:\n  return f\"Other:{self.name}\"\n  \n  \nPlatform=Union[\nOtherPlatform,\nLiteral[\n\"MacOS\",\n\"Linux\",\n\"Windows\",\n\"FreeBSD\",\n\"OpenBSD\",\n\"iOS\",\n\"Android\",\n\"Unknown\",\n],\n]\n\n\ndef get_platform()->Platform:\n try:\n  system=platform.system().lower()\n  platform_name=platform.platform().lower()\n except Exception:\n  return \"Unknown\"\n  \n if \"iphone\"in platform_name or \"ipad\"in platform_name:\n \n \n \n \n  return \"iOS\"\n  \n if system ==\"darwin\":\n  return \"MacOS\"\n  \n if system ==\"windows\":\n  return \"Windows\"\n  \n if \"android\"in platform_name:\n \n \n  return \"Android\"\n  \n if system ==\"linux\":\n \n  distro_id=distro.id()\n  if distro_id ==\"freebsd\":\n   return \"FreeBSD\"\n   \n  if distro_id ==\"openbsd\":\n   return \"OpenBSD\"\n   \n  return \"Linux\"\n  \n if platform_name:\n  return OtherPlatform(platform_name)\n  \n return \"Unknown\"\n \n \n@lru_cache(maxsize=None)\ndef platform_headers(version:str,*,platform:Platform |None)->Dict[str,str]:\n return{\n \"X-Stainless-Lang\":\"python\",\n \"X-Stainless-Package-Version\":version,\n \"X-Stainless-OS\":str(platform or get_platform()),\n \"X-Stainless-Arch\":str(get_architecture()),\n \"X-Stainless-Runtime\":get_python_runtime(),\n \"X-Stainless-Runtime-Version\":get_python_version(),\n }\n \n \nclass OtherArch:\n def __init__(self,name:str)->None:\n  self.name=name\n  \n @override\n def __str__(self)->str:\n  return f\"other:{self.name}\"\n  \n  \nArch=Union[OtherArch,Literal[\"x32\",\"x64\",\"arm\",\"arm64\",\"unknown\"]]\n\n\ndef get_python_runtime()->str:\n try:\n  return platform.python_implementation()\n except Exception:\n  return \"unknown\"\n  \n  \ndef get_python_version()->str:\n try:\n  return platform.python_version()\n except Exception:\n  return \"unknown\"\n  \n  \ndef get_architecture()->Arch:\n try:\n  machine=platform.machine().lower()\n except Exception:\n  return \"unknown\"\n  \n if machine in(\"arm64\",\"aarch64\"):\n  return \"arm64\"\n  \n  \n if machine ==\"arm\":\n  return \"arm\"\n  \n if machine ==\"x86_64\":\n  return \"x64\"\n  \n  \n if sys.maxsize <=2 **32:\n  return \"x32\"\n  \n if machine:\n  return OtherArch(machine)\n  \n return \"unknown\"\n \n \ndef _merge_mappings(\nobj1:Mapping[_T_co,Union[_T,Omit]],\nobj2:Mapping[_T_co,Union[_T,Omit]],\n)->Dict[_T_co,_T]:\n ''\n\n\n \n merged={**obj1,**obj2}\n return{key:value for key,value in merged.items()if not isinstance(value,Omit)}\n", ["__future__", "anyio", "asyncio", "distro", "email", "email.utils", "groq", "groq._compat", "groq._constants", "groq._exceptions", "groq._files", "groq._models", "groq._qs", "groq._response", "groq._streaming", "groq._types", "groq._utils", "httpx", "httpx._config", "inspect", "json", "logging", "platform", "pydantic", "random", "sys", "time", "types", "typing", "typing_extensions", "uuid", "warnings"]], "groq._version": [".py", "\n\n__title__=\"groq\"\n__version__=\"0.13.1\"\n", []], "groq._client": [".py", "\n\nfrom __future__ import annotations\n\nimport os\nfrom typing import Any,Union,Mapping\nfrom typing_extensions import Self,override\n\nimport httpx\n\nfrom. import _exceptions\nfrom._qs import Querystring\nfrom._types import(\nNOT_GIVEN,\nOmit,\nTimeout,\nNotGiven,\nTransport,\nProxiesTypes,\nRequestOptions,\n)\nfrom._utils import(\nis_given,\nget_async_library,\n)\nfrom._version import __version__\nfrom.resources import models,embeddings\nfrom._streaming import Stream as Stream,AsyncStream as AsyncStream\nfrom._exceptions import GroqError,APIStatusError\nfrom._base_client import(\nDEFAULT_MAX_RETRIES,\nSyncAPIClient,\nAsyncAPIClient,\n)\nfrom.resources.chat import chat\nfrom.resources.audio import audio\n\n__all__=[\"Timeout\",\"Transport\",\"ProxiesTypes\",\"RequestOptions\",\"Groq\",\"AsyncGroq\",\"Client\",\"AsyncClient\"]\n\n\nclass Groq(SyncAPIClient):\n chat:chat.Chat\n embeddings:embeddings.Embeddings\n audio:audio.Audio\n models:models.Models\n with_raw_response:GroqWithRawResponse\n with_streaming_response:GroqWithStreamedResponse\n \n \n api_key:str\n \n def __init__(\n self,\n *,\n api_key:str |None=None,\n base_url:str |httpx.URL |None=None,\n timeout:Union[float,Timeout,None,NotGiven]=NOT_GIVEN,\n max_retries:int=DEFAULT_MAX_RETRIES,\n default_headers:Mapping[str,str]|None=None,\n default_query:Mapping[str,object]|None=None,\n \n \n \n http_client:httpx.Client |None=None,\n \n \n \n \n \n \n \n \n _strict_response_validation:bool=False,\n )->None:\n  ''\n\n\n  \n  if api_key is None:\n   api_key=os.environ.get(\"GROQ_API_KEY\")\n  if api_key is None:\n   raise GroqError(\n   \"The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\"\n   )\n  self.api_key=api_key\n  \n  if base_url is None:\n   base_url=os.environ.get(\"GROQ_BASE_URL\")\n  if base_url is None:\n   base_url=f\"https://api.groq.com\"\n   \n  super().__init__(\n  version=__version__,\n  base_url=base_url,\n  max_retries=max_retries,\n  timeout=timeout,\n  http_client=http_client,\n  custom_headers=default_headers,\n  custom_query=default_query,\n  _strict_response_validation=_strict_response_validation,\n  )\n  \n  self.chat=chat.Chat(self)\n  self.embeddings=embeddings.Embeddings(self)\n  self.audio=audio.Audio(self)\n  self.models=models.Models(self)\n  self.with_raw_response=GroqWithRawResponse(self)\n  self.with_streaming_response=GroqWithStreamedResponse(self)\n  \n @property\n @override\n def qs(self)->Querystring:\n  return Querystring(array_format=\"comma\")\n  \n @property\n @override\n def auth_headers(self)->dict[str,str]:\n  api_key=self.api_key\n  return{\"Authorization\":f\"Bearer {api_key}\"}\n  \n @property\n @override\n def default_headers(self)->dict[str,str |Omit]:\n  return{\n  **super().default_headers,\n  \"X-Stainless-Async\":\"false\",\n  **self._custom_headers,\n  }\n  \n def copy(\n self,\n *,\n api_key:str |None=None,\n base_url:str |httpx.URL |None=None,\n timeout:float |Timeout |None |NotGiven=NOT_GIVEN,\n http_client:httpx.Client |None=None,\n max_retries:int |NotGiven=NOT_GIVEN,\n default_headers:Mapping[str,str]|None=None,\n set_default_headers:Mapping[str,str]|None=None,\n default_query:Mapping[str,object]|None=None,\n set_default_query:Mapping[str,object]|None=None,\n _extra_kwargs:Mapping[str,Any]={},\n )->Self:\n  ''\n\n  \n  if default_headers is not None and set_default_headers is not None:\n   raise ValueError(\"The `default_headers` and `set_default_headers` arguments are mutually exclusive\")\n   \n  if default_query is not None and set_default_query is not None:\n   raise ValueError(\"The `default_query` and `set_default_query` arguments are mutually exclusive\")\n   \n  headers=self._custom_headers\n  if default_headers is not None:\n   headers={**headers,**default_headers}\n  elif set_default_headers is not None:\n   headers=set_default_headers\n   \n  params=self._custom_query\n  if default_query is not None:\n   params={**params,**default_query}\n  elif set_default_query is not None:\n   params=set_default_query\n   \n  http_client=http_client or self._client\n  return self.__class__(\n  api_key=api_key or self.api_key,\n  base_url=base_url or self.base_url,\n  timeout=self.timeout if isinstance(timeout,NotGiven)else timeout,\n  http_client=http_client,\n  max_retries=max_retries if is_given(max_retries)else self.max_retries,\n  default_headers=headers,\n  default_query=params,\n  **_extra_kwargs,\n  )\n  \n  \n  \n with_options=copy\n \n @override\n def _make_status_error(\n self,\n err_msg:str,\n *,\n body:object,\n response:httpx.Response,\n )->APIStatusError:\n  if response.status_code ==400:\n   return _exceptions.BadRequestError(err_msg,response=response,body=body)\n   \n  if response.status_code ==401:\n   return _exceptions.AuthenticationError(err_msg,response=response,body=body)\n   \n  if response.status_code ==403:\n   return _exceptions.PermissionDeniedError(err_msg,response=response,body=body)\n   \n  if response.status_code ==404:\n   return _exceptions.NotFoundError(err_msg,response=response,body=body)\n   \n  if response.status_code ==409:\n   return _exceptions.ConflictError(err_msg,response=response,body=body)\n   \n  if response.status_code ==422:\n   return _exceptions.UnprocessableEntityError(err_msg,response=response,body=body)\n   \n  if response.status_code ==429:\n   return _exceptions.RateLimitError(err_msg,response=response,body=body)\n   \n  if response.status_code >=500:\n   return _exceptions.InternalServerError(err_msg,response=response,body=body)\n  return APIStatusError(err_msg,response=response,body=body)\n  \n  \nclass AsyncGroq(AsyncAPIClient):\n chat:chat.AsyncChat\n embeddings:embeddings.AsyncEmbeddings\n audio:audio.AsyncAudio\n models:models.AsyncModels\n with_raw_response:AsyncGroqWithRawResponse\n with_streaming_response:AsyncGroqWithStreamedResponse\n \n \n api_key:str\n \n def __init__(\n self,\n *,\n api_key:str |None=None,\n base_url:str |httpx.URL |None=None,\n timeout:Union[float,Timeout,None,NotGiven]=NOT_GIVEN,\n max_retries:int=DEFAULT_MAX_RETRIES,\n default_headers:Mapping[str,str]|None=None,\n default_query:Mapping[str,object]|None=None,\n \n \n \n http_client:httpx.AsyncClient |None=None,\n \n \n \n \n \n \n \n \n _strict_response_validation:bool=False,\n )->None:\n  ''\n\n\n  \n  if api_key is None:\n   api_key=os.environ.get(\"GROQ_API_KEY\")\n  if api_key is None:\n   raise GroqError(\n   \"The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\"\n   )\n  self.api_key=api_key\n  \n  if base_url is None:\n   base_url=os.environ.get(\"GROQ_BASE_URL\")\n  if base_url is None:\n   base_url=f\"https://api.groq.com\"\n   \n  super().__init__(\n  version=__version__,\n  base_url=base_url,\n  max_retries=max_retries,\n  timeout=timeout,\n  http_client=http_client,\n  custom_headers=default_headers,\n  custom_query=default_query,\n  _strict_response_validation=_strict_response_validation,\n  )\n  \n  self.chat=chat.AsyncChat(self)\n  self.embeddings=embeddings.AsyncEmbeddings(self)\n  self.audio=audio.AsyncAudio(self)\n  self.models=models.AsyncModels(self)\n  self.with_raw_response=AsyncGroqWithRawResponse(self)\n  self.with_streaming_response=AsyncGroqWithStreamedResponse(self)\n  \n @property\n @override\n def qs(self)->Querystring:\n  return Querystring(array_format=\"comma\")\n  \n @property\n @override\n def auth_headers(self)->dict[str,str]:\n  api_key=self.api_key\n  return{\"Authorization\":f\"Bearer {api_key}\"}\n  \n @property\n @override\n def default_headers(self)->dict[str,str |Omit]:\n  return{\n  **super().default_headers,\n  \"X-Stainless-Async\":f\"async:{get_async_library()}\",\n  **self._custom_headers,\n  }\n  \n def copy(\n self,\n *,\n api_key:str |None=None,\n base_url:str |httpx.URL |None=None,\n timeout:float |Timeout |None |NotGiven=NOT_GIVEN,\n http_client:httpx.AsyncClient |None=None,\n max_retries:int |NotGiven=NOT_GIVEN,\n default_headers:Mapping[str,str]|None=None,\n set_default_headers:Mapping[str,str]|None=None,\n default_query:Mapping[str,object]|None=None,\n set_default_query:Mapping[str,object]|None=None,\n _extra_kwargs:Mapping[str,Any]={},\n )->Self:\n  ''\n\n  \n  if default_headers is not None and set_default_headers is not None:\n   raise ValueError(\"The `default_headers` and `set_default_headers` arguments are mutually exclusive\")\n   \n  if default_query is not None and set_default_query is not None:\n   raise ValueError(\"The `default_query` and `set_default_query` arguments are mutually exclusive\")\n   \n  headers=self._custom_headers\n  if default_headers is not None:\n   headers={**headers,**default_headers}\n  elif set_default_headers is not None:\n   headers=set_default_headers\n   \n  params=self._custom_query\n  if default_query is not None:\n   params={**params,**default_query}\n  elif set_default_query is not None:\n   params=set_default_query\n   \n  http_client=http_client or self._client\n  return self.__class__(\n  api_key=api_key or self.api_key,\n  base_url=base_url or self.base_url,\n  timeout=self.timeout if isinstance(timeout,NotGiven)else timeout,\n  http_client=http_client,\n  max_retries=max_retries if is_given(max_retries)else self.max_retries,\n  default_headers=headers,\n  default_query=params,\n  **_extra_kwargs,\n  )\n  \n  \n  \n with_options=copy\n \n @override\n def _make_status_error(\n self,\n err_msg:str,\n *,\n body:object,\n response:httpx.Response,\n )->APIStatusError:\n  if response.status_code ==400:\n   return _exceptions.BadRequestError(err_msg,response=response,body=body)\n   \n  if response.status_code ==401:\n   return _exceptions.AuthenticationError(err_msg,response=response,body=body)\n   \n  if response.status_code ==403:\n   return _exceptions.PermissionDeniedError(err_msg,response=response,body=body)\n   \n  if response.status_code ==404:\n   return _exceptions.NotFoundError(err_msg,response=response,body=body)\n   \n  if response.status_code ==409:\n   return _exceptions.ConflictError(err_msg,response=response,body=body)\n   \n  if response.status_code ==422:\n   return _exceptions.UnprocessableEntityError(err_msg,response=response,body=body)\n   \n  if response.status_code ==429:\n   return _exceptions.RateLimitError(err_msg,response=response,body=body)\n   \n  if response.status_code >=500:\n   return _exceptions.InternalServerError(err_msg,response=response,body=body)\n  return APIStatusError(err_msg,response=response,body=body)\n  \n  \nclass GroqWithRawResponse:\n def __init__(self,client:Groq)->None:\n  self.chat=chat.ChatWithRawResponse(client.chat)\n  self.embeddings=embeddings.EmbeddingsWithRawResponse(client.embeddings)\n  self.audio=audio.AudioWithRawResponse(client.audio)\n  self.models=models.ModelsWithRawResponse(client.models)\n  \n  \nclass AsyncGroqWithRawResponse:\n def __init__(self,client:AsyncGroq)->None:\n  self.chat=chat.AsyncChatWithRawResponse(client.chat)\n  self.embeddings=embeddings.AsyncEmbeddingsWithRawResponse(client.embeddings)\n  self.audio=audio.AsyncAudioWithRawResponse(client.audio)\n  self.models=models.AsyncModelsWithRawResponse(client.models)\n  \n  \nclass GroqWithStreamedResponse:\n def __init__(self,client:Groq)->None:\n  self.chat=chat.ChatWithStreamingResponse(client.chat)\n  self.embeddings=embeddings.EmbeddingsWithStreamingResponse(client.embeddings)\n  self.audio=audio.AudioWithStreamingResponse(client.audio)\n  self.models=models.ModelsWithStreamingResponse(client.models)\n  \n  \nclass AsyncGroqWithStreamedResponse:\n def __init__(self,client:AsyncGroq)->None:\n  self.chat=chat.AsyncChatWithStreamingResponse(client.chat)\n  self.embeddings=embeddings.AsyncEmbeddingsWithStreamingResponse(client.embeddings)\n  self.audio=audio.AsyncAudioWithStreamingResponse(client.audio)\n  self.models=models.AsyncModelsWithStreamingResponse(client.models)\n  \n  \nClient=Groq\n\nAsyncClient=AsyncGroq\n", ["__future__", "groq", "groq._base_client", "groq._exceptions", "groq._qs", "groq._streaming", "groq._types", "groq._utils", "groq._version", "groq.resources", "groq.resources.audio", "groq.resources.audio.audio", "groq.resources.chat", "groq.resources.chat.chat", "groq.resources.embeddings", "groq.resources.models", "httpx", "os", "typing", "typing_extensions"]], "groq._utils": [".py", "from._sync import asyncify as asyncify\nfrom._proxy import LazyProxy as LazyProxy\nfrom._utils import(\nflatten as flatten,\nis_dict as is_dict,\nis_list as is_list,\nis_given as is_given,\nis_tuple as is_tuple,\njson_safe as json_safe,\nlru_cache as lru_cache,\nis_mapping as is_mapping,\nis_tuple_t as is_tuple_t,\nparse_date as parse_date,\nis_iterable as is_iterable,\nis_sequence as is_sequence,\ncoerce_float as coerce_float,\nis_mapping_t as is_mapping_t,\nremoveprefix as removeprefix,\nremovesuffix as removesuffix,\nextract_files as extract_files,\nis_sequence_t as is_sequence_t,\nrequired_args as required_args,\ncoerce_boolean as coerce_boolean,\ncoerce_integer as coerce_integer,\nfile_from_path as file_from_path,\nparse_datetime as parse_datetime,\nstrip_not_given as strip_not_given,\ndeepcopy_minimal as deepcopy_minimal,\nget_async_library as get_async_library,\nmaybe_coerce_float as maybe_coerce_float,\nget_required_header as get_required_header,\nmaybe_coerce_boolean as maybe_coerce_boolean,\nmaybe_coerce_integer as maybe_coerce_integer,\n)\nfrom._typing import(\nis_list_type as is_list_type,\nis_union_type as is_union_type,\nextract_type_arg as extract_type_arg,\nis_iterable_type as is_iterable_type,\nis_required_type as is_required_type,\nis_annotated_type as is_annotated_type,\nis_type_alias_type as is_type_alias_type,\nstrip_annotated_type as strip_annotated_type,\nextract_type_var_from_base as extract_type_var_from_base,\n)\nfrom._streams import consume_sync_iterator as consume_sync_iterator,consume_async_iterator as consume_async_iterator\nfrom._transform import(\nPropertyInfo as PropertyInfo,\ntransform as transform,\nasync_transform as async_transform,\nmaybe_transform as maybe_transform,\nasync_maybe_transform as async_maybe_transform,\n)\nfrom._reflection import(\nfunction_has_argument as function_has_argument,\nassert_signatures_in_sync as assert_signatures_in_sync,\n)\n", ["groq._utils._proxy", "groq._utils._reflection", "groq._utils._streams", "groq._utils._sync", "groq._utils._transform", "groq._utils._typing", "groq._utils._utils"], 1], "groq._utils._utils": [".py", "from __future__ import annotations\n\nimport os\nimport re\nimport inspect\nimport functools\nfrom typing import(\nAny,\nTuple,\nMapping,\nTypeVar,\nCallable,\nIterable,\nSequence,\ncast,\noverload,\n)\nfrom pathlib import Path\nfrom datetime import date,datetime\nfrom typing_extensions import TypeGuard\n\nimport sniffio\n\nfrom.._types import NotGiven,FileTypes,NotGivenOr,HeadersLike\nfrom.._compat import parse_date as parse_date,parse_datetime as parse_datetime\n\n_T=TypeVar(\"_T\")\n_TupleT=TypeVar(\"_TupleT\",bound=Tuple[object,...])\n_MappingT=TypeVar(\"_MappingT\",bound=Mapping[str,object])\n_SequenceT=TypeVar(\"_SequenceT\",bound=Sequence[object])\nCallableT=TypeVar(\"CallableT\",bound=Callable[...,Any])\n\n\ndef flatten(t:Iterable[Iterable[_T]])->list[_T]:\n return[item for sublist in t for item in sublist]\n \n \ndef extract_files(\n\n\nquery:Mapping[str,object],\n*,\npaths:Sequence[Sequence[str]],\n)->list[tuple[str,FileTypes]]:\n ''\n\n\n\n\n \n files:list[tuple[str,FileTypes]]=[]\n for path in paths:\n  files.extend(_extract_items(query,path,index=0,flattened_key=None))\n return files\n \n \ndef _extract_items(\nobj:object,\npath:Sequence[str],\n*,\nindex:int,\nflattened_key:str |None,\n)->list[tuple[str,FileTypes]]:\n try:\n  key=path[index]\n except IndexError:\n  if isinstance(obj,NotGiven):\n  \n   return[]\n   \n   \n  from.._files import assert_is_file_content\n  \n  \n  assert_is_file_content(obj,key=flattened_key)\n  assert flattened_key is not None\n  return[(flattened_key,cast(FileTypes,obj))]\n  \n index +=1\n if is_dict(obj):\n  try:\n  \n   if(len(path))==index:\n    item=obj.pop(key)\n   else:\n    item=obj[key]\n  except KeyError:\n  \n  \n  \n   return[]\n  if flattened_key is None:\n   flattened_key=key\n  else:\n   flattened_key +=f\"[{key}]\"\n  return _extract_items(\n  item,\n  path,\n  index=index,\n  flattened_key=flattened_key,\n  )\n elif is_list(obj):\n  if key !=\"<array>\":\n   return[]\n   \n  return flatten(\n  [\n  _extract_items(\n  item,\n  path,\n  index=index,\n  flattened_key=flattened_key+\"[]\"if flattened_key is not None else \"[]\",\n  )\n  for item in obj\n  ]\n  )\n  \n  \n return[]\n \n \ndef is_given(obj:NotGivenOr[_T])->TypeGuard[_T]:\n return not isinstance(obj,NotGiven)\n \n \n \n \n \n \n \n \n \n \n \n \ndef is_tuple(obj:object)->TypeGuard[tuple[object,...]]:\n return isinstance(obj,tuple)\n \n \ndef is_tuple_t(obj:_TupleT |object)->TypeGuard[_TupleT]:\n return isinstance(obj,tuple)\n \n \ndef is_sequence(obj:object)->TypeGuard[Sequence[object]]:\n return isinstance(obj,Sequence)\n \n \ndef is_sequence_t(obj:_SequenceT |object)->TypeGuard[_SequenceT]:\n return isinstance(obj,Sequence)\n \n \ndef is_mapping(obj:object)->TypeGuard[Mapping[str,object]]:\n return isinstance(obj,Mapping)\n \n \ndef is_mapping_t(obj:_MappingT |object)->TypeGuard[_MappingT]:\n return isinstance(obj,Mapping)\n \n \ndef is_dict(obj:object)->TypeGuard[dict[object,object]]:\n return isinstance(obj,dict)\n \n \ndef is_list(obj:object)->TypeGuard[list[object]]:\n return isinstance(obj,list)\n \n \ndef is_iterable(obj:object)->TypeGuard[Iterable[object]]:\n return isinstance(obj,Iterable)\n \n \ndef deepcopy_minimal(item:_T)->_T:\n ''\n\n\n\n\n\n \n if is_mapping(item):\n  return cast(_T,{k:deepcopy_minimal(v)for k,v in item.items()})\n if is_list(item):\n  return cast(_T,[deepcopy_minimal(entry)for entry in item])\n return item\n \n \n \ndef human_join(seq:Sequence[str],*,delim:str=\", \",final:str=\"or\")->str:\n size=len(seq)\n if size ==0:\n  return \"\"\n  \n if size ==1:\n  return seq[0]\n  \n if size ==2:\n  return f\"{seq[0]} {final} {seq[1]}\"\n  \n return delim.join(seq[:-1])+f\" {final} {seq[-1]}\"\n \n \ndef quote(string:str)->str:\n ''\n return f\"'{string}'\"\n \n \ndef required_args(*variants:Sequence[str])->Callable[[CallableT],CallableT]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def inner(func:CallableT)->CallableT:\n  params=inspect.signature(func).parameters\n  positional=[\n  name\n  for name,param in params.items()\n  if param.kind\n  in{\n  param.POSITIONAL_ONLY,\n  param.POSITIONAL_OR_KEYWORD,\n  }\n  ]\n  \n  @functools.wraps(func)\n  def wrapper(*args:object,**kwargs:object)->object:\n   given_params:set[str]=set()\n   for i,_ in enumerate(args):\n    try:\n     given_params.add(positional[i])\n    except IndexError:\n     raise TypeError(\n     f\"{func.__name__}() takes {len(positional)} argument(s) but {len(args)} were given\"\n     )from None\n     \n   for key in kwargs.keys():\n    given_params.add(key)\n    \n   for variant in variants:\n    matches=all((param in given_params for param in variant))\n    if matches:\n     break\n   else:\n    if len(variants)>1:\n     variations=human_join(\n     [\"(\"+human_join([quote(arg)for arg in variant],final=\"and\")+\")\"for variant in variants]\n     )\n     msg=f\"Missing required arguments; Expected either {variations} arguments to be given\"\n    else:\n     assert len(variants)>0\n     \n     \n     missing=list(set(variants[0])-given_params)\n     if len(missing)>1:\n      msg=f\"Missing required arguments: {human_join([quote(arg)for arg in missing])}\"\n     else:\n      msg=f\"Missing required argument: {quote(missing[0])}\"\n    raise TypeError(msg)\n   return func(*args,**kwargs)\n   \n  return wrapper\n  \n return inner\n \n \n_K=TypeVar(\"_K\")\n_V=TypeVar(\"_V\")\n\n\n@overload\ndef strip_not_given(obj:None)->None:...\n\n\n@overload\ndef strip_not_given(obj:Mapping[_K,_V |NotGiven])->dict[_K,_V]:...\n\n\n@overload\ndef strip_not_given(obj:object)->object:...\n\n\ndef strip_not_given(obj:object |None)->object:\n ''\n if obj is None:\n  return None\n  \n if not is_mapping(obj):\n  return obj\n  \n return{key:value for key,value in obj.items()if not isinstance(value,NotGiven)}\n \n \ndef coerce_integer(val:str)->int:\n return int(val,base=10)\n \n \ndef coerce_float(val:str)->float:\n return float(val)\n \n \ndef coerce_boolean(val:str)->bool:\n return val ==\"true\"or val ==\"1\"or val ==\"on\"\n \n \ndef maybe_coerce_integer(val:str |None)->int |None:\n if val is None:\n  return None\n return coerce_integer(val)\n \n \ndef maybe_coerce_float(val:str |None)->float |None:\n if val is None:\n  return None\n return coerce_float(val)\n \n \ndef maybe_coerce_boolean(val:str |None)->bool |None:\n if val is None:\n  return None\n return coerce_boolean(val)\n \n \ndef removeprefix(string:str,prefix:str)->str:\n ''\n\n\n \n if string.startswith(prefix):\n  return string[len(prefix):]\n return string\n \n \ndef removesuffix(string:str,suffix:str)->str:\n ''\n\n\n \n if string.endswith(suffix):\n  return string[:-len(suffix)]\n return string\n \n \ndef file_from_path(path:str)->FileTypes:\n contents=Path(path).read_bytes()\n file_name=os.path.basename(path)\n return(file_name,contents)\n \n \ndef get_required_header(headers:HeadersLike,header:str)->str:\n lower_header=header.lower()\n if is_mapping_t(headers):\n \n  for k,v in headers.items():\n   if k.lower()==lower_header and isinstance(v,str):\n    return v\n    \n    \n intercaps_header=re.sub(r\"([^\\w])(\\w)\",lambda pat:pat.group(1)+pat.group(2).upper(),header.capitalize())\n \n for normalized_header in[header,lower_header,header.upper(),intercaps_header]:\n  value=headers.get(normalized_header)\n  if value:\n   return value\n   \n raise ValueError(f\"Could not find {header} header\")\n \n \ndef get_async_library()->str:\n try:\n  return sniffio.current_async_library()\n except Exception:\n  return \"false\"\n  \n  \ndef lru_cache(*,maxsize:int |None=128)->Callable[[CallableT],CallableT]:\n ''\n\n \n wrapper=functools.lru_cache(\n maxsize=maxsize,\n )\n return cast(Any,wrapper)\n \n \ndef json_safe(data:object)->object:\n ''\n\n \n if is_mapping(data):\n  return{json_safe(key):json_safe(value)for key,value in data.items()}\n  \n if is_iterable(data)and not isinstance(data,(str,bytes,bytearray)):\n  return[json_safe(item)for item in data]\n  \n if isinstance(data,(datetime,date)):\n  return data.isoformat()\n  \n return data\n", ["__future__", "datetime", "functools", "groq._compat", "groq._files", "groq._types", "inspect", "os", "pathlib", "re", "sniffio", "typing", "typing_extensions"]], "groq._utils._streams": [".py", "from typing import Any\nfrom typing_extensions import Iterator,AsyncIterator\n\n\ndef consume_sync_iterator(iterator:Iterator[Any])->None:\n for _ in iterator:\n  ...\n  \n  \nasync def consume_async_iterator(iterator:AsyncIterator[Any])->None:\n async for _ in iterator:\n  ...\n", ["typing", "typing_extensions"]], "groq._utils._reflection": [".py", "from __future__ import annotations\n\nimport inspect\nfrom typing import Any,Callable\n\n\ndef function_has_argument(func:Callable[...,Any],arg_name:str)->bool:\n ''\n sig=inspect.signature(func)\n return arg_name in sig.parameters\n \n \ndef assert_signatures_in_sync(\nsource_func:Callable[...,Any],\ncheck_func:Callable[...,Any],\n*,\nexclude_params:set[str]=set(),\n)->None:\n ''\n \n check_sig=inspect.signature(check_func)\n source_sig=inspect.signature(source_func)\n \n errors:list[str]=[]\n \n for name,source_param in source_sig.parameters.items():\n  if name in exclude_params:\n   continue\n   \n  custom_param=check_sig.parameters.get(name)\n  if not custom_param:\n   errors.append(f\"the `{name}` param is missing\")\n   continue\n   \n  if custom_param.annotation !=source_param.annotation:\n   errors.append(\n   f\"types for the `{name}` param are do not match; source={repr(source_param.annotation)} checking={repr(custom_param.annotation)}\"\n   )\n   continue\n   \n if errors:\n  raise AssertionError(f\"{len(errors)} errors encountered when comparing signatures:\\n\\n\"+\"\\n\\n\".join(errors))\n", ["__future__", "inspect", "typing"]], "groq._utils._logs": [".py", "import os\nimport logging\n\nlogger:logging.Logger=logging.getLogger(\"groq\")\nhttpx_logger:logging.Logger=logging.getLogger(\"httpx\")\n\n\ndef _basic_config()->None:\n\n logging.basicConfig(\n format=\"[%(asctime)s - %(name)s:%(lineno)d - %(levelname)s] %(message)s\",\n datefmt=\"%Y-%m-%d %H:%M:%S\",\n )\n \n \ndef setup_logging()->None:\n env=os.environ.get(\"GROQ_LOG\")\n if env ==\"debug\":\n  _basic_config()\n  logger.setLevel(logging.DEBUG)\n  httpx_logger.setLevel(logging.DEBUG)\n elif env ==\"info\":\n  _basic_config()\n  logger.setLevel(logging.INFO)\n  httpx_logger.setLevel(logging.INFO)\n", ["logging", "os"]], "groq._utils._transform": [".py", "from __future__ import annotations\n\nimport io\nimport base64\nimport pathlib\nfrom typing import Any,Mapping,TypeVar,cast\nfrom datetime import date,datetime\nfrom typing_extensions import Literal,get_args,override,get_type_hints\n\nimport anyio\nimport pydantic\n\nfrom._utils import(\nis_list,\nis_mapping,\nis_iterable,\n)\nfrom.._files import is_base64_file_input\nfrom._typing import(\nis_list_type,\nis_union_type,\nextract_type_arg,\nis_iterable_type,\nis_required_type,\nis_annotated_type,\nstrip_annotated_type,\n)\nfrom.._compat import model_dump,is_typeddict\n\n_T=TypeVar(\"_T\")\n\n\n\n\n\n\nPropertyFormat=Literal[\"iso8601\",\"base64\",\"custom\"]\n\n\nclass PropertyInfo:\n ''\n\n\n\n\n\n\n\n \n \n alias:str |None\n format:PropertyFormat |None\n format_template:str |None\n discriminator:str |None\n \n def __init__(\n self,\n *,\n alias:str |None=None,\n format:PropertyFormat |None=None,\n format_template:str |None=None,\n discriminator:str |None=None,\n )->None:\n  self.alias=alias\n  self.format=format\n  self.format_template=format_template\n  self.discriminator=discriminator\n  \n @override\n def __repr__(self)->str:\n  return f\"{self.__class__.__name__}(alias='{self.alias}', format={self.format}, format_template='{self.format_template}', discriminator='{self.discriminator}')\"\n  \n  \ndef maybe_transform(\ndata:object,\nexpected_type:object,\n)->Any |None:\n ''\n\n\n \n if data is None:\n  return None\n return transform(data,expected_type)\n \n \n \ndef transform(\ndata:_T,\nexpected_type:object,\n)->_T:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n transformed=_transform_recursive(data,annotation=cast(type,expected_type))\n return cast(_T,transformed)\n \n \ndef _get_annotated_type(type_:type)->type |None:\n ''\n\n\n \n if is_required_type(type_):\n \n  type_=get_args(type_)[0]\n  \n if is_annotated_type(type_):\n  return type_\n  \n return None\n \n \ndef _maybe_transform_key(key:str,type_:type)->str:\n ''\n\n\n \n annotated_type=_get_annotated_type(type_)\n if annotated_type is None:\n \n  return key\n  \n  \n annotations=get_args(annotated_type)[1:]\n for annotation in annotations:\n  if isinstance(annotation,PropertyInfo)and annotation.alias is not None:\n   return annotation.alias\n   \n return key\n \n \ndef _transform_recursive(\ndata:object,\n*,\nannotation:type,\ninner_type:type |None=None,\n)->object:\n ''\n\n\n\n\n\n\n\n\n\n\n \n if inner_type is None:\n  inner_type=annotation\n  \n stripped_type=strip_annotated_type(inner_type)\n if is_typeddict(stripped_type)and is_mapping(data):\n  return _transform_typeddict(data,stripped_type)\n  \n if(\n \n (is_list_type(stripped_type)and is_list(data))\n \n or(is_iterable_type(stripped_type)and is_iterable(data)and not isinstance(data,str))\n ):\n \n \n  if isinstance(data,dict):\n   return cast(object,data)\n   \n  inner_type=extract_type_arg(stripped_type,0)\n  return[_transform_recursive(d,annotation=annotation,inner_type=inner_type)for d in data]\n  \n if is_union_type(stripped_type):\n \n \n \n \n  for subtype in get_args(stripped_type):\n   data=_transform_recursive(data,annotation=annotation,inner_type=subtype)\n  return data\n  \n if isinstance(data,pydantic.BaseModel):\n  return model_dump(data,exclude_unset=True,mode=\"json\")\n  \n annotated_type=_get_annotated_type(annotation)\n if annotated_type is None:\n  return data\n  \n  \n annotations=get_args(annotated_type)[1:]\n for annotation in annotations:\n  if isinstance(annotation,PropertyInfo)and annotation.format is not None:\n   return _format_data(data,annotation.format,annotation.format_template)\n   \n return data\n \n \ndef _format_data(data:object,format_:PropertyFormat,format_template:str |None)->object:\n if isinstance(data,(date,datetime)):\n  if format_ ==\"iso8601\":\n   return data.isoformat()\n   \n  if format_ ==\"custom\"and format_template is not None:\n   return data.strftime(format_template)\n   \n if format_ ==\"base64\"and is_base64_file_input(data):\n  binary:str |bytes |None=None\n  \n  if isinstance(data,pathlib.Path):\n   binary=data.read_bytes()\n  elif isinstance(data,io.IOBase):\n   binary=data.read()\n   \n   if isinstance(binary,str):\n    binary=binary.encode()\n    \n  if not isinstance(binary,bytes):\n   raise RuntimeError(f\"Could not read bytes from {data}; Received {type(binary)}\")\n   \n  return base64.b64encode(binary).decode(\"ascii\")\n  \n return data\n \n \ndef _transform_typeddict(\ndata:Mapping[str,object],\nexpected_type:type,\n)->Mapping[str,object]:\n result:dict[str,object]={}\n annotations=get_type_hints(expected_type,include_extras=True)\n for key,value in data.items():\n  type_=annotations.get(key)\n  if type_ is None:\n  \n   result[key]=value\n  else:\n   result[_maybe_transform_key(key,type_)]=_transform_recursive(value,annotation=type_)\n return result\n \n \nasync def async_maybe_transform(\ndata:object,\nexpected_type:object,\n)->Any |None:\n ''\n\n\n \n if data is None:\n  return None\n return await async_transform(data,expected_type)\n \n \nasync def async_transform(\ndata:_T,\nexpected_type:object,\n)->_T:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n transformed=await _async_transform_recursive(data,annotation=cast(type,expected_type))\n return cast(_T,transformed)\n \n \nasync def _async_transform_recursive(\ndata:object,\n*,\nannotation:type,\ninner_type:type |None=None,\n)->object:\n ''\n\n\n\n\n\n\n\n\n\n\n \n if inner_type is None:\n  inner_type=annotation\n  \n stripped_type=strip_annotated_type(inner_type)\n if is_typeddict(stripped_type)and is_mapping(data):\n  return await _async_transform_typeddict(data,stripped_type)\n  \n if(\n \n (is_list_type(stripped_type)and is_list(data))\n \n or(is_iterable_type(stripped_type)and is_iterable(data)and not isinstance(data,str))\n ):\n \n \n  if isinstance(data,dict):\n   return cast(object,data)\n   \n  inner_type=extract_type_arg(stripped_type,0)\n  return[await _async_transform_recursive(d,annotation=annotation,inner_type=inner_type)for d in data]\n  \n if is_union_type(stripped_type):\n \n \n \n \n  for subtype in get_args(stripped_type):\n   data=await _async_transform_recursive(data,annotation=annotation,inner_type=subtype)\n  return data\n  \n if isinstance(data,pydantic.BaseModel):\n  return model_dump(data,exclude_unset=True,mode=\"json\")\n  \n annotated_type=_get_annotated_type(annotation)\n if annotated_type is None:\n  return data\n  \n  \n annotations=get_args(annotated_type)[1:]\n for annotation in annotations:\n  if isinstance(annotation,PropertyInfo)and annotation.format is not None:\n   return await _async_format_data(data,annotation.format,annotation.format_template)\n   \n return data\n \n \nasync def _async_format_data(data:object,format_:PropertyFormat,format_template:str |None)->object:\n if isinstance(data,(date,datetime)):\n  if format_ ==\"iso8601\":\n   return data.isoformat()\n   \n  if format_ ==\"custom\"and format_template is not None:\n   return data.strftime(format_template)\n   \n if format_ ==\"base64\"and is_base64_file_input(data):\n  binary:str |bytes |None=None\n  \n  if isinstance(data,pathlib.Path):\n   binary=await anyio.Path(data).read_bytes()\n  elif isinstance(data,io.IOBase):\n   binary=data.read()\n   \n   if isinstance(binary,str):\n    binary=binary.encode()\n    \n  if not isinstance(binary,bytes):\n   raise RuntimeError(f\"Could not read bytes from {data}; Received {type(binary)}\")\n   \n  return base64.b64encode(binary).decode(\"ascii\")\n  \n return data\n \n \nasync def _async_transform_typeddict(\ndata:Mapping[str,object],\nexpected_type:type,\n)->Mapping[str,object]:\n result:dict[str,object]={}\n annotations=get_type_hints(expected_type,include_extras=True)\n for key,value in data.items():\n  type_=annotations.get(key)\n  if type_ is None:\n  \n   result[key]=value\n  else:\n   result[_maybe_transform_key(key,type_)]=await _async_transform_recursive(value,annotation=type_)\n return result\n", ["__future__", "anyio", "base64", "datetime", "groq._compat", "groq._files", "groq._utils._typing", "groq._utils._utils", "io", "pathlib", "pydantic", "typing", "typing_extensions"]], "groq._utils._sync": [".py", "from __future__ import annotations\n\nimport sys\nimport asyncio\nimport functools\nimport contextvars\nfrom typing import Any,TypeVar,Callable,Awaitable\nfrom typing_extensions import ParamSpec\n\nT_Retval=TypeVar(\"T_Retval\")\nT_ParamSpec=ParamSpec(\"T_ParamSpec\")\n\n\nif sys.version_info >=(3,9):\n to_thread=asyncio.to_thread\nelse:\n\n\n async def to_thread(\n func:Callable[T_ParamSpec,T_Retval],/,*args:T_ParamSpec.args,**kwargs:T_ParamSpec.kwargs\n )->Any:\n  ''\n\n\n\n\n\n\n\n  \n  loop=asyncio.events.get_running_loop()\n  ctx=contextvars.copy_context()\n  func_call=functools.partial(ctx.run,func,*args,**kwargs)\n  return await loop.run_in_executor(None,func_call)\n  \n  \n  \ndef asyncify(function:Callable[T_ParamSpec,T_Retval])->Callable[T_ParamSpec,Awaitable[T_Retval]]:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n async def wrapper(*args:T_ParamSpec.args,**kwargs:T_ParamSpec.kwargs)->T_Retval:\n  return await to_thread(function,*args,**kwargs)\n  \n return wrapper\n", ["__future__", "asyncio", "contextvars", "functools", "sys", "typing", "typing_extensions"]], "groq._utils._typing": [".py", "from __future__ import annotations\n\nimport sys\nimport typing\nimport typing_extensions\nfrom typing import Any,TypeVar,Iterable,cast\nfrom collections import abc as _c_abc\nfrom typing_extensions import(\nTypeIs,\nRequired,\nAnnotated,\nget_args,\nget_origin,\n)\n\nfrom.._types import InheritsGeneric\nfrom.._compat import is_union as _is_union\n\n\ndef is_annotated_type(typ:type)->bool:\n return get_origin(typ)==Annotated\n \n \ndef is_list_type(typ:type)->bool:\n return(get_origin(typ)or typ)==list\n \n \ndef is_iterable_type(typ:type)->bool:\n ''\n origin=get_origin(typ)or typ\n return origin ==Iterable or origin ==_c_abc.Iterable\n \n \ndef is_union_type(typ:type)->bool:\n return _is_union(get_origin(typ))\n \n \ndef is_required_type(typ:type)->bool:\n return get_origin(typ)==Required\n \n \ndef is_typevar(typ:type)->bool:\n\n\n return type(typ)==TypeVar\n \n \n_TYPE_ALIAS_TYPES:tuple[type[typing_extensions.TypeAliasType],...]=(typing_extensions.TypeAliasType,)\nif sys.version_info >=(3,12):\n _TYPE_ALIAS_TYPES=(*_TYPE_ALIAS_TYPES,typing.TypeAliasType)\n \n \ndef is_type_alias_type(tp:Any,/)->TypeIs[typing_extensions.TypeAliasType]:\n ''\n\n\n\n\n\n\n\n\n\n \n return isinstance(tp,_TYPE_ALIAS_TYPES)\n \n \n \ndef strip_annotated_type(typ:type)->type:\n if is_required_type(typ)or is_annotated_type(typ):\n  return strip_annotated_type(cast(type,get_args(typ)[0]))\n  \n return typ\n \n \ndef extract_type_arg(typ:type,index:int)->type:\n args=get_args(typ)\n try:\n  return cast(type,args[index])\n except IndexError as err:\n  raise RuntimeError(f\"Expected type {typ} to have a type argument at index {index} but it did not\")from err\n  \n  \ndef extract_type_var_from_base(\ntyp:type,\n*,\ngeneric_bases:tuple[type,...],\nindex:int,\nfailure_message:str |None=None,\n)->type:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n cls=cast(object,get_origin(typ)or typ)\n if cls in generic_bases:\n \n  return extract_type_arg(typ,index)\n  \n  \n  \n  \n  \n  \n  \n if isinstance(cls,InheritsGeneric):\n  target_base_class:Any |None=None\n  for base in cls.__orig_bases__:\n   if base.__origin__ in generic_bases:\n    target_base_class=base\n    break\n    \n  if target_base_class is None:\n   raise RuntimeError(\n   \"Could not find the generic base class;\\n\"\n   \"This should never happen;\\n\"\n   f\"Does {cls} inherit from one of {generic_bases} ?\"\n   )\n   \n  extracted=extract_type_arg(target_base_class,index)\n  if is_typevar(extracted):\n  \n  \n  \n  \n  \n  \n  \n  \n   return extract_type_arg(typ,index)\n   \n  return extracted\n  \n raise RuntimeError(failure_message or f\"Could not resolve inner type variable at index {index} for {typ}\")\n", ["__future__", "collections", "groq._compat", "groq._types", "sys", "typing", "typing_extensions"]], "groq._utils._proxy": [".py", "from __future__ import annotations\n\nfrom abc import ABC,abstractmethod\nfrom typing import Generic,TypeVar,Iterable,cast\nfrom typing_extensions import override\n\nT=TypeVar(\"T\")\n\n\nclass LazyProxy(Generic[T],ABC):\n ''\n\n\n \n \n \n \n \n def __getattr__(self,attr:str)->object:\n  proxied=self.__get_proxied__()\n  if isinstance(proxied,LazyProxy):\n   return proxied\n  return getattr(proxied,attr)\n  \n @override\n def __repr__(self)->str:\n  proxied=self.__get_proxied__()\n  if isinstance(proxied,LazyProxy):\n   return proxied.__class__.__name__\n  return repr(self.__get_proxied__())\n  \n @override\n def __str__(self)->str:\n  proxied=self.__get_proxied__()\n  if isinstance(proxied,LazyProxy):\n   return proxied.__class__.__name__\n  return str(proxied)\n  \n @override\n def __dir__(self)->Iterable[str]:\n  proxied=self.__get_proxied__()\n  if isinstance(proxied,LazyProxy):\n   return[]\n  return proxied.__dir__()\n  \n @property\n @override\n def __class__(self)->type:\n  proxied=self.__get_proxied__()\n  if issubclass(type(proxied),LazyProxy):\n   return type(proxied)\n  return proxied.__class__\n  \n def __get_proxied__(self)->T:\n  return self.__load__()\n  \n def __as_proxied__(self)->T:\n  ''\n  return cast(T,self)\n  \n @abstractmethod\n def __load__(self)->T:...\n", ["__future__", "abc", "typing", "typing_extensions"]], "groq.resources": [".py", "\n\nfrom.chat import(\nChat,\nAsyncChat,\nChatWithRawResponse,\nAsyncChatWithRawResponse,\nChatWithStreamingResponse,\nAsyncChatWithStreamingResponse,\n)\nfrom.audio import(\nAudio,\nAsyncAudio,\nAudioWithRawResponse,\nAsyncAudioWithRawResponse,\nAudioWithStreamingResponse,\nAsyncAudioWithStreamingResponse,\n)\nfrom.models import(\nModels,\nAsyncModels,\nModelsWithRawResponse,\nAsyncModelsWithRawResponse,\nModelsWithStreamingResponse,\nAsyncModelsWithStreamingResponse,\n)\nfrom.embeddings import(\nEmbeddings,\nAsyncEmbeddings,\nEmbeddingsWithRawResponse,\nAsyncEmbeddingsWithRawResponse,\nEmbeddingsWithStreamingResponse,\nAsyncEmbeddingsWithStreamingResponse,\n)\n\n__all__=[\n\"Chat\",\n\"AsyncChat\",\n\"ChatWithRawResponse\",\n\"AsyncChatWithRawResponse\",\n\"ChatWithStreamingResponse\",\n\"AsyncChatWithStreamingResponse\",\n\"Embeddings\",\n\"AsyncEmbeddings\",\n\"EmbeddingsWithRawResponse\",\n\"AsyncEmbeddingsWithRawResponse\",\n\"EmbeddingsWithStreamingResponse\",\n\"AsyncEmbeddingsWithStreamingResponse\",\n\"Audio\",\n\"AsyncAudio\",\n\"AudioWithRawResponse\",\n\"AsyncAudioWithRawResponse\",\n\"AudioWithStreamingResponse\",\n\"AsyncAudioWithStreamingResponse\",\n\"Models\",\n\"AsyncModels\",\n\"ModelsWithRawResponse\",\n\"AsyncModelsWithRawResponse\",\n\"ModelsWithStreamingResponse\",\n\"AsyncModelsWithStreamingResponse\",\n]\n", ["groq.resources.audio", "groq.resources.chat", "groq.resources.embeddings", "groq.resources.models"], 1], "groq.resources.models": [".py", "\n\nfrom __future__ import annotations\n\nimport httpx\n\nfrom.._types import NOT_GIVEN,Body,Query,Headers,NotGiven\nfrom.._compat import cached_property\nfrom.._resource import SyncAPIResource,AsyncAPIResource\nfrom.._response import(\nto_raw_response_wrapper,\nto_streamed_response_wrapper,\nasync_to_raw_response_wrapper,\nasync_to_streamed_response_wrapper,\n)\nfrom..types.model import Model\nfrom.._base_client import make_request_options\nfrom..types.model_deleted import ModelDeleted\nfrom..types.model_list_response import ModelListResponse\n\n__all__=[\"Models\",\"AsyncModels\"]\n\n\nclass Models(SyncAPIResource):\n @cached_property\n def with_raw_response(self)->ModelsWithRawResponse:\n  ''\n\n\n\n\n  \n  return ModelsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->ModelsWithStreamingResponse:\n  ''\n\n\n\n  \n  return ModelsWithStreamingResponse(self)\n  \n def retrieve(\n self,\n model:str,\n *,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Model:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if not model:\n   raise ValueError(f\"Expected a non-empty value for `model` but received {model !r}\")\n  return self._get(\n  f\"/openai/v1/models/{model}\",\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=Model,\n  )\n  \n def list(\n self,\n *,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ModelListResponse:\n  ''\n  return self._get(\n  \"/openai/v1/models\",\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=ModelListResponse,\n  )\n  \n def delete(\n self,\n model:str,\n *,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ModelDeleted:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if not model:\n   raise ValueError(f\"Expected a non-empty value for `model` but received {model !r}\")\n  return self._delete(\n  f\"/openai/v1/models/{model}\",\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=ModelDeleted,\n  )\n  \n  \nclass AsyncModels(AsyncAPIResource):\n @cached_property\n def with_raw_response(self)->AsyncModelsWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncModelsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncModelsWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncModelsWithStreamingResponse(self)\n  \n async def retrieve(\n self,\n model:str,\n *,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Model:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if not model:\n   raise ValueError(f\"Expected a non-empty value for `model` but received {model !r}\")\n  return await self._get(\n  f\"/openai/v1/models/{model}\",\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=Model,\n  )\n  \n async def list(\n self,\n *,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ModelListResponse:\n  ''\n  return await self._get(\n  \"/openai/v1/models\",\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=ModelListResponse,\n  )\n  \n async def delete(\n self,\n model:str,\n *,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ModelDeleted:\n  ''\n\n\n\n\n\n\n\n\n\n\n  \n  if not model:\n   raise ValueError(f\"Expected a non-empty value for `model` but received {model !r}\")\n  return await self._delete(\n  f\"/openai/v1/models/{model}\",\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=ModelDeleted,\n  )\n  \n  \nclass ModelsWithRawResponse:\n def __init__(self,models:Models)->None:\n  self._models=models\n  \n  self.retrieve=to_raw_response_wrapper(\n  models.retrieve,\n  )\n  self.list=to_raw_response_wrapper(\n  models.list,\n  )\n  self.delete=to_raw_response_wrapper(\n  models.delete,\n  )\n  \n  \nclass AsyncModelsWithRawResponse:\n def __init__(self,models:AsyncModels)->None:\n  self._models=models\n  \n  self.retrieve=async_to_raw_response_wrapper(\n  models.retrieve,\n  )\n  self.list=async_to_raw_response_wrapper(\n  models.list,\n  )\n  self.delete=async_to_raw_response_wrapper(\n  models.delete,\n  )\n  \n  \nclass ModelsWithStreamingResponse:\n def __init__(self,models:Models)->None:\n  self._models=models\n  \n  self.retrieve=to_streamed_response_wrapper(\n  models.retrieve,\n  )\n  self.list=to_streamed_response_wrapper(\n  models.list,\n  )\n  self.delete=to_streamed_response_wrapper(\n  models.delete,\n  )\n  \n  \nclass AsyncModelsWithStreamingResponse:\n def __init__(self,models:AsyncModels)->None:\n  self._models=models\n  \n  self.retrieve=async_to_streamed_response_wrapper(\n  models.retrieve,\n  )\n  self.list=async_to_streamed_response_wrapper(\n  models.list,\n  )\n  self.delete=async_to_streamed_response_wrapper(\n  models.delete,\n  )\n", ["__future__", "groq._base_client", "groq._compat", "groq._resource", "groq._response", "groq._types", "groq.types.model", "groq.types.model_deleted", "groq.types.model_list_response", "httpx"]], "groq.resources.embeddings": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import List,Union,Optional\nfrom typing_extensions import Literal\n\nimport httpx\n\nfrom..types import embedding_create_params\nfrom.._types import NOT_GIVEN,Body,Query,Headers,NotGiven\nfrom.._utils import(\nmaybe_transform,\nasync_maybe_transform,\n)\nfrom.._compat import cached_property\nfrom.._resource import SyncAPIResource,AsyncAPIResource\nfrom.._response import(\nto_raw_response_wrapper,\nto_streamed_response_wrapper,\nasync_to_raw_response_wrapper,\nasync_to_streamed_response_wrapper,\n)\nfrom.._base_client import make_request_options\nfrom..types.create_embedding_response import CreateEmbeddingResponse\n\n__all__=[\"Embeddings\",\"AsyncEmbeddings\"]\n\n\nclass Embeddings(SyncAPIResource):\n @cached_property\n def with_raw_response(self)->EmbeddingsWithRawResponse:\n  ''\n\n\n\n\n  \n  return EmbeddingsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->EmbeddingsWithStreamingResponse:\n  ''\n\n\n\n  \n  return EmbeddingsWithStreamingResponse(self)\n  \n def create(\n self,\n *,\n input:Union[str,List[str]],\n model:Union[str,Literal[\"nomic-embed-text-v1_5\"]],\n encoding_format:Literal[\"float\",\"base64\"]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->CreateEmbeddingResponse:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self._post(\n  \"/openai/v1/embeddings\",\n  body=maybe_transform(\n  {\n  \"input\":input,\n  \"model\":model,\n  \"encoding_format\":encoding_format,\n  \"user\":user,\n  },\n  embedding_create_params.EmbeddingCreateParams,\n  ),\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=CreateEmbeddingResponse,\n  )\n  \n  \nclass AsyncEmbeddings(AsyncAPIResource):\n @cached_property\n def with_raw_response(self)->AsyncEmbeddingsWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncEmbeddingsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncEmbeddingsWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncEmbeddingsWithStreamingResponse(self)\n  \n async def create(\n self,\n *,\n input:Union[str,List[str]],\n model:Union[str,Literal[\"nomic-embed-text-v1_5\"]],\n encoding_format:Literal[\"float\",\"base64\"]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->CreateEmbeddingResponse:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return await self._post(\n  \"/openai/v1/embeddings\",\n  body=await async_maybe_transform(\n  {\n  \"input\":input,\n  \"model\":model,\n  \"encoding_format\":encoding_format,\n  \"user\":user,\n  },\n  embedding_create_params.EmbeddingCreateParams,\n  ),\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=CreateEmbeddingResponse,\n  )\n  \n  \nclass EmbeddingsWithRawResponse:\n def __init__(self,embeddings:Embeddings)->None:\n  self._embeddings=embeddings\n  \n  self.create=to_raw_response_wrapper(\n  embeddings.create,\n  )\n  \n  \nclass AsyncEmbeddingsWithRawResponse:\n def __init__(self,embeddings:AsyncEmbeddings)->None:\n  self._embeddings=embeddings\n  \n  self.create=async_to_raw_response_wrapper(\n  embeddings.create,\n  )\n  \n  \nclass EmbeddingsWithStreamingResponse:\n def __init__(self,embeddings:Embeddings)->None:\n  self._embeddings=embeddings\n  \n  self.create=to_streamed_response_wrapper(\n  embeddings.create,\n  )\n  \n  \nclass AsyncEmbeddingsWithStreamingResponse:\n def __init__(self,embeddings:AsyncEmbeddings)->None:\n  self._embeddings=embeddings\n  \n  self.create=async_to_streamed_response_wrapper(\n  embeddings.create,\n  )\n", ["__future__", "groq._base_client", "groq._compat", "groq._resource", "groq._response", "groq._types", "groq._utils", "groq.types", "groq.types.create_embedding_response", "groq.types.embedding_create_params", "httpx", "typing", "typing_extensions"]], "groq.resources.audio": [".py", "\n\nfrom.audio import(\nAudio,\nAsyncAudio,\nAudioWithRawResponse,\nAsyncAudioWithRawResponse,\nAudioWithStreamingResponse,\nAsyncAudioWithStreamingResponse,\n)\nfrom.translations import(\nTranslations,\nAsyncTranslations,\nTranslationsWithRawResponse,\nAsyncTranslationsWithRawResponse,\nTranslationsWithStreamingResponse,\nAsyncTranslationsWithStreamingResponse,\n)\nfrom.transcriptions import(\nTranscriptions,\nAsyncTranscriptions,\nTranscriptionsWithRawResponse,\nAsyncTranscriptionsWithRawResponse,\nTranscriptionsWithStreamingResponse,\nAsyncTranscriptionsWithStreamingResponse,\n)\n\n__all__=[\n\"Transcriptions\",\n\"AsyncTranscriptions\",\n\"TranscriptionsWithRawResponse\",\n\"AsyncTranscriptionsWithRawResponse\",\n\"TranscriptionsWithStreamingResponse\",\n\"AsyncTranscriptionsWithStreamingResponse\",\n\"Translations\",\n\"AsyncTranslations\",\n\"TranslationsWithRawResponse\",\n\"AsyncTranslationsWithRawResponse\",\n\"TranslationsWithStreamingResponse\",\n\"AsyncTranslationsWithStreamingResponse\",\n\"Audio\",\n\"AsyncAudio\",\n\"AudioWithRawResponse\",\n\"AsyncAudioWithRawResponse\",\n\"AudioWithStreamingResponse\",\n\"AsyncAudioWithStreamingResponse\",\n]\n", ["groq.resources.audio.audio", "groq.resources.audio.transcriptions", "groq.resources.audio.translations"], 1], "groq.resources.audio.transcriptions": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import List,Union,Mapping,cast\nfrom typing_extensions import Literal\n\nimport httpx\n\nfrom ..._types import NOT_GIVEN,Body,Query,Headers,NotGiven,FileTypes\nfrom ..._utils import(\nextract_files,\nmaybe_transform,\ndeepcopy_minimal,\nasync_maybe_transform,\n)\nfrom ..._compat import cached_property\nfrom ..._resource import SyncAPIResource,AsyncAPIResource\nfrom ..._response import(\nto_raw_response_wrapper,\nto_streamed_response_wrapper,\nasync_to_raw_response_wrapper,\nasync_to_streamed_response_wrapper,\n)\nfrom ...types.audio import transcription_create_params\nfrom ..._base_client import make_request_options\nfrom ...types.audio.transcription import Transcription\n\n__all__=[\"Transcriptions\",\"AsyncTranscriptions\"]\n\n\nclass Transcriptions(SyncAPIResource):\n @cached_property\n def with_raw_response(self)->TranscriptionsWithRawResponse:\n  ''\n\n\n\n\n  \n  return TranscriptionsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->TranscriptionsWithStreamingResponse:\n  ''\n\n\n\n  \n  return TranscriptionsWithStreamingResponse(self)\n  \n def create(\n self,\n *,\n file:FileTypes,\n model:Union[str,Literal[\"whisper-large-v3\"]],\n language:Union[\n str,\n Literal[\n \"en\",\n \"zh\",\n \"de\",\n \"es\",\n \"ru\",\n \"ko\",\n \"fr\",\n \"ja\",\n \"pt\",\n \"tr\",\n \"pl\",\n \"ca\",\n \"nl\",\n \"ar\",\n \"sv\",\n \"it\",\n \"id\",\n \"hi\",\n \"fi\",\n \"vi\",\n \"he\",\n \"uk\",\n \"el\",\n \"ms\",\n \"cs\",\n \"ro\",\n \"da\",\n \"hu\",\n \"ta\",\n \"no\",\n \"th\",\n \"ur\",\n \"hr\",\n \"bg\",\n \"lt\",\n \"la\",\n \"mi\",\n \"ml\",\n \"cy\",\n \"sk\",\n \"te\",\n \"fa\",\n \"lv\",\n \"bn\",\n \"sr\",\n \"az\",\n \"sl\",\n \"kn\",\n \"et\",\n \"mk\",\n \"br\",\n \"eu\",\n \"is\",\n \"hy\",\n \"ne\",\n \"mn\",\n \"bs\",\n \"kk\",\n \"sq\",\n \"sw\",\n \"gl\",\n \"mr\",\n \"pa\",\n \"si\",\n \"km\",\n \"sn\",\n \"yo\",\n \"so\",\n \"af\",\n \"oc\",\n \"ka\",\n \"be\",\n \"tg\",\n \"sd\",\n \"gu\",\n \"am\",\n \"yi\",\n \"lo\",\n \"uz\",\n \"fo\",\n \"ht\",\n \"ps\",\n \"tk\",\n \"nn\",\n \"mt\",\n \"sa\",\n \"lb\",\n \"my\",\n \"bo\",\n \"tl\",\n \"mg\",\n \"as\",\n \"tt\",\n \"haw\",\n \"ln\",\n \"ha\",\n \"ba\",\n \"jv\",\n \"su\",\n \"yue\",\n ],\n ]\n |NotGiven=NOT_GIVEN,\n prompt:str |NotGiven=NOT_GIVEN,\n response_format:Literal[\"json\",\"text\",\"verbose_json\"]|NotGiven=NOT_GIVEN,\n temperature:float |NotGiven=NOT_GIVEN,\n timestamp_granularities:List[Literal[\"word\",\"segment\"]]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Transcription:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  body=deepcopy_minimal(\n  {\n  \"file\":file,\n  \"model\":model,\n  \"language\":language,\n  \"prompt\":prompt,\n  \"response_format\":response_format,\n  \"temperature\":temperature,\n  \"timestamp_granularities\":timestamp_granularities,\n  }\n  )\n  files=extract_files(cast(Mapping[str,object],body),paths=[[\"file\"]])\n  \n  \n  \n  extra_headers={\"Content-Type\":\"multipart/form-data\",**(extra_headers or{})}\n  return self._post(\n  \"/openai/v1/audio/transcriptions\",\n  body=maybe_transform(body,transcription_create_params.TranscriptionCreateParams),\n  files=files,\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=Transcription,\n  )\n  \n  \nclass AsyncTranscriptions(AsyncAPIResource):\n @cached_property\n def with_raw_response(self)->AsyncTranscriptionsWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncTranscriptionsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncTranscriptionsWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncTranscriptionsWithStreamingResponse(self)\n  \n async def create(\n self,\n *,\n file:FileTypes,\n model:Union[str,Literal[\"whisper-large-v3\"]],\n language:Union[\n str,\n Literal[\n \"en\",\n \"zh\",\n \"de\",\n \"es\",\n \"ru\",\n \"ko\",\n \"fr\",\n \"ja\",\n \"pt\",\n \"tr\",\n \"pl\",\n \"ca\",\n \"nl\",\n \"ar\",\n \"sv\",\n \"it\",\n \"id\",\n \"hi\",\n \"fi\",\n \"vi\",\n \"he\",\n \"uk\",\n \"el\",\n \"ms\",\n \"cs\",\n \"ro\",\n \"da\",\n \"hu\",\n \"ta\",\n \"no\",\n \"th\",\n \"ur\",\n \"hr\",\n \"bg\",\n \"lt\",\n \"la\",\n \"mi\",\n \"ml\",\n \"cy\",\n \"sk\",\n \"te\",\n \"fa\",\n \"lv\",\n \"bn\",\n \"sr\",\n \"az\",\n \"sl\",\n \"kn\",\n \"et\",\n \"mk\",\n \"br\",\n \"eu\",\n \"is\",\n \"hy\",\n \"ne\",\n \"mn\",\n \"bs\",\n \"kk\",\n \"sq\",\n \"sw\",\n \"gl\",\n \"mr\",\n \"pa\",\n \"si\",\n \"km\",\n \"sn\",\n \"yo\",\n \"so\",\n \"af\",\n \"oc\",\n \"ka\",\n \"be\",\n \"tg\",\n \"sd\",\n \"gu\",\n \"am\",\n \"yi\",\n \"lo\",\n \"uz\",\n \"fo\",\n \"ht\",\n \"ps\",\n \"tk\",\n \"nn\",\n \"mt\",\n \"sa\",\n \"lb\",\n \"my\",\n \"bo\",\n \"tl\",\n \"mg\",\n \"as\",\n \"tt\",\n \"haw\",\n \"ln\",\n \"ha\",\n \"ba\",\n \"jv\",\n \"su\",\n \"yue\",\n ],\n ]\n |NotGiven=NOT_GIVEN,\n prompt:str |NotGiven=NOT_GIVEN,\n response_format:Literal[\"json\",\"text\",\"verbose_json\"]|NotGiven=NOT_GIVEN,\n temperature:float |NotGiven=NOT_GIVEN,\n timestamp_granularities:List[Literal[\"word\",\"segment\"]]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Transcription:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  body=deepcopy_minimal(\n  {\n  \"file\":file,\n  \"model\":model,\n  \"language\":language,\n  \"prompt\":prompt,\n  \"response_format\":response_format,\n  \"temperature\":temperature,\n  \"timestamp_granularities\":timestamp_granularities,\n  }\n  )\n  files=extract_files(cast(Mapping[str,object],body),paths=[[\"file\"]])\n  \n  \n  \n  extra_headers={\"Content-Type\":\"multipart/form-data\",**(extra_headers or{})}\n  return await self._post(\n  \"/openai/v1/audio/transcriptions\",\n  body=await async_maybe_transform(body,transcription_create_params.TranscriptionCreateParams),\n  files=files,\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=Transcription,\n  )\n  \n  \nclass TranscriptionsWithRawResponse:\n def __init__(self,transcriptions:Transcriptions)->None:\n  self._transcriptions=transcriptions\n  \n  self.create=to_raw_response_wrapper(\n  transcriptions.create,\n  )\n  \n  \nclass AsyncTranscriptionsWithRawResponse:\n def __init__(self,transcriptions:AsyncTranscriptions)->None:\n  self._transcriptions=transcriptions\n  \n  self.create=async_to_raw_response_wrapper(\n  transcriptions.create,\n  )\n  \n  \nclass TranscriptionsWithStreamingResponse:\n def __init__(self,transcriptions:Transcriptions)->None:\n  self._transcriptions=transcriptions\n  \n  self.create=to_streamed_response_wrapper(\n  transcriptions.create,\n  )\n  \n  \nclass AsyncTranscriptionsWithStreamingResponse:\n def __init__(self,transcriptions:AsyncTranscriptions)->None:\n  self._transcriptions=transcriptions\n  \n  self.create=async_to_streamed_response_wrapper(\n  transcriptions.create,\n  )\n", ["__future__", "groq._base_client", "groq._compat", "groq._resource", "groq._response", "groq._types", "groq._utils", "groq.types.audio", "groq.types.audio.transcription", "groq.types.audio.transcription_create_params", "httpx", "typing", "typing_extensions"]], "groq.resources.audio.audio": [".py", "\n\nfrom __future__ import annotations\n\nfrom ..._compat import cached_property\nfrom ..._resource import SyncAPIResource,AsyncAPIResource\nfrom.translations import(\nTranslations,\nAsyncTranslations,\nTranslationsWithRawResponse,\nAsyncTranslationsWithRawResponse,\nTranslationsWithStreamingResponse,\nAsyncTranslationsWithStreamingResponse,\n)\nfrom.transcriptions import(\nTranscriptions,\nAsyncTranscriptions,\nTranscriptionsWithRawResponse,\nAsyncTranscriptionsWithRawResponse,\nTranscriptionsWithStreamingResponse,\nAsyncTranscriptionsWithStreamingResponse,\n)\n\n__all__=[\"Audio\",\"AsyncAudio\"]\n\n\nclass Audio(SyncAPIResource):\n @cached_property\n def transcriptions(self)->Transcriptions:\n  return Transcriptions(self._client)\n  \n @cached_property\n def translations(self)->Translations:\n  return Translations(self._client)\n  \n @cached_property\n def with_raw_response(self)->AudioWithRawResponse:\n  ''\n\n\n\n\n  \n  return AudioWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AudioWithStreamingResponse:\n  ''\n\n\n\n  \n  return AudioWithStreamingResponse(self)\n  \n  \nclass AsyncAudio(AsyncAPIResource):\n @cached_property\n def transcriptions(self)->AsyncTranscriptions:\n  return AsyncTranscriptions(self._client)\n  \n @cached_property\n def translations(self)->AsyncTranslations:\n  return AsyncTranslations(self._client)\n  \n @cached_property\n def with_raw_response(self)->AsyncAudioWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncAudioWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncAudioWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncAudioWithStreamingResponse(self)\n  \n  \nclass AudioWithRawResponse:\n def __init__(self,audio:Audio)->None:\n  self._audio=audio\n  \n @cached_property\n def transcriptions(self)->TranscriptionsWithRawResponse:\n  return TranscriptionsWithRawResponse(self._audio.transcriptions)\n  \n @cached_property\n def translations(self)->TranslationsWithRawResponse:\n  return TranslationsWithRawResponse(self._audio.translations)\n  \n  \nclass AsyncAudioWithRawResponse:\n def __init__(self,audio:AsyncAudio)->None:\n  self._audio=audio\n  \n @cached_property\n def transcriptions(self)->AsyncTranscriptionsWithRawResponse:\n  return AsyncTranscriptionsWithRawResponse(self._audio.transcriptions)\n  \n @cached_property\n def translations(self)->AsyncTranslationsWithRawResponse:\n  return AsyncTranslationsWithRawResponse(self._audio.translations)\n  \n  \nclass AudioWithStreamingResponse:\n def __init__(self,audio:Audio)->None:\n  self._audio=audio\n  \n @cached_property\n def transcriptions(self)->TranscriptionsWithStreamingResponse:\n  return TranscriptionsWithStreamingResponse(self._audio.transcriptions)\n  \n @cached_property\n def translations(self)->TranslationsWithStreamingResponse:\n  return TranslationsWithStreamingResponse(self._audio.translations)\n  \n  \nclass AsyncAudioWithStreamingResponse:\n def __init__(self,audio:AsyncAudio)->None:\n  self._audio=audio\n  \n @cached_property\n def transcriptions(self)->AsyncTranscriptionsWithStreamingResponse:\n  return AsyncTranscriptionsWithStreamingResponse(self._audio.transcriptions)\n  \n @cached_property\n def translations(self)->AsyncTranslationsWithStreamingResponse:\n  return AsyncTranslationsWithStreamingResponse(self._audio.translations)\n", ["__future__", "groq._compat", "groq._resource", "groq.resources.audio.transcriptions", "groq.resources.audio.translations"]], "groq.resources.audio.translations": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Union,Mapping,cast\nfrom typing_extensions import Literal\n\nimport httpx\n\nfrom ..._types import NOT_GIVEN,Body,Query,Headers,NotGiven,FileTypes\nfrom ..._utils import(\nextract_files,\nmaybe_transform,\ndeepcopy_minimal,\nasync_maybe_transform,\n)\nfrom ..._compat import cached_property\nfrom ..._resource import SyncAPIResource,AsyncAPIResource\nfrom ..._response import(\nto_raw_response_wrapper,\nto_streamed_response_wrapper,\nasync_to_raw_response_wrapper,\nasync_to_streamed_response_wrapper,\n)\nfrom ...types.audio import translation_create_params\nfrom ..._base_client import make_request_options\nfrom ...types.audio.translation import Translation\n\n__all__=[\"Translations\",\"AsyncTranslations\"]\n\n\nclass Translations(SyncAPIResource):\n @cached_property\n def with_raw_response(self)->TranslationsWithRawResponse:\n  ''\n\n\n\n\n  \n  return TranslationsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->TranslationsWithStreamingResponse:\n  ''\n\n\n\n  \n  return TranslationsWithStreamingResponse(self)\n  \n def create(\n self,\n *,\n file:FileTypes,\n model:Union[str,Literal[\"whisper-large-v3\"]],\n prompt:str |NotGiven=NOT_GIVEN,\n response_format:Literal[\"json\",\"text\",\"verbose_json\"]|NotGiven=NOT_GIVEN,\n temperature:float |NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Translation:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  body=deepcopy_minimal(\n  {\n  \"file\":file,\n  \"model\":model,\n  \"prompt\":prompt,\n  \"response_format\":response_format,\n  \"temperature\":temperature,\n  }\n  )\n  files=extract_files(cast(Mapping[str,object],body),paths=[[\"file\"]])\n  \n  \n  \n  extra_headers={\"Content-Type\":\"multipart/form-data\",**(extra_headers or{})}\n  return self._post(\n  \"/openai/v1/audio/translations\",\n  body=maybe_transform(body,translation_create_params.TranslationCreateParams),\n  files=files,\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=Translation,\n  )\n  \n  \nclass AsyncTranslations(AsyncAPIResource):\n @cached_property\n def with_raw_response(self)->AsyncTranslationsWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncTranslationsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncTranslationsWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncTranslationsWithStreamingResponse(self)\n  \n async def create(\n self,\n *,\n file:FileTypes,\n model:Union[str,Literal[\"whisper-large-v3\"]],\n prompt:str |NotGiven=NOT_GIVEN,\n response_format:Literal[\"json\",\"text\",\"verbose_json\"]|NotGiven=NOT_GIVEN,\n temperature:float |NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Translation:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  body=deepcopy_minimal(\n  {\n  \"file\":file,\n  \"model\":model,\n  \"prompt\":prompt,\n  \"response_format\":response_format,\n  \"temperature\":temperature,\n  }\n  )\n  files=extract_files(cast(Mapping[str,object],body),paths=[[\"file\"]])\n  \n  \n  \n  extra_headers={\"Content-Type\":\"multipart/form-data\",**(extra_headers or{})}\n  return await self._post(\n  \"/openai/v1/audio/translations\",\n  body=await async_maybe_transform(body,translation_create_params.TranslationCreateParams),\n  files=files,\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=Translation,\n  )\n  \n  \nclass TranslationsWithRawResponse:\n def __init__(self,translations:Translations)->None:\n  self._translations=translations\n  \n  self.create=to_raw_response_wrapper(\n  translations.create,\n  )\n  \n  \nclass AsyncTranslationsWithRawResponse:\n def __init__(self,translations:AsyncTranslations)->None:\n  self._translations=translations\n  \n  self.create=async_to_raw_response_wrapper(\n  translations.create,\n  )\n  \n  \nclass TranslationsWithStreamingResponse:\n def __init__(self,translations:Translations)->None:\n  self._translations=translations\n  \n  self.create=to_streamed_response_wrapper(\n  translations.create,\n  )\n  \n  \nclass AsyncTranslationsWithStreamingResponse:\n def __init__(self,translations:AsyncTranslations)->None:\n  self._translations=translations\n  \n  self.create=async_to_streamed_response_wrapper(\n  translations.create,\n  )\n", ["__future__", "groq._base_client", "groq._compat", "groq._resource", "groq._response", "groq._types", "groq._utils", "groq.types.audio", "groq.types.audio.translation", "groq.types.audio.translation_create_params", "httpx", "typing", "typing_extensions"]], "groq.resources.chat": [".py", "\n\nfrom.chat import(\nChat,\nAsyncChat,\nChatWithRawResponse,\nAsyncChatWithRawResponse,\nChatWithStreamingResponse,\nAsyncChatWithStreamingResponse,\n)\nfrom.completions import(\nCompletions,\nAsyncCompletions,\nCompletionsWithRawResponse,\nAsyncCompletionsWithRawResponse,\nCompletionsWithStreamingResponse,\nAsyncCompletionsWithStreamingResponse,\n)\n\n__all__=[\n\"Completions\",\n\"AsyncCompletions\",\n\"CompletionsWithRawResponse\",\n\"AsyncCompletionsWithRawResponse\",\n\"CompletionsWithStreamingResponse\",\n\"AsyncCompletionsWithStreamingResponse\",\n\"Chat\",\n\"AsyncChat\",\n\"ChatWithRawResponse\",\n\"AsyncChatWithRawResponse\",\n\"ChatWithStreamingResponse\",\n\"AsyncChatWithStreamingResponse\",\n]\n", ["groq.resources.chat.chat", "groq.resources.chat.completions"], 1], "groq.resources.chat.chat": [".py", "\n\nfrom __future__ import annotations\n\nfrom ..._compat import cached_property\nfrom ..._resource import SyncAPIResource,AsyncAPIResource\nfrom.completions import(\nCompletions,\nAsyncCompletions,\nCompletionsWithRawResponse,\nAsyncCompletionsWithRawResponse,\nCompletionsWithStreamingResponse,\nAsyncCompletionsWithStreamingResponse,\n)\n\n__all__=[\"Chat\",\"AsyncChat\"]\n\n\nclass Chat(SyncAPIResource):\n @cached_property\n def completions(self)->Completions:\n  return Completions(self._client)\n  \n @cached_property\n def with_raw_response(self)->ChatWithRawResponse:\n  ''\n\n\n\n\n  \n  return ChatWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->ChatWithStreamingResponse:\n  ''\n\n\n\n  \n  return ChatWithStreamingResponse(self)\n  \n  \nclass AsyncChat(AsyncAPIResource):\n @cached_property\n def completions(self)->AsyncCompletions:\n  return AsyncCompletions(self._client)\n  \n @cached_property\n def with_raw_response(self)->AsyncChatWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncChatWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncChatWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncChatWithStreamingResponse(self)\n  \n  \nclass ChatWithRawResponse:\n def __init__(self,chat:Chat)->None:\n  self._chat=chat\n  \n @cached_property\n def completions(self)->CompletionsWithRawResponse:\n  return CompletionsWithRawResponse(self._chat.completions)\n  \n  \nclass AsyncChatWithRawResponse:\n def __init__(self,chat:AsyncChat)->None:\n  self._chat=chat\n  \n @cached_property\n def completions(self)->AsyncCompletionsWithRawResponse:\n  return AsyncCompletionsWithRawResponse(self._chat.completions)\n  \n  \nclass ChatWithStreamingResponse:\n def __init__(self,chat:Chat)->None:\n  self._chat=chat\n  \n @cached_property\n def completions(self)->CompletionsWithStreamingResponse:\n  return CompletionsWithStreamingResponse(self._chat.completions)\n  \n  \nclass AsyncChatWithStreamingResponse:\n def __init__(self,chat:AsyncChat)->None:\n  self._chat=chat\n  \n @cached_property\n def completions(self)->AsyncCompletionsWithStreamingResponse:\n  return AsyncCompletionsWithStreamingResponse(self._chat.completions)\n", ["__future__", "groq._compat", "groq._resource", "groq.resources.chat.completions"]], "groq.resources.chat.completions": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Dict,List,Union,Iterable,Optional,overload\nfrom typing_extensions import Literal\n\nimport httpx\n\nfrom ..._types import NOT_GIVEN,Body,Query,Headers,NotGiven\nfrom ..._utils import(\nmaybe_transform,\nasync_maybe_transform,\n)\nfrom ..._compat import cached_property\nfrom ..._resource import SyncAPIResource,AsyncAPIResource\nfrom ..._response import(\nto_raw_response_wrapper,\nto_streamed_response_wrapper,\nasync_to_raw_response_wrapper,\nasync_to_streamed_response_wrapper,\n)\nfrom ..._streaming import Stream,AsyncStream\nfrom ...types.chat import completion_create_params\nfrom ..._base_client import make_request_options\nfrom ...types.chat.chat_completion import ChatCompletion\nfrom ...types.chat.chat_completion_chunk import ChatCompletionChunk\nfrom ...types.chat.chat_completion_tool_param import ChatCompletionToolParam\nfrom ...types.chat.chat_completion_message_param import ChatCompletionMessageParam\nfrom ...types.chat.chat_completion_tool_choice_option_param import ChatCompletionToolChoiceOptionParam\n\n__all__=[\"Completions\",\"AsyncCompletions\"]\n\n\nclass Completions(SyncAPIResource):\n @cached_property\n def with_raw_response(self)->CompletionsWithRawResponse:\n  ''\n\n\n\n\n  \n  return CompletionsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->CompletionsWithStreamingResponse:\n  ''\n\n\n\n  \n  return CompletionsWithStreamingResponse(self)\n  \n @overload\n def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:str,\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:Optional[Literal[False]]|NotGiven=NOT_GIVEN,\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ChatCompletion:\n  ...\n  \n @overload\n def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:str,\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:Literal[True],\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->Stream[ChatCompletionChunk]:\n  ...\n  \n @overload\n def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:str,\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:bool,\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ChatCompletion |Stream[ChatCompletionChunk]:\n  ...\n  \n def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:Union[str,Literal[\"gemma-7b-it\",\"llama3-70b-8192\",\"llama3-8b-8192\",\"mixtral-8x7b-32768\"]],\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:Optional[Literal[False]]|Literal[True]|NotGiven=NOT_GIVEN,\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ChatCompletion |Stream[ChatCompletionChunk]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return self._post(\n  \"/openai/v1/chat/completions\",\n  body=maybe_transform(\n  {\n  \"messages\":messages,\n  \"model\":model,\n  \"frequency_penalty\":frequency_penalty,\n  \"function_call\":function_call,\n  \"functions\":functions,\n  \"logit_bias\":logit_bias,\n  \"logprobs\":logprobs,\n  \"max_tokens\":max_tokens,\n  \"n\":n,\n  \"parallel_tool_calls\":parallel_tool_calls,\n  \"presence_penalty\":presence_penalty,\n  \"response_format\":response_format,\n  \"seed\":seed,\n  \"stop\":stop,\n  \"stream\":stream,\n  \"temperature\":temperature,\n  \"tool_choice\":tool_choice,\n  \"tools\":tools,\n  \"top_logprobs\":top_logprobs,\n  \"top_p\":top_p,\n  \"user\":user,\n  },\n  completion_create_params.CompletionCreateParams,\n  ),\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=ChatCompletion,\n  stream=stream or False,\n  stream_cls=Stream[ChatCompletionChunk],\n  )\n  \n  \nclass AsyncCompletions(AsyncAPIResource):\n @cached_property\n def with_raw_response(self)->AsyncCompletionsWithRawResponse:\n  ''\n\n\n\n\n  \n  return AsyncCompletionsWithRawResponse(self)\n  \n @cached_property\n def with_streaming_response(self)->AsyncCompletionsWithStreamingResponse:\n  ''\n\n\n\n  \n  return AsyncCompletionsWithStreamingResponse(self)\n  \n @overload\n async def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:str,\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:Optional[Literal[False]]|NotGiven=NOT_GIVEN,\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ChatCompletion:\n  ...\n  \n @overload\n async def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:str,\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:Literal[True],\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->AsyncStream[ChatCompletionChunk]:\n  ...\n  \n @overload\n async def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:str,\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:bool,\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ChatCompletion |AsyncStream[ChatCompletionChunk]:\n  ...\n  \n async def create(\n self,\n *,\n messages:Iterable[ChatCompletionMessageParam],\n model:Union[str,Literal[\"gemma-7b-it\",\"llama3-70b-8192\",\"llama3-8b-8192\",\"mixtral-8x7b-32768\"]],\n frequency_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n function_call:Optional[completion_create_params.FunctionCall]|NotGiven=NOT_GIVEN,\n functions:Optional[Iterable[completion_create_params.Function]]|NotGiven=NOT_GIVEN,\n logit_bias:Optional[Dict[str,int]]|NotGiven=NOT_GIVEN,\n logprobs:Optional[bool]|NotGiven=NOT_GIVEN,\n max_tokens:Optional[int]|NotGiven=NOT_GIVEN,\n n:Optional[int]|NotGiven=NOT_GIVEN,\n parallel_tool_calls:Optional[bool]|NotGiven=NOT_GIVEN,\n presence_penalty:Optional[float]|NotGiven=NOT_GIVEN,\n response_format:Optional[completion_create_params.ResponseFormat]|NotGiven=NOT_GIVEN,\n seed:Optional[int]|NotGiven=NOT_GIVEN,\n stop:Union[Optional[str],List[str],None]|NotGiven=NOT_GIVEN,\n stream:Optional[Literal[False]]|Literal[True]|NotGiven=NOT_GIVEN,\n temperature:Optional[float]|NotGiven=NOT_GIVEN,\n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]|NotGiven=NOT_GIVEN,\n tools:Optional[Iterable[ChatCompletionToolParam]]|NotGiven=NOT_GIVEN,\n top_logprobs:Optional[int]|NotGiven=NOT_GIVEN,\n top_p:Optional[float]|NotGiven=NOT_GIVEN,\n user:Optional[str]|NotGiven=NOT_GIVEN,\n \n \n extra_headers:Headers |None=None,\n extra_query:Query |None=None,\n extra_body:Body |None=None,\n timeout:float |httpx.Timeout |None |NotGiven=NOT_GIVEN,\n )->ChatCompletion |AsyncStream[ChatCompletionChunk]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  return await self._post(\n  \"/openai/v1/chat/completions\",\n  body=await async_maybe_transform(\n  {\n  \"messages\":messages,\n  \"model\":model,\n  \"frequency_penalty\":frequency_penalty,\n  \"function_call\":function_call,\n  \"functions\":functions,\n  \"logit_bias\":logit_bias,\n  \"logprobs\":logprobs,\n  \"max_tokens\":max_tokens,\n  \"n\":n,\n  \"parallel_tool_calls\":parallel_tool_calls,\n  \"presence_penalty\":presence_penalty,\n  \"response_format\":response_format,\n  \"seed\":seed,\n  \"stop\":stop,\n  \"stream\":stream,\n  \"temperature\":temperature,\n  \"tool_choice\":tool_choice,\n  \"tools\":tools,\n  \"top_logprobs\":top_logprobs,\n  \"top_p\":top_p,\n  \"user\":user,\n  },\n  completion_create_params.CompletionCreateParams,\n  ),\n  options=make_request_options(\n  extra_headers=extra_headers,extra_query=extra_query,extra_body=extra_body,timeout=timeout\n  ),\n  cast_to=ChatCompletion,\n  stream=stream or False,\n  stream_cls=AsyncStream[ChatCompletionChunk],\n  )\n  \n  \nclass CompletionsWithRawResponse:\n def __init__(self,completions:Completions)->None:\n  self._completions=completions\n  \n  self.create=to_raw_response_wrapper(\n  completions.create,\n  )\n  \n  \nclass AsyncCompletionsWithRawResponse:\n def __init__(self,completions:AsyncCompletions)->None:\n  self._completions=completions\n  \n  self.create=async_to_raw_response_wrapper(\n  completions.create,\n  )\n  \n  \nclass CompletionsWithStreamingResponse:\n def __init__(self,completions:Completions)->None:\n  self._completions=completions\n  \n  self.create=to_streamed_response_wrapper(\n  completions.create,\n  )\n  \n  \nclass AsyncCompletionsWithStreamingResponse:\n def __init__(self,completions:AsyncCompletions)->None:\n  self._completions=completions\n  \n  self.create=async_to_streamed_response_wrapper(\n  completions.create,\n  )\n", ["__future__", "groq._base_client", "groq._compat", "groq._resource", "groq._response", "groq._streaming", "groq._types", "groq._utils", "groq.types.chat", "groq.types.chat.chat_completion", "groq.types.chat.chat_completion_chunk", "groq.types.chat.chat_completion_message_param", "groq.types.chat.chat_completion_tool_choice_option_param", "groq.types.chat.chat_completion_tool_param", "groq.types.chat.completion_create_params", "httpx", "typing", "typing_extensions"]], "groq.types": [".py", "\n\nfrom __future__ import annotations\n\nfrom.model import Model as Model\nfrom.shared import(\nErrorObject as ErrorObject,\nFunctionDefinition as FunctionDefinition,\nFunctionParameters as FunctionParameters,\n)\nfrom.embedding import Embedding as Embedding\nfrom.model_deleted import ModelDeleted as ModelDeleted\nfrom.completion_usage import CompletionUsage as CompletionUsage\nfrom.model_list_response import ModelListResponse as ModelListResponse\nfrom.embedding_create_params import EmbeddingCreateParams as EmbeddingCreateParams\nfrom.create_embedding_response import CreateEmbeddingResponse as CreateEmbeddingResponse\n", ["__future__", "groq.types.completion_usage", "groq.types.create_embedding_response", "groq.types.embedding", "groq.types.embedding_create_params", "groq.types.model", "groq.types.model_deleted", "groq.types.model_list_response", "groq.types.shared"], 1], "groq.types.model": [".py", "\n\nfrom typing_extensions import Literal\n\nfrom.._models import BaseModel\n\n__all__=[\"Model\"]\n\n\nclass Model(BaseModel):\n id:str\n '' \n \n created:int\n '' \n \n object:Literal[\"model\"]\n '' \n \n owned_by:str\n '' \n", ["groq._models", "typing_extensions"]], "groq.types.model_list_response": [".py", "\n\nfrom typing import List\nfrom typing_extensions import Literal\n\nfrom.model import Model\nfrom.._models import BaseModel\n\n__all__=[\"ModelListResponse\"]\n\n\nclass ModelListResponse(BaseModel):\n data:List[Model]\n \n object:Literal[\"list\"]\n", ["groq._models", "groq.types.model", "typing", "typing_extensions"]], "groq.types.embedding_create_params": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import List,Union,Optional\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"EmbeddingCreateParams\"]\n\n\nclass EmbeddingCreateParams(TypedDict,total=False):\n input:Required[Union[str,List[str]]]\n ''\n\n\n\n\n \n \n model:Required[Union[str,Literal[\"nomic-embed-text-v1_5\"]]]\n '' \n \n encoding_format:Literal[\"float\",\"base64\"]\n '' \n \n user:Optional[str]\n ''\n\n\n \n", ["__future__", "typing", "typing_extensions"]], "groq.types.create_embedding_response": [".py", "\n\nfrom typing import List\nfrom typing_extensions import Literal\n\nfrom.._models import BaseModel\nfrom.embedding import Embedding\n\n__all__=[\"CreateEmbeddingResponse\",\"Usage\"]\n\n\nclass Usage(BaseModel):\n prompt_tokens:int\n '' \n \n total_tokens:int\n '' \n \n \nclass CreateEmbeddingResponse(BaseModel):\n data:List[Embedding]\n '' \n \n model:str\n '' \n \n object:Literal[\"list\"]\n '' \n \n usage:Usage\n '' \n", ["groq._models", "groq.types.embedding", "typing", "typing_extensions"]], "groq.types.model_deleted": [".py", "\n\n\nfrom.._models import BaseModel\n\n__all__=[\"ModelDeleted\"]\n\n\nclass ModelDeleted(BaseModel):\n id:str\n \n deleted:bool\n \n object:str\n", ["groq._models"]], "groq.types.completion_usage": [".py", "\n\nfrom typing import Optional\n\nfrom.._models import BaseModel\n\n__all__=[\"CompletionUsage\"]\n\n\nclass CompletionUsage(BaseModel):\n completion_tokens:int\n '' \n \n prompt_tokens:int\n '' \n \n total_tokens:int\n '' \n \n completion_time:Optional[float]=None\n '' \n \n prompt_time:Optional[float]=None\n '' \n \n queue_time:Optional[float]=None\n '' \n \n total_time:Optional[float]=None\n '' \n", ["groq._models", "typing"]], "groq.types.embedding": [".py", "\n\nfrom typing import List,Union\nfrom typing_extensions import Literal\n\nfrom.._models import BaseModel\n\n__all__=[\"Embedding\"]\n\n\nclass Embedding(BaseModel):\n embedding:Union[List[float],str]\n ''\n\n\n\n \n \n index:int\n '' \n \n object:Literal[\"embedding\"]\n '' \n", ["groq._models", "typing", "typing_extensions"]], "groq.types.audio": [".py", "\n\nfrom __future__ import annotations\n\nfrom.translation import Translation as Translation\nfrom.transcription import Transcription as Transcription\nfrom.translation_create_params import TranslationCreateParams as TranslationCreateParams\nfrom.transcription_create_params import TranscriptionCreateParams as TranscriptionCreateParams\n", ["__future__", "groq.types.audio.transcription", "groq.types.audio.transcription_create_params", "groq.types.audio.translation", "groq.types.audio.translation_create_params"], 1], "groq.types.audio.translation_create_params": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Union\nfrom typing_extensions import Literal,Required,TypedDict\n\nfrom ..._types import FileTypes\n\n__all__=[\"TranslationCreateParams\"]\n\n\nclass TranslationCreateParams(TypedDict,total=False):\n file:Required[FileTypes]\n ''\n\n\n \n \n model:Required[Union[str,Literal[\"whisper-large-v3\"]]]\n '' \n \n prompt:str\n ''\n\n\n\n \n \n response_format:Literal[\"json\",\"text\",\"verbose_json\"]\n ''\n\n\n \n \n temperature:float\n ''\n\n\n\n\n\n \n", ["__future__", "groq._types", "typing", "typing_extensions"]], "groq.types.audio.translation": [".py", "\n\n\nfrom ..._models import BaseModel\n\n__all__=[\"Translation\"]\n\n\nclass Translation(BaseModel):\n text:str\n", ["groq._models"]], "groq.types.audio.transcription_create_params": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import List,Union\nfrom typing_extensions import Literal,Required,TypedDict\n\nfrom ..._types import FileTypes\n\n__all__=[\"TranscriptionCreateParams\"]\n\n\nclass TranscriptionCreateParams(TypedDict,total=False):\n file:Required[FileTypes]\n ''\n\n\n \n \n model:Required[Union[str,Literal[\"whisper-large-v3\"]]]\n '' \n \n language:Union[\n str,\n Literal[\n \"en\",\n \"zh\",\n \"de\",\n \"es\",\n \"ru\",\n \"ko\",\n \"fr\",\n \"ja\",\n \"pt\",\n \"tr\",\n \"pl\",\n \"ca\",\n \"nl\",\n \"ar\",\n \"sv\",\n \"it\",\n \"id\",\n \"hi\",\n \"fi\",\n \"vi\",\n \"he\",\n \"uk\",\n \"el\",\n \"ms\",\n \"cs\",\n \"ro\",\n \"da\",\n \"hu\",\n \"ta\",\n \"no\",\n \"th\",\n \"ur\",\n \"hr\",\n \"bg\",\n \"lt\",\n \"la\",\n \"mi\",\n \"ml\",\n \"cy\",\n \"sk\",\n \"te\",\n \"fa\",\n \"lv\",\n \"bn\",\n \"sr\",\n \"az\",\n \"sl\",\n \"kn\",\n \"et\",\n \"mk\",\n \"br\",\n \"eu\",\n \"is\",\n \"hy\",\n \"ne\",\n \"mn\",\n \"bs\",\n \"kk\",\n \"sq\",\n \"sw\",\n \"gl\",\n \"mr\",\n \"pa\",\n \"si\",\n \"km\",\n \"sn\",\n \"yo\",\n \"so\",\n \"af\",\n \"oc\",\n \"ka\",\n \"be\",\n \"tg\",\n \"sd\",\n \"gu\",\n \"am\",\n \"yi\",\n \"lo\",\n \"uz\",\n \"fo\",\n \"ht\",\n \"ps\",\n \"tk\",\n \"nn\",\n \"mt\",\n \"sa\",\n \"lb\",\n \"my\",\n \"bo\",\n \"tl\",\n \"mg\",\n \"as\",\n \"tt\",\n \"haw\",\n \"ln\",\n \"ha\",\n \"ba\",\n \"jv\",\n \"su\",\n \"yue\",\n ],\n ]\n ''\n\n\n\n\n \n \n prompt:str\n ''\n\n\n\n\n \n \n response_format:Literal[\"json\",\"text\",\"verbose_json\"]\n ''\n\n\n \n \n temperature:float\n ''\n\n\n\n\n\n \n \n timestamp_granularities:List[Literal[\"word\",\"segment\"]]\n ''\n\n\n\n\n\n \n", ["__future__", "groq._types", "typing", "typing_extensions"]], "groq.types.audio.transcription": [".py", "\n\n\nfrom ..._models import BaseModel\n\n__all__=[\"Transcription\"]\n\n\nclass Transcription(BaseModel):\n text:str\n '' \n", ["groq._models"]], "groq.types.shared": [".py", "\n\nfrom.error_object import ErrorObject as ErrorObject\nfrom.function_definition import FunctionDefinition as FunctionDefinition\nfrom.function_parameters import FunctionParameters as FunctionParameters\n", ["groq.types.shared.error_object", "groq.types.shared.function_definition", "groq.types.shared.function_parameters"], 1], "groq.types.shared.function_parameters": [".py", "\n\nfrom typing import Dict\nfrom typing_extensions import TypeAlias\n\n__all__=[\"FunctionParameters\"]\n\nFunctionParameters:TypeAlias=Dict[str,object]\n", ["typing", "typing_extensions"]], "groq.types.shared.function_definition": [".py", "\n\nfrom typing import Optional\n\nfrom ..._models import BaseModel\nfrom.function_parameters import FunctionParameters\n\n__all__=[\"FunctionDefinition\"]\n\n\nclass FunctionDefinition(BaseModel):\n name:str\n ''\n\n\n\n \n \n description:Optional[str]=None\n ''\n\n\n \n \n parameters:Optional[FunctionParameters]=None\n ''\n\n\n\n\n\n\n \n", ["groq._models", "groq.types.shared.function_parameters", "typing"]], "groq.types.shared.error_object": [".py", "\n\nfrom typing import Optional\n\nfrom ..._models import BaseModel\n\n__all__=[\"ErrorObject\"]\n\n\nclass ErrorObject(BaseModel):\n code:Optional[str]=None\n \n message:str\n \n param:Optional[str]=None\n \n type:str\n", ["groq._models", "typing"]], "groq.types.shared_params": [".py", "\n\nfrom.function_definition import FunctionDefinition as FunctionDefinition\nfrom.function_parameters import FunctionParameters as FunctionParameters\n", ["groq.types.shared_params.function_definition", "groq.types.shared_params.function_parameters"], 1], "groq.types.shared_params.function_parameters": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Dict\nfrom typing_extensions import TypeAlias\n\n__all__=[\"FunctionParameters\"]\n\nFunctionParameters:TypeAlias=Dict[str,object]\n", ["__future__", "typing", "typing_extensions"]], "groq.types.shared_params.function_definition": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Required,TypedDict\n\nfrom.function_parameters import FunctionParameters\n\n__all__=[\"FunctionDefinition\"]\n\n\nclass FunctionDefinition(TypedDict,total=False):\n name:Required[str]\n ''\n\n\n\n \n \n description:str\n ''\n\n\n \n \n parameters:FunctionParameters\n ''\n\n\n\n\n\n\n \n", ["__future__", "groq.types.shared_params.function_parameters", "typing_extensions"]], "groq.types.chat.chat_completion_tool_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\nfrom..shared_params.function_definition import FunctionDefinition\n\n__all__=[\"ChatCompletionToolParam\"]\n\n\nclass ChatCompletionToolParam(TypedDict,total=False):\n function:Required[FunctionDefinition]\n \n type:Required[Literal[\"function\"]]\n '' \n", ["__future__", "groq.types.shared_params.function_definition", "typing_extensions"]], "groq.types.chat": [".py", "\n\nfrom __future__ import annotations\n\nfrom.chat_completion import ChatCompletion as ChatCompletion\nfrom.chat_completion_role import ChatCompletionRole as ChatCompletionRole\nfrom.chat_completion_chunk import ChatCompletionChunk as ChatCompletionChunk\nfrom.chat_completion_message import ChatCompletionMessage as ChatCompletionMessage\nfrom.completion_create_params import CompletionCreateParams as CompletionCreateParams\nfrom.chat_completion_tool_param import ChatCompletionToolParam as ChatCompletionToolParam\nfrom.chat_completion_message_param import ChatCompletionMessageParam as ChatCompletionMessageParam\nfrom.chat_completion_token_logprob import ChatCompletionTokenLogprob as ChatCompletionTokenLogprob\nfrom.chat_completion_message_tool_call import ChatCompletionMessageToolCall as ChatCompletionMessageToolCall\nfrom.chat_completion_content_part_param import ChatCompletionContentPartParam as ChatCompletionContentPartParam\nfrom.chat_completion_tool_message_param import ChatCompletionToolMessageParam as ChatCompletionToolMessageParam\nfrom.chat_completion_user_message_param import ChatCompletionUserMessageParam as ChatCompletionUserMessageParam\nfrom.chat_completion_system_message_param import ChatCompletionSystemMessageParam as ChatCompletionSystemMessageParam\nfrom.chat_completion_function_message_param import(\nChatCompletionFunctionMessageParam as ChatCompletionFunctionMessageParam,\n)\nfrom.chat_completion_assistant_message_param import(\nChatCompletionAssistantMessageParam as ChatCompletionAssistantMessageParam,\n)\nfrom.chat_completion_content_part_text_param import(\nChatCompletionContentPartTextParam as ChatCompletionContentPartTextParam,\n)\nfrom.chat_completion_message_tool_call_param import(\nChatCompletionMessageToolCallParam as ChatCompletionMessageToolCallParam,\n)\nfrom.chat_completion_named_tool_choice_param import(\nChatCompletionNamedToolChoiceParam as ChatCompletionNamedToolChoiceParam,\n)\nfrom.chat_completion_content_part_image_param import(\nChatCompletionContentPartImageParam as ChatCompletionContentPartImageParam,\n)\nfrom.chat_completion_tool_choice_option_param import(\nChatCompletionToolChoiceOptionParam as ChatCompletionToolChoiceOptionParam,\n)\nfrom.chat_completion_function_call_option_param import(\nChatCompletionFunctionCallOptionParam as ChatCompletionFunctionCallOptionParam,\n)\n", ["__future__", "groq.types.chat.chat_completion", "groq.types.chat.chat_completion_assistant_message_param", "groq.types.chat.chat_completion_chunk", "groq.types.chat.chat_completion_content_part_image_param", "groq.types.chat.chat_completion_content_part_param", "groq.types.chat.chat_completion_content_part_text_param", "groq.types.chat.chat_completion_function_call_option_param", "groq.types.chat.chat_completion_function_message_param", "groq.types.chat.chat_completion_message", "groq.types.chat.chat_completion_message_param", "groq.types.chat.chat_completion_message_tool_call", "groq.types.chat.chat_completion_message_tool_call_param", "groq.types.chat.chat_completion_named_tool_choice_param", "groq.types.chat.chat_completion_role", "groq.types.chat.chat_completion_system_message_param", "groq.types.chat.chat_completion_token_logprob", "groq.types.chat.chat_completion_tool_choice_option_param", "groq.types.chat.chat_completion_tool_message_param", "groq.types.chat.chat_completion_tool_param", "groq.types.chat.chat_completion_user_message_param", "groq.types.chat.completion_create_params"], 1], "groq.types.chat.chat_completion_message_tool_call_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionMessageToolCallParam\",\"Function\"]\n\n\nclass Function(TypedDict,total=False):\n arguments:Required[str]\n ''\n\n\n\n\n \n \n name:Required[str]\n '' \n \n \nclass ChatCompletionMessageToolCallParam(TypedDict,total=False):\n id:Required[str]\n '' \n \n function:Required[Function]\n '' \n \n type:Required[Literal[\"function\"]]\n '' \n", ["__future__", "typing_extensions"]], "groq.types.chat.chat_completion_token_logprob": [".py", "\n\nfrom typing import List,Optional\n\nfrom ..._models import BaseModel\n\n__all__=[\"ChatCompletionTokenLogprob\",\"TopLogprob\"]\n\n\nclass TopLogprob(BaseModel):\n token:str\n '' \n \n bytes:Optional[List[int]]=None\n ''\n\n\n\n\n \n \n logprob:float\n ''\n\n\n\n\n \n \n \nclass ChatCompletionTokenLogprob(BaseModel):\n token:str\n '' \n \n bytes:Optional[List[int]]=None\n ''\n\n\n\n\n \n \n logprob:float\n ''\n\n\n\n\n \n \n top_logprobs:List[TopLogprob]\n ''\n\n\n\n\n \n", ["groq._models", "typing"]], "groq.types.chat.chat_completion": [".py", "\n\nfrom typing import List,Optional\nfrom typing_extensions import Literal\n\nfrom ..._models import BaseModel\nfrom..completion_usage import CompletionUsage\nfrom.chat_completion_message import ChatCompletionMessage\nfrom.chat_completion_token_logprob import ChatCompletionTokenLogprob\n\n__all__=[\"ChatCompletion\",\"Choice\",\"ChoiceLogprobs\"]\n\n\nclass ChoiceLogprobs(BaseModel):\n content:Optional[List[ChatCompletionTokenLogprob]]=None\n '' \n \n \nclass Choice(BaseModel):\n finish_reason:Literal[\"stop\",\"length\",\"tool_calls\",\"function_call\"]\n ''\n\n\n\n\n\n \n \n index:int\n '' \n \n logprobs:Optional[ChoiceLogprobs]=None\n '' \n \n message:ChatCompletionMessage\n '' \n \n \nclass ChatCompletion(BaseModel):\n id:str\n '' \n \n choices:List[Choice]\n ''\n\n\n \n \n created:int\n '' \n \n model:str\n '' \n \n object:Literal[\"chat.completion\"]\n '' \n \n system_fingerprint:Optional[str]=None\n ''\n\n\n\n \n \n usage:Optional[CompletionUsage]=None\n '' \n", ["groq._models", "groq.types.chat.chat_completion_message", "groq.types.chat.chat_completion_token_logprob", "groq.types.completion_usage", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_message_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Union\nfrom typing_extensions import TypeAlias\n\nfrom.chat_completion_tool_message_param import ChatCompletionToolMessageParam\nfrom.chat_completion_user_message_param import ChatCompletionUserMessageParam\nfrom.chat_completion_system_message_param import ChatCompletionSystemMessageParam\nfrom.chat_completion_function_message_param import ChatCompletionFunctionMessageParam\nfrom.chat_completion_assistant_message_param import ChatCompletionAssistantMessageParam\n\n__all__=[\"ChatCompletionMessageParam\"]\n\nChatCompletionMessageParam:TypeAlias=Union[\nChatCompletionSystemMessageParam,\nChatCompletionUserMessageParam,\nChatCompletionAssistantMessageParam,\nChatCompletionToolMessageParam,\nChatCompletionFunctionMessageParam,\n]\n", ["__future__", "groq.types.chat.chat_completion_assistant_message_param", "groq.types.chat.chat_completion_function_message_param", "groq.types.chat.chat_completion_system_message_param", "groq.types.chat.chat_completion_tool_message_param", "groq.types.chat.chat_completion_user_message_param", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_content_part_text_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionContentPartTextParam\"]\n\n\nclass ChatCompletionContentPartTextParam(TypedDict,total=False):\n text:Required[str]\n '' \n \n type:Required[Literal[\"text\"]]\n '' \n", ["__future__", "typing_extensions"]], "groq.types.chat.chat_completion_chunk": [".py", "\n\n\nfrom typing import List,Optional\nfrom typing_extensions import Literal\n\nfrom ..._models import BaseModel\nfrom..completion_usage import CompletionUsage\nfrom.chat_completion_token_logprob import ChatCompletionTokenLogprob\n\n__all__=[\n\"ChatCompletionChunk\",\n\"Choice\",\n\"ChoiceDelta\",\n\"ChoiceDeltaFunctionCall\",\n\"ChoiceDeltaToolCall\",\n\"ChoiceDeltaToolCallFunction\",\n\"ChoiceLogprobs\",\n]\n\n\nclass ChoiceDeltaFunctionCall(BaseModel):\n arguments:Optional[str]=None\n ''\n\n\n\n\n \n \n name:Optional[str]=None\n '' \n \n \nclass ChoiceDeltaToolCallFunction(BaseModel):\n arguments:Optional[str]=None\n ''\n\n\n\n\n \n \n name:Optional[str]=None\n '' \n \n \nclass ChoiceDeltaToolCall(BaseModel):\n index:int\n \n id:Optional[str]=None\n '' \n \n function:Optional[ChoiceDeltaToolCallFunction]=None\n \n type:Optional[Literal[\"function\"]]=None\n '' \n \n \nclass ChoiceDelta(BaseModel):\n content:Optional[str]=None\n '' \n \n function_call:Optional[ChoiceDeltaFunctionCall]=None\n ''\n\n\n\n \n \n role:Optional[Literal[\"system\",\"user\",\"assistant\",\"tool\"]]=None\n '' \n \n tool_calls:Optional[List[ChoiceDeltaToolCall]]=None\n \n \nclass ChoiceLogprobs(BaseModel):\n content:Optional[List[ChatCompletionTokenLogprob]]=None\n '' \n \n \nclass Choice(BaseModel):\n delta:ChoiceDelta\n '' \n \n finish_reason:Optional[Literal[\"stop\",\"length\",\"tool_calls\",\"content_filter\",\"function_call\"]]=None\n ''\n\n\n\n\n\n\n \n \n index:int\n '' \n \n logprobs:Optional[ChoiceLogprobs]=None\n '' \n \n \nclass XGroq(BaseModel):\n id:Optional[str]\n ''\n\n\n \n \n usage:Optional[CompletionUsage]\n '' \n \n error:Optional[str]\n '' \n \n \nclass ChatCompletionChunk(BaseModel):\n id:str\n '' \n \n choices:List[Choice]\n ''\n\n\n\n \n \n created:int\n ''\n\n\n \n \n model:str\n '' \n \n object:Literal[\"chat.completion.chunk\"]\n '' \n \n system_fingerprint:Optional[str]=None\n ''\n\n\n\n \n \n usage:Optional[CompletionUsage]=None\n ''\n\n\n\n\n \n \n x_groq:Optional[XGroq]\n ''\n\n \n", ["groq._models", "groq.types.chat.chat_completion_token_logprob", "groq.types.completion_usage", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_function_call_option_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Required,TypedDict\n\n__all__=[\"ChatCompletionFunctionCallOptionParam\"]\n\n\nclass ChatCompletionFunctionCallOptionParam(TypedDict,total=False):\n name:Required[str]\n '' \n", ["__future__", "typing_extensions"]], "groq.types.chat.chat_completion_message_tool_call": [".py", "\n\nfrom typing_extensions import Literal\n\nfrom ..._models import BaseModel\n\n__all__=[\"ChatCompletionMessageToolCall\",\"Function\"]\n\n\nclass Function(BaseModel):\n arguments:str\n ''\n\n\n\n\n \n \n name:str\n '' \n \n \nclass ChatCompletionMessageToolCall(BaseModel):\n id:str\n '' \n \n function:Function\n '' \n \n type:Literal[\"function\"]\n '' \n", ["groq._models", "typing_extensions"]], "groq.types.chat.chat_completion_function_message_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Optional\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionFunctionMessageParam\"]\n\n\nclass ChatCompletionFunctionMessageParam(TypedDict,total=False):\n content:Required[Optional[str]]\n '' \n \n name:Required[str]\n '' \n \n role:Required[Literal[\"function\"]]\n '' \n", ["__future__", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_assistant_message_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Iterable,Optional\nfrom typing_extensions import Literal,Required,TypedDict\n\nfrom.chat_completion_message_tool_call_param import ChatCompletionMessageToolCallParam\n\n__all__=[\"ChatCompletionAssistantMessageParam\",\"FunctionCall\"]\n\n\nclass FunctionCall(TypedDict,total=False):\n arguments:str\n ''\n\n\n\n\n \n \n name:str\n '' \n \n \nclass ChatCompletionAssistantMessageParam(TypedDict,total=False):\n role:Required[Literal[\"assistant\"]]\n '' \n \n content:Optional[str]\n ''\n\n\n \n \n function_call:FunctionCall\n ''\n\n\n\n \n \n name:str\n ''\n\n\n\n \n \n tool_calls:Iterable[ChatCompletionMessageToolCallParam]\n '' \n", ["__future__", "groq.types.chat.chat_completion_message_tool_call_param", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_user_message_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Union,Iterable\nfrom typing_extensions import Literal,Required,TypedDict\n\nfrom.chat_completion_content_part_param import ChatCompletionContentPartParam\n\n__all__=[\"ChatCompletionUserMessageParam\"]\n\n\nclass ChatCompletionUserMessageParam(TypedDict,total=False):\n content:Required[Union[str,Iterable[ChatCompletionContentPartParam]]]\n '' \n \n role:Required[Literal[\"user\"]]\n '' \n \n name:str\n ''\n\n\n\n \n", ["__future__", "groq.types.chat.chat_completion_content_part_param", "typing", "typing_extensions"]], "groq.types.chat.completion_create_params": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Dict,List,Union,Iterable,Optional\nfrom typing_extensions import Literal,Required,TypeAlias,TypedDict\n\nfrom.chat_completion_tool_param import ChatCompletionToolParam\nfrom.chat_completion_message_param import ChatCompletionMessageParam\nfrom..shared_params.function_parameters import FunctionParameters\nfrom.chat_completion_tool_choice_option_param import ChatCompletionToolChoiceOptionParam\nfrom.chat_completion_function_call_option_param import ChatCompletionFunctionCallOptionParam\n\n__all__=[\"CompletionCreateParams\",\"FunctionCall\",\"Function\",\"ResponseFormat\"]\n\n\nclass CompletionCreateParams(TypedDict,total=False):\n messages:Required[Iterable[ChatCompletionMessageParam]]\n '' \n \n model:Required[Union[str,Literal[\"gemma-7b-it\",\"llama3-70b-8192\",\"llama3-8b-8192\",\"mixtral-8x7b-32768\"]]]\n ''\n\n\n\n \n \n frequency_penalty:Optional[float]\n ''\n\n\n\n \n \n function_call:Optional[FunctionCall]\n ''\n\n\n\n\n\n\n\n\n\n \n \n functions:Optional[Iterable[Function]]\n ''\n\n\n \n \n logit_bias:Optional[Dict[str,int]]\n ''\n\n\n \n \n logprobs:Optional[bool]\n ''\n\n\n\n \n \n max_tokens:Optional[int]\n ''\n\n\n\n \n \n n:Optional[int]\n ''\n\n\n\n \n \n parallel_tool_calls:Optional[bool]\n '' \n \n presence_penalty:Optional[float]\n ''\n\n\n\n \n \n response_format:Optional[ResponseFormat]\n ''\n\n\n\n\n\n\n \n \n seed:Optional[int]\n ''\n\n\n\n\n \n \n stop:Union[Optional[str],List[str],None]\n ''\n\n\n \n \n stream:Optional[bool]\n ''\n\n\n\n\n\n \n \n temperature:Optional[float]\n ''\n\n\n\n\n \n \n tool_choice:Optional[ChatCompletionToolChoiceOptionParam]\n ''\n\n\n\n\n\n\n\n\n\n \n \n tools:Optional[Iterable[ChatCompletionToolParam]]\n ''\n\n\n\n\n \n \n top_logprobs:Optional[int]\n ''\n\n\n\n\n \n \n top_p:Optional[float]\n ''\n\n\n\n\n \n \n user:Optional[str]\n ''\n\n\n \n \n \nFunctionCall:TypeAlias=Union[Literal[\"none\",\"auto\",\"required\"],ChatCompletionFunctionCallOptionParam]\n\n\nclass Function(TypedDict,total=False):\n name:Required[str]\n ''\n\n\n\n \n \n description:str\n ''\n\n\n \n \n parameters:FunctionParameters\n ''\n\n\n\n\n\n\n \n \n \nclass ResponseFormat(TypedDict,total=False):\n type:Literal[\"text\",\"json_object\"]\n '' \n", ["__future__", "groq.types.chat.chat_completion_function_call_option_param", "groq.types.chat.chat_completion_message_param", "groq.types.chat.chat_completion_tool_choice_option_param", "groq.types.chat.chat_completion_tool_param", "groq.types.shared_params.function_parameters", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_tool_choice_option_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Union\nfrom typing_extensions import Literal,TypeAlias\n\nfrom.chat_completion_named_tool_choice_param import ChatCompletionNamedToolChoiceParam\n\n__all__=[\"ChatCompletionToolChoiceOptionParam\"]\n\nChatCompletionToolChoiceOptionParam:TypeAlias=Union[\nLiteral[\"none\",\"auto\",\"required\"],ChatCompletionNamedToolChoiceParam\n]\n", ["__future__", "groq.types.chat.chat_completion_named_tool_choice_param", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_tool_message_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionToolMessageParam\"]\n\n\nclass ChatCompletionToolMessageParam(TypedDict,total=False):\n content:Required[str]\n '' \n \n role:Required[Literal[\"tool\"]]\n '' \n \n tool_call_id:Required[str]\n '' \n", ["__future__", "typing_extensions"]], "groq.types.chat.chat_completion_content_part_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing import Union\nfrom typing_extensions import TypeAlias\n\nfrom.chat_completion_content_part_text_param import ChatCompletionContentPartTextParam\nfrom.chat_completion_content_part_image_param import ChatCompletionContentPartImageParam\n\n__all__=[\"ChatCompletionContentPartParam\"]\n\nChatCompletionContentPartParam:TypeAlias=Union[\nChatCompletionContentPartTextParam,ChatCompletionContentPartImageParam\n]\n", ["__future__", "groq.types.chat.chat_completion_content_part_image_param", "groq.types.chat.chat_completion_content_part_text_param", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_system_message_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionSystemMessageParam\"]\n\n\nclass ChatCompletionSystemMessageParam(TypedDict,total=False):\n content:Required[str]\n '' \n \n role:Required[Literal[\"system\"]]\n '' \n \n name:str\n ''\n\n\n\n \n", ["__future__", "typing_extensions"]], "groq.types.chat.chat_completion_message": [".py", "\n\nfrom typing import List,Optional\nfrom typing_extensions import Literal\n\nfrom ..._models import BaseModel\nfrom.chat_completion_message_tool_call import ChatCompletionMessageToolCall\n\n__all__=[\"ChatCompletionMessage\",\"FunctionCall\"]\n\n\nclass FunctionCall(BaseModel):\n arguments:str\n ''\n\n\n\n\n \n \n name:str\n '' \n \n \nclass ChatCompletionMessage(BaseModel):\n content:Optional[str]=None\n '' \n \n role:Literal[\"assistant\"]\n '' \n \n function_call:Optional[FunctionCall]=None\n ''\n\n\n\n \n \n tool_calls:Optional[List[ChatCompletionMessageToolCall]]=None\n '' \n", ["groq._models", "groq.types.chat.chat_completion_message_tool_call", "typing", "typing_extensions"]], "groq.types.chat.chat_completion_role": [".py", "\n\nfrom typing_extensions import Literal,TypeAlias\n\n__all__=[\"ChatCompletionRole\"]\n\nChatCompletionRole:TypeAlias=Literal[\"system\",\"user\",\"assistant\",\"tool\",\"function\"]\n", ["typing_extensions"]], "groq.types.chat.chat_completion_content_part_image_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionContentPartImageParam\",\"ImageURL\"]\n\n\nclass ImageURL(TypedDict,total=False):\n url:Required[str]\n '' \n \n detail:Literal[\"auto\",\"low\",\"high\"]\n '' \n \n \nclass ChatCompletionContentPartImageParam(TypedDict,total=False):\n image_url:Required[ImageURL]\n \n type:Required[Literal[\"image_url\"]]\n '' \n", ["__future__", "typing_extensions"]], "groq.types.chat.chat_completion_named_tool_choice_param": [".py", "\n\nfrom __future__ import annotations\n\nfrom typing_extensions import Literal,Required,TypedDict\n\n__all__=[\"ChatCompletionNamedToolChoiceParam\",\"Function\"]\n\n\nclass Function(TypedDict,total=False):\n name:Required[str]\n '' \n \n \nclass ChatCompletionNamedToolChoiceParam(TypedDict,total=False):\n function:Required[Function]\n \n type:Required[Literal[\"function\"]]\n '' \n", ["__future__", "typing_extensions"]], "certifi": [".py", "from.core import contents,where\n\n__all__=[\"contents\",\"where\"]\n__version__=\"2024.12.14\"\n", ["certifi.core"], 1], "certifi.core": [".py", "''\n\n\n\n\n\nimport sys\nimport atexit\n\ndef exit_cacert_ctx()->None:\n _CACERT_CTX.__exit__(None,None,None)\n \n \nif sys.version_info >=(3,11):\n\n from importlib.resources import as_file,files\n \n _CACERT_CTX=None\n _CACERT_PATH=None\n \n def where()->str:\n \n \n \n \n \n  global _CACERT_CTX\n  global _CACERT_PATH\n  if _CACERT_PATH is None:\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   _CACERT_CTX=as_file(files(\"certifi\").joinpath(\"cacert.pem\"))\n   _CACERT_PATH=str(_CACERT_CTX.__enter__())\n   atexit.register(exit_cacert_ctx)\n   \n  return _CACERT_PATH\n  \n def contents()->str:\n  return files(\"certifi\").joinpath(\"cacert.pem\").read_text(encoding=\"ascii\")\n  \nelif sys.version_info >=(3,7):\n\n from importlib.resources import path as get_path,read_text\n \n _CACERT_CTX=None\n _CACERT_PATH=None\n \n def where()->str:\n \n \n \n \n \n  global _CACERT_CTX\n  global _CACERT_PATH\n  if _CACERT_PATH is None:\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   _CACERT_CTX=get_path(\"certifi\",\"cacert.pem\")\n   _CACERT_PATH=str(_CACERT_CTX.__enter__())\n   atexit.register(exit_cacert_ctx)\n   \n  return _CACERT_PATH\n  \n def contents()->str:\n  return read_text(\"certifi\",\"cacert.pem\",encoding=\"ascii\")\n  \nelse:\n import os\n import types\n from typing import Union\n \n Package=Union[types.ModuleType,str]\n Resource=Union[str,\"os.PathLike\"]\n \n \n \n \n \n def read_text(\n package:Package,\n resource:Resource,\n encoding:str='utf-8',\n errors:str='strict'\n )->str:\n  with open(where(),encoding=encoding)as data:\n   return data.read()\n   \n   \n   \n def where()->str:\n  f=os.path.dirname(__file__)\n  \n  return os.path.join(f,\"cacert.pem\")\n  \n def contents()->str:\n  return read_text(\"certifi\",\"cacert.pem\",encoding=\"ascii\")\n", ["atexit", "importlib.resources", "os", "sys", "types", "typing"]], "certifi.__main__": [".py", "import argparse\n\nfrom certifi import contents,where\n\nparser=argparse.ArgumentParser()\nparser.add_argument(\"-c\",\"--contents\",action=\"store_true\")\nargs=parser.parse_args()\n\nif args.contents:\n print(contents())\nelse:\n print(where())\n", ["argparse", "certifi"]], "h11": [".py", "\n\n\n\n\n\n\n\nfrom h11._connection import Connection,NEED_DATA,PAUSED\nfrom h11._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom h11._state import(\nCLIENT,\nCLOSED,\nDONE,\nERROR,\nIDLE,\nMIGHT_SWITCH_PROTOCOL,\nMUST_CLOSE,\nSEND_BODY,\nSEND_RESPONSE,\nSERVER,\nSWITCHED_PROTOCOL,\n)\nfrom h11._util import LocalProtocolError,ProtocolError,RemoteProtocolError\nfrom h11._version import __version__\n\nPRODUCT_ID=\"python-h11/\"+__version__\n\n\n__all__=(\n\"Connection\",\n\"NEED_DATA\",\n\"PAUSED\",\n\"ConnectionClosed\",\n\"Data\",\n\"EndOfMessage\",\n\"Event\",\n\"InformationalResponse\",\n\"Request\",\n\"Response\",\n\"CLIENT\",\n\"CLOSED\",\n\"DONE\",\n\"ERROR\",\n\"IDLE\",\n\"MUST_CLOSE\",\n\"SEND_BODY\",\n\"SEND_RESPONSE\",\n\"SERVER\",\n\"SWITCHED_PROTOCOL\",\n\"ProtocolError\",\n\"LocalProtocolError\",\n\"RemoteProtocolError\",\n)\n", ["h11._connection", "h11._events", "h11._state", "h11._util", "h11._version"], 1], "h11._writers": [".py", "\n\n\n\n\n\n\n\n\nfrom typing import Any,Callable,Dict,List,Tuple,Type,Union\n\nfrom._events import Data,EndOfMessage,Event,InformationalResponse,Request,Response\nfrom._headers import Headers\nfrom._state import CLIENT,IDLE,SEND_BODY,SEND_RESPONSE,SERVER\nfrom._util import LocalProtocolError,Sentinel\n\n__all__=[\"WRITERS\"]\n\nWriter=Callable[[bytes],Any]\n\n\ndef write_headers(headers:Headers,write:Writer)->None:\n\n\n\n raw_items=headers._full_items\n for raw_name,name,value in raw_items:\n  if name ==b\"host\":\n   write(b\"%s: %s\\r\\n\"%(raw_name,value))\n for raw_name,name,value in raw_items:\n  if name !=b\"host\":\n   write(b\"%s: %s\\r\\n\"%(raw_name,value))\n write(b\"\\r\\n\")\n \n \ndef write_request(request:Request,write:Writer)->None:\n if request.http_version !=b\"1.1\":\n  raise LocalProtocolError(\"I only send HTTP/1.1\")\n write(b\"%s %s HTTP/1.1\\r\\n\"%(request.method,request.target))\n write_headers(request.headers,write)\n \n \n \ndef write_any_response(\nresponse:Union[InformationalResponse,Response],write:Writer\n)->None:\n if response.http_version !=b\"1.1\":\n  raise LocalProtocolError(\"I only send HTTP/1.1\")\n status_bytes=str(response.status_code).encode(\"ascii\")\n \n \n \n \n \n \n \n \n write(b\"HTTP/1.1 %s %s\\r\\n\"%(status_bytes,response.reason))\n write_headers(response.headers,write)\n \n \nclass BodyWriter:\n def __call__(self,event:Event,write:Writer)->None:\n  if type(event)is Data:\n   self.send_data(event.data,write)\n  elif type(event)is EndOfMessage:\n   self.send_eom(event.headers,write)\n  else:\n   assert False\n   \n def send_data(self,data:bytes,write:Writer)->None:\n  pass\n  \n def send_eom(self,headers:Headers,write:Writer)->None:\n  pass\n  \n  \n  \n  \n  \n  \n  \n  \nclass ContentLengthWriter(BodyWriter):\n def __init__(self,length:int)->None:\n  self._length=length\n  \n def send_data(self,data:bytes,write:Writer)->None:\n  self._length -=len(data)\n  if self._length <0:\n   raise LocalProtocolError(\"Too much data for declared Content-Length\")\n  write(data)\n  \n def send_eom(self,headers:Headers,write:Writer)->None:\n  if self._length !=0:\n   raise LocalProtocolError(\"Too little data for declared Content-Length\")\n  if headers:\n   raise LocalProtocolError(\"Content-Length and trailers don't mix\")\n   \n   \nclass ChunkedWriter(BodyWriter):\n def send_data(self,data:bytes,write:Writer)->None:\n \n \n  if not data:\n   return\n  write(b\"%x\\r\\n\"%len(data))\n  write(data)\n  write(b\"\\r\\n\")\n  \n def send_eom(self,headers:Headers,write:Writer)->None:\n  write(b\"0\\r\\n\")\n  write_headers(headers,write)\n  \n  \nclass Http10Writer(BodyWriter):\n def send_data(self,data:bytes,write:Writer)->None:\n  write(data)\n  \n def send_eom(self,headers:Headers,write:Writer)->None:\n  if headers:\n   raise LocalProtocolError(\"can't send trailers to HTTP/1.0 client\")\n   \n   \n   \n   \nWritersType=Dict[\nUnion[Tuple[Type[Sentinel],Type[Sentinel]],Type[Sentinel]],\nUnion[\nDict[str,Type[BodyWriter]],\nCallable[[Union[InformationalResponse,Response],Writer],None],\nCallable[[Request,Writer],None],\n],\n]\n\nWRITERS:WritersType={\n(CLIENT,IDLE):write_request,\n(SERVER,IDLE):write_any_response,\n(SERVER,SEND_RESPONSE):write_any_response,\nSEND_BODY:{\n\"chunked\":ChunkedWriter,\n\"content-length\":ContentLengthWriter,\n\"http/1.0\":Http10Writer,\n},\n}\n", ["h11._events", "h11._headers", "h11._state", "h11._util", "typing"]], "h11._abnf": [".py", "\n\n\n\n\n\n\nOWS=r\"[ \\t]*\"\n\n\n\n\n\n\n\n\ntoken=r\"[-!#$%&'*+.^_`|~0-9a-zA-Z]+\"\n\n\n\nfield_name=token\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvchar=r\"[\\x21-\\x7e]\"\nvchar_or_obs_text=r\"[^\\x00\\s]\"\nfield_vchar=vchar_or_obs_text\nfield_content=r\"{field_vchar}+(?:[ \\t]+{field_vchar}+)*\".format(**globals())\n\n\n\nfield_value=r\"({field_content})?\".format(**globals())\n\n\nheader_field=(\nr\"(?P<field_name>{field_name})\"\nr\":\"\nr\"{OWS}\"\nr\"(?P<field_value>{field_value})\"\nr\"{OWS}\".format(**globals())\n)\n\n\n\n\n\n\n\n\n\n\n\nmethod=token\nrequest_target=r\"{vchar}+\".format(**globals())\nhttp_version=r\"HTTP/(?P<http_version>[0-9]\\.[0-9])\"\nrequest_line=(\nr\"(?P<method>{method})\"\nr\" \"\nr\"(?P<target>{request_target})\"\nr\" \"\nr\"{http_version}\".format(**globals())\n)\n\n\n\n\n\n\nstatus_code=r\"[0-9]{3}\"\nreason_phrase=r\"([ \\t]|{vchar_or_obs_text})*\".format(**globals())\nstatus_line=(\nr\"{http_version}\"\nr\" \"\nr\"(?P<status_code>{status_code})\"\n\n\n\n\n\nr\"(?: (?P<reason>{reason_phrase}))?\".format(**globals())\n)\n\nHEXDIG=r\"[0-9A-Fa-f]\"\n\n\n\n\n\nchunk_size=r\"({HEXDIG}){{1,20}}\".format(**globals())\n\n\n\n\n\nchunk_ext=r\";.*\"\nchunk_header=(\nr\"(?P<chunk_size>{chunk_size})\"\nr\"(?P<chunk_ext>{chunk_ext})?\"\nr\"{OWS}\\r\\n\".format(\n**globals()\n)\n\n)\n", []], "h11._headers": [".py", "import re\nfrom typing import AnyStr,cast,List,overload,Sequence,Tuple,TYPE_CHECKING,Union\n\nfrom._abnf import field_name,field_value\nfrom._util import bytesify,LocalProtocolError,validate\n\nif TYPE_CHECKING:\n from._events import Request\n \ntry:\n from typing import Literal\nexcept ImportError:\n from typing_extensions import Literal\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n_content_length_re=re.compile(rb\"[0-9]+\")\n_field_name_re=re.compile(field_name.encode(\"ascii\"))\n_field_value_re=re.compile(field_value.encode(\"ascii\"))\n\n\nclass Headers(Sequence[Tuple[bytes,bytes]]):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__=\"_full_items\"\n \n def __init__(self,full_items:List[Tuple[bytes,bytes,bytes]])->None:\n  self._full_items=full_items\n  \n def __bool__(self)->bool:\n  return bool(self._full_items)\n  \n def __eq__(self,other:object)->bool:\n  return list(self)==list(other)\n  \n def __len__(self)->int:\n  return len(self._full_items)\n  \n def __repr__(self)->str:\n  return \"<Headers(%s)>\"%repr(list(self))\n  \n def __getitem__(self,idx:int)->Tuple[bytes,bytes]:\n  _,name,value=self._full_items[idx]\n  return(name,value)\n  \n def raw_items(self)->List[Tuple[bytes,bytes]]:\n  return[(raw_name,value)for raw_name,_,value in self._full_items]\n  \n  \nHeaderTypes=Union[\nList[Tuple[bytes,bytes]],\nList[Tuple[bytes,str]],\nList[Tuple[str,bytes]],\nList[Tuple[str,str]],\n]\n\n\n@overload\ndef normalize_and_validate(headers:Headers,_parsed:Literal[True])->Headers:\n ...\n \n \n@overload\ndef normalize_and_validate(headers:HeaderTypes,_parsed:Literal[False])->Headers:\n ...\n \n \n@overload\ndef normalize_and_validate(\nheaders:Union[Headers,HeaderTypes],_parsed:bool=False\n)->Headers:\n ...\n \n \ndef normalize_and_validate(\nheaders:Union[Headers,HeaderTypes],_parsed:bool=False\n)->Headers:\n new_headers=[]\n seen_content_length=None\n saw_transfer_encoding=False\n for name,value in headers:\n \n \n \n  if not _parsed:\n   name=bytesify(name)\n   value=bytesify(value)\n   validate(_field_name_re,name,\"Illegal header name {!r}\",name)\n   validate(_field_value_re,value,\"Illegal header value {!r}\",value)\n  assert isinstance(name,bytes)\n  assert isinstance(value,bytes)\n  \n  raw_name=name\n  name=name.lower()\n  if name ==b\"content-length\":\n   lengths={length.strip()for length in value.split(b\",\")}\n   if len(lengths)!=1:\n    raise LocalProtocolError(\"conflicting Content-Length headers\")\n   value=lengths.pop()\n   validate(_content_length_re,value,\"bad Content-Length\")\n   if seen_content_length is None:\n    seen_content_length=value\n    new_headers.append((raw_name,name,value))\n   elif seen_content_length !=value:\n    raise LocalProtocolError(\"conflicting Content-Length headers\")\n  elif name ==b\"transfer-encoding\":\n  \n  \n  \n  \n   if saw_transfer_encoding:\n    raise LocalProtocolError(\n    \"multiple Transfer-Encoding headers\",error_status_hint=501\n    )\n    \n    \n   value=value.lower()\n   if value !=b\"chunked\":\n    raise LocalProtocolError(\n    \"Only Transfer-Encoding: chunked is supported\",\n    error_status_hint=501,\n    )\n   saw_transfer_encoding=True\n   new_headers.append((raw_name,name,value))\n  else:\n   new_headers.append((raw_name,name,value))\n return Headers(new_headers)\n \n \ndef get_comma_header(headers:Headers,name:bytes)->List[bytes]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n out:List[bytes]=[]\n for _,found_name,found_raw_value in headers._full_items:\n  if found_name ==name:\n   found_raw_value=found_raw_value.lower()\n   for found_split_value in found_raw_value.split(b\",\"):\n    found_split_value=found_split_value.strip()\n    if found_split_value:\n     out.append(found_split_value)\n return out\n \n \ndef set_comma_header(headers:Headers,name:bytes,new_values:List[bytes])->Headers:\n\n\n\n\n\n\n\n\n\n new_headers:List[Tuple[bytes,bytes]]=[]\n for found_raw_name,found_name,found_raw_value in headers._full_items:\n  if found_name !=name:\n   new_headers.append((found_raw_name,found_raw_value))\n for new_value in new_values:\n  new_headers.append((name.title(),new_value))\n return normalize_and_validate(new_headers)\n \n \ndef has_expect_100_continue(request:\"Request\")->bool:\n\n\n\n if request.http_version <b\"1.1\":\n  return False\n expect=get_comma_header(request.headers,b\"expect\")\n return b\"100-continue\"in expect\n", ["h11._abnf", "h11._events", "h11._util", "re", "typing", "typing_extensions"]], "h11._readers": [".py", "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport re\nfrom typing import Any,Callable,Dict,Iterable,NoReturn,Optional,Tuple,Type,Union\n\nfrom._abnf import chunk_header,header_field,request_line,status_line\nfrom._events import Data,EndOfMessage,InformationalResponse,Request,Response\nfrom._receivebuffer import ReceiveBuffer\nfrom._state import(\nCLIENT,\nCLOSED,\nDONE,\nIDLE,\nMUST_CLOSE,\nSEND_BODY,\nSEND_RESPONSE,\nSERVER,\n)\nfrom._util import LocalProtocolError,RemoteProtocolError,Sentinel,validate\n\n__all__=[\"READERS\"]\n\nheader_field_re=re.compile(header_field.encode(\"ascii\"))\nobs_fold_re=re.compile(rb\"[ \\t]+\")\n\n\ndef _obsolete_line_fold(lines:Iterable[bytes])->Iterable[bytes]:\n it=iter(lines)\n last:Optional[bytes]=None\n for line in it:\n  match=obs_fold_re.match(line)\n  if match:\n   if last is None:\n    raise LocalProtocolError(\"continuation line at start of headers\")\n   if not isinstance(last,bytearray):\n   \n    last=bytearray(last)\n   last +=b\" \"\n   last +=line[match.end():]\n  else:\n   if last is not None:\n    yield last\n   last=line\n if last is not None:\n  yield last\n  \n  \ndef _decode_header_lines(\nlines:Iterable[bytes],\n)->Iterable[Tuple[bytes,bytes]]:\n for line in _obsolete_line_fold(lines):\n  matches=validate(header_field_re,line,\"illegal header line: {!r}\",line)\n  yield(matches[\"field_name\"],matches[\"field_value\"])\n  \n  \nrequest_line_re=re.compile(request_line.encode(\"ascii\"))\n\n\ndef maybe_read_from_IDLE_client(buf:ReceiveBuffer)->Optional[Request]:\n lines=buf.maybe_extract_lines()\n if lines is None:\n  if buf.is_next_line_obviously_invalid_request_line():\n   raise LocalProtocolError(\"illegal request line\")\n  return None\n if not lines:\n  raise LocalProtocolError(\"no request line received\")\n matches=validate(\n request_line_re,lines[0],\"illegal request line: {!r}\",lines[0]\n )\n return Request(\n headers=list(_decode_header_lines(lines[1:])),_parsed=True,**matches\n )\n \n \nstatus_line_re=re.compile(status_line.encode(\"ascii\"))\n\n\ndef maybe_read_from_SEND_RESPONSE_server(\nbuf:ReceiveBuffer,\n)->Union[InformationalResponse,Response,None]:\n lines=buf.maybe_extract_lines()\n if lines is None:\n  if buf.is_next_line_obviously_invalid_request_line():\n   raise LocalProtocolError(\"illegal request line\")\n  return None\n if not lines:\n  raise LocalProtocolError(\"no response line received\")\n matches=validate(status_line_re,lines[0],\"illegal status line: {!r}\",lines[0])\n http_version=(\n b\"1.1\"if matches[\"http_version\"]is None else matches[\"http_version\"]\n )\n reason=b\"\"if matches[\"reason\"]is None else matches[\"reason\"]\n status_code=int(matches[\"status_code\"])\n class_:Union[Type[InformationalResponse],Type[Response]]=(\n InformationalResponse if status_code <200 else Response\n )\n return class_(\n headers=list(_decode_header_lines(lines[1:])),\n _parsed=True,\n status_code=status_code,\n reason=reason,\n http_version=http_version,\n )\n \n \nclass ContentLengthReader:\n def __init__(self,length:int)->None:\n  self._length=length\n  self._remaining=length\n  \n def __call__(self,buf:ReceiveBuffer)->Union[Data,EndOfMessage,None]:\n  if self._remaining ==0:\n   return EndOfMessage()\n  data=buf.maybe_extract_at_most(self._remaining)\n  if data is None:\n   return None\n  self._remaining -=len(data)\n  return Data(data=data)\n  \n def read_eof(self)->NoReturn:\n  raise RemoteProtocolError(\n  \"peer closed connection without sending complete message body \"\n  \"(received {} bytes, expected {})\".format(\n  self._length -self._remaining,self._length\n  )\n  )\n  \n  \nchunk_header_re=re.compile(chunk_header.encode(\"ascii\"))\n\n\nclass ChunkedReader:\n def __init__(self)->None:\n  self._bytes_in_chunk=0\n  \n  \n  \n  self._bytes_to_discard=0\n  self._reading_trailer=False\n  \n def __call__(self,buf:ReceiveBuffer)->Union[Data,EndOfMessage,None]:\n  if self._reading_trailer:\n   lines=buf.maybe_extract_lines()\n   if lines is None:\n    return None\n   return EndOfMessage(headers=list(_decode_header_lines(lines)))\n  if self._bytes_to_discard >0:\n   data=buf.maybe_extract_at_most(self._bytes_to_discard)\n   if data is None:\n    return None\n   self._bytes_to_discard -=len(data)\n   if self._bytes_to_discard >0:\n    return None\n    \n  assert self._bytes_to_discard ==0\n  if self._bytes_in_chunk ==0:\n  \n   chunk_header=buf.maybe_extract_next_line()\n   if chunk_header is None:\n    return None\n   matches=validate(\n   chunk_header_re,\n   chunk_header,\n   \"illegal chunk header: {!r}\",\n   chunk_header,\n   )\n   \n   self._bytes_in_chunk=int(matches[\"chunk_size\"],base=16)\n   if self._bytes_in_chunk ==0:\n    self._reading_trailer=True\n    return self(buf)\n   chunk_start=True\n  else:\n   chunk_start=False\n  assert self._bytes_in_chunk >0\n  data=buf.maybe_extract_at_most(self._bytes_in_chunk)\n  if data is None:\n   return None\n  self._bytes_in_chunk -=len(data)\n  if self._bytes_in_chunk ==0:\n   self._bytes_to_discard=2\n   chunk_end=True\n  else:\n   chunk_end=False\n  return Data(data=data,chunk_start=chunk_start,chunk_end=chunk_end)\n  \n def read_eof(self)->NoReturn:\n  raise RemoteProtocolError(\n  \"peer closed connection without sending complete message body \"\n  \"(incomplete chunked read)\"\n  )\n  \n  \nclass Http10Reader:\n def __call__(self,buf:ReceiveBuffer)->Optional[Data]:\n  data=buf.maybe_extract_at_most(999999999)\n  if data is None:\n   return None\n  return Data(data=data)\n  \n def read_eof(self)->EndOfMessage:\n  return EndOfMessage()\n  \n  \ndef expect_nothing(buf:ReceiveBuffer)->None:\n if buf:\n  raise LocalProtocolError(\"Got data when expecting EOF\")\n return None\n \n \nReadersType=Dict[\nUnion[Type[Sentinel],Tuple[Type[Sentinel],Type[Sentinel]]],\nUnion[Callable[...,Any],Dict[str,Callable[...,Any]]],\n]\n\nREADERS:ReadersType={\n(CLIENT,IDLE):maybe_read_from_IDLE_client,\n(SERVER,IDLE):maybe_read_from_SEND_RESPONSE_server,\n(SERVER,SEND_RESPONSE):maybe_read_from_SEND_RESPONSE_server,\n(CLIENT,DONE):expect_nothing,\n(CLIENT,MUST_CLOSE):expect_nothing,\n(CLIENT,CLOSED):expect_nothing,\n(SERVER,DONE):expect_nothing,\n(SERVER,MUST_CLOSE):expect_nothing,\n(SERVER,CLOSED):expect_nothing,\nSEND_BODY:{\n\"chunked\":ChunkedReader,\n\"content-length\":ContentLengthReader,\n\"http/1.0\":Http10Reader,\n},\n}\n", ["h11._abnf", "h11._events", "h11._receivebuffer", "h11._state", "h11._util", "re", "typing"]], "h11._version": [".py", "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__version__=\"0.14.0\"\n", []], "h11._events": [".py", "\n\n\n\n\n\n\nimport re\nfrom abc import ABC\nfrom dataclasses import dataclass,field\nfrom typing import Any,cast,Dict,List,Tuple,Union\n\nfrom._abnf import method,request_target\nfrom._headers import Headers,normalize_and_validate\nfrom._util import bytesify,LocalProtocolError,validate\n\n\n__all__=[\n\"Event\",\n\"Request\",\n\"InformationalResponse\",\n\"Response\",\n\"Data\",\n\"EndOfMessage\",\n\"ConnectionClosed\",\n]\n\nmethod_re=re.compile(method.encode(\"ascii\"))\nrequest_target_re=re.compile(request_target.encode(\"ascii\"))\n\n\nclass Event(ABC):\n ''\n\n \n \n __slots__=()\n \n \n@dataclass(init=False,frozen=True)\nclass Request(Event):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__=(\"method\",\"headers\",\"target\",\"http_version\")\n \n method:bytes\n headers:Headers\n target:bytes\n http_version:bytes\n \n def __init__(\n self,\n *,\n method:Union[bytes,str],\n headers:Union[Headers,List[Tuple[bytes,bytes]],List[Tuple[str,str]]],\n target:Union[bytes,str],\n http_version:Union[bytes,str]=b\"1.1\",\n _parsed:bool=False,\n )->None:\n  super().__init__()\n  if isinstance(headers,Headers):\n   object.__setattr__(self,\"headers\",headers)\n  else:\n   object.__setattr__(\n   self,\"headers\",normalize_and_validate(headers,_parsed=_parsed)\n   )\n  if not _parsed:\n   object.__setattr__(self,\"method\",bytesify(method))\n   object.__setattr__(self,\"target\",bytesify(target))\n   object.__setattr__(self,\"http_version\",bytesify(http_version))\n  else:\n   object.__setattr__(self,\"method\",method)\n   object.__setattr__(self,\"target\",target)\n   object.__setattr__(self,\"http_version\",http_version)\n   \n   \n   \n   \n   \n   \n  host_count=0\n  for name,value in self.headers:\n   if name ==b\"host\":\n    host_count +=1\n  if self.http_version ==b\"1.1\"and host_count ==0:\n   raise LocalProtocolError(\"Missing mandatory Host: header\")\n  if host_count >1:\n   raise LocalProtocolError(\"Found multiple Host: headers\")\n   \n  validate(method_re,self.method,\"Illegal method characters\")\n  validate(request_target_re,self.target,\"Illegal target characters\")\n  \n  \n __hash__=None\n \n \n@dataclass(init=False,frozen=True)\nclass _ResponseBase(Event):\n __slots__=(\"headers\",\"http_version\",\"reason\",\"status_code\")\n \n headers:Headers\n http_version:bytes\n reason:bytes\n status_code:int\n \n def __init__(\n self,\n *,\n headers:Union[Headers,List[Tuple[bytes,bytes]],List[Tuple[str,str]]],\n status_code:int,\n http_version:Union[bytes,str]=b\"1.1\",\n reason:Union[bytes,str]=b\"\",\n _parsed:bool=False,\n )->None:\n  super().__init__()\n  if isinstance(headers,Headers):\n   object.__setattr__(self,\"headers\",headers)\n  else:\n   object.__setattr__(\n   self,\"headers\",normalize_and_validate(headers,_parsed=_parsed)\n   )\n  if not _parsed:\n   object.__setattr__(self,\"reason\",bytesify(reason))\n   object.__setattr__(self,\"http_version\",bytesify(http_version))\n   if not isinstance(status_code,int):\n    raise LocalProtocolError(\"status code must be integer\")\n    \n    \n   object.__setattr__(self,\"status_code\",int(status_code))\n  else:\n   object.__setattr__(self,\"reason\",reason)\n   object.__setattr__(self,\"http_version\",http_version)\n   object.__setattr__(self,\"status_code\",status_code)\n   \n  self.__post_init__()\n  \n def __post_init__(self)->None:\n  pass\n  \n  \n __hash__=None\n \n \n@dataclass(init=False,frozen=True)\nclass InformationalResponse(_ResponseBase):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __post_init__(self)->None:\n  if not(100 <=self.status_code <200):\n   raise LocalProtocolError(\n   \"InformationalResponse status_code should be in range \"\n   \"[100, 200), not {}\".format(self.status_code)\n   )\n   \n   \n __hash__=None\n \n \n@dataclass(init=False,frozen=True)\nclass Response(_ResponseBase):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __post_init__(self)->None:\n  if not(200 <=self.status_code <1000):\n   raise LocalProtocolError(\n   \"Response status_code should be in range [200, 1000), not {}\".format(\n   self.status_code\n   )\n   )\n   \n   \n __hash__=None\n \n \n@dataclass(init=False,frozen=True)\nclass Data(Event):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__=(\"data\",\"chunk_start\",\"chunk_end\")\n \n data:bytes\n chunk_start:bool\n chunk_end:bool\n \n def __init__(\n self,data:bytes,chunk_start:bool=False,chunk_end:bool=False\n )->None:\n  object.__setattr__(self,\"data\",data)\n  object.__setattr__(self,\"chunk_start\",chunk_start)\n  object.__setattr__(self,\"chunk_end\",chunk_end)\n  \n  \n __hash__=None\n \n \n \n \n \n \n \n@dataclass(init=False,frozen=True)\nclass EndOfMessage(Event):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n __slots__=(\"headers\",)\n \n headers:Headers\n \n def __init__(\n self,\n *,\n headers:Union[\n Headers,List[Tuple[bytes,bytes]],List[Tuple[str,str]],None\n ]=None,\n _parsed:bool=False,\n )->None:\n  super().__init__()\n  if headers is None:\n   headers=Headers([])\n  elif not isinstance(headers,Headers):\n   headers=normalize_and_validate(headers,_parsed=_parsed)\n   \n  object.__setattr__(self,\"headers\",headers)\n  \n  \n __hash__=None\n \n \n@dataclass(frozen=True)\nclass ConnectionClosed(Event):\n ''\n\n\n\n\n\n\n\n \n \n pass\n", ["abc", "dataclasses", "h11._abnf", "h11._headers", "h11._util", "re", "typing"]], "h11._util": [".py", "from typing import Any,Dict,NoReturn,Pattern,Tuple,Type,TypeVar,Union\n\n__all__=[\n\"ProtocolError\",\n\"LocalProtocolError\",\n\"RemoteProtocolError\",\n\"validate\",\n\"bytesify\",\n]\n\n\nclass ProtocolError(Exception):\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(self,msg:str,error_status_hint:int=400)->None:\n  if type(self)is ProtocolError:\n   raise TypeError(\"tried to directly instantiate ProtocolError\")\n  Exception.__init__(self,msg)\n  self.error_status_hint=error_status_hint\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \nclass LocalProtocolError(ProtocolError):\n def _reraise_as_remote_protocol_error(self)->NoReturn:\n \n \n \n \n \n \n  self.__class__=RemoteProtocolError\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  raise self\n  \n  \nclass RemoteProtocolError(ProtocolError):\n pass\n \n \ndef validate(\nregex:Pattern[bytes],data:bytes,msg:str=\"malformed data\",*format_args:Any\n)->Dict[str,bytes]:\n match=regex.fullmatch(data)\n if not match:\n  if format_args:\n   msg=msg.format(*format_args)\n  raise LocalProtocolError(msg)\n return match.groupdict()\n \n \n \n \n \n \n \n \n \n \n \n_T_Sentinel=TypeVar(\"_T_Sentinel\",bound=\"Sentinel\")\n\n\nclass Sentinel(type):\n def __new__(\n cls:Type[_T_Sentinel],\n name:str,\n bases:Tuple[type,...],\n namespace:Dict[str,Any],\n **kwds:Any\n )->_T_Sentinel:\n  assert bases ==(Sentinel,)\n  v=super().__new__(cls,name,bases,namespace,**kwds)\n  v.__class__=v\n  return v\n  \n def __repr__(self)->str:\n  return self.__name__\n  \n  \n  \n  \n  \ndef bytesify(s:Union[bytes,bytearray,memoryview,int,str])->bytes:\n\n if type(s)is bytes:\n  return s\n if isinstance(s,str):\n  s=s.encode(\"ascii\")\n if isinstance(s,int):\n  raise TypeError(\"expected bytes-like object, not int\")\n return bytes(s)\n", ["typing"]], "h11._connection": [".py", "\n\nfrom typing import Any,Callable,cast,Dict,List,Optional,Tuple,Type,Union\n\nfrom._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom._headers import get_comma_header,has_expect_100_continue,set_comma_header\nfrom._readers import READERS,ReadersType\nfrom._receivebuffer import ReceiveBuffer\nfrom._state import(\n_SWITCH_CONNECT,\n_SWITCH_UPGRADE,\nCLIENT,\nConnectionState,\nDONE,\nERROR,\nMIGHT_SWITCH_PROTOCOL,\nSEND_BODY,\nSERVER,\nSWITCHED_PROTOCOL,\n)\nfrom._util import(\nLocalProtocolError,\nRemoteProtocolError,\nSentinel,\n)\nfrom._writers import WRITERS,WritersType\n\n\n__all__=[\"Connection\",\"NEED_DATA\",\"PAUSED\"]\n\n\nclass NEED_DATA(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass PAUSED(Sentinel,metaclass=Sentinel):\n pass\n \n \n \n \n \n \n \n \n \n \n \n \nDEFAULT_MAX_INCOMPLETE_EVENT_SIZE=16 *1024\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef _keep_alive(event:Union[Request,Response])->bool:\n connection=get_comma_header(event.headers,b\"connection\")\n if b\"close\"in connection:\n  return False\n if getattr(event,\"http_version\",b\"1.1\")<b\"1.1\":\n  return False\n return True\n \n \ndef _body_framing(\nrequest_method:bytes,event:Union[Request,Response]\n)->Tuple[str,Union[Tuple[()],Tuple[int]]]:\n\n\n\n\n assert type(event)in(Request,Response)\n \n \n \n \n \n \n \n \n \n \n \n \n \n if type(event)is Response:\n  if(\n  event.status_code in(204,304)\n  or request_method ==b\"HEAD\"\n  or(request_method ==b\"CONNECT\"and 200 <=event.status_code <300)\n  ):\n   return(\"content-length\",(0,))\n   \n   \n   \n  assert event.status_code >=200\n  \n  \n transfer_encodings=get_comma_header(event.headers,b\"transfer-encoding\")\n if transfer_encodings:\n  assert transfer_encodings ==[b\"chunked\"]\n  return(\"chunked\",())\n  \n  \n content_lengths=get_comma_header(event.headers,b\"content-length\")\n if content_lengths:\n  return(\"content-length\",(int(content_lengths[0]),))\n  \n  \n if type(event)is Request:\n  return(\"content-length\",(0,))\n else:\n  return(\"http/1.0\",())\n  \n  \n  \n  \n  \n  \n  \n  \n  \nclass Connection:\n ''\n\n\n\n\n\n\n\n\n\n\n\n\n \n \n def __init__(\n self,\n our_role:Type[Sentinel],\n max_incomplete_event_size:int=DEFAULT_MAX_INCOMPLETE_EVENT_SIZE,\n )->None:\n  self._max_incomplete_event_size=max_incomplete_event_size\n  \n  if our_role not in(CLIENT,SERVER):\n   raise ValueError(\"expected CLIENT or SERVER, not {!r}\".format(our_role))\n  self.our_role=our_role\n  self.their_role:Type[Sentinel]\n  if our_role is CLIENT:\n   self.their_role=SERVER\n  else:\n   self.their_role=CLIENT\n  self._cstate=ConnectionState()\n  \n  \n  \n  self._writer=self._get_io_object(self.our_role,None,WRITERS)\n  self._reader=self._get_io_object(self.their_role,None,READERS)\n  \n  \n  self._receive_buffer=ReceiveBuffer()\n  \n  \n  self._receive_buffer_closed=False\n  \n  \n  \n  \n  \n  \n  self.their_http_version:Optional[bytes]=None\n  self._request_method:Optional[bytes]=None\n  \n  \n  self.client_is_waiting_for_100_continue=False\n  \n @property\n def states(self)->Dict[Type[Sentinel],Type[Sentinel]]:\n  ''\n\n\n\n\n\n  \n  return dict(self._cstate.states)\n  \n @property\n def our_state(self)->Type[Sentinel]:\n  ''\n\n  \n  return self._cstate.states[self.our_role]\n  \n @property\n def their_state(self)->Type[Sentinel]:\n  ''\n\n  \n  return self._cstate.states[self.their_role]\n  \n @property\n def they_are_waiting_for_100_continue(self)->bool:\n  return self.their_role is CLIENT and self.client_is_waiting_for_100_continue\n  \n def start_next_cycle(self)->None:\n  ''\n\n\n\n\n\n\n\n\n\n  \n  old_states=dict(self._cstate.states)\n  self._cstate.start_next_cycle()\n  self._request_method=None\n  \n  \n  assert not self.client_is_waiting_for_100_continue\n  self._respond_to_state_changes(old_states)\n  \n def _process_error(self,role:Type[Sentinel])->None:\n  old_states=dict(self._cstate.states)\n  self._cstate.process_error(role)\n  self._respond_to_state_changes(old_states)\n  \n def _server_switch_event(self,event:Event)->Optional[Type[Sentinel]]:\n  if type(event)is InformationalResponse and event.status_code ==101:\n   return _SWITCH_UPGRADE\n  if type(event)is Response:\n   if(\n   _SWITCH_CONNECT in self._cstate.pending_switch_proposals\n   and 200 <=event.status_code <300\n   ):\n    return _SWITCH_CONNECT\n  return None\n  \n  \n def _process_event(self,role:Type[Sentinel],event:Event)->None:\n \n \n  old_states=dict(self._cstate.states)\n  if role is CLIENT and type(event)is Request:\n   if event.method ==b\"CONNECT\":\n    self._cstate.process_client_switch_proposal(_SWITCH_CONNECT)\n   if get_comma_header(event.headers,b\"upgrade\"):\n    self._cstate.process_client_switch_proposal(_SWITCH_UPGRADE)\n  server_switch_event=None\n  if role is SERVER:\n   server_switch_event=self._server_switch_event(event)\n  self._cstate.process_event(role,type(event),server_switch_event)\n  \n  \n  \n  if type(event)is Request:\n   self._request_method=event.method\n   \n  if role is self.their_role and type(event)in(\n  Request,\n  Response,\n  InformationalResponse,\n  ):\n   event=cast(Union[Request,Response,InformationalResponse],event)\n   self.their_http_version=event.http_version\n   \n   \n   \n   \n   \n   \n   \n  if type(event)in(Request,Response)and not _keep_alive(\n  cast(Union[Request,Response],event)\n  ):\n   self._cstate.process_keep_alive_disabled()\n   \n   \n  if type(event)is Request and has_expect_100_continue(event):\n   self.client_is_waiting_for_100_continue=True\n  if type(event)in(InformationalResponse,Response):\n   self.client_is_waiting_for_100_continue=False\n  if role is CLIENT and type(event)in(Data,EndOfMessage):\n   self.client_is_waiting_for_100_continue=False\n   \n  self._respond_to_state_changes(old_states,event)\n  \n def _get_io_object(\n self,\n role:Type[Sentinel],\n event:Optional[Event],\n io_dict:Union[ReadersType,WritersType],\n )->Optional[Callable[...,Any]]:\n \n  state=self._cstate.states[role]\n  if state is SEND_BODY:\n  \n  \n   framing_type,args=_body_framing(\n   cast(bytes,self._request_method),cast(Union[Request,Response],event)\n   )\n   return io_dict[SEND_BODY][framing_type](*args)\n  else:\n  \n  \n   return io_dict.get((role,state))\n   \n   \n   \n def _respond_to_state_changes(\n self,\n old_states:Dict[Type[Sentinel],Type[Sentinel]],\n event:Optional[Event]=None,\n )->None:\n \n  if self.our_state !=old_states[self.our_role]:\n   self._writer=self._get_io_object(self.our_role,event,WRITERS)\n  if self.their_state !=old_states[self.their_role]:\n   self._reader=self._get_io_object(self.their_role,event,READERS)\n   \n @property\n def trailing_data(self)->Tuple[bytes,bool]:\n  ''\n\n\n\n\n\n  \n  return(bytes(self._receive_buffer),self._receive_buffer_closed)\n  \n def receive_data(self,data:bytes)->None:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  if data:\n   if self._receive_buffer_closed:\n    raise RuntimeError(\"received close, then received more data?\")\n   self._receive_buffer +=data\n  else:\n   self._receive_buffer_closed=True\n   \n def _extract_next_receive_event(\n self,\n )->Union[Event,Type[NEED_DATA],Type[PAUSED]]:\n  state=self.their_state\n  \n  \n  \n  \n  if state is DONE and self._receive_buffer:\n   return PAUSED\n  if state is MIGHT_SWITCH_PROTOCOL or state is SWITCHED_PROTOCOL:\n   return PAUSED\n  assert self._reader is not None\n  event=self._reader(self._receive_buffer)\n  if event is None:\n   if not self._receive_buffer and self._receive_buffer_closed:\n   \n   \n   \n   \n    if hasattr(self._reader,\"read_eof\"):\n     event=self._reader.read_eof()\n    else:\n     event=ConnectionClosed()\n  if event is None:\n   event=NEED_DATA\n  return event\n  \n def next_event(self)->Union[Event,Type[NEED_DATA],Type[PAUSED]]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n  if self.their_state is ERROR:\n   raise RemoteProtocolError(\"Can't receive data when peer state is ERROR\")\n  try:\n   event=self._extract_next_receive_event()\n   if event not in[NEED_DATA,PAUSED]:\n    self._process_event(self.their_role,cast(Event,event))\n   if event is NEED_DATA:\n    if len(self._receive_buffer)>self._max_incomplete_event_size:\n    \n    \n     raise RemoteProtocolError(\n     \"Receive buffer too long\",error_status_hint=431\n     )\n    if self._receive_buffer_closed:\n    \n    \n     raise RemoteProtocolError(\"peer unexpectedly closed connection\")\n   return event\n  except BaseException as exc:\n   self._process_error(self.their_role)\n   if isinstance(exc,LocalProtocolError):\n    exc._reraise_as_remote_protocol_error()\n   else:\n    raise\n    \n def send(self,event:Event)->Optional[bytes]:\n  ''\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  data_list=self.send_with_data_passthrough(event)\n  if data_list is None:\n   return None\n  else:\n   return b\"\".join(data_list)\n   \n def send_with_data_passthrough(self,event:Event)->Optional[List[bytes]]:\n  ''\n\n\n\n\n\n  \n  if self.our_state is ERROR:\n   raise LocalProtocolError(\"Can't send data when our state is ERROR\")\n  try:\n   if type(event)is Response:\n    event=self._clean_up_response_headers_for_sending(event)\n    \n    \n    \n    \n    \n   writer=self._writer\n   self._process_event(self.our_role,event)\n   if type(event)is ConnectionClosed:\n    return None\n   else:\n   \n   \n    assert writer is not None\n    data_list:List[bytes]=[]\n    writer(event,data_list.append)\n    return data_list\n  except:\n   self._process_error(self.our_role)\n   raise\n   \n def send_failed(self)->None:\n  ''\n\n\n\n\n\n  \n  self._process_error(self.our_role)\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n def _clean_up_response_headers_for_sending(self,response:Response)->Response:\n  assert type(response)is Response\n  \n  headers=response.headers\n  need_close=False\n  \n  \n  \n  \n  \n  \n  \n  \n  method_for_choosing_headers=cast(bytes,self._request_method)\n  if method_for_choosing_headers ==b\"HEAD\":\n   method_for_choosing_headers=b\"GET\"\n  framing_type,_=_body_framing(method_for_choosing_headers,response)\n  if framing_type in(\"chunked\",\"http/1.0\"):\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   headers=set_comma_header(headers,b\"content-length\",[])\n   if self.their_http_version is None or self.their_http_version <b\"1.1\":\n   \n   \n   \n   \n    headers=set_comma_header(headers,b\"transfer-encoding\",[])\n    \n    \n    \n    \n    if self._request_method !=b\"HEAD\":\n     need_close=True\n   else:\n    headers=set_comma_header(headers,b\"transfer-encoding\",[b\"chunked\"])\n    \n  if not self._cstate.keep_alive or need_close:\n  \n   connection=set(get_comma_header(headers,b\"connection\"))\n   connection.discard(b\"keep-alive\")\n   connection.add(b\"close\")\n   headers=set_comma_header(headers,b\"connection\",sorted(connection))\n   \n  return Response(\n  headers=headers,\n  status_code=response.status_code,\n  http_version=response.http_version,\n  reason=response.reason,\n  )\n", ["h11._events", "h11._headers", "h11._readers", "h11._receivebuffer", "h11._state", "h11._util", "h11._writers", "typing"]], "h11._state": [".py", "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom typing import cast,Dict,Optional,Set,Tuple,Type,Union\n\nfrom._events import *\nfrom._util import LocalProtocolError,Sentinel\n\n\n__all__=[\n\"CLIENT\",\n\"SERVER\",\n\"IDLE\",\n\"SEND_RESPONSE\",\n\"SEND_BODY\",\n\"DONE\",\n\"MUST_CLOSE\",\n\"CLOSED\",\n\"MIGHT_SWITCH_PROTOCOL\",\n\"SWITCHED_PROTOCOL\",\n\"ERROR\",\n]\n\n\nclass CLIENT(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass SERVER(Sentinel,metaclass=Sentinel):\n pass\n \n \n \nclass IDLE(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass SEND_RESPONSE(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass SEND_BODY(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass DONE(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass MUST_CLOSE(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass CLOSED(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass ERROR(Sentinel,metaclass=Sentinel):\n pass\n \n \n \nclass MIGHT_SWITCH_PROTOCOL(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass SWITCHED_PROTOCOL(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass _SWITCH_UPGRADE(Sentinel,metaclass=Sentinel):\n pass\n \n \nclass _SWITCH_CONNECT(Sentinel,metaclass=Sentinel):\n pass\n \n \nEventTransitionType=Dict[\nType[Sentinel],\nDict[\nType[Sentinel],\nDict[Union[Type[Event],Tuple[Type[Event],Type[Sentinel]]],Type[Sentinel]],\n],\n]\n\nEVENT_TRIGGERED_TRANSITIONS:EventTransitionType={\nCLIENT:{\nIDLE:{Request:SEND_BODY,ConnectionClosed:CLOSED},\nSEND_BODY:{Data:SEND_BODY,EndOfMessage:DONE},\nDONE:{ConnectionClosed:CLOSED},\nMUST_CLOSE:{ConnectionClosed:CLOSED},\nCLOSED:{ConnectionClosed:CLOSED},\nMIGHT_SWITCH_PROTOCOL:{},\nSWITCHED_PROTOCOL:{},\nERROR:{},\n},\nSERVER:{\nIDLE:{\nConnectionClosed:CLOSED,\nResponse:SEND_BODY,\n\n(Request,CLIENT):SEND_RESPONSE,\n},\nSEND_RESPONSE:{\nInformationalResponse:SEND_RESPONSE,\nResponse:SEND_BODY,\n(InformationalResponse,_SWITCH_UPGRADE):SWITCHED_PROTOCOL,\n(Response,_SWITCH_CONNECT):SWITCHED_PROTOCOL,\n},\nSEND_BODY:{Data:SEND_BODY,EndOfMessage:DONE},\nDONE:{ConnectionClosed:CLOSED},\nMUST_CLOSE:{ConnectionClosed:CLOSED},\nCLOSED:{ConnectionClosed:CLOSED},\nSWITCHED_PROTOCOL:{},\nERROR:{},\n},\n}\n\nStateTransitionType=Dict[\nTuple[Type[Sentinel],Type[Sentinel]],Dict[Type[Sentinel],Type[Sentinel]]\n]\n\n\n\nSTATE_TRIGGERED_TRANSITIONS:StateTransitionType={\n\n\n(MIGHT_SWITCH_PROTOCOL,SWITCHED_PROTOCOL):{CLIENT:SWITCHED_PROTOCOL},\n\n(CLOSED,DONE):{SERVER:MUST_CLOSE},\n(CLOSED,IDLE):{SERVER:MUST_CLOSE},\n(ERROR,DONE):{SERVER:MUST_CLOSE},\n(DONE,CLOSED):{CLIENT:MUST_CLOSE},\n(IDLE,CLOSED):{CLIENT:MUST_CLOSE},\n(DONE,ERROR):{CLIENT:MUST_CLOSE},\n}\n\n\nclass ConnectionState:\n def __init__(self)->None:\n \n \n \n \n  self.keep_alive=True\n  \n  \n  \n  self.pending_switch_proposals:Set[Type[Sentinel]]=set()\n  \n  self.states:Dict[Type[Sentinel],Type[Sentinel]]={CLIENT:IDLE,SERVER:IDLE}\n  \n def process_error(self,role:Type[Sentinel])->None:\n  self.states[role]=ERROR\n  self._fire_state_triggered_transitions()\n  \n def process_keep_alive_disabled(self)->None:\n  self.keep_alive=False\n  self._fire_state_triggered_transitions()\n  \n def process_client_switch_proposal(self,switch_event:Type[Sentinel])->None:\n  self.pending_switch_proposals.add(switch_event)\n  self._fire_state_triggered_transitions()\n  \n def process_event(\n self,\n role:Type[Sentinel],\n event_type:Type[Event],\n server_switch_event:Optional[Type[Sentinel]]=None,\n )->None:\n  _event_type:Union[Type[Event],Tuple[Type[Event],Type[Sentinel]]]=event_type\n  if server_switch_event is not None:\n   assert role is SERVER\n   if server_switch_event not in self.pending_switch_proposals:\n    raise LocalProtocolError(\n    \"Received server {} event without a pending proposal\".format(\n    server_switch_event\n    )\n    )\n   _event_type=(event_type,server_switch_event)\n  if server_switch_event is None and _event_type is Response:\n   self.pending_switch_proposals=set()\n  self._fire_event_triggered_transitions(role,_event_type)\n  \n  \n  if _event_type is Request:\n   assert role is CLIENT\n   self._fire_event_triggered_transitions(SERVER,(Request,CLIENT))\n  self._fire_state_triggered_transitions()\n  \n def _fire_event_triggered_transitions(\n self,\n role:Type[Sentinel],\n event_type:Union[Type[Event],Tuple[Type[Event],Type[Sentinel]]],\n )->None:\n  state=self.states[role]\n  try:\n   new_state=EVENT_TRIGGERED_TRANSITIONS[role][state][event_type]\n  except KeyError:\n   event_type=cast(Type[Event],event_type)\n   raise LocalProtocolError(\n   \"can't handle event type {} when role={} and state={}\".format(\n   event_type.__name__,role,self.states[role]\n   )\n   )from None\n  self.states[role]=new_state\n  \n def _fire_state_triggered_transitions(self)->None:\n \n  while True:\n   start_states=dict(self.states)\n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   \n   if self.pending_switch_proposals:\n    if self.states[CLIENT]is DONE:\n     self.states[CLIENT]=MIGHT_SWITCH_PROTOCOL\n     \n   if not self.pending_switch_proposals:\n    if self.states[CLIENT]is MIGHT_SWITCH_PROTOCOL:\n     self.states[CLIENT]=DONE\n     \n   if not self.keep_alive:\n    for role in(CLIENT,SERVER):\n     if self.states[role]is DONE:\n      self.states[role]=MUST_CLOSE\n      \n      \n   joint_state=(self.states[CLIENT],self.states[SERVER])\n   changes=STATE_TRIGGERED_TRANSITIONS.get(joint_state,{})\n   self.states.update(changes)\n   \n   if self.states ==start_states:\n   \n    return\n    \n def start_next_cycle(self)->None:\n  if self.states !={CLIENT:DONE,SERVER:DONE}:\n   raise LocalProtocolError(\n   \"not in a reusable state. self.states={}\".format(self.states)\n   )\n   \n   \n  assert self.keep_alive\n  assert not self.pending_switch_proposals\n  self.states={CLIENT:IDLE,SERVER:IDLE}\n", ["h11._events", "h11._util", "typing"]], "h11._receivebuffer": [".py", "import re\nimport sys\nfrom typing import List,Optional,Union\n\n__all__=[\"ReceiveBuffer\"]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nblank_line_regex=re.compile(b\"\\n\\r?\\n\",re.MULTILINE)\n\n\nclass ReceiveBuffer:\n def __init__(self)->None:\n  self._data=bytearray()\n  self._next_line_search=0\n  self._multiple_lines_search=0\n  \n def __iadd__(self,byteslike:Union[bytes,bytearray])->\"ReceiveBuffer\":\n  self._data +=byteslike\n  return self\n  \n def __bool__(self)->bool:\n  return bool(len(self))\n  \n def __len__(self)->int:\n  return len(self._data)\n  \n  \n def __bytes__(self)->bytes:\n  return bytes(self._data)\n  \n def _extract(self,count:int)->bytearray:\n \n  out=self._data[:count]\n  del self._data[:count]\n  \n  self._next_line_search=0\n  self._multiple_lines_search=0\n  \n  return out\n  \n def maybe_extract_at_most(self,count:int)->Optional[bytearray]:\n  ''\n\n  \n  out=self._data[:count]\n  if not out:\n   return None\n   \n  return self._extract(count)\n  \n def maybe_extract_next_line(self)->Optional[bytearray]:\n  ''\n\n  \n  \n  search_start_index=max(0,self._next_line_search -1)\n  partial_idx=self._data.find(b\"\\r\\n\",search_start_index)\n  \n  if partial_idx ==-1:\n   self._next_line_search=len(self._data)\n   return None\n   \n   \n  idx=partial_idx+2\n  \n  return self._extract(idx)\n  \n def maybe_extract_lines(self)->Optional[List[bytearray]]:\n  ''\n\n  \n  \n  if self._data[:1]==b\"\\n\":\n   self._extract(1)\n   return[]\n   \n  if self._data[:2]==b\"\\r\\n\":\n   self._extract(2)\n   return[]\n   \n   \n  match=blank_line_regex.search(self._data,self._multiple_lines_search)\n  if match is None:\n   self._multiple_lines_search=max(0,len(self._data)-2)\n   return None\n   \n   \n  idx=match.span(0)[-1]\n  out=self._extract(idx)\n  lines=out.split(b\"\\n\")\n  \n  for line in lines:\n   if line.endswith(b\"\\r\"):\n    del line[-1]\n    \n  assert lines[-2]==lines[-1]==b\"\"\n  \n  del lines[-2:]\n  \n  return lines\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n def is_next_line_obviously_invalid_request_line(self)->bool:\n  try:\n  \n  \n   return self._data[0]<0x21\n  except IndexError:\n   return False\n", ["re", "sys", "typing"]], "h11.tests": [".py", "", [], 1], "h11.tests.test_util": [".py", "import re\nimport sys\nimport traceback\nfrom typing import NoReturn\n\nimport pytest\n\nfrom.._util import(\nbytesify,\nLocalProtocolError,\nProtocolError,\nRemoteProtocolError,\nSentinel,\nvalidate,\n)\n\n\ndef test_ProtocolError()->None:\n with pytest.raises(TypeError):\n  ProtocolError(\"abstract base class\")\n  \n  \ndef test_LocalProtocolError()->None:\n try:\n  raise LocalProtocolError(\"foo\")\n except LocalProtocolError as e:\n  assert str(e)==\"foo\"\n  assert e.error_status_hint ==400\n  \n try:\n  raise LocalProtocolError(\"foo\",error_status_hint=418)\n except LocalProtocolError as e:\n  assert str(e)==\"foo\"\n  assert e.error_status_hint ==418\n  \n def thunk()->NoReturn:\n  raise LocalProtocolError(\"a\",error_status_hint=420)\n  \n try:\n  try:\n   thunk()\n  except LocalProtocolError as exc1:\n   orig_traceback=\"\".join(traceback.format_tb(sys.exc_info()[2]))\n   exc1._reraise_as_remote_protocol_error()\n except RemoteProtocolError as exc2:\n  assert type(exc2)is RemoteProtocolError\n  assert exc2.args ==(\"a\",)\n  assert exc2.error_status_hint ==420\n  new_traceback=\"\".join(traceback.format_tb(sys.exc_info()[2]))\n  assert new_traceback.endswith(orig_traceback)\n  \n  \ndef test_validate()->None:\n my_re=re.compile(rb\"(?P<group1>[0-9]+)\\.(?P<group2>[0-9]+)\")\n with pytest.raises(LocalProtocolError):\n  validate(my_re,b\"0.\")\n  \n groups=validate(my_re,b\"0.1\")\n assert groups =={\"group1\":b\"0\",\"group2\":b\"1\"}\n \n \n with pytest.raises(LocalProtocolError):\n  validate(my_re,b\"0.1xx\")\n with pytest.raises(LocalProtocolError):\n  validate(my_re,b\"0.1\\n\")\n  \n  \ndef test_validate_formatting()->None:\n my_re=re.compile(rb\"foo\")\n \n with pytest.raises(LocalProtocolError)as excinfo:\n  validate(my_re,b\"\",\"oops\")\n assert \"oops\"in str(excinfo.value)\n \n with pytest.raises(LocalProtocolError)as excinfo:\n  validate(my_re,b\"\",\"oops {}\")\n assert \"oops {}\"in str(excinfo.value)\n \n with pytest.raises(LocalProtocolError)as excinfo:\n  validate(my_re,b\"\",\"oops {} xx\",10)\n assert \"oops 10 xx\"in str(excinfo.value)\n \n \ndef test_make_sentinel()->None:\n class S(Sentinel,metaclass=Sentinel):\n  pass\n  \n assert repr(S)==\"S\"\n assert S ==S\n assert type(S).__name__ ==\"S\"\n assert S in{S}\n assert type(S)is S\n \n class S2(Sentinel,metaclass=Sentinel):\n  pass\n  \n assert repr(S2)==\"S2\"\n assert S !=S2\n assert S not in{S2}\n assert type(S)is not type(S2)\n \n \ndef test_bytesify()->None:\n assert bytesify(b\"123\")==b\"123\"\n assert bytesify(bytearray(b\"123\"))==b\"123\"\n assert bytesify(\"123\")==b\"123\"\n \n with pytest.raises(UnicodeEncodeError):\n  bytesify(\"\\u1234\")\n  \n with pytest.raises(TypeError):\n  bytesify(10)\n", ["h11._util", "pytest", "re", "sys", "traceback", "typing"]], "h11.tests.test_io": [".py", "from typing import Any,Callable,Generator,List\n\nimport pytest\n\nfrom.._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom.._headers import Headers,normalize_and_validate\nfrom.._readers import(\n_obsolete_line_fold,\nChunkedReader,\nContentLengthReader,\nHttp10Reader,\nREADERS,\n)\nfrom.._receivebuffer import ReceiveBuffer\nfrom.._state import(\nCLIENT,\nCLOSED,\nDONE,\nIDLE,\nMIGHT_SWITCH_PROTOCOL,\nMUST_CLOSE,\nSEND_BODY,\nSEND_RESPONSE,\nSERVER,\nSWITCHED_PROTOCOL,\n)\nfrom.._util import LocalProtocolError\nfrom.._writers import(\nChunkedWriter,\nContentLengthWriter,\nHttp10Writer,\nwrite_any_response,\nwrite_headers,\nwrite_request,\nWRITERS,\n)\nfrom.helpers import normalize_data_events\n\nSIMPLE_CASES=[\n(\n(CLIENT,IDLE),\nRequest(\nmethod=\"GET\",\ntarget=\"/a\",\nheaders=[(\"Host\",\"foo\"),(\"Connection\",\"close\")],\n),\nb\"GET /a HTTP/1.1\\r\\nHost: foo\\r\\nConnection: close\\r\\n\\r\\n\",\n),\n(\n(SERVER,SEND_RESPONSE),\nResponse(status_code=200,headers=[(\"Connection\",\"close\")],reason=b\"OK\"),\nb\"HTTP/1.1 200 OK\\r\\nConnection: close\\r\\n\\r\\n\",\n),\n(\n(SERVER,SEND_RESPONSE),\nResponse(status_code=200,headers=[],reason=b\"OK\"),\nb\"HTTP/1.1 200 OK\\r\\n\\r\\n\",\n),\n(\n(SERVER,SEND_RESPONSE),\nInformationalResponse(\nstatus_code=101,headers=[(\"Upgrade\",\"websocket\")],reason=b\"Upgrade\"\n),\nb\"HTTP/1.1 101 Upgrade\\r\\nUpgrade: websocket\\r\\n\\r\\n\",\n),\n(\n(SERVER,SEND_RESPONSE),\nInformationalResponse(status_code=101,headers=[],reason=b\"Upgrade\"),\nb\"HTTP/1.1 101 Upgrade\\r\\n\\r\\n\",\n),\n]\n\n\ndef dowrite(writer:Callable[...,None],obj:Any)->bytes:\n got_list:List[bytes]=[]\n writer(obj,got_list.append)\n return b\"\".join(got_list)\n \n \ndef tw(writer:Any,obj:Any,expected:Any)->None:\n got=dowrite(writer,obj)\n assert got ==expected\n \n \ndef makebuf(data:bytes)->ReceiveBuffer:\n buf=ReceiveBuffer()\n buf +=data\n return buf\n \n \ndef tr(reader:Any,data:bytes,expected:Any)->None:\n def check(got:Any)->None:\n  assert got ==expected\n  \n  \n  for name,value in getattr(got,\"headers\",[]):\n   assert type(name)is bytes\n   assert type(value)is bytes\n   \n   \n buf=makebuf(data)\n check(reader(buf))\n assert not buf\n \n \n buf=ReceiveBuffer()\n for i in range(len(data)):\n  assert reader(buf)is None\n  buf +=data[i:i+1]\n check(reader(buf))\n \n \n buf=makebuf(data)\n buf +=b\"trailing\"\n check(reader(buf))\n assert bytes(buf)==b\"trailing\"\n \n \ndef test_writers_simple()->None:\n for((role,state),event,binary)in SIMPLE_CASES:\n  tw(WRITERS[role,state],event,binary)\n  \n  \ndef test_readers_simple()->None:\n for((role,state),event,binary)in SIMPLE_CASES:\n  tr(READERS[role,state],binary,event)\n  \n  \ndef test_writers_unusual()->None:\n\n tw(\n write_headers,\n normalize_and_validate([(\"foo\",\"bar\"),(\"baz\",\"quux\")]),\n b\"foo: bar\\r\\nbaz: quux\\r\\n\\r\\n\",\n )\n tw(write_headers,Headers([]),b\"\\r\\n\")\n \n \n with pytest.raises(LocalProtocolError):\n  tw(\n  write_request,\n  Request(\n  method=\"GET\",\n  target=\"/\",\n  headers=[(\"Host\",\"foo\"),(\"Connection\",\"close\")],\n  http_version=\"1.0\",\n  ),\n  None,\n  )\n with pytest.raises(LocalProtocolError):\n  tw(\n  write_any_response,\n  Response(\n  status_code=200,headers=[(\"Connection\",\"close\")],http_version=\"1.0\"\n  ),\n  None,\n  )\n  \n  \ndef test_readers_unusual()->None:\n\n tr(\n READERS[CLIENT,IDLE],\n b\"HEAD /foo HTTP/1.0\\r\\nSome: header\\r\\n\\r\\n\",\n Request(\n method=\"HEAD\",\n target=\"/foo\",\n headers=[(\"Some\",\"header\")],\n http_version=\"1.0\",\n ),\n )\n \n \n tr(\n READERS[CLIENT,IDLE],\n b\"HEAD /foo HTTP/1.0\\r\\n\\r\\n\",\n Request(method=\"HEAD\",target=\"/foo\",headers=[],http_version=\"1.0\"),\n )\n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.0 200 OK\\r\\nSome: header\\r\\n\\r\\n\",\n Response(\n status_code=200,\n headers=[(\"Some\",\"header\")],\n http_version=\"1.0\",\n reason=b\"OK\",\n ),\n )\n \n \n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.0 200 OK\\r\\n\"b\"Foo: a a a a a \\r\\n\\r\\n\",\n Response(\n status_code=200,\n headers=[(\"Foo\",\"a a a a a\")],\n http_version=\"1.0\",\n reason=b\"OK\",\n ),\n )\n \n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.0 200 OK\\r\\n\"b\"Foo:\\r\\n\\r\\n\",\n Response(\n status_code=200,headers=[(\"Foo\",\"\")],http_version=\"1.0\",reason=b\"OK\"\n ),\n )\n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.0 200 OK\\r\\n\"b\"Foo: \\t \\t \\r\\n\\r\\n\",\n Response(\n status_code=200,headers=[(\"Foo\",\"\")],http_version=\"1.0\",reason=b\"OK\"\n ),\n )\n \n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.0 200\\r\\n\"b\"Foo: bar\\r\\n\\r\\n\",\n Response(\n status_code=200,headers=[(\"Foo\",\"bar\")],http_version=\"1.0\",reason=b\"\"\n ),\n )\n \n \n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.1 200 OK\\r\\nSomeHeader: val\\n\\r\\n\",\n Response(\n status_code=200,\n headers=[(\"SomeHeader\",\"val\")],\n http_version=\"1.1\",\n reason=\"OK\",\n ),\n )\n \n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.1 200 OK\\nSomeHeader1: val1\\nSomeHeader2: val2\\n\\n\",\n Response(\n status_code=200,\n headers=[(\"SomeHeader1\",\"val1\"),(\"SomeHeader2\",\"val2\")],\n http_version=\"1.1\",\n reason=\"OK\",\n ),\n )\n \n \n tr(\n READERS[SERVER,SEND_RESPONSE],\n b\"HTTP/1.1 200 OK\\r\\nSomeHeader1: val1\\nSomeHeader2: val2\\n\\r\\n\",\n Response(\n status_code=200,\n headers=[(\"SomeHeader1\",\"val1\"),(\"SomeHeader2\",\"val2\")],\n http_version=\"1.1\",\n reason=\"OK\",\n ),\n )\n \n \n tr(\n READERS[CLIENT,IDLE],\n b\"HEAD /foo HTTP/1.1\\r\\n\"\n b\"Host: example.com\\r\\n\"\n b\"Some: multi-line\\r\\n\"\n b\" header\\r\\n\"\n b\"\\tnonsense\\r\\n\"\n b\"    \\t   \\t\\tI guess\\r\\n\"\n b\"Connection: close\\r\\n\"\n b\"More-nonsense: in the\\r\\n\"\n b\"    last header  \\r\\n\\r\\n\",\n Request(\n method=\"HEAD\",\n target=\"/foo\",\n headers=[\n (\"Host\",\"example.com\"),\n (\"Some\",\"multi-line header nonsense I guess\"),\n (\"Connection\",\"close\"),\n (\"More-nonsense\",\"in the last header\"),\n ],\n ),\n )\n \n with pytest.raises(LocalProtocolError):\n  tr(\n  READERS[CLIENT,IDLE],\n  b\"HEAD /foo HTTP/1.1\\r\\n\"b\"  folded: line\\r\\n\\r\\n\",\n  None,\n  )\n  \n with pytest.raises(LocalProtocolError):\n  tr(\n  READERS[CLIENT,IDLE],\n  b\"HEAD /foo HTTP/1.1\\r\\n\"b\"foo  : line\\r\\n\\r\\n\",\n  None,\n  )\n with pytest.raises(LocalProtocolError):\n  tr(\n  READERS[CLIENT,IDLE],\n  b\"HEAD /foo HTTP/1.1\\r\\n\"b\"foo\\t: line\\r\\n\\r\\n\",\n  None,\n  )\n with pytest.raises(LocalProtocolError):\n  tr(\n  READERS[CLIENT,IDLE],\n  b\"HEAD /foo HTTP/1.1\\r\\n\"b\"foo\\t: line\\r\\n\\r\\n\",\n  None,\n  )\n with pytest.raises(LocalProtocolError):\n  tr(READERS[CLIENT,IDLE],b\"HEAD /foo HTTP/1.1\\r\\n\"b\": line\\r\\n\\r\\n\",None)\n  \n  \ndef test__obsolete_line_fold_bytes()->None:\n\n\n\n\n\n assert list(_obsolete_line_fold([b\"aaa\",b\"bbb\",b\"  ccc\",b\"ddd\"]))==[\n b\"aaa\",\n bytearray(b\"bbb ccc\"),\n b\"ddd\",\n ]\n \n \ndef _run_reader_iter(\nreader:Any,buf:bytes,do_eof:bool\n)->Generator[Any,None,None]:\n while True:\n  event=reader(buf)\n  if event is None:\n   break\n  yield event\n  \n  \n  if type(event)is EndOfMessage:\n   break\n if do_eof:\n  assert not buf\n  yield reader.read_eof()\n  \n  \ndef _run_reader(*args:Any)->List[Event]:\n events=list(_run_reader_iter(*args))\n return normalize_data_events(events)\n \n \ndef t_body_reader(thunk:Any,data:bytes,expected:Any,do_eof:bool=False)->None:\n\n print(\"Test 1\")\n buf=makebuf(data)\n assert _run_reader(thunk(),buf,do_eof)==expected\n \n \n print(\"Test 2\")\n reader=thunk()\n buf=ReceiveBuffer()\n events=[]\n for i in range(len(data)):\n  events +=_run_reader(reader,buf,False)\n  buf +=data[i:i+1]\n events +=_run_reader(reader,buf,do_eof)\n assert normalize_data_events(events)==expected\n \n is_complete=any(type(event)is EndOfMessage for event in expected)\n if is_complete and not do_eof:\n  buf=makebuf(data+b\"trailing\")\n  assert _run_reader(thunk(),buf,False)==expected\n  \n  \ndef test_ContentLengthReader()->None:\n t_body_reader(lambda:ContentLengthReader(0),b\"\",[EndOfMessage()])\n \n t_body_reader(\n lambda:ContentLengthReader(10),\n b\"0123456789\",\n [Data(data=b\"0123456789\"),EndOfMessage()],\n )\n \n \ndef test_Http10Reader()->None:\n t_body_reader(Http10Reader,b\"\",[EndOfMessage()],do_eof=True)\n t_body_reader(Http10Reader,b\"asdf\",[Data(data=b\"asdf\")],do_eof=False)\n t_body_reader(\n Http10Reader,b\"asdf\",[Data(data=b\"asdf\"),EndOfMessage()],do_eof=True\n )\n \n \ndef test_ChunkedReader()->None:\n t_body_reader(ChunkedReader,b\"0\\r\\n\\r\\n\",[EndOfMessage()])\n \n t_body_reader(\n ChunkedReader,\n b\"0\\r\\nSome: header\\r\\n\\r\\n\",\n [EndOfMessage(headers=[(\"Some\",\"header\")])],\n )\n \n t_body_reader(\n ChunkedReader,\n b\"5\\r\\n01234\\r\\n\"\n +b\"10\\r\\n0123456789abcdef\\r\\n\"\n +b\"0\\r\\n\"\n +b\"Some: header\\r\\n\\r\\n\",\n [\n Data(data=b\"012340123456789abcdef\"),\n EndOfMessage(headers=[(\"Some\",\"header\")]),\n ],\n )\n \n t_body_reader(\n ChunkedReader,\n b\"5\\r\\n01234\\r\\n\"+b\"10\\r\\n0123456789abcdef\\r\\n\"+b\"0\\r\\n\\r\\n\",\n [Data(data=b\"012340123456789abcdef\"),EndOfMessage()],\n )\n \n \n t_body_reader(\n ChunkedReader,\n b\"aA\\r\\n\"+b\"x\"*0xAA+b\"\\r\\n\"+b\"0\\r\\n\\r\\n\",\n [Data(data=b\"x\"*0xAA),EndOfMessage()],\n )\n \n \n with pytest.raises(LocalProtocolError):\n \n \n  t_body_reader(ChunkedReader,b\"9\"*100+b\"\\r\\nxxx\",[Data(data=b\"xxx\")])\n  \n  \n with pytest.raises(LocalProtocolError):\n  t_body_reader(ChunkedReader,b\"10\\x00\\r\\nxxx\",None)\n  \n  \n t_body_reader(\n ChunkedReader,\n b\"5; hello=there\\r\\n\"\n +b\"xxxxx\"\n +b\"\\r\\n\"\n +b'0; random=\"junk\"; some=more; canbe=lonnnnngg\\r\\n\\r\\n',\n [Data(data=b\"xxxxx\"),EndOfMessage()],\n )\n \n t_body_reader(\n ChunkedReader,\n b\"5   \t \\r\\n01234\\r\\n\"+b\"0\\r\\n\\r\\n\",\n [Data(data=b\"01234\"),EndOfMessage()],\n )\n \n \ndef test_ContentLengthWriter()->None:\n w=ContentLengthWriter(5)\n assert dowrite(w,Data(data=b\"123\"))==b\"123\"\n assert dowrite(w,Data(data=b\"45\"))==b\"45\"\n assert dowrite(w,EndOfMessage())==b\"\"\n \n w=ContentLengthWriter(5)\n with pytest.raises(LocalProtocolError):\n  dowrite(w,Data(data=b\"123456\"))\n  \n w=ContentLengthWriter(5)\n dowrite(w,Data(data=b\"123\"))\n with pytest.raises(LocalProtocolError):\n  dowrite(w,Data(data=b\"456\"))\n  \n w=ContentLengthWriter(5)\n dowrite(w,Data(data=b\"123\"))\n with pytest.raises(LocalProtocolError):\n  dowrite(w,EndOfMessage())\n  \n w=ContentLengthWriter(5)\n dowrite(w,Data(data=b\"123\"))==b\"123\"\n dowrite(w,Data(data=b\"45\"))==b\"45\"\n with pytest.raises(LocalProtocolError):\n  dowrite(w,EndOfMessage(headers=[(\"Etag\",\"asdf\")]))\n  \n  \ndef test_ChunkedWriter()->None:\n w=ChunkedWriter()\n assert dowrite(w,Data(data=b\"aaa\"))==b\"3\\r\\naaa\\r\\n\"\n assert dowrite(w,Data(data=b\"a\"*20))==b\"14\\r\\n\"+b\"a\"*20+b\"\\r\\n\"\n \n assert dowrite(w,Data(data=b\"\"))==b\"\"\n \n assert dowrite(w,EndOfMessage())==b\"0\\r\\n\\r\\n\"\n \n assert(\n dowrite(w,EndOfMessage(headers=[(\"Etag\",\"asdf\"),(\"a\",\"b\")]))\n ==b\"0\\r\\nEtag: asdf\\r\\na: b\\r\\n\\r\\n\"\n )\n \n \ndef test_Http10Writer()->None:\n w=Http10Writer()\n assert dowrite(w,Data(data=b\"1234\"))==b\"1234\"\n assert dowrite(w,EndOfMessage())==b\"\"\n \n with pytest.raises(LocalProtocolError):\n  dowrite(w,EndOfMessage(headers=[(\"Etag\",\"asdf\")]))\n  \n  \ndef test_reject_garbage_after_request_line()->None:\n with pytest.raises(LocalProtocolError):\n  tr(READERS[SERVER,SEND_RESPONSE],b\"HTTP/1.0 200 OK\\x00xxxx\\r\\n\\r\\n\",None)\n  \n  \ndef test_reject_garbage_after_response_line()->None:\n with pytest.raises(LocalProtocolError):\n  tr(\n  READERS[CLIENT,IDLE],\n  b\"HEAD /foo HTTP/1.1 xxxxxx\\r\\n\"b\"Host: a\\r\\n\\r\\n\",\n  None,\n  )\n  \n  \ndef test_reject_garbage_in_header_line()->None:\n with pytest.raises(LocalProtocolError):\n  tr(\n  READERS[CLIENT,IDLE],\n  b\"HEAD /foo HTTP/1.1\\r\\n\"b\"Host: foo\\x00bar\\r\\n\\r\\n\",\n  None,\n  )\n  \n  \ndef test_reject_non_vchar_in_path()->None:\n for bad_char in b\"\\x00\\x20\\x7f\\xee\":\n  message=bytearray(b\"HEAD /\")\n  message.append(bad_char)\n  message.extend(b\" HTTP/1.1\\r\\nHost: foobar\\r\\n\\r\\n\")\n  with pytest.raises(LocalProtocolError):\n   tr(READERS[CLIENT,IDLE],message,None)\n   \n   \n   \ndef test_allow_some_garbage_in_cookies()->None:\n tr(\n READERS[CLIENT,IDLE],\n b\"HEAD /foo HTTP/1.1\\r\\n\"\n b\"Host: foo\\r\\n\"\n b\"Set-Cookie: ___utmvafIumyLc=kUd\\x01UpAt; path=/; Max-Age=900\\r\\n\"\n b\"\\r\\n\",\n Request(\n method=\"HEAD\",\n target=\"/foo\",\n headers=[\n (\"Host\",\"foo\"),\n (\"Set-Cookie\",\"___utmvafIumyLc=kUd\\x01UpAt; path=/; Max-Age=900\"),\n ],\n ),\n )\n \n \ndef test_host_comes_first()->None:\n tw(\n write_headers,\n normalize_and_validate([(\"foo\",\"bar\"),(\"Host\",\"example.com\")]),\n b\"Host: example.com\\r\\nfoo: bar\\r\\n\\r\\n\",\n )\n", ["h11._events", "h11._headers", "h11._readers", "h11._receivebuffer", "h11._state", "h11._util", "h11._writers", "h11.tests.helpers", "pytest", "typing"]], "h11.tests.test_events": [".py", "from http import HTTPStatus\n\nimport pytest\n\nfrom.. import _events\nfrom.._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom.._util import LocalProtocolError\n\n\ndef test_events()->None:\n with pytest.raises(LocalProtocolError):\n \n  req=Request(\n  method=\"GET\",target=\"/\",headers=[(\"a\",\"b\")],http_version=\"1.1\"\n  )\n  \n req=Request(method=\"GET\",target=\"/\",headers=[(\"a\",\"b\")],http_version=\"1.0\")\n \n assert req.method ==b\"GET\"\n assert req.target ==b\"/\"\n assert req.headers ==[(b\"a\",b\"b\")]\n assert req.http_version ==b\"1.0\"\n \n \n req=Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"a\",\"b\"),(\"hOSt\",\"example.com\")],\n http_version=\"1.1\",\n )\n \n assert req.headers ==[(b\"a\",b\"b\"),(b\"host\",b\"example.com\")]\n \n \n with pytest.raises(LocalProtocolError):\n  req=Request(\n  method=\"GET\",\n  target=\"/\",\n  headers=[(\"Host\",\"a\"),(\"Host\",\"a\")],\n  http_version=\"1.1\",\n  )\n  \n with pytest.raises(LocalProtocolError):\n  req=Request(\n  method=\"GET\",\n  target=\"/\",\n  headers=[(\"Host\",\"a\"),(\"Host\",\"a\")],\n  http_version=\"1.0\",\n  )\n  \n  \n for bad_char in \"\\x00\\r\\n\\f\\v\":\n  with pytest.raises(LocalProtocolError):\n   req=Request(\n   method=\"GET\",\n   target=\"/\",\n   headers=[(\"Host\",\"a\"),(\"Foo\",\"asd\"+bad_char)],\n   http_version=\"1.0\",\n   )\n   \n   \n   \n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"a\"),(\"Foo\",\"asd\\x01\\x02\\x7f\")],\n http_version=\"1.0\",\n )\n \n \n for bad_byte in b\"\\x00\\x20\\x7f\\xee\":\n  target=bytearray(b\"/\")\n  target.append(bad_byte)\n  with pytest.raises(LocalProtocolError):\n   Request(\n   method=\"GET\",target=target,headers=[(\"Host\",\"a\")],http_version=\"1.1\"\n   )\n   \n   \n with pytest.raises(LocalProtocolError):\n  Request(\n  method=\"GET / HTTP/1.1\",\n  target=target,\n  headers=[(\"Host\",\"a\")],\n  http_version=\"1.1\",\n  )\n  \n ir=InformationalResponse(status_code=100,headers=[(\"Host\",\"a\")])\n assert ir.status_code ==100\n assert ir.headers ==[(b\"host\",b\"a\")]\n assert ir.http_version ==b\"1.1\"\n \n with pytest.raises(LocalProtocolError):\n  InformationalResponse(status_code=200,headers=[(\"Host\",\"a\")])\n  \n resp=Response(status_code=204,headers=[],http_version=\"1.0\")\n assert resp.status_code ==204\n assert resp.headers ==[]\n assert resp.http_version ==b\"1.0\"\n \n with pytest.raises(LocalProtocolError):\n  resp=Response(status_code=100,headers=[],http_version=\"1.0\")\n  \n with pytest.raises(LocalProtocolError):\n  Response(status_code=\"100\",headers=[],http_version=\"1.0\")\n  \n with pytest.raises(LocalProtocolError):\n  InformationalResponse(status_code=b\"100\",headers=[],http_version=\"1.0\")\n  \n d=Data(data=b\"asdf\")\n assert d.data ==b\"asdf\"\n \n eom=EndOfMessage()\n assert eom.headers ==[]\n \n cc=ConnectionClosed()\n assert repr(cc)==\"ConnectionClosed()\"\n \n \ndef test_intenum_status_code()->None:\n\n\n r=Response(status_code=HTTPStatus.OK,headers=[],http_version=\"1.0\")\n assert r.status_code ==HTTPStatus.OK\n assert type(r.status_code)is not type(HTTPStatus.OK)\n assert type(r.status_code)is int\n \n \ndef test_header_casing()->None:\n r=Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.org\"),(\"Connection\",\"keep-alive\")],\n http_version=\"1.1\",\n )\n assert len(r.headers)==2\n assert r.headers[0]==(b\"host\",b\"example.org\")\n assert r.headers ==[(b\"host\",b\"example.org\"),(b\"connection\",b\"keep-alive\")]\n assert r.headers.raw_items()==[\n (b\"Host\",b\"example.org\"),\n (b\"Connection\",b\"keep-alive\"),\n ]\n", ["h11", "h11._events", "h11._util", "http", "pytest"]], "h11.tests.test_helpers": [".py", "from.._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom.helpers import normalize_data_events\n\n\ndef test_normalize_data_events()->None:\n assert normalize_data_events(\n [\n Data(data=bytearray(b\"1\")),\n Data(data=b\"2\"),\n Response(status_code=200,headers=[]),\n Data(data=b\"3\"),\n Data(data=b\"4\"),\n EndOfMessage(),\n Data(data=b\"5\"),\n Data(data=b\"6\"),\n Data(data=b\"7\"),\n ]\n )==[\n Data(data=b\"12\"),\n Response(status_code=200,headers=[]),\n Data(data=b\"34\"),\n EndOfMessage(),\n Data(data=b\"567\"),\n ]\n", ["h11._events", "h11.tests.helpers"]], "h11.tests.test_connection": [".py", "from typing import Any,cast,Dict,List,Optional,Tuple,Type\n\nimport pytest\n\nfrom.._connection import _body_framing,_keep_alive,Connection,NEED_DATA,PAUSED\nfrom.._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom.._state import(\nCLIENT,\nCLOSED,\nDONE,\nERROR,\nIDLE,\nMIGHT_SWITCH_PROTOCOL,\nMUST_CLOSE,\nSEND_BODY,\nSEND_RESPONSE,\nSERVER,\nSWITCHED_PROTOCOL,\n)\nfrom.._util import LocalProtocolError,RemoteProtocolError,Sentinel\nfrom.helpers import ConnectionPair,get_all_events,receive_and_get\n\n\ndef test__keep_alive()->None:\n assert _keep_alive(\n Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"Example.com\")])\n )\n assert not _keep_alive(\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"Example.com\"),(\"Connection\",\"close\")],\n )\n )\n assert not _keep_alive(\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"Example.com\"),(\"Connection\",\"a, b, cLOse, foo\")],\n )\n )\n assert not _keep_alive(\n Request(method=\"GET\",target=\"/\",headers=[],http_version=\"1.0\")\n )\n \n assert _keep_alive(Response(status_code=200,headers=[]))\n assert not _keep_alive(Response(status_code=200,headers=[(\"Connection\",\"close\")]))\n assert not _keep_alive(\n Response(status_code=200,headers=[(\"Connection\",\"a, b, cLOse, foo\")])\n )\n assert not _keep_alive(Response(status_code=200,headers=[],http_version=\"1.0\"))\n \n \ndef test__body_framing()->None:\n def headers(cl:Optional[int],te:bool)->List[Tuple[str,str]]:\n  headers=[]\n  if cl is not None:\n   headers.append((\"Content-Length\",str(cl)))\n  if te:\n   headers.append((\"Transfer-Encoding\",\"chunked\"))\n  return headers\n  \n def resp(\n status_code:int=200,cl:Optional[int]=None,te:bool=False\n )->Response:\n  return Response(status_code=status_code,headers=headers(cl,te))\n  \n def req(cl:Optional[int]=None,te:bool=False)->Request:\n  h=headers(cl,te)\n  h +=[(\"Host\",\"example.com\")]\n  return Request(method=\"GET\",target=\"/\",headers=h)\n  \n  \n for kwargs in[{},{\"cl\":100},{\"te\":True},{\"cl\":100,\"te\":True}]:\n  kwargs=cast(Dict[str,Any],kwargs)\n  for meth,r in[\n  (b\"HEAD\",resp(**kwargs)),\n  (b\"GET\",resp(status_code=204,**kwargs)),\n  (b\"GET\",resp(status_code=304,**kwargs)),\n  ]:\n   assert _body_framing(meth,r)==(\"content-length\",(0,))\n   \n   \n for kwargs in[{\"te\":True},{\"cl\":100,\"te\":True}]:\n  kwargs=cast(Dict[str,Any],kwargs)\n  for meth,r in[(None,req(**kwargs)),(b\"GET\",resp(**kwargs))]:\n   assert _body_framing(meth,r)==(\"chunked\",())\n   \n   \n for meth,r in[(None,req(cl=100)),(b\"GET\",resp(cl=100))]:\n  assert _body_framing(meth,r)==(\"content-length\",(100,))\n  \n  \n assert _body_framing(None,req())==(\"content-length\",(0,))\n assert _body_framing(b\"GET\",resp())==(\"http/1.0\",())\n \n \ndef test_Connection_basics_and_content_length()->None:\n with pytest.raises(ValueError):\n  Connection(\"CLIENT\")\n  \n p=ConnectionPair()\n assert p.conn[CLIENT].our_role is CLIENT\n assert p.conn[CLIENT].their_role is SERVER\n assert p.conn[SERVER].our_role is SERVER\n assert p.conn[SERVER].their_role is CLIENT\n \n data=p.send(\n CLIENT,\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Content-Length\",\"10\")],\n ),\n )\n assert data ==(\n b\"GET / HTTP/1.1\\r\\n\"b\"Host: example.com\\r\\n\"b\"Content-Length: 10\\r\\n\\r\\n\"\n )\n \n for conn in p.conns:\n  assert conn.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n assert p.conn[CLIENT].our_state is SEND_BODY\n assert p.conn[CLIENT].their_state is SEND_RESPONSE\n assert p.conn[SERVER].our_state is SEND_RESPONSE\n assert p.conn[SERVER].their_state is SEND_BODY\n \n assert p.conn[CLIENT].their_http_version is None\n assert p.conn[SERVER].their_http_version ==b\"1.1\"\n \n data=p.send(SERVER,InformationalResponse(status_code=100,headers=[]))\n assert data ==b\"HTTP/1.1 100 \\r\\n\\r\\n\"\n \n data=p.send(SERVER,Response(status_code=200,headers=[(\"Content-Length\",\"11\")]))\n assert data ==b\"HTTP/1.1 200 \\r\\nContent-Length: 11\\r\\n\\r\\n\"\n \n for conn in p.conns:\n  assert conn.states =={CLIENT:SEND_BODY,SERVER:SEND_BODY}\n  \n assert p.conn[CLIENT].their_http_version ==b\"1.1\"\n assert p.conn[SERVER].their_http_version ==b\"1.1\"\n \n data=p.send(CLIENT,Data(data=b\"12345\"))\n assert data ==b\"12345\"\n data=p.send(\n CLIENT,Data(data=b\"67890\"),expect=[Data(data=b\"67890\"),EndOfMessage()]\n )\n assert data ==b\"67890\"\n data=p.send(CLIENT,EndOfMessage(),expect=[])\n assert data ==b\"\"\n \n for conn in p.conns:\n  assert conn.states =={CLIENT:DONE,SERVER:SEND_BODY}\n  \n data=p.send(SERVER,Data(data=b\"1234567890\"))\n assert data ==b\"1234567890\"\n data=p.send(SERVER,Data(data=b\"1\"),expect=[Data(data=b\"1\"),EndOfMessage()])\n assert data ==b\"1\"\n data=p.send(SERVER,EndOfMessage(),expect=[])\n assert data ==b\"\"\n \n for conn in p.conns:\n  assert conn.states =={CLIENT:DONE,SERVER:DONE}\n  \n  \ndef test_chunked()->None:\n p=ConnectionPair()\n \n p.send(\n CLIENT,\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Transfer-Encoding\",\"chunked\")],\n ),\n )\n data=p.send(CLIENT,Data(data=b\"1234567890\",chunk_start=True,chunk_end=True))\n assert data ==b\"a\\r\\n1234567890\\r\\n\"\n data=p.send(CLIENT,Data(data=b\"abcde\",chunk_start=True,chunk_end=True))\n assert data ==b\"5\\r\\nabcde\\r\\n\"\n data=p.send(CLIENT,Data(data=b\"\"),expect=[])\n assert data ==b\"\"\n data=p.send(CLIENT,EndOfMessage(headers=[(\"hello\",\"there\")]))\n assert data ==b\"0\\r\\nhello: there\\r\\n\\r\\n\"\n \n p.send(\n SERVER,Response(status_code=200,headers=[(\"Transfer-Encoding\",\"chunked\")])\n )\n p.send(SERVER,Data(data=b\"54321\",chunk_start=True,chunk_end=True))\n p.send(SERVER,Data(data=b\"12345\",chunk_start=True,chunk_end=True))\n p.send(SERVER,EndOfMessage())\n \n for conn in p.conns:\n  assert conn.states =={CLIENT:DONE,SERVER:DONE}\n  \n  \ndef test_chunk_boundaries()->None:\n conn=Connection(our_role=SERVER)\n \n request=(\n b\"POST / HTTP/1.1\\r\\n\"\n b\"Host: example.com\\r\\n\"\n b\"Transfer-Encoding: chunked\\r\\n\"\n b\"\\r\\n\"\n )\n conn.receive_data(request)\n assert conn.next_event()==Request(\n method=\"POST\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Transfer-Encoding\",\"chunked\")],\n )\n assert conn.next_event()is NEED_DATA\n \n conn.receive_data(b\"5\\r\\nhello\\r\\n\")\n assert conn.next_event()==Data(data=b\"hello\",chunk_start=True,chunk_end=True)\n \n conn.receive_data(b\"5\\r\\nhel\")\n assert conn.next_event()==Data(data=b\"hel\",chunk_start=True,chunk_end=False)\n \n conn.receive_data(b\"l\")\n assert conn.next_event()==Data(data=b\"l\",chunk_start=False,chunk_end=False)\n \n conn.receive_data(b\"o\\r\\n\")\n assert conn.next_event()==Data(data=b\"o\",chunk_start=False,chunk_end=True)\n \n conn.receive_data(b\"5\\r\\nhello\")\n assert conn.next_event()==Data(data=b\"hello\",chunk_start=True,chunk_end=True)\n \n conn.receive_data(b\"\\r\\n\")\n assert conn.next_event()==NEED_DATA\n \n conn.receive_data(b\"0\\r\\n\\r\\n\")\n assert conn.next_event()==EndOfMessage()\n \n \ndef test_client_talking_to_http10_server()->None:\n c=Connection(CLIENT)\n c.send(Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"example.com\")]))\n c.send(EndOfMessage())\n assert c.our_state is DONE\n \n assert receive_and_get(c,b\"HTTP/1.0 200 OK\\r\\n\\r\\n\")==[\n Response(status_code=200,headers=[],http_version=\"1.0\",reason=b\"OK\")\n ]\n assert c.our_state is MUST_CLOSE\n assert receive_and_get(c,b\"12345\")==[Data(data=b\"12345\")]\n assert receive_and_get(c,b\"67890\")==[Data(data=b\"67890\")]\n assert receive_and_get(c,b\"\")==[EndOfMessage(),ConnectionClosed()]\n assert c.their_state is CLOSED\n \n \ndef test_server_talking_to_http10_client()->None:\n c=Connection(SERVER)\n \n \n assert receive_and_get(c,b\"GET / HTTP/1.0\\r\\n\\r\\n\")==[\n Request(method=\"GET\",target=\"/\",headers=[],http_version=\"1.0\"),\n EndOfMessage(),\n ]\n assert c.their_state is MUST_CLOSE\n \n \n assert(\n c.send(Response(status_code=200,headers=[]))\n ==b\"HTTP/1.1 200 \\r\\nConnection: close\\r\\n\\r\\n\"\n )\n \n assert c.send(Data(data=b\"12345\"))==b\"12345\"\n assert c.send(EndOfMessage())==b\"\"\n assert c.our_state is MUST_CLOSE\n \n \n c=Connection(SERVER)\n \n assert receive_and_get(c,b\"POST / HTTP/1.0\\r\\nContent-Length: 10\\r\\n\\r\\n1\")==[\n Request(\n method=\"POST\",\n target=\"/\",\n headers=[(\"Content-Length\",\"10\")],\n http_version=\"1.0\",\n ),\n Data(data=b\"1\"),\n ]\n assert receive_and_get(c,b\"234567890\")==[Data(data=b\"234567890\"),EndOfMessage()]\n assert c.their_state is MUST_CLOSE\n assert receive_and_get(c,b\"\")==[ConnectionClosed()]\n \n \ndef test_automatic_transfer_encoding_in_response()->None:\n\n\n\n\n for user_headers in[\n [(\"Transfer-Encoding\",\"chunked\")],\n [],\n \n \n [(\"Transfer-Encoding\",\"chunked\"),(\"Content-Length\",\"100\")],\n ]:\n  user_headers=cast(List[Tuple[str,str]],user_headers)\n  p=ConnectionPair()\n  p.send(\n  CLIENT,\n  [\n  Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"example.com\")]),\n  EndOfMessage(),\n  ],\n  )\n  \n  \n  p.send(\n  SERVER,\n  Response(status_code=200,headers=user_headers),\n  expect=Response(\n  status_code=200,headers=[(\"Transfer-Encoding\",\"chunked\")]\n  ),\n  )\n  \n  \n  \n  c=Connection(SERVER)\n  receive_and_get(c,b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n  assert(\n  c.send(Response(status_code=200,headers=user_headers))\n  ==b\"HTTP/1.1 200 \\r\\nConnection: close\\r\\n\\r\\n\"\n  )\n  assert c.send(Data(data=b\"12345\"))==b\"12345\"\n  \n  \ndef test_automagic_connection_close_handling()->None:\n p=ConnectionPair()\n \n \n p.send(\n CLIENT,\n [\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Connection\",\"close\")],\n ),\n EndOfMessage(),\n ],\n )\n for conn in p.conns:\n  assert conn.states[CLIENT]is MUST_CLOSE\n  \n p.send(\n SERVER,\n \n [Response(status_code=204,headers=[]),EndOfMessage()],\n \n expect=[\n Response(status_code=204,headers=[(\"connection\",\"close\")]),\n EndOfMessage(),\n ],\n )\n for conn in p.conns:\n  assert conn.states =={CLIENT:MUST_CLOSE,SERVER:MUST_CLOSE}\n  \n  \ndef test_100_continue()->None:\n def setup()->ConnectionPair:\n  p=ConnectionPair()\n  p.send(\n  CLIENT,\n  Request(\n  method=\"GET\",\n  target=\"/\",\n  headers=[\n  (\"Host\",\"example.com\"),\n  (\"Content-Length\",\"100\"),\n  (\"Expect\",\"100-continue\"),\n  ],\n  ),\n  )\n  for conn in p.conns:\n   assert conn.client_is_waiting_for_100_continue\n  assert not p.conn[CLIENT].they_are_waiting_for_100_continue\n  assert p.conn[SERVER].they_are_waiting_for_100_continue\n  return p\n  \n  \n p=setup()\n p.send(SERVER,InformationalResponse(status_code=100,headers=[]))\n for conn in p.conns:\n  assert not conn.client_is_waiting_for_100_continue\n  assert not conn.they_are_waiting_for_100_continue\n  \n  \n p=setup()\n p.send(\n SERVER,Response(status_code=200,headers=[(\"Transfer-Encoding\",\"chunked\")])\n )\n for conn in p.conns:\n  assert not conn.client_is_waiting_for_100_continue\n  assert not conn.they_are_waiting_for_100_continue\n  \n  \n p=setup()\n p.send(CLIENT,Data(data=b\"12345\"))\n for conn in p.conns:\n  assert not conn.client_is_waiting_for_100_continue\n  assert not conn.they_are_waiting_for_100_continue\n  \n  \ndef test_max_incomplete_event_size_countermeasure()->None:\n\n c=Connection(SERVER)\n c.receive_data(b\"GET / HTTP/1.0\\r\\nEndless: \")\n assert c.next_event()is NEED_DATA\n with pytest.raises(RemoteProtocolError):\n  while True:\n   c.receive_data(b\"a\"*1024)\n   c.next_event()\n   \n   \n   \n c=Connection(SERVER,max_incomplete_event_size=5000)\n c.receive_data(b\"GET / HTTP/1.0\\r\\nBig: \")\n c.receive_data(b\"a\"*4000)\n c.receive_data(b\"\\r\\n\\r\\n\")\n assert get_all_events(c)==[\n Request(\n method=\"GET\",target=\"/\",http_version=\"1.0\",headers=[(\"big\",\"a\"*4000)]\n ),\n EndOfMessage(),\n ]\n \n c=Connection(SERVER,max_incomplete_event_size=4000)\n c.receive_data(b\"GET / HTTP/1.0\\r\\nBig: \")\n c.receive_data(b\"a\"*4000)\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \n  \n c=Connection(SERVER,max_incomplete_event_size=5000)\n c.receive_data(b\"GET / HTTP/1.0\\r\\nContent-Length: 10000\")\n c.receive_data(b\"\\r\\n\\r\\n\"+b\"a\"*10000)\n assert get_all_events(c)==[\n Request(\n method=\"GET\",\n target=\"/\",\n http_version=\"1.0\",\n headers=[(\"Content-Length\",\"10000\")],\n ),\n Data(data=b\"a\"*10000),\n EndOfMessage(),\n ]\n \n c=Connection(SERVER,max_incomplete_event_size=100)\n \n \n c.receive_data(\n b\"GET /1 HTTP/1.1\\r\\nHost: a\\r\\n\\r\\n\"\n b\"GET /2 HTTP/1.1\\r\\nHost: b\\r\\n\\r\\n\"+b\"X\"*1000\n )\n assert get_all_events(c)==[\n Request(method=\"GET\",target=\"/1\",headers=[(\"host\",\"a\")]),\n EndOfMessage(),\n ]\n \n c.receive_data(b\"X\"*1000)\n \n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n c.start_next_cycle()\n assert get_all_events(c)==[\n Request(method=\"GET\",target=\"/2\",headers=[(\"host\",\"b\")]),\n EndOfMessage(),\n ]\n \n \n \n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n c.start_next_cycle()\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \ndef test_reuse_simple()->None:\n p=ConnectionPair()\n p.send(\n CLIENT,\n [Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"a\")]),EndOfMessage()],\n )\n p.send(\n SERVER,\n [\n Response(status_code=200,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n EndOfMessage(),\n ],\n )\n for conn in p.conns:\n  assert conn.states =={CLIENT:DONE,SERVER:DONE}\n  conn.start_next_cycle()\n  \n p.send(\n CLIENT,\n [\n Request(method=\"DELETE\",target=\"/foo\",headers=[(\"Host\",\"a\")]),\n EndOfMessage(),\n ],\n )\n p.send(\n SERVER,\n [\n Response(status_code=404,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n EndOfMessage(),\n ],\n )\n \n \ndef test_pipelining()->None:\n\n c=Connection(SERVER)\n assert c.next_event()is NEED_DATA\n \n c.receive_data(\n b\"GET /1 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n b\"12345\"\n b\"GET /2 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n b\"67890\"\n b\"GET /3 HTTP/1.1\\r\\nHost: a.com\\r\\n\\r\\n\"\n )\n assert get_all_events(c)==[\n Request(\n method=\"GET\",\n target=\"/1\",\n headers=[(\"Host\",\"a.com\"),(\"Content-Length\",\"5\")],\n ),\n Data(data=b\"12345\"),\n EndOfMessage(),\n ]\n assert c.their_state is DONE\n assert c.our_state is SEND_RESPONSE\n \n assert c.next_event()is PAUSED\n \n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n assert c.their_state is DONE\n assert c.our_state is DONE\n \n c.start_next_cycle()\n \n assert get_all_events(c)==[\n Request(\n method=\"GET\",\n target=\"/2\",\n headers=[(\"Host\",\"a.com\"),(\"Content-Length\",\"5\")],\n ),\n Data(data=b\"67890\"),\n EndOfMessage(),\n ]\n assert c.next_event()is PAUSED\n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n c.start_next_cycle()\n \n assert get_all_events(c)==[\n Request(method=\"GET\",target=\"/3\",headers=[(\"Host\",\"a.com\")]),\n EndOfMessage(),\n ]\n \n assert c.next_event()is NEED_DATA\n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n \n \n assert c.next_event()is NEED_DATA\n c.receive_data(b\"SADF\")\n assert c.next_event()is PAUSED\n assert c.trailing_data ==(b\"SADF\",False)\n \n c.receive_data(b\"\")\n assert c.trailing_data ==(b\"SADF\",True)\n assert c.next_event()is PAUSED\n c.receive_data(b\"\")\n assert c.next_event()is PAUSED\n \n with pytest.raises(RuntimeError):\n  c.receive_data(b\"FDSA\")\n  \n  \ndef test_protocol_switch()->None:\n for(req,deny,accept)in[\n (\n Request(\n method=\"CONNECT\",\n target=\"example.com:443\",\n headers=[(\"Host\",\"foo\"),(\"Content-Length\",\"1\")],\n ),\n Response(status_code=404,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n Response(status_code=200,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n ),\n (\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"foo\"),(\"Content-Length\",\"1\"),(\"Upgrade\",\"a, b\")],\n ),\n Response(status_code=200,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n InformationalResponse(status_code=101,headers=[(\"Upgrade\",\"a\")]),\n ),\n (\n Request(\n method=\"CONNECT\",\n target=\"example.com:443\",\n headers=[(\"Host\",\"foo\"),(\"Content-Length\",\"1\"),(\"Upgrade\",\"a, b\")],\n ),\n Response(status_code=404,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n \n Response(status_code=200,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n ),\n (\n Request(\n method=\"CONNECT\",\n target=\"example.com:443\",\n headers=[(\"Host\",\"foo\"),(\"Content-Length\",\"1\"),(\"Upgrade\",\"a, b\")],\n ),\n Response(status_code=404,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n \n InformationalResponse(status_code=101,headers=[(\"Upgrade\",\"b\")]),\n ),\n ]:\n \n  def setup()->ConnectionPair:\n   p=ConnectionPair()\n   p.send(CLIENT,req)\n   \n   \n   for conn in p.conns:\n    assert conn.states[CLIENT]is SEND_BODY\n   p.send(CLIENT,[Data(data=b\"1\"),EndOfMessage()])\n   for conn in p.conns:\n    assert conn.states[CLIENT]is MIGHT_SWITCH_PROTOCOL\n   assert p.conn[SERVER].next_event()is PAUSED\n   return p\n   \n   \n  p=setup()\n  p.send(SERVER,deny)\n  for conn in p.conns:\n   assert conn.states =={CLIENT:DONE,SERVER:SEND_BODY}\n  p.send(SERVER,EndOfMessage())\n  \n  for conn in p.conns:\n   conn.start_next_cycle()\n   \n   \n  p=setup()\n  p.send(SERVER,accept)\n  for conn in p.conns:\n   assert conn.states =={CLIENT:SWITCHED_PROTOCOL,SERVER:SWITCHED_PROTOCOL}\n   conn.receive_data(b\"123\")\n   assert conn.next_event()is PAUSED\n   conn.receive_data(b\"456\")\n   assert conn.next_event()is PAUSED\n   assert conn.trailing_data ==(b\"123456\",False)\n   \n   \n   \n   \n   \n  p=setup()\n  sc=p.conn[SERVER]\n  sc.receive_data(b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n  assert sc.next_event()is PAUSED\n  assert sc.trailing_data ==(b\"GET / HTTP/1.0\\r\\n\\r\\n\",False)\n  sc.send(deny)\n  assert sc.next_event()is PAUSED\n  sc.send(EndOfMessage())\n  sc.start_next_cycle()\n  assert get_all_events(sc)==[\n  Request(method=\"GET\",target=\"/\",headers=[],http_version=\"1.0\"),\n  EndOfMessage(),\n  ]\n  \n  \n  \n  \n  p=setup()\n  sc=p.conn[SERVER]\n  sc.receive_data(b\"\")\n  assert sc.next_event()is PAUSED\n  assert sc.trailing_data ==(b\"\",True)\n  p.send(SERVER,accept)\n  assert sc.next_event()is PAUSED\n  \n  p=setup()\n  sc=p.conn[SERVER]\n  sc.receive_data(b\"\")\n  assert sc.next_event()is PAUSED\n  sc.send(deny)\n  assert sc.next_event()==ConnectionClosed()\n  \n  \n  \n  p=setup()\n  with pytest.raises(LocalProtocolError):\n   p.conn[CLIENT].send(\n   Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"a\")])\n   )\n  p=setup()\n  p.send(SERVER,accept)\n  with pytest.raises(LocalProtocolError):\n   p.conn[SERVER].send(Data(data=b\"123\"))\n   \n   \ndef test_close_simple()->None:\n\n\n for(who_shot_first,who_shot_second)in[(CLIENT,SERVER),(SERVER,CLIENT)]:\n \n  def setup()->ConnectionPair:\n   p=ConnectionPair()\n   p.send(who_shot_first,ConnectionClosed())\n   for conn in p.conns:\n    assert conn.states =={\n    who_shot_first:CLOSED,\n    who_shot_second:MUST_CLOSE,\n    }\n   return p\n   \n   \n   \n  p=setup()\n  assert p.conn[who_shot_second].next_event()==ConnectionClosed()\n  assert p.conn[who_shot_second].next_event()==ConnectionClosed()\n  p.conn[who_shot_second].receive_data(b\"\")\n  assert p.conn[who_shot_second].next_event()==ConnectionClosed()\n  \n  p=setup()\n  p.send(who_shot_second,ConnectionClosed())\n  for conn in p.conns:\n   assert conn.our_state is CLOSED\n   assert conn.their_state is CLOSED\n   \n   \n   \n  p=setup()\n  with pytest.raises(RuntimeError):\n   p.conn[who_shot_second].receive_data(b\"123\")\n   \n  p=setup()\n  p.conn[who_shot_first].receive_data(b\"GET\")\n  with pytest.raises(RemoteProtocolError):\n   p.conn[who_shot_first].next_event()\n   \n   \ndef test_close_different_states()->None:\n req=[\n Request(method=\"GET\",target=\"/foo\",headers=[(\"Host\",\"a\")]),\n EndOfMessage(),\n ]\n resp=[\n Response(status_code=200,headers=[(b\"transfer-encoding\",b\"chunked\")]),\n EndOfMessage(),\n ]\n \n \n p=ConnectionPair()\n p.send(CLIENT,ConnectionClosed())\n for conn in p.conns:\n  assert conn.states =={CLIENT:CLOSED,SERVER:MUST_CLOSE}\n  \n  \n p=ConnectionPair()\n p.send(CLIENT,req)\n p.send(CLIENT,ConnectionClosed())\n for conn in p.conns:\n  assert conn.states =={CLIENT:CLOSED,SERVER:SEND_RESPONSE}\n  \n  \n p=ConnectionPair()\n p.send(CLIENT,req)\n with pytest.raises(LocalProtocolError):\n  p.conn[SERVER].send(ConnectionClosed())\n p.conn[CLIENT].receive_data(b\"\")\n with pytest.raises(RemoteProtocolError):\n  p.conn[CLIENT].next_event()\n  \n  \n p=ConnectionPair()\n p.send(CLIENT,req)\n p.send(SERVER,resp)\n p.send(SERVER,ConnectionClosed())\n for conn in p.conns:\n  assert conn.states =={CLIENT:MUST_CLOSE,SERVER:CLOSED}\n  \n  \n p=ConnectionPair()\n p.send(CLIENT,req)\n p.send(SERVER,resp)\n p.send(CLIENT,ConnectionClosed())\n p.send(SERVER,ConnectionClosed())\n p.send(CLIENT,ConnectionClosed())\n p.send(SERVER,ConnectionClosed())\n \n \n p=ConnectionPair()\n p.send(\n CLIENT,\n Request(\n method=\"GET\",target=\"/\",headers=[(\"Host\",\"a\"),(\"Content-Length\",\"10\")]\n ),\n )\n with pytest.raises(LocalProtocolError):\n  p.conn[CLIENT].send(ConnectionClosed())\n p.conn[SERVER].receive_data(b\"\")\n with pytest.raises(RemoteProtocolError):\n  p.conn[SERVER].next_event()\n  \n  \n  \n  \ndef test_pipelined_close()->None:\n c=Connection(SERVER)\n \n c.receive_data(\n b\"GET /1 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n b\"12345\"\n b\"GET /2 HTTP/1.1\\r\\nHost: a.com\\r\\nContent-Length: 5\\r\\n\\r\\n\"\n b\"67890\"\n )\n c.receive_data(b\"\")\n assert get_all_events(c)==[\n Request(\n method=\"GET\",\n target=\"/1\",\n headers=[(\"host\",\"a.com\"),(\"content-length\",\"5\")],\n ),\n Data(data=b\"12345\"),\n EndOfMessage(),\n ]\n assert c.states[CLIENT]is DONE\n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n assert c.states[SERVER]is DONE\n c.start_next_cycle()\n assert get_all_events(c)==[\n Request(\n method=\"GET\",\n target=\"/2\",\n headers=[(\"host\",\"a.com\"),(\"content-length\",\"5\")],\n ),\n Data(data=b\"67890\"),\n EndOfMessage(),\n ConnectionClosed(),\n ]\n assert c.states =={CLIENT:CLOSED,SERVER:SEND_RESPONSE}\n c.send(Response(status_code=200,headers=[]))\n c.send(EndOfMessage())\n assert c.states =={CLIENT:CLOSED,SERVER:MUST_CLOSE}\n c.send(ConnectionClosed())\n assert c.states =={CLIENT:CLOSED,SERVER:CLOSED}\n \n \ndef test_sendfile()->None:\n class SendfilePlaceholder:\n  def __len__(self)->int:\n   return 10\n   \n placeholder=SendfilePlaceholder()\n \n def setup(\n header:Tuple[str,str],http_version:str\n )->Tuple[Connection,Optional[List[bytes]]]:\n  c=Connection(SERVER)\n  receive_and_get(\n  c,\"GET / HTTP/{}\\r\\nHost: a\\r\\n\\r\\n\".format(http_version).encode(\"ascii\")\n  )\n  headers=[]\n  if header:\n   headers.append(header)\n  c.send(Response(status_code=200,headers=headers))\n  return c,c.send_with_data_passthrough(Data(data=placeholder))\n  \n c,data=setup((\"Content-Length\",\"10\"),\"1.1\")\n assert data ==[placeholder]\n \n \n c.send(EndOfMessage())\n \n _,data=setup((\"Transfer-Encoding\",\"chunked\"),\"1.1\")\n assert placeholder in data\n data[data.index(placeholder)]=b\"x\"*10\n assert b\"\".join(data)==b\"a\\r\\nxxxxxxxxxx\\r\\n\"\n \n c,data=setup(None,\"1.0\")\n assert data ==[placeholder]\n assert c.our_state is SEND_BODY\n \n \ndef test_errors()->None:\n\n for role in[CLIENT,SERVER]:\n  c=Connection(our_role=role)\n  c.receive_data(b\"gibberish\\r\\n\\r\\n\")\n  with pytest.raises(RemoteProtocolError):\n   c.next_event()\n   \n  assert c.their_state is ERROR\n  assert c.our_state is not ERROR\n  print(c._cstate.states)\n  with pytest.raises(RemoteProtocolError):\n   c.next_event()\n   \n  if role is SERVER:\n   assert(\n   c.send(Response(status_code=400,headers=[]))\n   ==b\"HTTP/1.1 400 \\r\\nConnection: close\\r\\n\\r\\n\"\n   )\n   \n   \n   \n   \n def conn(role:Type[Sentinel])->Connection:\n  c=Connection(our_role=role)\n  if role is SERVER:\n  \n   receive_and_get(c,b\"GET / HTTP/1.0\\r\\n\\r\\n\")\n   assert c.our_state is SEND_RESPONSE\n  return c\n  \n for role in[CLIENT,SERVER]:\n  if role is CLIENT:\n  \n  \n   good=Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"example.com\")])\n   bad=Request(\n   method=\"GET\",\n   target=\"/\",\n   headers=[(\"Host\",\"example.com\")],\n   http_version=\"1.0\",\n   )\n  elif role is SERVER:\n   good=Response(status_code=200,headers=[])\n   bad=Response(status_code=200,headers=[],http_version=\"1.0\")\n   \n  c=conn(role)\n  c.send(good)\n  assert c.our_state is not ERROR\n  \n  c=conn(role)\n  with pytest.raises(LocalProtocolError):\n   c.send(bad)\n  assert c.our_state is ERROR\n  assert c.their_state is not ERROR\n  \n  with pytest.raises(LocalProtocolError):\n   c.send(good)\n   \n   \n  c=conn(role)\n  c.send_failed()\n  assert c.our_state is ERROR\n  assert c.their_state is not ERROR\n  \n  c.send_failed()\n  assert c.our_state is ERROR\n  assert c.their_state is not ERROR\n  \n  \ndef test_idle_receive_nothing()->None:\n\n for role in[CLIENT,SERVER]:\n  c=Connection(role)\n  assert c.next_event()is NEED_DATA\n  \n  \ndef test_connection_drop()->None:\n c=Connection(SERVER)\n c.receive_data(b\"GET /\")\n assert c.next_event()is NEED_DATA\n c.receive_data(b\"\")\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \ndef test_408_request_timeout()->None:\n\n\n p=ConnectionPair()\n p.send(SERVER,Response(status_code=408,headers=[(b\"connection\",b\"close\")]))\n \n \n \ndef test_empty_request()->None:\n c=Connection(SERVER)\n c.receive_data(b\"\\r\\n\")\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \n  \ndef test_empty_response()->None:\n c=Connection(CLIENT)\n c.send(Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"a\")]))\n c.receive_data(b\"\\r\\n\")\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \n@pytest.mark.parametrize(\n\"data\",\n[\nb\"\\x00\",\nb\"\\x20\",\nb\"\\x16\\x03\\x01\\x00\\xa5\",\n],\n)\ndef test_early_detection_of_invalid_request(data:bytes)->None:\n c=Connection(SERVER)\n \n c.receive_data(data)\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \n@pytest.mark.parametrize(\n\"data\",\n[\nb\"\\x00\",\nb\"\\x20\",\nb\"\\x16\\x03\\x03\\x00\\x31\",\n],\n)\ndef test_early_detection_of_invalid_response(data:bytes)->None:\n c=Connection(CLIENT)\n \n c.receive_data(data)\n with pytest.raises(RemoteProtocolError):\n  c.next_event()\n  \n  \n  \n  \n  \n  \ndef test_HEAD_framing_headers()->None:\n def setup(method:bytes,http_version:bytes)->Connection:\n  c=Connection(SERVER)\n  c.receive_data(\n  method+b\" / HTTP/\"+http_version+b\"\\r\\n\"+b\"Host: example.com\\r\\n\\r\\n\"\n  )\n  assert type(c.next_event())is Request\n  assert type(c.next_event())is EndOfMessage\n  return c\n  \n for method in[b\"GET\",b\"HEAD\"]:\n \n  c=setup(method,b\"1.1\")\n  assert(\n  c.send(Response(status_code=200,headers=[]))==b\"HTTP/1.1 200 \\r\\n\"\n  b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n  )\n  \n  \n  c=setup(method,b\"1.0\")\n  assert(\n  c.send(Response(status_code=200,headers=[]))==b\"HTTP/1.1 200 \\r\\n\"\n  b\"Connection: close\\r\\n\\r\\n\"\n  )\n  \n  \n  c=setup(method,b\"1.1\")\n  assert(\n  c.send(\n  Response(\n  status_code=200,\n  headers=[\n  (\"Content-Length\",\"100\"),\n  (\"Transfer-Encoding\",\"chunked\"),\n  ],\n  )\n  )\n  ==b\"HTTP/1.1 200 \\r\\n\"\n  b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n  )\n  \n  \ndef test_special_exceptions_for_lost_connection_in_message_body()->None:\n c=Connection(SERVER)\n c.receive_data(\n b\"POST / HTTP/1.1\\r\\n\"b\"Host: example.com\\r\\n\"b\"Content-Length: 100\\r\\n\\r\\n\"\n )\n assert type(c.next_event())is Request\n assert c.next_event()is NEED_DATA\n c.receive_data(b\"12345\")\n assert c.next_event()==Data(data=b\"12345\")\n c.receive_data(b\"\")\n with pytest.raises(RemoteProtocolError)as excinfo:\n  c.next_event()\n assert \"received 5 bytes\"in str(excinfo.value)\n assert \"expected 100\"in str(excinfo.value)\n \n c=Connection(SERVER)\n c.receive_data(\n b\"POST / HTTP/1.1\\r\\n\"\n b\"Host: example.com\\r\\n\"\n b\"Transfer-Encoding: chunked\\r\\n\\r\\n\"\n )\n assert type(c.next_event())is Request\n assert c.next_event()is NEED_DATA\n c.receive_data(b\"8\\r\\n012345\")\n assert c.next_event().data ==b\"012345\"\n c.receive_data(b\"\")\n with pytest.raises(RemoteProtocolError)as excinfo:\n  c.next_event()\n assert \"incomplete chunked read\"in str(excinfo.value)\n", ["h11._connection", "h11._events", "h11._state", "h11._util", "h11.tests.helpers", "pytest", "typing"]], "h11.tests.test_receivebuffer": [".py", "import re\nfrom typing import Tuple\n\nimport pytest\n\nfrom.._receivebuffer import ReceiveBuffer\n\n\ndef test_receivebuffer()->None:\n b=ReceiveBuffer()\n assert not b\n assert len(b)==0\n assert bytes(b)==b\"\"\n \n b +=b\"123\"\n assert b\n assert len(b)==3\n assert bytes(b)==b\"123\"\n \n assert bytes(b)==b\"123\"\n \n assert b.maybe_extract_at_most(2)==b\"12\"\n assert b\n assert len(b)==1\n assert bytes(b)==b\"3\"\n \n assert bytes(b)==b\"3\"\n \n assert b.maybe_extract_at_most(10)==b\"3\"\n assert bytes(b)==b\"\"\n \n assert b.maybe_extract_at_most(10)is None\n assert not b\n \n \n \n \n \n b +=b\"123\\n456\\r\\n789\\r\\n\"\n \n assert b.maybe_extract_next_line()==b\"123\\n456\\r\\n\"\n assert bytes(b)==b\"789\\r\\n\"\n \n assert b.maybe_extract_next_line()==b\"789\\r\\n\"\n assert bytes(b)==b\"\"\n \n b +=b\"12\\r\"\n assert b.maybe_extract_next_line()is None\n assert bytes(b)==b\"12\\r\"\n \n b +=b\"345\\n\\r\"\n assert b.maybe_extract_next_line()is None\n assert bytes(b)==b\"12\\r345\\n\\r\"\n \n \n \n b +=b\"\\n6789aaa123\\r\\n\"\n assert b.maybe_extract_next_line()==b\"12\\r345\\n\\r\\n\"\n assert b.maybe_extract_next_line()==b\"6789aaa123\\r\\n\"\n assert b.maybe_extract_next_line()is None\n assert bytes(b)==b\"\"\n \n \n \n \n \n b +=b\"123\\r\\na: b\\r\\nfoo:bar\\r\\n\\r\\ntrailing\"\n lines=b.maybe_extract_lines()\n assert lines ==[b\"123\",b\"a: b\",b\"foo:bar\"]\n assert bytes(b)==b\"trailing\"\n \n assert b.maybe_extract_lines()is None\n \n b +=b\"\\r\\n\\r\"\n assert b.maybe_extract_lines()is None\n \n assert b.maybe_extract_at_most(100)==b\"trailing\\r\\n\\r\"\n assert not b\n \n \n \n b +=b\"\\r\\ntrailing\"\n assert b.maybe_extract_lines()==[]\n assert bytes(b)==b\"trailing\"\n \n \n@pytest.mark.parametrize(\n\"data\",\n[\npytest.param(\n(\nb\"HTTP/1.1 200 OK\\r\\n\",\nb\"Content-type: text/plain\\r\\n\",\nb\"Connection: close\\r\\n\",\nb\"\\r\\n\",\nb\"Some body\",\n),\nid=\"with_crlf_delimiter\",\n),\npytest.param(\n(\nb\"HTTP/1.1 200 OK\\n\",\nb\"Content-type: text/plain\\n\",\nb\"Connection: close\\n\",\nb\"\\n\",\nb\"Some body\",\n),\nid=\"with_lf_only_delimiter\",\n),\npytest.param(\n(\nb\"HTTP/1.1 200 OK\\n\",\nb\"Content-type: text/plain\\r\\n\",\nb\"Connection: close\\n\",\nb\"\\n\",\nb\"Some body\",\n),\nid=\"with_mixed_crlf_and_lf\",\n),\n],\n)\ndef test_receivebuffer_for_invalid_delimiter(data:Tuple[bytes])->None:\n b=ReceiveBuffer()\n \n for line in data:\n  b +=line\n  \n lines=b.maybe_extract_lines()\n \n assert lines ==[\n b\"HTTP/1.1 200 OK\",\n b\"Content-type: text/plain\",\n b\"Connection: close\",\n ]\n assert bytes(b)==b\"Some body\"\n", ["h11._receivebuffer", "pytest", "re", "typing"]], "h11.tests.test_state": [".py", "import pytest\n\nfrom.._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom.._state import(\n_SWITCH_CONNECT,\n_SWITCH_UPGRADE,\nCLIENT,\nCLOSED,\nConnectionState,\nDONE,\nIDLE,\nMIGHT_SWITCH_PROTOCOL,\nMUST_CLOSE,\nSEND_BODY,\nSEND_RESPONSE,\nSERVER,\nSWITCHED_PROTOCOL,\n)\nfrom.._util import LocalProtocolError\n\n\ndef test_ConnectionState()->None:\n cs=ConnectionState()\n \n \n \n assert cs.states =={CLIENT:IDLE,SERVER:IDLE}\n \n cs.process_event(CLIENT,Request)\n \n assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n \n \n with pytest.raises(LocalProtocolError):\n  cs.process_event(CLIENT,Request)\n assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n \n cs.process_event(SERVER,InformationalResponse)\n assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n \n cs.process_event(SERVER,Response)\n assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_BODY}\n \n cs.process_event(CLIENT,EndOfMessage)\n cs.process_event(SERVER,EndOfMessage)\n assert cs.states =={CLIENT:DONE,SERVER:DONE}\n \n \n \n cs.process_event(SERVER,ConnectionClosed)\n assert cs.states =={CLIENT:MUST_CLOSE,SERVER:CLOSED}\n \n \ndef test_ConnectionState_keep_alive()->None:\n\n cs=ConnectionState()\n cs.process_event(CLIENT,Request)\n cs.process_keep_alive_disabled()\n cs.process_event(CLIENT,EndOfMessage)\n assert cs.states =={CLIENT:MUST_CLOSE,SERVER:SEND_RESPONSE}\n \n cs.process_event(SERVER,Response)\n cs.process_event(SERVER,EndOfMessage)\n assert cs.states =={CLIENT:MUST_CLOSE,SERVER:MUST_CLOSE}\n \n \ndef test_ConnectionState_keep_alive_in_DONE()->None:\n\n\n\n cs=ConnectionState()\n cs.process_event(CLIENT,Request)\n cs.process_event(CLIENT,EndOfMessage)\n assert cs.states[CLIENT]is DONE\n cs.process_keep_alive_disabled()\n assert cs.states[CLIENT]is MUST_CLOSE\n \n \ndef test_ConnectionState_switch_denied()->None:\n for switch_type in(_SWITCH_CONNECT,_SWITCH_UPGRADE):\n  for deny_early in(True,False):\n   cs=ConnectionState()\n   cs.process_client_switch_proposal(switch_type)\n   cs.process_event(CLIENT,Request)\n   cs.process_event(CLIENT,Data)\n   assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n   \n   assert switch_type in cs.pending_switch_proposals\n   \n   if deny_early:\n   \n    cs.process_event(SERVER,Response)\n    assert not cs.pending_switch_proposals\n    \n   cs.process_event(CLIENT,EndOfMessage)\n   \n   if deny_early:\n    assert cs.states =={CLIENT:DONE,SERVER:SEND_BODY}\n   else:\n    assert cs.states =={\n    CLIENT:MIGHT_SWITCH_PROTOCOL,\n    SERVER:SEND_RESPONSE,\n    }\n    \n    cs.process_event(SERVER,InformationalResponse)\n    assert cs.states =={\n    CLIENT:MIGHT_SWITCH_PROTOCOL,\n    SERVER:SEND_RESPONSE,\n    }\n    \n    cs.process_event(SERVER,Response)\n    assert cs.states =={CLIENT:DONE,SERVER:SEND_BODY}\n    assert not cs.pending_switch_proposals\n    \n    \n_response_type_for_switch={\n_SWITCH_UPGRADE:InformationalResponse,\n_SWITCH_CONNECT:Response,\nNone:Response,\n}\n\n\ndef test_ConnectionState_protocol_switch_accepted()->None:\n for switch_event in[_SWITCH_UPGRADE,_SWITCH_CONNECT]:\n  cs=ConnectionState()\n  cs.process_client_switch_proposal(switch_event)\n  cs.process_event(CLIENT,Request)\n  cs.process_event(CLIENT,Data)\n  assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n  \n  cs.process_event(CLIENT,EndOfMessage)\n  assert cs.states =={CLIENT:MIGHT_SWITCH_PROTOCOL,SERVER:SEND_RESPONSE}\n  \n  cs.process_event(SERVER,InformationalResponse)\n  assert cs.states =={CLIENT:MIGHT_SWITCH_PROTOCOL,SERVER:SEND_RESPONSE}\n  \n  cs.process_event(SERVER,_response_type_for_switch[switch_event],switch_event)\n  assert cs.states =={CLIENT:SWITCHED_PROTOCOL,SERVER:SWITCHED_PROTOCOL}\n  \n  \ndef test_ConnectionState_double_protocol_switch()->None:\n\n\n for server_switch in[None,_SWITCH_UPGRADE,_SWITCH_CONNECT]:\n  cs=ConnectionState()\n  cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n  cs.process_client_switch_proposal(_SWITCH_CONNECT)\n  cs.process_event(CLIENT,Request)\n  cs.process_event(CLIENT,EndOfMessage)\n  assert cs.states =={CLIENT:MIGHT_SWITCH_PROTOCOL,SERVER:SEND_RESPONSE}\n  cs.process_event(\n  SERVER,_response_type_for_switch[server_switch],server_switch\n  )\n  if server_switch is None:\n   assert cs.states =={CLIENT:DONE,SERVER:SEND_BODY}\n  else:\n   assert cs.states =={CLIENT:SWITCHED_PROTOCOL,SERVER:SWITCHED_PROTOCOL}\n   \n   \ndef test_ConnectionState_inconsistent_protocol_switch()->None:\n for client_switches,server_switch in[\n ([],_SWITCH_CONNECT),\n ([],_SWITCH_UPGRADE),\n ([_SWITCH_UPGRADE],_SWITCH_CONNECT),\n ([_SWITCH_CONNECT],_SWITCH_UPGRADE),\n ]:\n  cs=ConnectionState()\n  for client_switch in client_switches:\n   cs.process_client_switch_proposal(client_switch)\n  cs.process_event(CLIENT,Request)\n  with pytest.raises(LocalProtocolError):\n   cs.process_event(SERVER,Response,server_switch)\n   \n   \ndef test_ConnectionState_keepalive_protocol_switch_interaction()->None:\n\n cs=ConnectionState()\n cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n cs.process_event(CLIENT,Request)\n cs.process_keep_alive_disabled()\n cs.process_event(CLIENT,Data)\n assert cs.states =={CLIENT:SEND_BODY,SERVER:SEND_RESPONSE}\n \n \n cs.process_event(CLIENT,EndOfMessage)\n assert cs.states =={CLIENT:MIGHT_SWITCH_PROTOCOL,SERVER:SEND_RESPONSE}\n \n \n cs.process_event(SERVER,Response)\n assert cs.states =={CLIENT:MUST_CLOSE,SERVER:SEND_BODY}\n \n \ndef test_ConnectionState_reuse()->None:\n cs=ConnectionState()\n \n with pytest.raises(LocalProtocolError):\n  cs.start_next_cycle()\n  \n cs.process_event(CLIENT,Request)\n cs.process_event(CLIENT,EndOfMessage)\n \n with pytest.raises(LocalProtocolError):\n  cs.start_next_cycle()\n  \n cs.process_event(SERVER,Response)\n cs.process_event(SERVER,EndOfMessage)\n \n cs.start_next_cycle()\n assert cs.states =={CLIENT:IDLE,SERVER:IDLE}\n \n \n \n cs.process_event(CLIENT,Request)\n cs.process_keep_alive_disabled()\n cs.process_event(CLIENT,EndOfMessage)\n cs.process_event(SERVER,Response)\n cs.process_event(SERVER,EndOfMessage)\n \n with pytest.raises(LocalProtocolError):\n  cs.start_next_cycle()\n  \n  \n  \n cs=ConnectionState()\n cs.process_event(CLIENT,Request)\n cs.process_event(CLIENT,EndOfMessage)\n cs.process_event(CLIENT,ConnectionClosed)\n cs.process_event(SERVER,Response)\n cs.process_event(SERVER,EndOfMessage)\n \n with pytest.raises(LocalProtocolError):\n  cs.start_next_cycle()\n  \n  \n  \n cs=ConnectionState()\n cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n cs.process_event(CLIENT,Request)\n cs.process_event(CLIENT,EndOfMessage)\n cs.process_event(SERVER,InformationalResponse,_SWITCH_UPGRADE)\n \n with pytest.raises(LocalProtocolError):\n  cs.start_next_cycle()\n  \n  \n  \n cs=ConnectionState()\n cs.process_client_switch_proposal(_SWITCH_UPGRADE)\n cs.process_event(CLIENT,Request)\n cs.process_event(CLIENT,EndOfMessage)\n cs.process_event(SERVER,Response)\n cs.process_event(SERVER,EndOfMessage)\n \n cs.start_next_cycle()\n assert cs.states =={CLIENT:IDLE,SERVER:IDLE}\n \n \ndef test_server_request_is_illegal()->None:\n\n\n cs=ConnectionState()\n with pytest.raises(LocalProtocolError):\n  cs.process_event(SERVER,Request)\n", ["h11._events", "h11._state", "h11._util", "pytest"]], "h11.tests.test_against_stdlib_http": [".py", "import json\nimport os.path\nimport socket\nimport socketserver\nimport threading\nfrom contextlib import closing,contextmanager\nfrom http.server import SimpleHTTPRequestHandler\nfrom typing import Callable,Generator\nfrom urllib.request import urlopen\n\nimport h11\n\n\n@contextmanager\ndef socket_server(\nhandler:Callable[...,socketserver.BaseRequestHandler]\n)->Generator[socketserver.TCPServer,None,None]:\n httpd=socketserver.TCPServer((\"127.0.0.1\",0),handler)\n thread=threading.Thread(\n target=httpd.serve_forever,kwargs={\"poll_interval\":0.01}\n )\n thread.daemon=True\n try:\n  thread.start()\n  yield httpd\n finally:\n  httpd.shutdown()\n  \n  \ntest_file_path=os.path.join(os.path.dirname(__file__),\"data/test-file\")\nwith open(test_file_path,\"rb\")as f:\n test_file_data=f.read()\n \n \nclass SingleMindedRequestHandler(SimpleHTTPRequestHandler):\n def translate_path(self,path:str)->str:\n  return test_file_path\n  \n  \ndef test_h11_as_client()->None:\n with socket_server(SingleMindedRequestHandler)as httpd:\n  with closing(socket.create_connection(httpd.server_address))as s:\n   c=h11.Connection(h11.CLIENT)\n   \n   s.sendall(\n   c.send(\n   h11.Request(\n   method=\"GET\",target=\"/foo\",headers=[(\"Host\",\"localhost\")]\n   )\n   )\n   )\n   s.sendall(c.send(h11.EndOfMessage()))\n   \n   data=bytearray()\n   while True:\n    event=c.next_event()\n    print(event)\n    if event is h11.NEED_DATA:\n    \n    \n     c.receive_data(s.recv(10))\n     continue\n    if type(event)is h11.Response:\n     assert event.status_code ==200\n    if type(event)is h11.Data:\n     data +=event.data\n    if type(event)is h11.EndOfMessage:\n     break\n   assert bytes(data)==test_file_data\n   \n   \nclass H11RequestHandler(socketserver.BaseRequestHandler):\n def handle(self)->None:\n  with closing(self.request)as s:\n   c=h11.Connection(h11.SERVER)\n   request=None\n   while True:\n    event=c.next_event()\n    if event is h11.NEED_DATA:\n    \n    \n     c.receive_data(s.recv(10))\n     continue\n    if type(event)is h11.Request:\n     request=event\n    if type(event)is h11.EndOfMessage:\n     break\n   assert request is not None\n   info=json.dumps(\n   {\n   \"method\":request.method.decode(\"ascii\"),\n   \"target\":request.target.decode(\"ascii\"),\n   \"headers\":{\n   name.decode(\"ascii\"):value.decode(\"ascii\")\n   for(name,value)in request.headers\n   },\n   }\n   )\n   s.sendall(c.send(h11.Response(status_code=200,headers=[])))\n   s.sendall(c.send(h11.Data(data=info.encode(\"ascii\"))))\n   s.sendall(c.send(h11.EndOfMessage()))\n   \n   \ndef test_h11_as_server()->None:\n with socket_server(H11RequestHandler)as httpd:\n  host,port=httpd.server_address\n  url=\"http://{}:{}/some-path\".format(host,port)\n  with closing(urlopen(url))as f:\n   assert f.getcode()==200\n   data=f.read()\n info=json.loads(data.decode(\"ascii\"))\n print(info)\n assert info[\"method\"]==\"GET\"\n assert info[\"target\"]==\"/some-path\"\n assert \"urllib\"in info[\"headers\"][\"user-agent\"]\n", ["contextlib", "h11", "http.server", "json", "os.path", "socket", "socketserver", "threading", "typing", "urllib.request"]], "h11.tests.test_headers": [".py", "import pytest\n\nfrom.._events import Request\nfrom.._headers import(\nget_comma_header,\nhas_expect_100_continue,\nHeaders,\nnormalize_and_validate,\nset_comma_header,\n)\nfrom.._util import LocalProtocolError\n\n\ndef test_normalize_and_validate()->None:\n assert normalize_and_validate([(\"foo\",\"bar\")])==[(b\"foo\",b\"bar\")]\n assert normalize_and_validate([(b\"foo\",b\"bar\")])==[(b\"foo\",b\"bar\")]\n \n \n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(b\"foo \",\"bar\")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(b\" foo\",\"bar\")])\n  \n  \n with pytest.raises(LocalProtocolError)as excinfo:\n  normalize_and_validate([(b\"foo bar\",b\"baz\")])\n assert \"foo bar\"in str(excinfo.value)\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(b\"foo\\x00bar\",b\"baz\")])\n  \n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(b\"foo\\xffbar\",b\"baz\")])\n  \n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(b\"foo\\x01bar\",b\"baz\")])\n  \n  \n with pytest.raises(LocalProtocolError)as excinfo:\n  normalize_and_validate([(\"foo\",\"bar\\rbaz\")])\n assert \"bar\\\\rbaz\"in str(excinfo.value)\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"foo\",\"bar\\nbaz\")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"foo\",\"bar\\x00baz\")])\n  \n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"foo\",\"barbaz  \")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"foo\",\"  barbaz\")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"foo\",\"barbaz\\t\")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"foo\",\"\\tbarbaz\")])\n  \n  \n assert normalize_and_validate([(\"Content-Length\",\"1\")])==[\n (b\"content-length\",b\"1\")\n ]\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"Content-Length\",\"asdf\")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"Content-Length\",\"1x\")])\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"Content-Length\",\"1\"),(\"Content-Length\",\"2\")])\n assert normalize_and_validate(\n [(\"Content-Length\",\"0\"),(\"Content-Length\",\"0\")]\n )==[(b\"content-length\",b\"0\")]\n assert normalize_and_validate([(\"Content-Length\",\"0 , 0\")])==[\n (b\"content-length\",b\"0\")\n ]\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate(\n  [(\"Content-Length\",\"1\"),(\"Content-Length\",\"1\"),(\"Content-Length\",\"2\")]\n  )\n with pytest.raises(LocalProtocolError):\n  normalize_and_validate([(\"Content-Length\",\"1 , 1,2\")])\n  \n  \n assert normalize_and_validate([(\"Transfer-Encoding\",\"chunked\")])==[\n (b\"transfer-encoding\",b\"chunked\")\n ]\n assert normalize_and_validate([(\"Transfer-Encoding\",\"cHuNkEd\")])==[\n (b\"transfer-encoding\",b\"chunked\")\n ]\n with pytest.raises(LocalProtocolError)as excinfo:\n  normalize_and_validate([(\"Transfer-Encoding\",\"gzip\")])\n assert excinfo.value.error_status_hint ==501\n with pytest.raises(LocalProtocolError)as excinfo:\n  normalize_and_validate(\n  [(\"Transfer-Encoding\",\"chunked\"),(\"Transfer-Encoding\",\"gzip\")]\n  )\n assert excinfo.value.error_status_hint ==501\n \n \ndef test_get_set_comma_header()->None:\n headers=normalize_and_validate(\n [\n (\"Connection\",\"close\"),\n (\"whatever\",\"something\"),\n (\"connectiON\",\"fOo,, , BAR\"),\n ]\n )\n \n assert get_comma_header(headers,b\"connection\")==[b\"close\",b\"foo\",b\"bar\"]\n \n headers=set_comma_header(headers,b\"newthing\",[\"a\",\"b\"])\n \n with pytest.raises(LocalProtocolError):\n  set_comma_header(headers,b\"newthing\",[\"  a\",\"b\"])\n  \n assert headers ==[\n (b\"connection\",b\"close\"),\n (b\"whatever\",b\"something\"),\n (b\"connection\",b\"fOo,, , BAR\"),\n (b\"newthing\",b\"a\"),\n (b\"newthing\",b\"b\"),\n ]\n \n headers=set_comma_header(headers,b\"whatever\",[\"different thing\"])\n \n assert headers ==[\n (b\"connection\",b\"close\"),\n (b\"connection\",b\"fOo,, , BAR\"),\n (b\"newthing\",b\"a\"),\n (b\"newthing\",b\"b\"),\n (b\"whatever\",b\"different thing\"),\n ]\n \n \ndef test_has_100_continue()->None:\n assert has_expect_100_continue(\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Expect\",\"100-continue\")],\n )\n )\n assert not has_expect_100_continue(\n Request(method=\"GET\",target=\"/\",headers=[(\"Host\",\"example.com\")])\n )\n \n assert has_expect_100_continue(\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Expect\",\"100-Continue\")],\n )\n )\n \n assert not has_expect_100_continue(\n Request(\n method=\"GET\",\n target=\"/\",\n headers=[(\"Host\",\"example.com\"),(\"Expect\",\"100-continue\")],\n http_version=\"1.0\",\n )\n )\n", ["h11._events", "h11._headers", "h11._util", "pytest"]], "h11.tests.helpers": [".py", "from typing import cast,List,Type,Union,ValuesView\n\nfrom.._connection import Connection,NEED_DATA,PAUSED\nfrom.._events import(\nConnectionClosed,\nData,\nEndOfMessage,\nEvent,\nInformationalResponse,\nRequest,\nResponse,\n)\nfrom.._state import CLIENT,CLOSED,DONE,MUST_CLOSE,SERVER\nfrom.._util import Sentinel\n\ntry:\n from typing import Literal\nexcept ImportError:\n from typing_extensions import Literal\n \n \ndef get_all_events(conn:Connection)->List[Event]:\n got_events=[]\n while True:\n  event=conn.next_event()\n  if event in(NEED_DATA,PAUSED):\n   break\n  event=cast(Event,event)\n  got_events.append(event)\n  if type(event)is ConnectionClosed:\n   break\n return got_events\n \n \ndef receive_and_get(conn:Connection,data:bytes)->List[Event]:\n conn.receive_data(data)\n return get_all_events(conn)\n \n \n \n \ndef normalize_data_events(in_events:List[Event])->List[Event]:\n out_events:List[Event]=[]\n for event in in_events:\n  if type(event)is Data:\n   event=Data(data=bytes(event.data),chunk_start=False,chunk_end=False)\n  if out_events and type(out_events[-1])is type(event)is Data:\n   out_events[-1]=Data(\n   data=out_events[-1].data+event.data,\n   chunk_start=out_events[-1].chunk_start,\n   chunk_end=out_events[-1].chunk_end,\n   )\n  else:\n   out_events.append(event)\n return out_events\n \n \n \n \n \n \nclass ConnectionPair:\n def __init__(self)->None:\n  self.conn={CLIENT:Connection(CLIENT),SERVER:Connection(SERVER)}\n  self.other={CLIENT:SERVER,SERVER:CLIENT}\n  \n @property\n def conns(self)->ValuesView[Connection]:\n  return self.conn.values()\n  \n  \n def send(\n self,\n role:Type[Sentinel],\n send_events:Union[List[Event],Event],\n expect:Union[List[Event],Event,Literal[\"match\"]]=\"match\",\n )->bytes:\n  if not isinstance(send_events,list):\n   send_events=[send_events]\n  data=b\"\"\n  closed=False\n  for send_event in send_events:\n   new_data=self.conn[role].send(send_event)\n   if new_data is None:\n    closed=True\n   else:\n    data +=new_data\n    \n    \n    \n  if data:\n   self.conn[self.other[role]].receive_data(data)\n  if closed:\n   self.conn[self.other[role]].receive_data(b\"\")\n  got_events=get_all_events(self.conn[self.other[role]])\n  if expect ==\"match\":\n   expect=send_events\n  if not isinstance(expect,list):\n   expect=[expect]\n  assert got_events ==expect\n  return data\n", ["h11._connection", "h11._events", "h11._state", "h11._util", "typing", "typing_extensions"]]}
__BRYTHON__.update_VFS(scripts)
